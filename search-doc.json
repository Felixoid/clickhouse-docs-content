[{"title":"ClickHouse Cloud Service","type":0,"sectionRef":"#","url":"en/about-us/cloud","content":"ClickHouse Cloud Service We are building a serverless hosted ClickHouse offering that is: Cloud-agnosticNo infrastructure to manageAutomatic scalingConsumption-based pricingTurnkey data migration servicesWorld-class security and privacy guaranteesReduce total cost of ownership and let us take the worry out of operating ClickHouse, at any scale.","keywords":""},{"title":"ClickHouse Commercial Support Service","type":0,"sectionRef":"#","url":"en/about-us/support","content":"ClickHouse Commercial Support Service ClickHouse provides Support Services for our customers running ClickHouse in production environments. Our objective is a Support Services team that represents the ClickHouse product – unparalleled performance, ease of use, and exceptionally fast, high-quality results. For details, visit our Support Services page.","keywords":""},{"title":"ClickHouse History","type":0,"sectionRef":"#","url":"en/about-us/history","content":"","keywords":""},{"title":"Usage in Yandex.Metrica and Other Yandex Services​","type":1,"pageTitle":"ClickHouse History","url":"en/about-us/history#usage-in-yandex-metrica-and-other-yandex-services","content":"ClickHouse serves multiple purposes in Yandex.Metrica. Its main task is to build reports in online mode using non-aggregated data. It uses a cluster of 374 servers, which store over 20.3 trillion rows in the database. The volume of compressed data is about 2 PB, without accounting for duplicates and replicas. The volume of uncompressed data (in TSV format) would be approximately 17 PB. ClickHouse also plays a key role in the following processes: Storing data for Session Replay from Yandex.Metrica.Processing intermediate data.Building global reports with Analytics.Running queries for debugging the Yandex.Metrica engine.Analyzing logs from the API and the user interface. Nowadays, there are multiple dozen ClickHouse installations in other Yandex services and departments: search verticals, e-commerce, advertisement, business analytics, mobile development, personal services, and others. "},{"title":"Aggregated and Non-aggregated Data​","type":1,"pageTitle":"ClickHouse History","url":"en/about-us/history#aggregated-and-non-aggregated-data","content":"There is a widespread opinion that to calculate statistics effectively, you must aggregate data since this reduces the volume of data. But data aggregation comes with a lot of limitations: You must have a pre-defined list of required reports.The user can’t make custom reports.When aggregating over a large number of distinct keys, the data volume is barely reduced, so aggregation is useless.For a large number of reports, there are too many aggregation variations (combinatorial explosion).When aggregating keys with high cardinality (such as URLs), the volume of data is not reduced by much (less than twofold).For this reason, the volume of data with aggregation might grow instead of shrink.Users do not view all the reports we generate for them. A large portion of those calculations is useless.The logical integrity of data may be violated for various aggregations. If we do not aggregate anything and work with non-aggregated data, this might reduce the volume of calculations. However, with aggregation, a significant part of the work is taken offline and completed relatively calmly. In contrast, online calculations require calculating as fast as possible, since the user is waiting for the result. Yandex.Metrica has a specialized system for aggregating data called Metrage, which was used for the majority of reports. Starting in 2009, Yandex.Metrica also used a specialized OLAP database for non-aggregated data called OLAPServer, which was previously used for the report builder. OLAPServer worked well for non-aggregated data, but it had many restrictions that did not allow it to be used for all reports as desired. These included the lack of support for data types (only numbers), and the inability to incrementally update data in real-time (it could only be done by rewriting data daily). OLAPServer is not a DBMS, but a specialized DB. The initial goal for ClickHouse was to remove the limitations of OLAPServer and solve the problem of working with non-aggregated data for all reports, but over the years, it has grown into a general-purpose database management system suitable for a wide range of analytical tasks. "},{"title":"Performance","type":0,"sectionRef":"#","url":"en/about-us/performance","content":"","keywords":""},{"title":"Throughput for a Single Large Query​","type":1,"pageTitle":"Performance","url":"en/about-us/performance#throughput-for-a-single-large-query","content":"Throughput can be measured in rows per second or megabytes per second. If the data is placed in the page cache, a query that is not too complex is processed on modern hardware at a speed of approximately 2-10 GB/s of uncompressed data on a single server (for the most straightforward cases, the speed may reach 30 GB/s). If data is not placed in the page cache, the speed depends on the disk subsystem and the data compression rate. For example, if the disk subsystem allows reading data at 400 MB/s, and the data compression rate is 3, the speed is expected to be around 1.2 GB/s. To get the speed in rows per second, divide the speed in bytes per second by the total size of the columns used in the query. For example, if 10 bytes of columns are extracted, the speed is expected to be around 100-200 million rows per second. The processing speed increases almost linearly for distributed processing, but only if the number of rows resulting from aggregation or sorting is not too large. "},{"title":"Latency When Processing Short Queries​","type":1,"pageTitle":"Performance","url":"en/about-us/performance#latency-when-processing-short-queries","content":"If a query uses a primary key and does not select too many columns and rows to process (hundreds of thousands), you can expect less than 50 milliseconds of latency (single digits of milliseconds in the best case) if data is placed in the page cache. Otherwise, latency is mostly dominated by the number of seeks. If you use rotating disk drives, for a system that is not overloaded, the latency can be estimated with this formula: seek time (10 ms) * count of columns queried * count of data parts. "},{"title":"Throughput When Processing a Large Quantity of Short Queries​","type":1,"pageTitle":"Performance","url":"en/about-us/performance#throughput-when-processing-a-large-quantity-of-short-queries","content":"Under the same conditions, ClickHouse can handle several hundred queries per second on a single server (up to several thousand in the best case). Since this scenario is not typical for analytical DBMSs, we recommend expecting a maximum of 100 queries per second. "},{"title":"Performance When Inserting Data​","type":1,"pageTitle":"Performance","url":"en/about-us/performance#performance-when-inserting-data","content":"We recommend inserting data in packets of at least 1000 rows, or no more than a single request per second. When inserting to a MergeTree table from a tab-separated dump, the insertion speed can be from 50 to 200 MB/s. If the inserted rows are around 1 KB in size, the speed will be from 50,000 to 200,000 rows per second. If the rows are small, the performance can be higher in rows per second (on Banner System data -&gt; 500,000 rows per second; on Graphite data -&gt; 1,000,000 rows per second). To improve performance, you can make multiple INSERT queries in parallel, which scales linearly. "},{"title":"Distinctive Features of ClickHouse","type":0,"sectionRef":"#","url":"en/about-us/distinctive-features","content":"","keywords":""},{"title":"True Column-Oriented Database Management System​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#true-column-oriented-database-management-system","content":"In a real column-oriented DBMS, no extra data is stored with the values. Among other things, this means that constant-length values must be supported, to avoid storing their length “number” next to the values. For example, a billion UInt8-type values should consume around 1 GB uncompressed, or this strongly affects the CPU use. It is essential to store data compactly (without any “garbage”) even when uncompressed since the speed of decompression (CPU usage) depends mainly on the volume of uncompressed data. It is worth noting because there are systems that can store values of different columns separately, but that can’t effectively process analytical queries due to their optimization for other scenarios. Examples are HBase, BigTable, Cassandra, and HyperTable. You would get throughput around a hundred thousand rows per second in these systems, but not hundreds of millions of rows per second. It’s also worth noting that ClickHouse is a database management system, not a single database. ClickHouse allows creating tables and databases in runtime, loading data, and running queries without reconfiguring and restarting the server. "},{"title":"Data Compression​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#data-compression","content":"Some column-oriented DBMSs do not use data compression. However, data compression does play a key role in achieving excellent performance. In addition to efficient general-purpose compression codecs with different trade-offs between disk space and CPU consumption, ClickHouse provides specialized codecs for specific kinds of data, which allow ClickHouse to compete with and outperform more niche databases, like time-series ones. "},{"title":"Disk Storage of Data​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#disk-storage-of-data","content":"Keeping data physically sorted by primary key makes it possible to extract data for its specific values or value ranges with low latency, less than a few dozen milliseconds. Some column-oriented DBMSs (such as SAP HANA and Google PowerDrill) can only work in RAM. This approach encourages the allocation of a larger hardware budget than is necessary for real-time analysis. ClickHouse is designed to work on regular hard drives, which means the cost per GB of data storage is low, but SSD and additional RAM are also fully used if available. "},{"title":"Parallel Processing on Multiple Cores​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#parallel-processing-on-multiple-cores","content":"Large queries are parallelized naturally, taking all the necessary resources available on the current server. "},{"title":"Distributed Processing on Multiple Servers​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#distributed-processing-on-multiple-servers","content":"Almost none of the columnar DBMSs mentioned above have support for distributed query processing. In ClickHouse, data can reside on different shards. Each shard can be a group of replicas used for fault tolerance. All shards are used to run a query in parallel, transparently for the user. "},{"title":"SQL Support​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#sql-support","content":"ClickHouse supports a declarative query language based on SQL that is identical to the ANSI SQL standard in many cases. Supported queries include GROUP BY, ORDER BY, subqueries in FROM, JOIN clause, IN operator, window functions and scalar subqueries. Correlated (dependent) subqueries are not supported at the time of writing but might become available in the future. "},{"title":"Vector Computation Engine​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#vector-engine","content":"Data is not only stored by columns but is processed by vectors (parts of columns), which allows achieving high CPU efficiency. "},{"title":"Real-time Data Updates​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#real-time-data-updates","content":"ClickHouse supports tables with a primary key. To quickly perform queries on the range of the primary key, the data is sorted incrementally using the merge tree. Due to this, data can continually be added to the table. No locks are taken when new data is ingested. "},{"title":"Primary Index​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#primary-index","content":"Having a data physically sorted by primary key makes it possible to extract data for its specific values or value ranges with low latency, less than a few dozen milliseconds. "},{"title":"Secondary Indexes​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#secondary-indexes","content":"Unlike other database management systems, secondary indexes in ClickHouse does not point to specific rows or row ranges. Instead, they allow the database to know in advance that all rows in some data parts wouldn’t match the query filtering conditions and do not read them at all, thus they are called data skipping indexes. "},{"title":"Suitable for Online Queries​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#suitable-for-online-queries","content":"Most OLAP database management systems do not aim for online queries with sub-second latencies. In alternative systems, report building time of tens of seconds or even minutes is often considered acceptable. Sometimes it takes even more which forces to prepare reports offline (in advance or by responding with “come back later”). In ClickHouse low latency means that queries can be processed without delay and without trying to prepare an answer in advance, right at the same moment while the user interface page is loading. In other words, online. "},{"title":"Support for Approximated Calculations​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#support-for-approximated-calculations","content":"ClickHouse provides various ways to trade accuracy for performance: Aggregate functions for approximated calculation of the number of distinct values, medians, and quantiles.Running a query based on a part (sample) of data and getting an approximated result. In this case, proportionally less data is retrieved from the disk.Running an aggregation for a limited number of random keys, instead of for all keys. Under certain conditions for key distribution in the data, this provides a reasonably accurate result while using fewer resources. "},{"title":"Adaptive Join Algorithm​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#adaptive-join-algorithm","content":"ClickHouse adaptively chooses how to JOIN multiple tables, by preferring hash-join algorithm and falling back to the merge-join algorithm if there’s more than one large table. "},{"title":"Data Replication and Data Integrity Support​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#data-replication-and-data-integrity-support","content":"ClickHouse uses asynchronous multi-master replication. After being written to any available replica, all the remaining replicas retrieve their copy in the background. The system maintains identical data on different replicas. Recovery after most failures is performed automatically, or semi-automatically in complex cases. For more information, see the section Data replication. "},{"title":"Role-Based Access Control​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#role-based-access-control","content":"ClickHouse implements user account management using SQL queries and allows for role-based access control configuration similar to what can be found in ANSI SQL standard and popular relational database management systems. "},{"title":"Features that Can Be Considered Disadvantages​","type":1,"pageTitle":"Distinctive Features of ClickHouse","url":"en/about-us/distinctive-features#clickhouse-features-that-can-be-considered-disadvantages","content":"No full-fledged transactions.Lack of ability to modify or delete already inserted data with a high rate and low latency. There are batch deletes and updates available to clean up or modify data, for example, to comply with GDPR.The sparse index makes ClickHouse not so efficient for point queries retrieving single rows by their keys. "},{"title":"Connecting Grafana to ClickHouse","type":0,"sectionRef":"#","url":"en/connect-a-ui/grafana-and-clickhouse","content":"","keywords":"clickhouse grafana connect integrate"},{"title":"1. Install the Grafana Plugin for ClickHouse​","type":1,"pageTitle":"Connecting Grafana to ClickHouse","url":"en/connect-a-ui/grafana-and-clickhouse#1--install-the-grafana-plugin-for-clickhouse","content":"Before Grafana can talk to ClickHouse, you need to install the appropriate Grafana plugin. Assuming you are logged in to Grafana, follow these steps: From the Configuration page, select the Plugins tab. Search for ClickHouse and click on the Signed plugin by Grafana Labs: On the next screen, click the Install button: "},{"title":"2. Define a ClickHouse data source​","type":1,"pageTitle":"Connecting Grafana to ClickHouse","url":"en/connect-a-ui/grafana-and-clickhouse#2-define-a-clickhouse-data-source","content":"Once the installation is complete, click the Create a ClickHouse data source button. (You can also add a data source from the Data sources tab on the Configuration page.) Either scroll down and find the ClickHouse data source type, or you can search for it in the search bar of the Add data source page. Either way, select the ClickHouse data source type and the following dialog appears: Enter your server settings and credentials. The key settings are: Name: a Grafana setting - give your data source any name you likeServer address: the URL of your ClickHouse serviceServer port: 9000 for unsecure, 9440 for secure (unless you modified the ClickHouse ports)Username and Password: enter your ClickHouse user credentials. If you have not configured users and passwords, then try default for the username and leave the password empty.Default database: a Grafana setting - you can specify a database that Grafana defaults to when using this data source (this property can be left blank) Click the Save &amp; test button to verify that Grafana can connect to your ClickHouse service. If successful, you will see a Data source is working message: "},{"title":"3. Build a dashboard​","type":1,"pageTitle":"Connecting Grafana to ClickHouse","url":"en/connect-a-ui/grafana-and-clickhouse#3-build-a-dashboard","content":"From the left menu, click on the Dashboards icon and select Browse. Then select the New Dashboard button: Click the Add a new panel button. From here, you can build a visualization based on a query. From the Data source dropdown, select your ClickHouse data source that you defined earlier. Then you can either use the Query Builder to build a query visually, or switch to the SQL Editor and enter a SQL query (as shown here): That's it! You are now ready to build visualizations and dashboards in Grafana. "},{"title":"Connecting Metabase to ClickHouse","type":0,"sectionRef":"#","url":"en/connect-a-ui/metabase-and-clickhouse","content":"","keywords":"clickhouse metabase connect integrate ui"},{"title":"1. Download the ClickHouse plugin for Metabase​","type":1,"pageTitle":"Connecting Metabase to ClickHouse","url":"en/connect-a-ui/metabase-and-clickhouse#1--download-the-clickhouse-plugin-for-metabase","content":"If you do not have a plugins folder, create one as a subfolder of where you have metabase.jar saved. The plugin is a JAR file named clickhouse.metabase-driver.jar. Download the latest version of the JAR file at https://github.com/enqueue/metabase-clickhouse-driver/releases/latest Save clickhouse.metabase-driver.jar in your plugins folder. Start (or restart) Metabase so that the driver gets loaded properly. Access Metabse at http://hostname:3000. On the initial startup, you will see a welcome screen and have to work your way through a list of questions. If prompted to select a database, select &quot;I'll add my data later&quot;: "},{"title":"2. Connect Metabase to ClickHouse​","type":1,"pageTitle":"Connecting Metabase to ClickHouse","url":"en/connect-a-ui/metabase-and-clickhouse#2--connect-metabase-to-clickhouse","content":"Click on the gear icon in the top-right corner and select Admin Settings to visit your Metabase admin page. Click on Add a database. Alternately, you can click on the Databases tab and select the Add database button. If your driver installation worked, you will see ClickHouse in the dropdown menu for Database type: Give your database a Display name, which is a Metabase setting - so use any name you like. Enter the connection details of your ClickHouse database. For example: Click the Save button and Metabase will scan your database for tables. "},{"title":"3. Run a SQL query​","type":1,"pageTitle":"Connecting Metabase to ClickHouse","url":"en/connect-a-ui/metabase-and-clickhouse#3-run-a-sql-query","content":"Exit the Admin settings by clicking the Exit admin button in the top-right corner. In the top-right corner, click the + New menu and notice you can ask questions, run SQL queries, and build a dashboard: For example, here is a SQL query executed on a table named hits that returns the top ten most-visited URLs: "},{"title":"4. Ask a question​","type":1,"pageTitle":"Connecting Metabase to ClickHouse","url":"en/connect-a-ui/metabase-and-clickhouse#4-ask-a-question","content":"Click on + New and select Question. Notice you can build a question by starting wtih a database and table. For example, the following question is being asked of a table named hits in the Web Traffic Database: Here is a simple question that calculates the top 20 most-visited URLs in the table: Click the Visualize button to see the results in a tabular view. Below the results, click the Visualization button to change the visualization to a bar chart (or any of the other options avaialable): Find more information about Metabase and how to build dashboards by visiting the Metabase documentation. "},{"title":"Browse ClickHouse Source Code","type":0,"sectionRef":"#","url":"en/development/browse-code","content":"Browse ClickHouse Source Code You can use the Woboq online code browser available here. It provides code navigation and semantic highlighting, search and indexing. The code snapshot is updated daily. Also, you can browse sources on GitHub as usual. If you’re interested what IDE to use, we recommend CLion, QT Creator, VS Code and KDevelop (with caveats). You can use any favorite IDE. Vim and Emacs also count.","keywords":""},{"title":"Connect Superset to ClickHouse","type":0,"sectionRef":"#","url":"en/connect-a-ui/superset-and-clickhouse","content":"","keywords":"clickhouse superset connect integrate ui"},{"title":"1. Install the Drivers​","type":1,"pageTitle":"Connect Superset to ClickHouse","url":"en/connect-a-ui/superset-and-clickhouse#1-install-the-drivers","content":"Superset uses the clickhouse-sqlalchemy driver, which requires the clickhouse-driver to connect to ClickHouse. The details of clickhouse-driver are at https://pypi.org/project/clickhouse-driver/ and can be installed with the following command: pip install clickhouse-driver Now install the ClickHouse SQLAlchemy driver: pip install clickhouse-sqlalchemy Start (or restart) Superset. "},{"title":"2. Connect Superset to ClickHouse​","type":1,"pageTitle":"Connect Superset to ClickHouse","url":"en/connect-a-ui/superset-and-clickhouse#2-connect-superset-to-clickhouse","content":"Within Superset, select Data from the top menu and then Databases from the drop-down menu. Add a new database by clicking the + Database button: In the first step, select ClickHouse as the type of database: In the second step, enter a display name for your database and the connection URI. The DISPLAY NAME can be any name you prefer. The SQLALCHEMY URI is the important setting - it has the following format: clickhouse+native://username:password@hostname/database_name In the example below, ClickHouse is running on localhost with the default user and no password. The name of the database is covid19db. Use the TEST CONNECTION button to verify that Superset is connecting to your ClickHouse database properly: Click the CONNECT button to complete the setup wizard, and you should see your database in the list of databases. "},{"title":"3. Add a Dataset​","type":1,"pageTitle":"Connect Superset to ClickHouse","url":"en/connect-a-ui/superset-and-clickhouse#3-add-a-dataset","content":"To define new charts (visualizations) in Superset, you need to define a dataset. From the top menu in Superset, select Data, then Datasets from the drop-down menu. Click the button for adding a dataset. Select your new database as the datasource and you should see the tables defined in your database. For example, the covid19db database has a table named daily_totals: Click the ADD button at the bottom of the dialog window and your table appears in the list of datasets. You are ready to build a dashboard and analyze your ClickHouse data! "},{"title":"4. Creating charts and a dashboard in Superset​","type":1,"pageTitle":"Connect Superset to ClickHouse","url":"en/connect-a-ui/superset-and-clickhouse#4--creating-charts-and-a-dashboard-in-superset","content":"If you are familiar with Superset, then you will feel right at home with this next section. If you are new to Superset, well...it's like a lot of the other cool visualization tools out there in the world - it doesn't take long to get started, but the details and nuances get learned over time as you use the tool. You start with a dashboard. From the top menu in Superset, select Dashboards. Click the button in the upper-right to add a new dashboard. The following dashboard is named Covid-19 Dashboard: To create a new chart, select Charts from the top menu and click the button to add a new chart. You will be shown a lot of options. The following example shows a Big Number chart using the daily_totals dataset from the CHOOSE A DATASET drop-down: You need to add a metric to a Big Number. The column named DATA and the section named Query with a METRIC field show a red warning because they are not defined yet. To add a metric, click Add metric and a small dialog window appears: The following example uses the SUM metric, found on the the SIMPLE tab. It sums the values of the new_cases column: To view the actual number, click the RUN QUERY button: Click the SAVE button to save the chart, then select Covid-19 Dashboard under the ADD TO DASHBOARD drop-down, then SAVE &amp; GO TO DASHBOARD saves the chart and adds it to the dashboard: That's it. Building dashboards in Superset based on data in ClickHouse opens up a whole world of blazing fast data analytics! "},{"title":"How to Build ClickHouse on Linux for AARCH64 (ARM64) Architecture","type":0,"sectionRef":"#","url":"en/development/build-cross-arm","content":"","keywords":""},{"title":"Install Clang-13​","type":1,"pageTitle":"How to Build ClickHouse on Linux for AARCH64 (ARM64) Architecture","url":"en/development/build-cross-arm#install-clang-13","content":"Follow the instructions from https://apt.llvm.org/ for your Ubuntu or Debian setup or do sudo bash -c &quot;$(wget -O - https://apt.llvm.org/llvm.sh)&quot;  "},{"title":"Install Cross-Compilation Toolset​","type":1,"pageTitle":"How to Build ClickHouse on Linux for AARCH64 (ARM64) Architecture","url":"en/development/build-cross-arm#install-cross-compilation-toolset","content":"cd ClickHouse mkdir -p build-aarch64/cmake/toolchain/linux-aarch64 wget 'https://developer.arm.com/-/media/Files/downloads/gnu-a/8.3-2019.03/binrel/gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz?revision=2e88a73f-d233-4f96-b1f4-d8b36e9bb0b9&amp;la=en' -O gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz tar xJf gcc-arm-8.3-2019.03-x86_64-aarch64-linux-gnu.tar.xz -C build-aarch64/cmake/toolchain/linux-aarch64 --strip-components=1  "},{"title":"Build ClickHouse​","type":1,"pageTitle":"How to Build ClickHouse on Linux for AARCH64 (ARM64) Architecture","url":"en/development/build-cross-arm#build-clickhouse","content":"cd ClickHouse mkdir build-arm64 CC=clang-13 CXX=clang++-13 cmake . -Bbuild-arm64 -DCMAKE_TOOLCHAIN_FILE=cmake/linux/toolchain-aarch64.cmake ninja -C build-arm64  The resulting binary will run only on Linux with the AARCH64 CPU architecture. "},{"title":"How to Build ClickHouse on Linux for RISC-V 64 Architecture","type":0,"sectionRef":"#","url":"en/development/build-cross-riscv","content":"","keywords":""},{"title":"Install Clang-13​","type":1,"pageTitle":"How to Build ClickHouse on Linux for RISC-V 64 Architecture","url":"en/development/build-cross-riscv#install-clang-13","content":"Follow the instructions from https://apt.llvm.org/ for your Ubuntu or Debian setup or do sudo bash -c &quot;$(wget -O - https://apt.llvm.org/llvm.sh)&quot;  "},{"title":"Build ClickHouse​","type":1,"pageTitle":"How to Build ClickHouse on Linux for RISC-V 64 Architecture","url":"en/development/build-cross-riscv#build-clickhouse","content":"cd ClickHouse mkdir build-riscv64 CC=clang-13 CXX=clang++-13 cmake . -Bbuild-riscv64 -G Ninja -DCMAKE_TOOLCHAIN_FILE=cmake/linux/toolchain-riscv64.cmake -DGLIBC_COMPATIBILITY=OFF -DENABLE_LDAP=OFF -DOPENSSL_NO_ASM=ON -DENABLE_JEMALLOC=ON -DENABLE_PARQUET=OFF -DENABLE_ORC=OFF -DUSE_UNWIND=OFF -DENABLE_GRPC=OFF -DENABLE_HDFS=OFF -DENABLE_MYSQL=OFF ninja -C build-riscv64  The resulting binary will run only on Linux with the RISC-V 64 CPU architecture. "},{"title":"Connecting Tableau to ClickHouse","type":0,"sectionRef":"#","url":"en/connect-a-ui/tableau-and-clickhouse","content":"","keywords":"clickhouse tableau connect integrate ui"},{"title":"1. Download the JDBC Driver​","type":1,"pageTitle":"Connecting Tableau to ClickHouse","url":"en/connect-a-ui/tableau-and-clickhouse#1--download-the-jdbc-driver","content":"The Tableau connector is an extension of the ClickHouse JDBC driver, so you need to download the JDBC driver and save it in the correct folder. Download the latest version of the ClickHouse JDBC driver at https://github.com/ClickHouse/clickhouse-jdbc/releases/. (We used this version of the driver for this tutorial.)  note Make sure you download the clickhouse-jdbc-x.x.x-shaded.jar JAR file. Store the JDBC driver in the following folder (based on your OS): Operating System\tDestination folderMacOS\t~/Library/Tableau/Drivers Windows\tC:\\Program Files\\Tableau\\Drivers That's it. The driver will be found the next time you start Tableau.  "},{"title":"3. Download the Connector​","type":1,"pageTitle":"Connecting Tableau to ClickHouse","url":"en/connect-a-ui/tableau-and-clickhouse#3-download-the-connector","content":"ANALYTIKA PLUS has built a handy connector for simplifying connections to ClickHouse from Tableau. You can view the details of the project in Github. Follow these steps to download the connector... The connector is built in a taco file (short for Tableau Connector). Download the latest version at https://github.com/analytikaplus/clickhouse-tableau-connector-jdbc/releases/. (For this lesson, we downloaded v0.1.1 of clickhouse_jdbc.taco.) Store clickhouse_jdbc.taco in the following folder (based on your OS): Operating System\tDestination folderMacOS\t~/Documents/My Tableau Repository/Connectors Windows\tC:\\Users[Windows User]\\Documents\\My Tableau Repository\\Connectors  The connector is now ready to go.  "},{"title":"4. Configure a ClickHouse data source in Tableau​","type":1,"pageTitle":"Connecting Tableau to ClickHouse","url":"en/connect-a-ui/tableau-and-clickhouse#4--configure-a-clickhouse-data-source-in-tableau","content":"Now that you have the driver and connector in the approriate folders on your machine, let's see how to define a data source in Tableau that connects to the TPCD database in ClickHouse. Start Tableau. (If you already had it running, then restart it.) From the left-side menu, click on More under the To a Server section. If everything worked properly, you should see ClickHouse (JDBC) by ANALYTIKA PLUS in the list of installed connectors:  Click on ClickHouse (JDBC) by ANALYTIKA PLUS and a dialog window pops up. Enter the following details: Setting\tValueServer\tlocalhost Port\t8123 Database\tdefault Username\tdefault Password\tleave blank  Your settings should look like:  note Our ClickHouse database is named TPCD, but you must set the Database to default in the dialog above, then select TPCD for the Schema in the next step. (This is likely due to a bug in the connector, so this behavior could change, but for now you must use default as the database.) Click the Sign In button and you should see a new Tableau workbook:  Select TPCD from the Schema dropdown and you should see the list of tables in TPCD:  You are now ready to build some visualizations in Tableau!  "},{"title":"5. Building Visualizations in Tableau​","type":1,"pageTitle":"Connecting Tableau to ClickHouse","url":"en/connect-a-ui/tableau-and-clickhouse#5-building-visualizations-in-tableau","content":"Now that have a ClickHouse data source configured in Tableau, let's visualize the data... Drag the CUSTOMER table onto the workbook. Notice the columns appear, but the data table is empty:  Click the Update Now button and 100 rows from CUSTOMER will populate the table. Drag the ORDERS table into the workbook, then set Custkey as the relationship field between the two tables:  You now have the ORDERS and LINEITEM tables associated with each other as your data source, so you can use this relationship to answer questions about the data. Select the Sheet 1 tab at the bottom of the workbook.  Suppose you want to know how many specific items were ordered each year. Drag Orderdate from ORDERS into the Columns section (the horizontal field), then drag Quantity from LINEITEM into the Rows. Tableau will generate the following line chart:  Not a very exciting line chart, but the dataset was generated by a script and built for testing query performance, so you will notice there is not a lot of variations in the simulated orders of the TCPD data. Suppose you want to know the average order amount (in dollars) by quarter and also by shipping mode (air, mail, ship, truck, etc.): Click the New Worksheet tab create a new sheetDrag OrderDate from ORDERS into Columns and change it from Year to QuarterDrag Shipmode from LINEITEM into Rows  You should see the following:  The Abc values are just filling in the space until you drag a metric onto the table. Drag Totalprice from ORDERS onto the table. Notice the default calculation is to SUM the Totalpricess:  Click on SUM and change the Measure to Average. From the same dropdown menu, select Format change the Numbers to Currency (Standard):  Well done! You have successfully connected Tableau to ClickHouse, and you have opened up a whole world of possibilities for analyzing and visualizing your ClickHouse data. note Tableau is great, and we love that it connects so nicely to ClickHouse! If you are new to Tableau, check out their documentation for help on building dashboards and visualizations.  Summary: You can connect Tableau to ClickHouse using the generic ODBC/JDBC ClickHouse driver, but we really like how this tool from ANALYTIKA PLUS simplifies the process of setting up the connection. If you have any issues with the connector, feel free to reach out to ANALYTIKA PLUS on GitHub. "},{"title":"How to Build ClickHouse on Linux for Mac OS X","type":0,"sectionRef":"#","url":"en/development/build-cross-osx","content":"","keywords":""},{"title":"Install Clang-13​","type":1,"pageTitle":"How to Build ClickHouse on Linux for Mac OS X","url":"en/development/build-cross-osx#install-clang-13","content":"Follow the instructions from https://apt.llvm.org/ for your Ubuntu or Debian setup. For example the commands for Bionic are like: sudo echo &quot;deb [trusted=yes] http://apt.llvm.org/bionic/ llvm-toolchain-bionic-13 main&quot; &gt;&gt; /etc/apt/sources.list sudo apt-get install clang-13  "},{"title":"Install Cross-Compilation Toolset​","type":1,"pageTitle":"How to Build ClickHouse on Linux for Mac OS X","url":"en/development/build-cross-osx#install-cross-compilation-toolset","content":"Let’s remember the path where we install cctools as ${CCTOOLS} mkdir ${CCTOOLS} cd ${CCTOOLS} git clone https://github.com/tpoechtrager/apple-libtapi.git cd apple-libtapi INSTALLPREFIX=${CCTOOLS} ./build.sh ./install.sh cd .. git clone https://github.com/tpoechtrager/cctools-port.git cd cctools-port/cctools ./configure --prefix=$(readlink -f ${CCTOOLS}) --with-libtapi=$(readlink -f ${CCTOOLS}) --target=x86_64-apple-darwin make install  Also, we need to download macOS X SDK into the working tree. cd ClickHouse wget 'https://github.com/phracker/MacOSX-SDKs/releases/download/10.15/MacOSX10.15.sdk.tar.xz' mkdir -p build-darwin/cmake/toolchain/darwin-x86_64 tar xJf MacOSX10.15.sdk.tar.xz -C build-darwin/cmake/toolchain/darwin-x86_64 --strip-components=1  "},{"title":"Build ClickHouse​","type":1,"pageTitle":"How to Build ClickHouse on Linux for Mac OS X","url":"en/development/build-cross-osx#build-clickhouse","content":"cd ClickHouse mkdir build-darwin cd build-darwin CC=clang-13 CXX=clang++-13 cmake -DCMAKE_AR:FILEPATH=${CCTOOLS}/bin/aarch64-apple-darwin-ar -DCMAKE_INSTALL_NAME_TOOL=${CCTOOLS}/bin/aarch64-apple-darwin-install_name_tool -DCMAKE_RANLIB:FILEPATH=${CCTOOLS}/bin/aarch64-apple-darwin-ranlib -DLINKER_NAME=${CCTOOLS}/bin/aarch64-apple-darwin-ld -DCMAKE_TOOLCHAIN_FILE=cmake/darwin/toolchain-x86_64.cmake .. ninja  The resulting binary will have a Mach-O executable format and can’t be run on Linux. "},{"title":"How to add test queries to ClickHouse CI","type":0,"sectionRef":"#","url":"en/development/adding_test_queries","content":"","keywords":""},{"title":"Why adding tests​","type":1,"pageTitle":"How to add test queries to ClickHouse CI","url":"en/development/adding_test_queries#why-adding-tests","content":"Why/when you should add a test case into ClickHouse code: 1) you use some complicated scenarios / feature combinations / you have some corner case which is probably not widely used 2) you see that certain behavior gets changed between version w/o notifications in the changelog 3) you just want to help to improve ClickHouse quality and ensure the features you use will not be broken in the future releases 4) once the test is added/accepted, you can be sure the corner case you check will never be accidentally broken. 5) you will be a part of great open-source community 6) your name will be visible in the system.contributors table! 7) you will make a world bit better :) "},{"title":"Steps to do​","type":1,"pageTitle":"How to add test queries to ClickHouse CI","url":"en/development/adding_test_queries#steps-to-do","content":"Prerequisite​ I assume you run some Linux machine (you can use docker / virtual machines on other OS) and any modern browser / internet connection, and you have some basic Linux &amp; SQL skills. Any highly specialized knowledge is not needed (so you don't need to know C++ or know something about how ClickHouse CI works). Preparation​ 1) create GitHub account (if you haven't one yet) 2) setup git # for Ubuntu sudo apt-get update sudo apt-get install git git config --global user.name &quot;John Doe&quot; # fill with your name git config --global user.email &quot;email@example.com&quot; # fill with your email  3) fork ClickHouse project - just open https://github.com/ClickHouse/ClickHouse and press fork button in the top right corner: 4) clone your fork to some folder on your PC, for example, ~/workspace/ClickHouse mkdir ~/workspace &amp;&amp; cd ~/workspace git clone https://github.com/&lt; your GitHub username&gt;/ClickHouse cd ClickHouse git remote add upstream https://github.com/ClickHouse/ClickHouse  New branch for the test​ 1) create a new branch from the latest clickhouse master cd ~/workspace/ClickHouse git fetch upstream git checkout -b name_for_a_branch_with_my_test upstream/master  Install &amp; run clickhouse​ 1) install clickhouse-server (follow official docs) 2) install test configurations (it will use Zookeeper mock implementation and adjust some settings) cd ~/workspace/ClickHouse/tests/config sudo ./install.sh  3) run clickhouse-server sudo systemctl restart clickhouse-server  Creating the test file​ 1) find the number for your test - find the file with the biggest number in tests/queries/0_stateless/ $ cd ~/workspace/ClickHouse $ ls tests/queries/0_stateless/[0-9]*.reference | tail -n 1 tests/queries/0_stateless/01520_client_print_query_id.reference  Currently, the last number for the test is 01520, so my test will have the number 01521 2) create an SQL file with the next number and name of the feature you test touch tests/queries/0_stateless/01521_dummy_test.sql  3) edit SQL file with your favorite editor (see hint of creating tests below) vim tests/queries/0_stateless/01521_dummy_test.sql  4) run the test, and put the result of that into the reference file: clickhouse-client -nmT &lt; tests/queries/0_stateless/01521_dummy_test.sql | tee tests/queries/0_stateless/01521_dummy_test.reference  5) ensure everything is correct, if the test output is incorrect (due to some bug for example), adjust the reference file using text editor. How to create a good test​ A test should be minimal - create only tables related to tested functionality, remove unrelated columns and parts of queryfast - should not take longer than a few seconds (better subseconds)correct - fails then feature is not working - deterministic isolated / stateless don't rely on some environment thingsdon't rely on timing when possible try to cover corner cases (zeros / Nulls / empty sets / throwing exceptions)to test that query return errors, you can put special comment after the query: -- { serverError 60 } or -- { clientError 20 }don't switch databases (unless necessary)you can create several table replicas on the same node if neededyou can use one of the test cluster definitions when needed (see system.clusters)use number / numbers_mt / zeros / zeros_mt and similar for queries / to initialize data when applicableclean up the created objects after test and before the test (DROP IF EXISTS) - in case of some dirty stateprefer sync mode of operations (mutations, merges, etc.)use other SQL files in the 0_stateless folder as an exampleensure the feature / feature combination you want to test is not yet covered with existing tests Test naming rules​ It's important to name tests correctly, so one could turn some tests subset off in clickhouse-test invocation. Tester flag\tWhat should be in test name\tWhen flag should be added\t--[no-]zookeeper\t&quot;zookeeper&quot; or &quot;replica&quot;\tTest uses tables from ReplicatedMergeTree family --[no-]shard\t&quot;shard&quot; or &quot;distributed&quot; or &quot;global&quot;\tTest using connections to 127.0.0.2 or similar --[no-]long\t&quot;long&quot; or &quot;deadlock&quot; or &quot;race&quot;\tTest runs longer than 60 seconds\t Commit / push / create PR.​ 1) commit &amp; push your changes cd ~/workspace/ClickHouse git add tests/queries/0_stateless/01521_dummy_test.sql git add tests/queries/0_stateless/01521_dummy_test.reference git commit # use some nice commit message when possible git push origin HEAD  2) use a link which was shown during the push, to create a PR into the main repo 3) adjust the PR title and contents, in Changelog category (leave one) keepBuild/Testing/Packaging Improvement, fill the rest of the fields if you want. "},{"title":"Integrating Rust libraries","type":0,"sectionRef":"#","url":"en/development/integrating_rust_libraries","content":"Integrating Rust libraries Rust library integration will be described based on BLAKE3 hash-function integration. The first step is forking a library and making neccessary changes for Rust and C/C++ compatibility. After forking library repository you need to change target settings in Cargo.toml file. Firstly, you need to switch build to static library. Secondly, you need to add cbindgen crate to the crate list. We will use it later to generate C-header automatically. The next step is creating or editing the build.rs script for your library - and enable cbindgen to generate the header during library build. These lines were added to BLAKE3 build script for the same purpose: let crate_dir = env::var(&quot;CARGO_MANIFEST_DIR&quot;).unwrap(); let package_name = env::var(&quot;CARGO_PKG_NAME&quot;).unwrap(); let output_file = (&quot;include/&quot;.to_owned() + &amp;format!(&quot;{}.h&quot;, package_name)).to_string(); match cbindgen::generate(&amp;crate_dir) { Ok(header) =&gt; { header.write_to_file(&amp;output_file); } Err(err) =&gt; { panic!(&quot;{}&quot;, err) } } As you can see, script sets the output directory and launches header generation. The next step is to add CMake files into library directory, so it can build with other submodules. As you can see, BLAKE3 main directory contains two CMake files - CMakeLists.txt and build_rust_lib.cmake. The second one is a function, which calls cargo build and sets all needed paths for library build. You should copy it to your library and then you can adjust cargo flags and other settings for you library needs. When finished with CMake configuration, you should move on to create a C/C++ compatible API for your library. Let us see BLAKE3's method blake3_apply_shim: #[no_mangle] pub unsafe extern &quot;C&quot; fn blake3_apply_shim( begin: *const c_char, _size: u32, out_char_data: *mut u8, ) -&gt; *mut c_char { if begin.is_null() { let err_str = CString::new(&quot;input was a null pointer&quot;).unwrap(); return err_str.into_raw(); } let mut hasher = Hasher::new(); let input_bytes = CStr::from_ptr(begin); let input_res = input_bytes.to_bytes(); hasher.update(input_res); let mut reader = hasher.finalize_xof(); reader.fill(std::slice::from_raw_parts_mut(out_char_data, OUT_LEN)); std::ptr::null_mut() } This method gets C-compatible string, its size and output string pointer as input. Then, it converts C-compatible inputs into types that are used by actual library methods and calls them. After that, it should convert library methods' outputs back into C-compatible type. In that particular case library supported direct writing into pointer by method fill(), so the convertion was not needed. The main advice here is to create less methods, so you will need to do less convertions on each method call and won't create much overhead. Also, you should use attribute #[no_mangle] and extern &quot;C&quot; for every C-compatible attribute. Without it library can compile incorrectly and cbindgen won't launch header autogeneration. After all these steps you can test your library in a small project to find all problems with compatibility or header generation. If any problems occur during header generation, you can try to configure it with cbindgen.toml file (you can find an example of it in BLAKE3 directory or a template here: https://github.com/eqrion/cbindgen/blob/master/template.toml). If everything works correctly, you can finally integrate its methods into ClickHouse. In addition, some problems with integration are worth noting here: 1) Some architectures may require special cargo flags or build.rs configurations, so you may want to test cross-compilation for different platforms first. 2) MemorySanitizer can cause false-positive reports as it's unable to see if some variables in Rust are initialized or not. It was solved with writing a method with more explicit definition for some variables, although this implementation of method is slower and is used only to fix MemorySanitizer builds.","keywords":""},{"title":"Database Engines","type":0,"sectionRef":"#","url":"en/engines/database-engines/","content":"Database Engines Database engines allow you to work with tables. By default, ClickHouse uses the Atomic database engine, which provides configurable table engines and an SQL dialect. Here is a complete list of available database engines. Follow the links for more details: Atomic MySQL MaterializedMySQL Lazy PostgreSQL Replicated SQLite","keywords":""},{"title":"Lazy","type":0,"sectionRef":"#","url":"en/engines/database-engines/lazy","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"Lazy","url":"en/engines/database-engines/lazy#creating-a-database","content":"CREATE DATABASE testlazy ENGINE = Lazy(expiration_time_in_seconds);  Original article "},{"title":"How to Build ClickHouse on Linux","type":0,"sectionRef":"#","url":"en/development/build","content":"","keywords":""},{"title":"Normal Build for Development on Ubuntu​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"en/development/build#normal-build-for-development-on-ubuntu","content":"The following tutorial is based on the Ubuntu Linux system. With appropriate changes, it should also work on any other Linux distribution. "},{"title":"Install Git, CMake, Python and Ninja​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"en/development/build#install-git-cmake-python-and-ninja","content":"$ sudo apt-get install git cmake python ninja-build  Or cmake3 instead of cmake on older systems. "},{"title":"Install the latest clang (recommended)​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"en/development/build#install-the-latest-clang-recommended","content":"On Ubuntu/Debian you can use the automatic installation script (check official webpage) sudo bash -c &quot;$(wget -O - https://apt.llvm.org/llvm.sh)&quot;  For other Linux distribution - check the availability of the prebuild packages or build clang from sources. Use the latest clang for Builds​ $ export CC=clang-14 $ export CXX=clang++-14  In this example we use version 14 that is the latest as of Feb 2022. Gcc can also be used though it is discouraged. "},{"title":"Checkout ClickHouse Sources​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"en/development/build#checkout-clickhouse-sources","content":"$ git clone --recursive git@github.com:ClickHouse/ClickHouse.git  or $ git clone --recursive https://github.com/ClickHouse/ClickHouse.git  "},{"title":"Build ClickHouse​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"en/development/build#build-clickhouse","content":"$ cd ClickHouse $ mkdir build $ cd build $ cmake .. $ ninja  To create an executable, run ninja clickhouse. This will create the programs/clickhouse executable, which can be used with client or server arguments. "},{"title":"How to Build ClickHouse on Any Linux​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"en/development/build#how-to-build-clickhouse-on-any-linux","content":"The build requires the following components: Git (is used only to checkout the sources, it’s not needed for the build)CMake 3.14 or newerNinjaC++ compiler: clang-13 or newerLinker: lld If all the components are installed, you may build in the same way as the steps above. Example for Ubuntu Eoan: sudo apt update sudo apt install git cmake ninja-build clang++ python git clone --recursive https://github.com/ClickHouse/ClickHouse.git mkdir build &amp;&amp; cd build cmake ../ClickHouse ninja  Example for OpenSUSE Tumbleweed: sudo zypper install git cmake ninja clang-c++ python lld git clone --recursive https://github.com/ClickHouse/ClickHouse.git mkdir build &amp;&amp; cd build cmake ../ClickHouse ninja  Example for Fedora Rawhide: sudo yum update yum --nogpg install git cmake make clang-c++ python3 git clone --recursive https://github.com/ClickHouse/ClickHouse.git mkdir build &amp;&amp; cd build cmake ../ClickHouse make -j $(nproc)  Here is an example of how to build clang and all the llvm infrastructure from sources:  git clone git@github.com:llvm/llvm-project.git mkdir llvm-build &amp;&amp; cd llvm-build cmake -DCMAKE_BUILD_TYPE:STRING=Release -DLLVM_ENABLE_PROJECTS=all ../llvm-project/llvm/ make -j16 sudo make install hash clang clang --version  You can install the older clang like clang-11 from packages and then use it to build the new clang from sources. Here is an example of how to install the new cmake from the official website: wget https://github.com/Kitware/CMake/releases/download/v3.22.2/cmake-3.22.2-linux-x86_64.sh chmod +x cmake-3.22.2-linux-x86_64.sh ./cmake-3.22.2-linux-x86_64.sh export PATH=/home/milovidov/work/cmake-3.22.2-linux-x86_64/bin/:${PATH} hash cmake  "},{"title":"How to Build ClickHouse Debian Package​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"en/development/build#how-to-build-clickhouse-debian-package","content":""},{"title":"Install Git​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"en/development/build#install-git","content":"$ sudo apt-get update $ sudo apt-get install git python debhelper lsb-release fakeroot sudo debian-archive-keyring debian-keyring  "},{"title":"Checkout ClickHouse Sources​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"en/development/build#checkout-clickhouse-sources-1","content":"$ git clone --recursive --branch master https://github.com/ClickHouse/ClickHouse.git $ cd ClickHouse  "},{"title":"Run Release Script​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"en/development/build#run-release-script","content":"$ ./release  "},{"title":"You Don’t Have to Build ClickHouse​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"en/development/build#you-dont-have-to-build-clickhouse","content":"ClickHouse is available in pre-built binaries and packages. Binaries are portable and can be run on any Linux flavour. They are built for stable, prestable and testing releases as long as for every commit to master and for every pull request. To find the freshest build from master, go to commits page, click on the first green checkmark or red cross near commit, and click to the “Details” link right after “ClickHouse Build Check”. "},{"title":"Faster builds for development: Split build configuration​","type":1,"pageTitle":"How to Build ClickHouse on Linux","url":"en/development/build#split-build","content":"Normally, ClickHouse is statically linked into a single static clickhouse binary with minimal dependencies. This is convenient for distribution, but it means that on every change the entire binary needs to be linked, which is slow and may be inconvenient for development. There is an alternative configuration which instead creates dynamically loaded shared libraries and separate binaries clickhouse-server, clickhouse-client etc., allowing for faster incremental builds. To use it, add the following flags to your cmake invocation: -DUSE_STATIC_LIBRARIES=0 -DSPLIT_SHARED_LIBRARIES=1 -DCLICKHOUSE_SPLIT_BINARY=1  Note that the split build has several drawbacks: There is no single clickhouse binary, and you have to run clickhouse-server, clickhouse-client, etc.Risk of segfault if you run any of the programs while rebuilding the project.You cannot run the integration tests since they only work a single complete binary.You can't easily copy the binaries elsewhere. Instead of moving a single binary you'll need to copy all binaries and libraries. Original article "},{"title":"Atomic","type":0,"sectionRef":"#","url":"en/engines/database-engines/atomic","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"Atomic","url":"en/engines/database-engines/atomic#creating-a-database","content":"CREATE DATABASE test [ENGINE = Atomic];  "},{"title":"Specifics and recommendations​","type":1,"pageTitle":"Atomic","url":"en/engines/database-engines/atomic#specifics-and-recommendations","content":""},{"title":"Table UUID​","type":1,"pageTitle":"Atomic","url":"en/engines/database-engines/atomic#table-uuid","content":"All tables in database Atomic have persistent UUID and store data in directory /clickhouse_path/store/xxx/xxxyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy/, where xxxyyyyy-yyyy-yyyy-yyyy-yyyyyyyyyyyy is UUID of the table. Usually, the UUID is generated automatically, but the user can also explicitly specify the UUID in the same way when creating the table (this is not recommended). For example: CREATE TABLE name UUID '28f1c61c-2970-457a-bffe-454156ddcfef' (n UInt64) ENGINE = ...;  note You can use the show_table_uuid_in_table_create_query_if_not_nil setting to display the UUID with the SHOW CREATE query. "},{"title":"RENAME TABLE​","type":1,"pageTitle":"Atomic","url":"en/engines/database-engines/atomic#rename-table","content":"RENAME queries are performed without changing the UUID or moving table data. These queries do not wait for the completion of queries using the table and are executed instantly. "},{"title":"DROP/DETACH TABLE​","type":1,"pageTitle":"Atomic","url":"en/engines/database-engines/atomic#drop-detach-table","content":"On DROP TABLE no data is removed, database Atomic just marks table as dropped by moving metadata to /clickhouse_path/metadata_dropped/ and notifies background thread. Delay before final table data deletion is specified by the database_atomic_delay_before_drop_table_sec setting. You can specify synchronous mode using SYNC modifier. Use the database_atomic_wait_for_drop_and_detach_synchronously setting to do this. In this case DROP waits for running SELECT, INSERT and other queries which are using the table to finish. Table will be actually removed when it's not in use. "},{"title":"EXCHANGE TABLES/DICTIONARIES​","type":1,"pageTitle":"Atomic","url":"en/engines/database-engines/atomic#exchange-tables","content":"EXCHANGE query swaps tables or dictionaries atomically. For instance, instead of this non-atomic operation: RENAME TABLE new_table TO tmp, old_table TO new_table, tmp TO old_table;  you can use one atomic query: EXCHANGE TABLES new_table AND old_table;  "},{"title":"ReplicatedMergeTree in Atomic Database​","type":1,"pageTitle":"Atomic","url":"en/engines/database-engines/atomic#replicatedmergetree-in-atomic-database","content":"For ReplicatedMergeTree tables, it is recommended not to specify engine parameters - path in ZooKeeper and replica name. In this case, configuration parameters default_replica_path and default_replica_name will be used. If you want to specify engine parameters explicitly, it is recommended to use {uuid} macros. This is useful so that unique paths are automatically generated for each table in ZooKeeper. "},{"title":"See Also​","type":1,"pageTitle":"Atomic","url":"en/engines/database-engines/atomic#see-also","content":"system.databases system table "},{"title":"How to Build ClickHouse on Mac OS X","type":0,"sectionRef":"#","url":"en/development/build-osx","content":"","keywords":""},{"title":"Install Homebrew​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"en/development/build-osx#install-homebrew","content":"First install Homebrew "},{"title":"For Apple's Clang (discouraged): Install Xcode and Command Line Tools​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"en/development/build-osx#install-xcode-and-command-line-tools","content":"Install the latest Xcode from App Store. Open it at least once to accept the end-user license agreement and automatically install the required components. Then, make sure that the latest Command Line Tools are installed and selected in the system: sudo rm -rf /Library/Developer/CommandLineTools sudo xcode-select --install  "},{"title":"Install Required Compilers, Tools, and Libraries​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"en/development/build-osx#install-required-compilers-tools-and-libraries","content":"brew update brew install cmake ninja libtool gettext llvm gcc binutils  "},{"title":"Checkout ClickHouse Sources​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"en/development/build-osx#checkout-clickhouse-sources","content":"git clone --recursive git@github.com:ClickHouse/ClickHouse.git # ...alternatively, you can use https://github.com/ClickHouse/ClickHouse.git as the repo URL.  "},{"title":"Build ClickHouse​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"en/development/build-osx#build-clickhouse","content":"To build using Homebrew's vanilla Clang compiler (the only recommended way): cd ClickHouse mkdir build export CC=$(brew --prefix llvm)/bin/clang export CXX=$(brew --prefix llvm)/bin/clang++ cmake -G Ninja -DCMAKE_BUILD_TYPE=RelWithDebInfo -S . -B build cmake --build build # The resulting binary will be created at: build/programs/clickhouse  To build using Xcode's native AppleClang compiler in Xcode IDE (this option is only for development builds and workflows, and is not recommended unless you know what you are doing): cd ClickHouse rm -rf build mkdir build cd build XCODE_IDE=1 ALLOW_APPLECLANG=1 cmake -G Xcode -DCMAKE_BUILD_TYPE=Debug -DENABLE_JEMALLOC=OFF .. cmake --open . # ...then, in Xcode IDE select ALL_BUILD scheme and start the building process. # The resulting binary will be created at: ./programs/Debug/clickhouse  To build using Homebrew's vanilla GCC compiler (this option is only for development experiments, and is absolutely not recommended unless you really know what you are doing): cd ClickHouse mkdir build export CC=$(brew --prefix gcc)/bin/gcc-11 export CXX=$(brew --prefix gcc)/bin/g++-11 cmake -G Ninja -DCMAKE_BUILD_TYPE=RelWithDebInfo -S . -B build cmake --build build # The resulting binary will be created at: build/programs/clickhouse  "},{"title":"Caveats​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"en/development/build-osx#caveats","content":"If you intend to run clickhouse-server, make sure to increase the system’s maxfiles variable. note You’ll need to use sudo. To do so, create the /Library/LaunchDaemons/limit.maxfiles.plist file with the following content: &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;!DOCTYPE plist PUBLIC &quot;-//Apple//DTD PLIST 1.0//EN&quot; &quot;http://www.apple.com/DTDs/PropertyList-1.0.dtd&quot;&gt; &lt;plist version=&quot;1.0&quot;&gt; &lt;dict&gt; &lt;key&gt;Label&lt;/key&gt; &lt;string&gt;limit.maxfiles&lt;/string&gt; &lt;key&gt;ProgramArguments&lt;/key&gt; &lt;array&gt; &lt;string&gt;launchctl&lt;/string&gt; &lt;string&gt;limit&lt;/string&gt; &lt;string&gt;maxfiles&lt;/string&gt; &lt;string&gt;524288&lt;/string&gt; &lt;string&gt;524288&lt;/string&gt; &lt;/array&gt; &lt;key&gt;RunAtLoad&lt;/key&gt; &lt;true/&gt; &lt;key&gt;ServiceIPC&lt;/key&gt; &lt;false/&gt; &lt;/dict&gt; &lt;/plist&gt;  Give the file correct permissions: sudo chown root:wheel /Library/LaunchDaemons/limit.maxfiles.plist  Validate that the file is correct: plutil /Library/LaunchDaemons/limit.maxfiles.plist  Load the file (or reboot): sudo launchctl load -w /Library/LaunchDaemons/limit.maxfiles.plist  To check if it’s working, use the ulimit -n or launchctl limit maxfiles commands. "},{"title":"Running ClickHouse server​","type":1,"pageTitle":"How to Build ClickHouse on Mac OS X","url":"en/development/build-osx#running-clickhouse-server","content":"cd ClickHouse ./build/programs/clickhouse-server --config-file ./programs/server/config.xml  Original article "},{"title":"Continuous Integration Checks","type":0,"sectionRef":"#","url":"en/development/continuous-integration","content":"","keywords":""},{"title":"Merge With Master​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#merge-with-master","content":"Verifies that the PR can be merged to master. If not, it will fail with the message 'Cannot fetch mergecommit'. To fix this check, resolve the conflict as described in the GitHub documentation, or merge the master branch to your pull request branch using git. "},{"title":"Docs check​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#docs-check","content":"Tries to build the ClickHouse documentation website. It can fail if you changed something in the documentation. Most probable reason is that some cross-link in the documentation is wrong. Go to the check report and look for ERROR and WARNING messages. "},{"title":"Report Details​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#report-details","content":"Status page exampledocs_output.txt contains the building log. Successful result example "},{"title":"Description Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#description-check","content":"Check that the description of your pull request conforms to the templatePULL_REQUEST_TEMPLATE.md. You have to specify a changelog category for your change (e.g., Bug Fix), and write a user-readable message describing the change for CHANGELOG.md "},{"title":"Push To Dockerhub​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#push-to-dockerhub","content":"Builds docker images used for build and tests, then pushes them to DockerHub. "},{"title":"Marker Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#marker-check","content":"This check means that the CI system started to process the pull request. When it has 'pending' status, it means that not all checks have been started yet. After all checks have been started, it changes status to 'success'. "},{"title":"Style Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#style-check","content":"Performs some simple regex-based checks of code style, using the utils/check-style/check-style binary (note that it can be run locally). If it fails, fix the style errors following the code style guide. "},{"title":"Report Details​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#report-details-1","content":"Status page exampleoutput.txt contains the check resulting errors (invalid tabulation etc), blank page means no errors. Successful result example. "},{"title":"Fast Test​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#fast-test","content":"Normally this is the first check that is ran for a PR. It builds ClickHouse and runs most of stateless functional tests, omitting some. If it fails, further checks are not started until it is fixed. Look at the report to see which tests fail, then reproduce the failure locally as described here. "},{"title":"Report Details​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#report-details-2","content":"Status page example Status Page Files​ runlog.out.log is the general log that includes all other logs.test_log.txtsubmodule_log.txt contains the messages about cloning and checkouting needed submodules.stderr.logstdout.logclickhouse-server.logclone_log.txtinstall_log.txtclickhouse-server.err.logbuild_log.txtcmake_log.txt contains messages about the C/C++ and Linux flags check. Status Page Columns​ Test name contains the name of the test (without the path e.g. all types of tests will be stripped to the name).Test status -- one of Skipped, Success, or Fail.Test time, sec. -- empty on this test. "},{"title":"Build Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#build-check","content":"Builds ClickHouse in various configurations for use in further steps. You have to fix the builds that fail. Build logs often has enough information to fix the error, but you might have to reproduce the failure locally. The cmake options can be found in the build log, grepping for cmake. Use these options and follow the general build process. "},{"title":"Report Details​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#report-details-3","content":"Status page example. Compiler: gcc-9 or clang-10 (or clang-10-xx for other architectures e.g. clang-10-freebsd).Build type: Debug or RelWithDebInfo (cmake).Sanitizer: none (without sanitizers), address (ASan), memory (MSan), undefined (UBSan), or thread (TSan).Splitted splitted is a split buildStatus: success or failBuild log: link to the building and files copying log, useful when build failed.Build time.Artifacts: build result files (with XXX being the server version e.g. 20.8.1.4344). clickhouse-client_XXX_all.debclickhouse-common-static-dbg_XXX[+asan, +msan, +ubsan, +tsan]_amd64.debclickhouse-common-staticXXX_amd64.debclickhouse-server_XXX_all.debclickhouse_XXX_amd64.buildinfoclickhouse_XXX_amd64.changesclickhouse: Main built binary.clickhouse-odbc-bridgeunit_tests_dbms: GoogleTest binary with ClickHouse unit tests.shared_build.tgz: build with shared libraries.performance.tgz: Special package for performance tests. "},{"title":"Special Build Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#special-build-check","content":"Performs static analysis and code style checks using clang-tidy. The report is similar to the build check. Fix the errors found in the build log. "},{"title":"Functional Stateless Tests​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#functional-stateless-tests","content":"Runs stateless functional tests for ClickHouse binaries built in various configurations -- release, debug, with sanitizers, etc. Look at the report to see which tests fail, then reproduce the failure locally as described here. Note that you have to use the correct build configuration to reproduce -- a test might fail under AddressSanitizer but pass in Debug. Download the binary from CI build checks page, or build it locally. "},{"title":"Functional Stateful Tests​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#functional-stateful-tests","content":"Runs stateful functional tests. Treat them in the same way as the functional stateless tests. The difference is that they require hits and visits tables from the clickstream dataset to run. "},{"title":"Integration Tests​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#integration-tests","content":"Runs integration tests. "},{"title":"Testflows Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#testflows-check","content":"Runs some tests using Testflows test system. See here how to run them locally. "},{"title":"Stress Test​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#stress-test","content":"Runs stateless functional tests concurrently from several clients to detect concurrency-related errors. If it fails: * Fix all other test failures first; * Look at the report to find the server logs and check them for possible causes of error.  "},{"title":"Split Build Smoke Test​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#split-build-smoke-test","content":"Checks that the server build in split buildconfiguration can start and run simple queries. If it fails: * Fix other test errors first; * Build the server in [split build](/docs/en/development/build#split-build) configuration locally and check whether it can start and run `select 1`.  "},{"title":"Compatibility Check​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#compatibility-check","content":"Checks that clickhouse binary runs on distributions with old libc versions. If it fails, ask a maintainer for help. "},{"title":"AST Fuzzer​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#ast-fuzzer","content":"Runs randomly generated queries to catch program errors. If it fails, ask a maintainer for help. "},{"title":"Performance Tests​","type":1,"pageTitle":"Continuous Integration Checks","url":"en/development/continuous-integration#performance-tests","content":"Measure changes in query performance. This is the longest check that takes just below 6 hours to run. The performance test report is described in detail here. "},{"title":"Overview of ClickHouse Architecture","type":0,"sectionRef":"#","url":"en/development/architecture","content":"","keywords":""},{"title":"Columns​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#columns","content":"IColumn interface is used to represent columns in memory (actually, chunks of columns). This interface provides helper methods for the implementation of various relational operators. Almost all operations are immutable: they do not modify the original column, but create a new modified one. For example, the IColumn :: filter method accepts a filter byte mask. It is used for the WHERE and HAVING relational operators. Additional examples: the IColumn :: permute method to support ORDER BY, the IColumn :: cut method to support LIMIT. Various IColumn implementations (ColumnUInt8, ColumnString, and so on) are responsible for the memory layout of columns. The memory layout is usually a contiguous array. For the integer type of columns, it is just one contiguous array, like std :: vector. For String and Array columns, it is two vectors: one for all array elements, placed contiguously, and a second one for offsets to the beginning of each array. There is also ColumnConst that stores just one value in memory, but looks like a column. "},{"title":"Field​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#field","content":"Nevertheless, it is possible to work with individual values as well. To represent an individual value, the Field is used. Field is just a discriminated union of UInt64, Int64, Float64, String and Array. IColumn has the operator [] method to get the n-th value as a Field, and the insert method to append a Field to the end of a column. These methods are not very efficient, because they require dealing with temporary Field objects representing an individual value. There are more efficient methods, such as insertFrom, insertRangeFrom, and so on. Field does not have enough information about a specific data type for a table. For example, UInt8, UInt16, UInt32, and UInt64 are all represented as UInt64 in a Field. "},{"title":"Leaky Abstractions​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#leaky-abstractions","content":"IColumn has methods for common relational transformations of data, but they do not meet all needs. For example, ColumnUInt64 does not have a method to calculate the sum of two columns, and ColumnString does not have a method to run a substring search. These countless routines are implemented outside of IColumn. Various functions on columns can be implemented in a generic, non-efficient way using IColumn methods to extract Field values, or in a specialized way using knowledge of inner memory layout of data in a specific IColumn implementation. It is implemented by casting functions to a specific IColumn type and deal with internal representation directly. For example, ColumnUInt64 has the getData method that returns a reference to an internal array, then a separate routine reads or fills that array directly. We have “leaky abstractions” to allow efficient specializations of various routines. "},{"title":"Data Types​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#data_types","content":"IDataType is responsible for serialization and deserialization: for reading and writing chunks of columns or individual values in binary or text form. IDataType directly corresponds to data types in tables. For example, there are DataTypeUInt32, DataTypeDateTime, DataTypeString and so on. IDataType and IColumn are only loosely related to each other. Different data types can be represented in memory by the same IColumn implementations. For example, DataTypeUInt32 and DataTypeDateTime are both represented by ColumnUInt32 or ColumnConstUInt32. In addition, the same data type can be represented by different IColumn implementations. For example, DataTypeUInt8 can be represented by ColumnUInt8 or ColumnConstUInt8. IDataType only stores metadata. For instance, DataTypeUInt8 does not store anything at all (except virtual pointer vptr) and DataTypeFixedString stores just N (the size of fixed-size strings). IDataType has helper methods for various data formats. Examples are methods to serialize a value with possible quoting, to serialize a value for JSON, and to serialize a value as part of the XML format. There is no direct correspondence to data formats. For example, the different data formats Pretty and TabSeparated can use the same serializeTextEscaped helper method from the IDataType interface. "},{"title":"Block​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#block","content":"A Block is a container that represents a subset (chunk) of a table in memory. It is just a set of triples: (IColumn, IDataType, column name). During query execution, data is processed by Blocks. If we have a Block, we have data (in the IColumn object), we have information about its type (in IDataType) that tells us how to deal with that column, and we have the column name. It could be either the original column name from the table or some artificial name assigned for getting temporary results of calculations. When we calculate some function over columns in a block, we add another column with its result to the block, and we do not touch columns for arguments of the function because operations are immutable. Later, unneeded columns can be removed from the block, but not modified. It is convenient for the elimination of common subexpressions. Blocks are created for every processed chunk of data. Note that for the same type of calculation, the column names and types remain the same for different blocks, and only column data changes. It is better to split block data from the block header because small block sizes have a high overhead of temporary strings for copying shared_ptrs and column names. "},{"title":"Block Streams​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#block-streams","content":"Block streams are for processing data. We use streams of blocks to read data from somewhere, perform data transformations, or write data to somewhere. IBlockInputStream has the read method to fetch the next block while available. IBlockOutputStream has the write method to push the block somewhere. Streams are responsible for: Reading or writing to a table. The table just returns a stream for reading or writing blocks.Implementing data formats. For example, if you want to output data to a terminal in Pretty format, you create a block output stream where you push blocks, and it formats them.Performing data transformations. Let’s say you have IBlockInputStream and want to create a filtered stream. You create FilterBlockInputStream and initialize it with your stream. Then when you pull a block from FilterBlockInputStream, it pulls a block from your stream, filters it, and returns the filtered block to you. Query execution pipelines are represented this way. There are more sophisticated transformations. For example, when you pull from AggregatingBlockInputStream, it reads all data from its source, aggregates it, and then returns a stream of aggregated data for you. Another example: UnionBlockInputStream accepts many input sources in the constructor and also a number of threads. It launches multiple threads and reads from multiple sources in parallel. Block streams use the “pull” approach to control flow: when you pull a block from the first stream, it consequently pulls the required blocks from nested streams, and the entire execution pipeline will work. Neither “pull” nor “push” is the best solution, because control flow is implicit, and that limits the implementation of various features like simultaneous execution of multiple queries (merging many pipelines together). This limitation could be overcome with coroutines or just running extra threads that wait for each other. We may have more possibilities if we make control flow explicit: if we locate the logic for passing data from one calculation unit to another outside of those calculation units. Read this article for more thoughts. We should note that the query execution pipeline creates temporary data at each step. We try to keep block size small enough so that temporary data fits in the CPU cache. With that assumption, writing and reading temporary data is almost free in comparison with other calculations. We could consider an alternative, which is to fuse many operations in the pipeline together. It could make the pipeline as short as possible and remove much of the temporary data, which could be an advantage, but it also has drawbacks. For example, a split pipeline makes it easy to implement caching intermediate data, stealing intermediate data from similar queries running at the same time, and merging pipelines for similar queries. "},{"title":"Formats​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#formats","content":"Data formats are implemented with block streams. There are “presentational” formats only suitable for the output of data to the client, such as Pretty format, which provides only IBlockOutputStream. And there are input/output formats, such as TabSeparated or JSONEachRow. There are also row streams: IRowInputStream and IRowOutputStream. They allow you to pull/push data by individual rows, not by blocks. And they are only needed to simplify the implementation of row-oriented formats. The wrappers BlockInputStreamFromRowInputStream and BlockOutputStreamFromRowOutputStream allow you to convert row-oriented streams to regular block-oriented streams. "},{"title":"I/O​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#io","content":"For byte-oriented input/output, there are ReadBuffer and WriteBuffer abstract classes. They are used instead of C++ iostreams. Don’t worry: every mature C++ project is using something other than iostreams for good reasons. ReadBuffer and WriteBuffer are just a contiguous buffer and a cursor pointing to the position in that buffer. Implementations may own or not own the memory for the buffer. There is a virtual method to fill the buffer with the following data (for ReadBuffer) or to flush the buffer somewhere (for WriteBuffer). The virtual methods are rarely called. Implementations of ReadBuffer/WriteBuffer are used for working with files and file descriptors and network sockets, for implementing compression (CompressedWriteBuffer is initialized with another WriteBuffer and performs compression before writing data to it), and for other purposes – the names ConcatReadBuffer, LimitReadBuffer, and HashingWriteBuffer speak for themselves. Read/WriteBuffers only deal with bytes. There are functions from ReadHelpers and WriteHelpers header files to help with formatting input/output. For example, there are helpers to write a number in decimal format. Let’s look at what happens when you want to write a result set in JSON format to stdout. You have a result set ready to be fetched from IBlockInputStream. You create WriteBufferFromFileDescriptor(STDOUT_FILENO) to write bytes to stdout. You create JSONRowOutputStream, initialized with that WriteBuffer, to write rows in JSON to stdout. You create BlockOutputStreamFromRowOutputStream on top of it, to represent it as IBlockOutputStream. Then you call copyData to transfer data from IBlockInputStream to IBlockOutputStream, and everything works. Internally, JSONRowOutputStream will write various JSON delimiters and call the IDataType::serializeTextJSON method with a reference to IColumn and the row number as arguments. Consequently, IDataType::serializeTextJSON will call a method from WriteHelpers.h: for example, writeText for numeric types and writeJSONString for DataTypeString. "},{"title":"Tables​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#tables","content":"The IStorage interface represents tables. Different implementations of that interface are different table engines. Examples are StorageMergeTree, StorageMemory, and so on. Instances of these classes are just tables. The key IStorage methods are read and write. There are also alter, rename, drop, and so on. The read method accepts the following arguments: the set of columns to read from a table, the AST query to consider, and the desired number of streams to return. It returns one or multiple IBlockInputStream objects and information about the stage of data processing that was completed inside a table engine during query execution. In most cases, the read method is only responsible for reading the specified columns from a table, not for any further data processing. All further data processing is done by the query interpreter and is outside the responsibility of IStorage. But there are notable exceptions: The AST query is passed to the read method, and the table engine can use it to derive index usage and to read fewer data from a table.Sometimes the table engine can process data itself to a specific stage. For example, StorageDistributed can send a query to remote servers, ask them to process data to a stage where data from different remote servers can be merged, and return that preprocessed data. The query interpreter then finishes processing the data. The table’s read method can return multiple IBlockInputStream objects to allow parallel data processing. These multiple block input streams can read from a table in parallel. Then you can wrap these streams with various transformations (such as expression evaluation or filtering) that can be calculated independently and create a UnionBlockInputStream on top of them, to read from multiple streams in parallel. There are also TableFunctions. These are functions that return a temporary IStorage object to use in the FROM clause of a query. To get a quick idea of how to implement your table engine, look at something simple, like StorageMemory or StorageTinyLog. As the result of the read method, IStorage returns QueryProcessingStage – information about what parts of the query were already calculated inside storage. "},{"title":"Parsers​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#parsers","content":"A hand-written recursive descent parser parses a query. For example, ParserSelectQuery just recursively calls the underlying parsers for various parts of the query. Parsers create an AST. The AST is represented by nodes, which are instances of IAST. Parser generators are not used for historical reasons. "},{"title":"Interpreters​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#interpreters","content":"Interpreters are responsible for creating the query execution pipeline from an AST. There are simple interpreters, such as InterpreterExistsQuery and InterpreterDropQuery, or the more sophisticated InterpreterSelectQuery. The query execution pipeline is a combination of block input or output streams. For example, the result of interpreting the SELECT query is the IBlockInputStream to read the result set from; the result of the INSERT query is the IBlockOutputStream to write data for insertion to, and the result of interpreting the INSERT SELECT query is the IBlockInputStream that returns an empty result set on the first read, but that copies data from SELECT to INSERT at the same time. InterpreterSelectQuery uses ExpressionAnalyzer and ExpressionActions machinery for query analysis and transformations. This is where most rule-based query optimizations are done. ExpressionAnalyzer is quite messy and should be rewritten: various query transformations and optimizations should be extracted to separate classes to allow modular transformations of query. "},{"title":"Functions​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#functions","content":"There are ordinary functions and aggregate functions. For aggregate functions, see the next section. Ordinary functions do not change the number of rows – they work as if they are processing each row independently. In fact, functions are not called for individual rows, but for Block’s of data to implement vectorized query execution. There are some miscellaneous functions, like blockSize, rowNumberInBlock, and runningAccumulate, that exploit block processing and violate the independence of rows. ClickHouse has strong typing, so there’s no implicit type conversion. If a function does not support a specific combination of types, it throws an exception. But functions can work (be overloaded) for many different combinations of types. For example, the plus function (to implement the + operator) works for any combination of numeric types: UInt8 + Float32, UInt16 + Int8, and so on. Also, some variadic functions can accept any number of arguments, such as the concat function. Implementing a function may be slightly inconvenient because a function explicitly dispatches supported data types and supported IColumns. For example, the plus function has code generated by instantiation of a C++ template for each combination of numeric types, and constant or non-constant left and right arguments. It is an excellent place to implement runtime code generation to avoid template code bloat. Also, it makes it possible to add fused functions like fused multiply-add or to make multiple comparisons in one loop iteration. Due to vectorized query execution, functions are not short-circuited. For example, if you write WHERE f(x) AND g(y), both sides are calculated, even for rows, when f(x) is zero (except when f(x) is a zero constant expression). But if the selectivity of the f(x) condition is high, and calculation of f(x) is much cheaper than g(y), it’s better to implement multi-pass calculation. It would first calculate f(x), then filter columns by the result, and then calculate g(y) only for smaller, filtered chunks of data. "},{"title":"Aggregate Functions​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#aggregate-functions","content":"Aggregate functions are stateful functions. They accumulate passed values into some state and allow you to get results from that state. They are managed with the IAggregateFunction interface. States can be rather simple (the state for AggregateFunctionCount is just a single UInt64 value) or quite complex (the state of AggregateFunctionUniqCombined is a combination of a linear array, a hash table, and a HyperLogLog probabilistic data structure). States are allocated in Arena (a memory pool) to deal with multiple states while executing a high-cardinality GROUP BY query. States can have a non-trivial constructor and destructor: for example, complicated aggregation states can allocate additional memory themselves. It requires some attention to creating and destroying states and properly passing their ownership and destruction order. Aggregation states can be serialized and deserialized to pass over the network during distributed query execution or to write them on the disk where there is not enough RAM. They can even be stored in a table with the DataTypeAggregateFunction to allow incremental aggregation of data. The serialized data format for aggregate function states is not versioned right now. It is ok if aggregate states are only stored temporarily. But we have the AggregatingMergeTree table engine for incremental aggregation, and people are already using it in production. It is the reason why backward compatibility is required when changing the serialized format for any aggregate function in the future. "},{"title":"Server​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#server","content":"The server implements several different interfaces: An HTTP interface for any foreign clients.A TCP interface for the native ClickHouse client and for cross-server communication during distributed query execution.An interface for transferring data for replication. Internally, it is just a primitive multithreaded server without coroutines or fibers. Since the server is not designed to process a high rate of simple queries but to process a relatively low rate of complex queries, each of them can process a vast amount of data for analytics. The server initializes the Context class with the necessary environment for query execution: the list of available databases, users and access rights, settings, clusters, the process list, the query log, and so on. Interpreters use this environment. We maintain full backward and forward compatibility for the server TCP protocol: old clients can talk to new servers, and new clients can talk to old servers. But we do not want to maintain it eternally, and we are removing support for old versions after about one year. note For most external applications, we recommend using the HTTP interface because it is simple and easy to use. The TCP protocol is more tightly linked to internal data structures: it uses an internal format for passing blocks of data, and it uses custom framing for compressed data. We haven’t released a C library for that protocol because it requires linking most of the ClickHouse codebase, which is not practical. "},{"title":"Configuration​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#configuration","content":"ClickHouse Server is based on POCO C++ Libraries and uses Poco::Util::AbstractConfiguration to represent it's configuration. Configuration is held by Poco::Util::ServerApplication class inherited by DaemonBase class, which in turn is inherited by DB::Server class, implementing clickhouse-server itself. So config can be accessed by ServerApplication::config() method. Config is read from multiple files (in XML or YAML format) and merged into single AbstractConfiguration by ConfigProcessor class. Configuration is loaded at server startup and can be reloaded later if one of config files is updated, removed or added. ConfigReloader class is responsible for periodic monitoring of these changes and reload procedure as well. SYSTEM RELOAD CONFIG query also triggers config to be reloaded. For queries and subsystems other than Server config is accessible using Context::getConfigRef() method. Every subsystem that is capable of reloading it's config without server restart should register itself in reload callback in Server::main() method. Note that if newer config has an error, most subsystems will ignore new config, log warning messages and keep working with previously loaded config. Due to the nature of AbstractConfiguration it is not possible to pass reference to specific section, so String config_prefix is usually used instead. "},{"title":"Threads and jobs​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#threads-and-jobs","content":"To execute queries and do side activities ClickHouse allocates threads from one of thread pools to avoid frequent thread creation and destruction. There are a few thread pools, which are selected depending on a purpose and structure of a job: Server pool for incoming client sessions.Global thread pool for general purpose jobs, background activities and standalone threads.IO thread pool for jobs that are mostly blocked on some IO and are not CPU-intensive.Background pools for periodic tasks.Pools for preemptable tasks that can be split into steps. Server pool is a Poco::ThreadPool class instance defined in Server::main() method. It can have at most max_connection threads. Every thread is dedicated to a single active connection. Global thread pool is GlobalThreadPool singleton class. To allocate thread from it ThreadFromGlobalPool is used. It has an interface similar to std::thread, but pulls thread from the global pool and does all necessary initializations. It is configured with the following settings: max_thread_pool_size - limit on thread count in pool.max_thread_pool_free_size - limit on idle thread count waiting for new jobs.thread_pool_queue_size - limit on scheduled job count. Global pool is universal and all pools described below are implemented on top of it. This can be thought of as a hierarchy of pools. Any specialized pool takes its threads from the global pool using ThreadPool class. So the main purpose of any specialized pool is to apply limit on the number of simultaneous jobs and do job scheduling. If there are more jobs scheduled than threads in a pool, ThreadPool accumulates jobs in a queue with priorities. Each job has an integer priority. Default priority is zero. All jobs with higher priority values are started before any job with lower priority value. But there is no difference between already executing jobs, thus priority matters only when the pool in overloaded. IO thread pool is implemented as a plain ThreadPool accessible via IOThreadPool::get() method. It is configured in the same way as global pool with max_io_thread_pool_size, max_io_thread_pool_free_size and io_thread_pool_queue_size settings. The main purpose of IO thread pool is to avoid exhaustion of the global pool with IO jobs, which could prevent queries from fully utilizing CPU. For periodic task execution there is BackgroundSchedulePool class. You can register tasks using BackgroundSchedulePool::TaskHolder objects and the pool ensures that no task runs two jobs at the same time. It also allows you to postpone task execution to a specific instant in the future or temporarily deactivate task. Global Context provides a few instances of this class for different purposes. For general purpose tasks Context::getSchedulePool() is used. There are also specialized thread pools for preemptable tasks. Such IExecutableTask task can be split into ordered sequence of jobs, called steps. To schedule these tasks in a manner allowing short tasks to be prioritied over long ones MergeTreeBackgroundExecutor is used. As name suggests it is used for background MergeTree related operations such as merges, mutations, fetches and moves. Pool instances are available using Context::getCommonExecutor() and other similar methods. No matter what pool is used for a job, at start ThreadStatus instance is created for this job. It encapsulates all per-thread information: thread id, query id, performance counters, resource consumption and many other useful data. Job can access it via thread local pointer by CurrentThread::get() call, so we do not need to pass it to every function. If thread is related to query execution, then the most important thing attached to ThreadStatus is query context ContextPtr. Every query has its master thread in the server pool. Master thread does the attachment by holding an ThreadStatus::QueryScope query_scope(query_context) object. Master thread also creates a thread group represented with ThreadGroupStatus object. Every additional thread that is allocated during this query execution is attached to its thread group by CurrentThread::attachTo(thread_group) call. Thread groups are used to aggregate profile event counters and track memory consumption by all threads dedicated to a single task (see MemoryTracker and ProfileEvents::Counters classes for more information). "},{"title":"Distributed Query Execution​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#distributed-query-execution","content":"Servers in a cluster setup are mostly independent. You can create a Distributed table on one or all servers in a cluster. The Distributed table does not store data itself – it only provides a “view” to all local tables on multiple nodes of a cluster. When you SELECT from a Distributed table, it rewrites that query, chooses remote nodes according to load balancing settings, and sends the query to them. The Distributed table requests remote servers to process a query just up to a stage where intermediate results from different servers can be merged. Then it receives the intermediate results and merges them. The distributed table tries to distribute as much work as possible to remote servers and does not send much intermediate data over the network. Things become more complicated when you have subqueries in IN or JOIN clauses, and each of them uses a Distributed table. We have different strategies for the execution of these queries. There is no global query plan for distributed query execution. Each node has its local query plan for its part of the job. We only have simple one-pass distributed query execution: we send queries for remote nodes and then merge the results. But this is not feasible for complicated queries with high cardinality GROUP BYs or with a large amount of temporary data for JOIN. In such cases, we need to “reshuffle” data between servers, which requires additional coordination. ClickHouse does not support that kind of query execution, and we need to work on it. "},{"title":"Merge Tree​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#merge-tree","content":"MergeTree is a family of storage engines that supports indexing by primary key. The primary key can be an arbitrary tuple of columns or expressions. Data in a MergeTree table is stored in “parts”. Each part stores data in the primary key order, so data is ordered lexicographically by the primary key tuple. All the table columns are stored in separate column.bin files in these parts. The files consist of compressed blocks. Each block is usually from 64 KB to 1 MB of uncompressed data, depending on the average value size. The blocks consist of column values placed contiguously one after the other. Column values are in the same order for each column (the primary key defines the order), so when you iterate by many columns, you get values for the corresponding rows. The primary key itself is “sparse”. It does not address every single row, but only some ranges of data. A separate primary.idx file has the value of the primary key for each N-th row, where N is called index_granularity (usually, N = 8192). Also, for each column, we have column.mrk files with “marks”, which are offsets to each N-th row in the data file. Each mark is a pair: the offset in the file to the beginning of the compressed block, and the offset in the decompressed block to the beginning of data. Usually, compressed blocks are aligned by marks, and the offset in the decompressed block is zero. Data for primary.idx always resides in memory, and data for column.mrk files is cached. When we are going to read something from a part in MergeTree, we look at primary.idx data and locate ranges that could contain requested data, then look at column.mrk data and calculate offsets for where to start reading those ranges. Because of sparseness, excess data may be read. ClickHouse is not suitable for a high load of simple point queries, because the entire range with index_granularity rows must be read for each key, and the entire compressed block must be decompressed for each column. We made the index sparse because we must be able to maintain trillions of rows per single server without noticeable memory consumption for the index. Also, because the primary key is sparse, it is not unique: it cannot check the existence of the key in the table at INSERT time. You could have many rows with the same key in a table. When you INSERT a bunch of data into MergeTree, that bunch is sorted by primary key order and forms a new part. There are background threads that periodically select some parts and merge them into a single sorted part to keep the number of parts relatively low. That’s why it is called MergeTree. Of course, merging leads to “write amplification”. All parts are immutable: they are only created and deleted, but not modified. When SELECT is executed, it holds a snapshot of the table (a set of parts). After merging, we also keep old parts for some time to make a recovery after failure easier, so if we see that some merged part is probably broken, we can replace it with its source parts. MergeTree is not an LSM tree because it does not contain MEMTABLE and LOG: inserted data is written directly to the filesystem. This behavior makes MergeTree much more suitable to insert data in batches. Therefore frequently inserting small amounts of rows is not ideal for MergeTree. For example, a couple of rows per second is OK, but doing it a thousand times a second is not optimal for MergeTree. However, there is an async insert mode for small inserts to overcome this limitation. We did it this way for simplicity’s sake, and because we are already inserting data in batches in our applications There are MergeTree engines that are doing additional work during background merges. Examples are CollapsingMergeTree and AggregatingMergeTree. This could be treated as special support for updates. Keep in mind that these are not real updates because users usually have no control over the time when background merges are executed, and data in a MergeTree table is almost always stored in more than one part, not in completely merged form. "},{"title":"Replication​","type":1,"pageTitle":"Overview of ClickHouse Architecture","url":"en/development/architecture#replication","content":"Replication in ClickHouse can be configured on a per-table basis. You could have some replicated and some non-replicated tables on the same server. You could also have tables replicated in different ways, such as one table with two-factor replication and another with three-factor. Replication is implemented in the ReplicatedMergeTree storage engine. The path in ZooKeeper is specified as a parameter for the storage engine. All tables with the same path in ZooKeeper become replicas of each other: they synchronize their data and maintain consistency. Replicas can be added and removed dynamically simply by creating or dropping a table. Replication uses an asynchronous multi-master scheme. You can insert data into any replica that has a session with ZooKeeper, and data is replicated to all other replicas asynchronously. Because ClickHouse does not support UPDATEs, replication is conflict-free. As there is no quorum acknowledgment of inserts, just-inserted data might be lost if one node fails. Metadata for replication is stored in ZooKeeper. There is a replication log that lists what actions to do. Actions are: get part; merge parts; drop a partition, and so on. Each replica copies the replication log to its queue and then executes the actions from the queue. For example, on insertion, the “get the part” action is created in the log, and every replica downloads that part. Merges are coordinated between replicas to get byte-identical results. All parts are merged in the same way on all replicas. One of the leaders initiates a new merge first and writes “merge parts” actions to the log. Multiple replicas (or all) can be leaders at the same time. A replica can be prevented from becoming a leader using the merge_tree setting replicated_can_become_leader. The leaders are responsible for scheduling background merges. Replication is physical: only compressed parts are transferred between nodes, not queries. Merges are processed on each replica independently in most cases to lower the network costs by avoiding network amplification. Large merged parts are sent over the network only in cases of significant replication lag. Besides, each replica stores its state in ZooKeeper as the set of parts and its checksums. When the state on the local filesystem diverges from the reference state in ZooKeeper, the replica restores its consistency by downloading missing and broken parts from other replicas. When there is some unexpected or broken data in the local filesystem, ClickHouse does not remove it, but moves it to a separate directory and forgets it. note The ClickHouse cluster consists of independent shards, and each shard consists of replicas. The cluster is not elastic, so after adding a new shard, data is not rebalanced between shards automatically. Instead, the cluster load is supposed to be adjusted to be uneven. This implementation gives you more control, and it is ok for relatively small clusters, such as tens of nodes. But for clusters with hundreds of nodes that we are using in production, this approach becomes a significant drawback. We should implement a table engine that spans across the cluster with dynamically replicated regions that could be split and balanced between clusters automatically. Original article "},{"title":"MySQL","type":0,"sectionRef":"#","url":"en/engines/database-engines/mysql","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"MySQL","url":"en/engines/database-engines/mysql#creating-a-database","content":"CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster] ENGINE = MySQL('host:port', ['database' | database], 'user', 'password')  Engine Parameters host:port — MySQL server address.database — Remote database name.user — MySQL user.password — User password. "},{"title":"Data Types Support​","type":1,"pageTitle":"MySQL","url":"en/engines/database-engines/mysql#data_types-support","content":"MySQL\tClickHouseUNSIGNED TINYINT\tUInt8 TINYINT\tInt8 UNSIGNED SMALLINT\tUInt16 SMALLINT\tInt16 UNSIGNED INT, UNSIGNED MEDIUMINT\tUInt32 INT, MEDIUMINT\tInt32 UNSIGNED BIGINT\tUInt64 BIGINT\tInt64 FLOAT\tFloat32 DOUBLE\tFloat64 DATE\tDate DATETIME, TIMESTAMP\tDateTime BINARY\tFixedString All other MySQL data types are converted into String. Nullable is supported. "},{"title":"Global Variables Support​","type":1,"pageTitle":"MySQL","url":"en/engines/database-engines/mysql#global-variables-support","content":"For better compatibility you may address global variables in MySQL style, as @@identifier. These variables are supported: versionmax_allowed_packet warning By now these variables are stubs and don't correspond to anything. Example: SELECT @@version;  "},{"title":"Examples of Use​","type":1,"pageTitle":"MySQL","url":"en/engines/database-engines/mysql#examples-of-use","content":"Table in MySQL: mysql&gt; USE test; Database changed mysql&gt; CREATE TABLE `mysql_table` ( -&gt; `int_id` INT NOT NULL AUTO_INCREMENT, -&gt; `float` FLOAT NOT NULL, -&gt; PRIMARY KEY (`int_id`)); Query OK, 0 rows affected (0,09 sec) mysql&gt; insert into mysql_table (`int_id`, `float`) VALUES (1,2); Query OK, 1 row affected (0,00 sec) mysql&gt; select * from mysql_table; +------+-----+ | int_id | value | +------+-----+ | 1 | 2 | +------+-----+ 1 row in set (0,00 sec)  Database in ClickHouse, exchanging data with the MySQL server: CREATE DATABASE mysql_db ENGINE = MySQL('localhost:3306', 'test', 'my_user', 'user_password')  SHOW DATABASES  ┌─name─────┐ │ default │ │ mysql_db │ │ system │ └──────────┘  SHOW TABLES FROM mysql_db  ┌─name─────────┐ │ mysql_table │ └──────────────┘  SELECT * FROM mysql_db.mysql_table  ┌─int_id─┬─value─┐ │ 1 │ 2 │ └────────┴───────┘  INSERT INTO mysql_db.mysql_table VALUES (3,4)  SELECT * FROM mysql_db.mysql_table  ┌─int_id─┬─value─┐ │ 1 │ 2 │ │ 3 │ 4 │ └────────┴───────┘  Original article "},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"en/engines/database-engines/postgresql","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"PostgreSQL","url":"en/engines/database-engines/postgresql#creating-a-database","content":"CREATE DATABASE test_database ENGINE = PostgreSQL('host:port', 'database', 'user', 'password'[, `schema`, `use_table_cache`]);  Engine Parameters host:port — PostgreSQL server address.database — Remote database name.user — PostgreSQL user.password — User password.schema — PostgreSQL schema.use_table_cache — Defines if the database table structure is cached or not. Optional. Default value: 0. "},{"title":"Data Types Support​","type":1,"pageTitle":"PostgreSQL","url":"en/engines/database-engines/postgresql#data_types-support","content":"PostgerSQL\tClickHouseDATE\tDate TIMESTAMP\tDateTime REAL\tFloat32 DOUBLE\tFloat64 DECIMAL, NUMERIC\tDecimal SMALLINT\tInt16 INTEGER\tInt32 BIGINT\tInt64 SERIAL\tUInt32 BIGSERIAL\tUInt64 TEXT, CHAR\tString INTEGER\tNullable(Int32) ARRAY\tArray "},{"title":"Examples of Use​","type":1,"pageTitle":"PostgreSQL","url":"en/engines/database-engines/postgresql#examples-of-use","content":"Database in ClickHouse, exchanging data with the PostgreSQL server: CREATE DATABASE test_database ENGINE = PostgreSQL('postgres1:5432', 'test_database', 'postgres', 'mysecretpassword', 1);  SHOW DATABASES;  ┌─name──────────┐ │ default │ │ test_database │ │ system │ └───────────────┘  SHOW TABLES FROM test_database;  ┌─name───────┐ │ test_table │ └────────────┘  Reading data from the PostgreSQL table: SELECT * FROM test_database.test_table;  ┌─id─┬─value─┐ │ 1 │ 2 │ └────┴───────┘  Writing data to the PostgreSQL table: INSERT INTO test_database.test_table VALUES (3,4); SELECT * FROM test_database.test_table;  ┌─int_id─┬─value─┐ │ 1 │ 2 │ │ 3 │ 4 │ └────────┴───────┘  Consider the table structure was modified in PostgreSQL: postgre&gt; ALTER TABLE test_table ADD COLUMN data Text  As the use_table_cache parameter was set to 1 when the database was created, the table structure in ClickHouse was cached and therefore not modified: DESCRIBE TABLE test_database.test_table;  ┌─name───┬─type──────────────┐ │ id │ Nullable(Integer) │ │ value │ Nullable(Integer) │ └────────┴───────────────────┘  After detaching the table and attaching it again, the structure was updated: DETACH TABLE test_database.test_table; ATTACH TABLE test_database.test_table; DESCRIBE TABLE test_database.test_table;  ┌─name───┬─type──────────────┐ │ id │ Nullable(Integer) │ │ value │ Nullable(Integer) │ │ data │ Nullable(String) │ └────────┴───────────────────┘  Original article "},{"title":"Table Engines for Integrations","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/","content":"Table Engines for Integrations ClickHouse provides various means for integrating with external systems, including table engines. Like with all other table engines, the configuration is done using CREATE TABLE or ALTER TABLE queries. Then from a user perspective, the configured integration looks like a normal table, but queries to it are proxied to the external system. This transparent querying is one of the key advantages of this approach over alternative integration methods, like external dictionaries or table functions, which require to use custom query methods on each use. List of supported integrations: ODBCJDBCMySQLMongoDBHDFSS3KafkaEmbeddedRocksDBRabbitMQPostgreSQLSQLiteHive","keywords":""},{"title":"Third-Party Libraries Used","type":0,"sectionRef":"#","url":"en/development/contrib","content":"","keywords":""},{"title":"Guidelines for adding new third-party libraries and maintaining custom changes in them​","type":1,"pageTitle":"Third-Party Libraries Used","url":"en/development/contrib#adding-third-party-libraries","content":"All external third-party code should reside in the dedicated directories under contrib directory of ClickHouse repo. Prefer Git submodules, when available.Fork/mirror the official repo in Clickhouse-extras. Prefer official GitHub repos, when available.Branch from the branch you want to integrate, e.g., master -&gt; clickhouse/master, or release/vX.Y.Z -&gt; clickhouse/release/vX.Y.Z.All forks in Clickhouse-extras can be automatically synchronized with upstreams. clickhouse/... branches will remain unaffected, since virtually nobody is going to use that naming pattern in their upstream repos.Add submodules under contrib of ClickHouse repo that refer the above forks/mirrors. Set the submodules to track the corresponding clickhouse/... branches.Every time the custom changes have to be made in the library code, a dedicated branch should be created, like clickhouse/my-fix. Then this branch should be merged into the branch, that is tracked by the submodule, e.g., clickhouse/master or clickhouse/release/vX.Y.Z.No code should be pushed in any branch of the forks in Clickhouse-extras, whose names do not follow clickhouse/... pattern.Always write the custom changes with the official repo in mind. Once the PR is merged from (a feature/fix branch in) your personal fork into the fork in Clickhouse-extras, and the submodule is bumped in ClickHouse repo, consider opening another PR from (a feature/fix branch in) the fork in Clickhouse-extras to the official repo of the library. This will make sure, that 1) the contribution has more than a single use case and importance, 2) others will also benefit from it, 3) the change will not remain a maintenance burden solely on ClickHouse developers.When a submodule needs to start using a newer code from the original branch (e.g., master), and since the custom changes might be merged in the branch it is tracking (e.g., clickhouse/master) and so it may diverge from its original counterpart (i.e., master), a careful merge should be carried out first, i.e., master -&gt; clickhouse/master, and only then the submodule can be bumped in ClickHouse. "},{"title":"CMake in ClickHouse","type":0,"sectionRef":"#","url":"en/development/cmake-in-clickhouse","content":"","keywords":""},{"title":"CMake files types​","type":1,"pageTitle":"CMake in ClickHouse","url":"en/development/cmake-in-clickhouse#cmake-files-types","content":"ClickHouse's source CMake files (located in the root directory and in /src).Arch-dependent CMake files (located in /cmake/os_name).Libraries finders (search for contrib libraries, located in /contrib/*/CMakeLists.txt).Contrib build CMake files (used instead of libraries' own CMake files, located in /cmake/modules) "},{"title":"List of CMake flags​","type":1,"pageTitle":"CMake in ClickHouse","url":"en/development/cmake-in-clickhouse#list-of-cmake-flags","content":"The flag name is a link to its position in the code.If an option's default value is itself an option, it's also a link to its position in this list. "},{"title":"ClickHouse modes​","type":1,"pageTitle":"CMake in ClickHouse","url":"en/development/cmake-in-clickhouse#clickhouse-modes","content":"Name\tDefault value\tDescription\tCommentENABLE_CLICKHOUSE_ALL\tON\tEnable all ClickHouse modes by default\tThe clickhouse binary is a multi purpose tool that contains multiple execution modes (client, server, etc.), each of them may be built and linked as a separate library. If you do not know what modes you need, turn this option OFF and enable SERVER and CLIENT only. ENABLE_CLICKHOUSE_BENCHMARK\tENABLE_CLICKHOUSE_ALL\tQueries benchmarking mode\thttps://clickhouse.com/docs/en/operations/utilities/clickhouse-benchmark/ ENABLE_CLICKHOUSE_CLIENT\tENABLE_CLICKHOUSE_ALL\tClient mode (interactive tui/shell that connects to the server) ENABLE_CLICKHOUSE_COMPRESSOR\tENABLE_CLICKHOUSE_ALL\tData compressor and decompressor\thttps://clickhouse.com/docs/en/operations/utilities/clickhouse-compressor/ ENABLE_CLICKHOUSE_COPIER\tENABLE_CLICKHOUSE_ALL\tInter-cluster data copying mode\thttps://clickhouse.com/docs/en/operations/utilities/clickhouse-copier/ ENABLE_CLICKHOUSE_EXTRACT_FROM_CONFIG\tENABLE_CLICKHOUSE_ALL\tConfigs processor (extract values etc.) ENABLE_CLICKHOUSE_FORMAT\tENABLE_CLICKHOUSE_ALL\tQueries pretty-printer and formatter with syntax highlighting ENABLE_CLICKHOUSE_GIT_IMPORT\tENABLE_CLICKHOUSE_ALL\tA tool to analyze Git repositories\thttps://presentations.clickhouse.com/matemarketing_2020/ ENABLE_CLICKHOUSE_INSTALL\tOFF\tInstall ClickHouse without .deb/.rpm/.tgz packages (having the binary only) ENABLE_CLICKHOUSE_KEEPER\tENABLE_CLICKHOUSE_ALL\tClickHouse alternative to ZooKeeper ENABLE_CLICKHOUSE_KEEPER_CONVERTER\tENABLE_CLICKHOUSE_ALL\tUtil allows to convert ZooKeeper logs and snapshots into clickhouse-keeper snapshot ENABLE_CLICKHOUSE_LIBRARY_BRIDGE\tENABLE_CLICKHOUSE_ALL\tHTTP-server working like a proxy to Library dictionary source ENABLE_CLICKHOUSE_LOCAL\tENABLE_CLICKHOUSE_ALL\tLocal files fast processing mode\thttps://clickhouse.com/docs/en/operations/utilities/clickhouse-local/ ENABLE_CLICKHOUSE_OBFUSCATOR\tENABLE_CLICKHOUSE_ALL\tTable data obfuscator (convert real data to benchmark-ready one)\thttps://clickhouse.com/docs/en/operations/utilities/clickhouse-obfuscator/ ENABLE_CLICKHOUSE_ODBC_BRIDGE\tENABLE_CLICKHOUSE_ALL\tHTTP-server working like a proxy to ODBC driver ENABLE_CLICKHOUSE_SERVER\tENABLE_CLICKHOUSE_ALL\tServer mode (main mode) ENABLE_CLICKHOUSE_STATIC_FILES_DISK_UPLOADER\tENABLE_CLICKHOUSE_ALL\tA tool to export table data files to be later put to a static files web server\t "},{"title":"External libraries​","type":1,"pageTitle":"CMake in ClickHouse","url":"en/development/cmake-in-clickhouse#external-libraries","content":"Note that ClickHouse uses forks of these libraries, see https://github.com/ClickHouse-Extras. Name\tDefault value\tDescription\tCommentENABLE_AVX\t0\tUse AVX instructions on x86_64 ENABLE_AVX2\t0\tUse AVX2 instructions on x86_64 ENABLE_AVX2_FOR_SPEC_OP\t0\tUse avx2 instructions for specific operations on x86_64 ENABLE_AVX512\t0\tUse AVX512 instructions on x86_64 ENABLE_AVX512_FOR_SPEC_OP\t0\tUse avx512 instructions for specific operations on x86_64 ENABLE_BMI\t0\tUse BMI instructions on x86_64 ENABLE_CCACHE\tENABLE_CCACHE_BY_DEFAULT\tSpeedup re-compilations using ccache (external tool)\thttps://ccache.dev/ ENABLE_CLANG_TIDY\tOFF\tUse clang-tidy static analyzer\thttps://clang.llvm.org/extra/clang-tidy/ ENABLE_PCLMULQDQ\t1\tUse pclmulqdq instructions on x86_64 ENABLE_POPCNT\t1\tUse popcnt instructions on x86_64 ENABLE_SSE41\t1\tUse SSE4.1 instructions on x86_64 ENABLE_SSE42\t1\tUse SSE4.2 instructions on x86_64 ENABLE_SSSE3\t1\tUse SSSE3 instructions on x86_64\t "},{"title":"Other flags​","type":1,"pageTitle":"CMake in ClickHouse","url":"en/development/cmake-in-clickhouse#other-flags","content":"Name\tDefault value\tDescription\tCommentADD_GDB_INDEX_FOR_GOLD\tOFF\tAdd .gdb-index to resulting binaries for gold linker.\tIgnored if lld is used ARCH_NATIVE\t0\tAdd -march=native compiler flag. This makes your binaries non-portable but more performant code may be generated. This option overrides ENABLE_* options for specific instruction set. Highly not recommended to use. BUILD_STANDALONE_KEEPER\tOFF\tBuild keeper as small standalone binary CLICKHOUSE_SPLIT_BINARY\tOFF\tMake several binaries (clickhouse-server, clickhouse-client etc.) instead of one bundled COMPILER_PIPE\tON\t-pipe compiler option\tLess /tmp usage, more RAM usage. ENABLE_BUILD_PATH_MAPPING\tON\tEnable remap file source paths in debug info, predefined preprocessor macros and __builtin_FILE(). It's to generate reproducible builds. See https://reproducible-builds.org/docs/build-path\tReproducible builds If turned ON, remap file source paths in debug info, predefined preprocessor macros and __builtin_FILE(). ENABLE_CHECK_HEAVY_BUILDS\tOFF\tDon't allow C++ translation units to compile too long or to take too much memory while compiling.\tTake care to add prlimit in command line before ccache, or else ccache thinks that prlimit is compiler, and clang++ is its input file, and refuses to work with multiple inputs, e.g in ccache log: [2021-03-31T18:06:32.655327 36900] Command line: /usr/bin/ccache prlimit --as=10000000000 --data=5000000000 --cpu=600 /usr/bin/clang++-11 - ...... std=gnu++2a -MD -MT src/CMakeFiles/dbms.dir/Storages/MergeTree/IMergeTreeDataPart.cpp.o -MF src/CMakeFiles/dbms.dir/Storages/MergeTree/IMergeTreeDataPart.cpp.o.d -o src/CMakeFiles/dbms.dir/Storages/MergeTree/IMergeTreeDataPart.cpp.o -c ../src/Storages/MergeTree/IMergeTreeDataPart.cpp [2021-03-31T18:06:32.656704 36900] Multiple input files: /usr/bin/clang++-11 and ../src/Storages/MergeTree/IMergeTreeDataPart.cpp Another way would be to use --ccache-skip option before clang++-11 to make ccache ignore it. ENABLE_EXAMPLES\tOFF\tBuild all example programs in 'examples' subdirectories ENABLE_FUZZING\tOFF\tFuzzy testing using libfuzzer ENABLE_LIBRARIES\tON\tEnable all external libraries by default\tTurns on all external libs like s3, kafka, ODBC, ... ENABLE_MULTITARGET_CODE\tON\tEnable platform-dependent code\tClickHouse developers may use platform-dependent code under some macro (e.g. ifdef ENABLE_MULTITARGET). If turned ON, this option defines such macro. See src/Functions/TargetSpecific.h ENABLE_TESTS\tON\tProvide unit_test_dbms target with Google.Test unit tests\tIf turned ON, assumes the user has either the system GTest library or the bundled one. ENABLE_THINLTO\tON\tClang-specific link time optimization\thttps://clang.llvm.org/docs/ThinLTO.html Applies to clang only. Disabled when building with tests or sanitizers. FAIL_ON_UNSUPPORTED_OPTIONS_COMBINATION\tON\tStop/Fail CMake configuration if some ENABLE_XXX option is defined (either ON or OFF) but is not possible to satisfy\tIf turned off: e.g. when ENABLE_FOO is ON, but FOO tool was not found, the CMake will continue. GLIBC_COMPATIBILITY\tON\tEnable compatibility with older glibc libraries.\tOnly for Linux, x86_64 or aarch64. INSTALL_STRIPPED_BINARIES\tOFF\tBuild stripped binaries with debug info in separate directory LINKER_NAME\tOFF\tLinker name or full path\tExample values: lld-10, gold. PARALLEL_COMPILE_JOBS\t&quot;&quot;\tMaximum number of concurrent compilation jobs\t1 if not set PARALLEL_LINK_JOBS\t&quot;&quot;\tMaximum number of concurrent link jobs\t1 if not set SANITIZE\t&quot;&quot;\tEnable one of the code sanitizers\tPossible values: - address (ASan) - memory (MSan) - thread (TSan) - undefined (UBSan) - &quot;&quot; (no sanitizing) SPLIT_SHARED_LIBRARIES\tOFF\tKeep all internal libraries as separate .so files\tDEVELOPER ONLY. Faster linking if turned on. STRIP_DEBUG_SYMBOLS_FUNCTIONS\tSTRIP_DSF_DEFAULT\tDo not generate debugger info for ClickHouse functions\tProvides faster linking and lower binary size. Tradeoff is the inability to debug some source files with e.g. gdb (empty stack frames and no local variables).&quot; USE_DEBUG_HELPERS\tUSE_DEBUG_HELPERS\tEnable debug helpers USE_STATIC_LIBRARIES\tON\tDisable to use shared libraries USE_UNWIND\tENABLE_LIBRARIES\tEnable libunwind (better stacktraces) WERROR\tOFF\tEnable -Werror compiler option\tUsing system libs can cause a lot of warnings in includes (on macro expansion). WEVERYTHING\tON\tEnable -Weverything option with some exceptions.\tAdd some warnings that are not available even with -Wall -Wextra -Wpedantic. Intended for exploration of new compiler warnings that may be found useful. Applies to clang only WITH_COVERAGE\tOFF\tProfile the resulting binary/binaries\tCompiler-specific coverage flags e.g. -fcoverage-mapping for gcc "},{"title":"Developer's guide for adding new CMake options​","type":1,"pageTitle":"CMake in ClickHouse","url":"en/development/cmake-in-clickhouse#developers-guide-for-adding-new-cmake-options","content":"Don't be obvious. Be informative.​ Bad: option (ENABLE_TESTS &quot;Enables testing&quot; OFF)  This description is quite useless as it neither gives the viewer any additional information nor explains the option purpose. Better: option(ENABLE_TESTS &quot;Provide unit_test_dbms target with Google.test unit tests&quot; OFF)  If the option's purpose can't be guessed by its name, or the purpose guess may be misleading, or option has some pre-conditions, leave a comment above the option() line and explain what it does. The best way would be linking the docs page (if it exists). The comment is parsed into a separate column (see below). Even better: # implies ${TESTS_ARE_ENABLED} # see tests/CMakeLists.txt for implementation detail. option(ENABLE_TESTS &quot;Provide unit_test_dbms target with Google.test unit tests&quot; OFF)  If the option's state could produce unwanted (or unusual) result, explicitly warn the user.​ Suppose you have an option that may strip debug symbols from the ClickHouse's part. This can speed up the linking process, but produces a binary that cannot be debugged. In that case, prefer explicitly raising a warning telling the developer that he may be doing something wrong. Also, such options should be disabled if applies. Bad: option(STRIP_DEBUG_SYMBOLS_FUNCTIONS &quot;Do not generate debugger info for ClickHouse functions. ${STRIP_DSF_DEFAULT}) if (STRIP_DEBUG_SYMBOLS_FUNCTIONS) target_compile_options(clickhouse_functions PRIVATE &quot;-g0&quot;) endif()  Better: # Provides faster linking and lower binary size. # Tradeoff is the inability to debug some source files with e.g. gdb # (empty stack frames and no local variables).&quot; option(STRIP_DEBUG_SYMBOLS_FUNCTIONS &quot;Do not generate debugger info for ClickHouse functions.&quot; ${STRIP_DSF_DEFAULT}) if (STRIP_DEBUG_SYMBOLS_FUNCTIONS) message(WARNING &quot;Not generating debugger info for ClickHouse functions&quot;) target_compile_options(clickhouse_functions PRIVATE &quot;-g0&quot;) endif()  In the option's description, explain WHAT the option does rather than WHY it does something.​ The WHY explanation should be placed in the comment. You may find that the option's name is self-descriptive. Bad: option(ENABLE_THINLTO &quot;Enable Thin LTO. Only applicable for clang. It's also suppressed when building with tests or sanitizers.&quot; ON)  Better: # Only applicable for clang. # Turned off when building with tests or sanitizers. option(ENABLE_THINLTO &quot;Clang-specific link time optimisation&quot; ON).  Don't assume other developers know as much as you do.​ In ClickHouse, there are many tools used that an ordinary developer may not know. If you are in doubt, give a link to the tool's docs. It won't take much of your time. Bad: option(ENABLE_THINLTO &quot;Enable Thin LTO. Only applicable for clang. It's also suppressed when building with tests or sanitizers.&quot; ON)  Better (combined with the above hint): # https://clang.llvm.org/docs/ThinLTO.html # Only applicable for clang. # Turned off when building with tests or sanitizers. option(ENABLE_THINLTO &quot;Clang-specific link time optimisation&quot; ON).  Other example, bad: option (USE_INCLUDE_WHAT_YOU_USE &quot;Use 'include-what-you-use' tool&quot; OFF)  Better: # https://github.com/include-what-you-use/include-what-you-use option (USE_INCLUDE_WHAT_YOU_USE &quot;Reduce unneeded #include s (external tool)&quot; OFF)  Prefer consistent default values.​ CMake allows you to pass a plethora of values representing boolean true/false, e.g. 1, ON, YES, .... Prefer the ON/OFF values, if possible. "},{"title":"SQLite","type":0,"sectionRef":"#","url":"en/engines/database-engines/sqlite","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"SQLite","url":"en/engines/database-engines/sqlite#creating-a-database","content":" CREATE DATABASE sqlite_database ENGINE = SQLite('db_path')  Engine Parameters db_path — Path to a file with SQLite database. "},{"title":"Data Types Support​","type":1,"pageTitle":"SQLite","url":"en/engines/database-engines/sqlite#data_types-support","content":"SQLite\tClickHouseINTEGER\tInt32 REAL\tFloat32 TEXT\tString BLOB\tString "},{"title":"Specifics and Recommendations​","type":1,"pageTitle":"SQLite","url":"en/engines/database-engines/sqlite#specifics-and-recommendations","content":"SQLite stores the entire database (definitions, tables, indices, and the data itself) as a single cross-platform file on a host machine. During writing SQLite locks the entire database file, therefore write operations are performed sequentially. Read operations can be multitasked. SQLite does not require service management (such as startup scripts) or access control based on GRANT and passwords. Access control is handled by means of file-system permissions given to the database file itself. "},{"title":"Usage Example​","type":1,"pageTitle":"SQLite","url":"en/engines/database-engines/sqlite#usage-example","content":"Database in ClickHouse, connected to the SQLite: CREATE DATABASE sqlite_db ENGINE = SQLite('sqlite.db'); SHOW TABLES FROM sqlite_db;  ┌──name───┐ │ table1 │ │ table2 │ └─────────┘  Shows the tables: SELECT * FROM sqlite_db.table1;  ┌─col1──┬─col2─┐ │ line1 │ 1 │ │ line2 │ 2 │ │ line3 │ 3 │ └───────┴──────┘  Inserting data into SQLite table from ClickHouse table: CREATE TABLE clickhouse_table(`col1` String,`col2` Int16) ENGINE = MergeTree() ORDER BY col2; INSERT INTO clickhouse_table VALUES ('text',10); INSERT INTO sqlite_db.table1 SELECT * FROM clickhouse_table; SELECT * FROM sqlite_db.table1;  ┌─col1──┬─col2─┐ │ line1 │ 1 │ │ line2 │ 2 │ │ line3 │ 3 │ │ text │ 10 │ └───────┴──────┘  "},{"title":"[experimental] Replicated","type":0,"sectionRef":"#","url":"en/engines/database-engines/replicated","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"[experimental] Replicated","url":"en/engines/database-engines/replicated#creating-a-database","content":" CREATE DATABASE testdb ENGINE = Replicated('zoo_path', 'shard_name', 'replica_name') [SETTINGS ...]  Engine Parameters zoo_path — ZooKeeper path. The same ZooKeeper path corresponds to the same database.shard_name — Shard name. Database replicas are grouped into shards by shard_name.replica_name — Replica name. Replica names must be different for all replicas of the same shard. warning For ReplicatedMergeTree tables if no arguments provided, then default arguments are used: /clickhouse/tables/{uuid}/{shard} and {replica}. These can be changed in the server settings default_replica_path and default_replica_name. Macro {uuid} is unfolded to table's uuid, {shard} and {replica} are unfolded to values from server config, not from database engine arguments. But in the future, it will be possible to use shard_name and replica_name of Replicated database. "},{"title":"Specifics and Recommendations​","type":1,"pageTitle":"[experimental] Replicated","url":"en/engines/database-engines/replicated#specifics-and-recommendations","content":"DDL queries with Replicated database work in a similar way to ON CLUSTER queries, but with minor differences. First, the DDL request tries to execute on the initiator (the host that originally received the request from the user). If the request is not fulfilled, then the user immediately receives an error, other hosts do not try to fulfill it. If the request has been successfully completed on the initiator, then all other hosts will automatically retry until they complete it. The initiator will try to wait for the query to be completed on other hosts (no longer than distributed_ddl_task_timeout) and will return a table with the query execution statuses on each host. The behavior in case of errors is regulated by the distributed_ddl_output_mode setting, for a Replicated database it is better to set it to null_status_on_timeout — i.e. if some hosts did not have time to execute the request for distributed_ddl_task_timeout, then do not throw an exception, but show the NULL status for them in the table. The system.clusters system table contains a cluster named like the replicated database, which consists of all replicas of the database. This cluster is updated automatically when creating/deleting replicas, and it can be used for Distributed tables. When creating a new replica of the database, this replica creates tables by itself. If the replica has been unavailable for a long time and has lagged behind the replication log — it checks its local metadata with the current metadata in ZooKeeper, moves the extra tables with data to a separate non-replicated database (so as not to accidentally delete anything superfluous), creates the missing tables, updates the table names if they have been renamed. The data is replicated at the ReplicatedMergeTree level, i.e. if the table is not replicated, the data will not be replicated (the database is responsible only for metadata). ALTER TABLE ATTACH|FETCH|DROP|DROP DETACHED|DETACH PARTITION|PART queries are allowed but not replicated. The database engine will only add/fetch/remove the partition/part to the current replica. However, if the table itself uses a Replicated table engine, then the data will be replicated after using ATTACH. "},{"title":"Usage Example​","type":1,"pageTitle":"[experimental] Replicated","url":"en/engines/database-engines/replicated#usage-example","content":"Creating a cluster with three hosts: node1 :) CREATE DATABASE r ENGINE=Replicated('some/path/r','shard1','replica1'); node2 :) CREATE DATABASE r ENGINE=Replicated('some/path/r','shard1','other_replica'); node3 :) CREATE DATABASE r ENGINE=Replicated('some/path/r','other_shard','{replica}');  Running the DDL-query: CREATE TABLE r.rmt (n UInt64) ENGINE=ReplicatedMergeTree ORDER BY n;  ┌─────hosts────────────┬──status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐ │ shard1|replica1 │ 0 │ │ 2 │ 0 │ │ shard1|other_replica │ 0 │ │ 1 │ 0 │ │ other_shard|r1 │ 0 │ │ 0 │ 0 │ └──────────────────────┴─────────┴───────┴─────────────────────┴──────────────────┘  Showing the system table: SELECT cluster, shard_num, replica_num, host_name, host_address, port, is_local FROM system.clusters WHERE cluster='r';  ┌─cluster─┬─shard_num─┬─replica_num─┬─host_name─┬─host_address─┬─port─┬─is_local─┐ │ r │ 1 │ 1 │ node3 │ 127.0.0.1 │ 9002 │ 0 │ │ r │ 2 │ 1 │ node2 │ 127.0.0.1 │ 9001 │ 0 │ │ r │ 2 │ 2 │ node1 │ 127.0.0.1 │ 9000 │ 1 │ └─────────┴───────────┴─────────────┴───────────┴──────────────┴──────┴──────────┘  Creating a distributed table and inserting the data: node2 :) CREATE TABLE r.d (n UInt64) ENGINE=Distributed('r','r','rmt', n % 2); node3 :) INSERT INTO r SELECT * FROM numbers(10); node1 :) SELECT materialize(hostName()) AS host, groupArray(n) FROM r.d GROUP BY host;  ┌─hosts─┬─groupArray(n)─┐ │ node1 │ [1,3,5,7,9] │ │ node2 │ [0,2,4,6,8] │ └───────┴───────────────┘  Adding replica on the one more host: node4 :) CREATE DATABASE r ENGINE=Replicated('some/path/r','other_shard','r2');  The cluster configuration will look like this: ┌─cluster─┬─shard_num─┬─replica_num─┬─host_name─┬─host_address─┬─port─┬─is_local─┐ │ r │ 1 │ 1 │ node3 │ 127.0.0.1 │ 9002 │ 0 │ │ r │ 1 │ 2 │ node4 │ 127.0.0.1 │ 9003 │ 0 │ │ r │ 2 │ 1 │ node2 │ 127.0.0.1 │ 9001 │ 0 │ │ r │ 2 │ 2 │ node1 │ 127.0.0.1 │ 9000 │ 1 │ └─────────┴───────────┴─────────────┴───────────┴──────────────┴──────┴──────────┘  The distributed table also will get data from the new host: node2 :) SELECT materialize(hostName()) AS host, groupArray(n) FROM r.d GROUP BY host;  ┌─hosts─┬─groupArray(n)─┐ │ node2 │ [1,3,5,7,9] │ │ node4 │ [0,2,4,6,8] │ └───────┴───────────────┘  "},{"title":"ClickHouse Testing","type":0,"sectionRef":"#","url":"en/development/tests","content":"","keywords":""},{"title":"Functional Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#functional-tests","content":"Functional tests are the most simple and convenient to use. Most of ClickHouse features can be tested with functional tests and they are mandatory to use for every change in ClickHouse code that can be tested that way. Each functional test sends one or multiple queries to the running ClickHouse server and compares the result with reference. Tests are located in queries directory. There are two subdirectories: stateless and stateful. Stateless tests run queries without any preloaded test data - they often create small synthetic datasets on the fly, within the test itself. Stateful tests require preloaded test data from CLickHouse and it is available to general public. Each test can be one of two types: .sql and .sh. .sql test is the simple SQL script that is piped to clickhouse-client --multiquery. .sh test is a script that is run by itself. SQL tests are generally preferable to .sh tests. You should use .sh tests only when you have to test some feature that cannot be exercised from pure SQL, such as piping some input data into clickhouse-client or testing clickhouse-local. "},{"title":"Running a Test Locally​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#functional-test-locally","content":"Start the ClickHouse server locally, listening on the default port (9000). To run, for example, the test 01428_hash_set_nan_key, change to the repository folder and run the following command: PATH=$PATH:&lt;path to clickhouse-client&gt; tests/clickhouse-test 01428_hash_set_nan_key  For more options, see tests/clickhouse-test --help. You can simply run all tests or run subset of tests filtered by substring in test name: ./clickhouse-test substring. There are also options to run tests in parallel or in randomized order. "},{"title":"Adding a New Test​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#adding-a-new-test","content":"To add new test, create a .sql or .sh file in queries/0_stateless directory, check it manually and then generate .reference file in the following way: clickhouse-client --multiquery &lt; 00000_test.sql &gt; 00000_test.reference or ./00000_test.sh &gt; ./00000_test.reference. Tests should use (create, drop, etc) only tables in test database that is assumed to be created beforehand; also tests can use temporary tables. "},{"title":"Choosing the Test Name​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#choosing-the-test-name","content":"The name of the test starts with a five-digit prefix followed by a descriptive name, such as 00422_hash_function_constexpr.sql. To choose the prefix, find the largest prefix already present in the directory, and increment it by one. In the meantime, some other tests might be added with the same numeric prefix, but this is OK and does not lead to any problems, you don't have to change it later. Some tests are marked with zookeeper, shard or long in their names. zookeeper is for tests that are using ZooKeeper. shard is for tests that requires server to listen 127.0.0.*; distributed or global have the same meaning. long is for tests that run slightly longer that one second. You can disable these groups of tests using --no-zookeeper, --no-shard and --no-long options, respectively. Make sure to add a proper prefix to your test name if it needs ZooKeeper or distributed queries. "},{"title":"Checking for an Error that Must Occur​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#checking-for-an-error-that-must-occur","content":"Sometimes you want to test that a server error occurs for an incorrect query. We support special annotations for this in SQL tests, in the following form: select x; -- { serverError 49 }  This test ensures that the server returns an error with code 49 about unknown column x. If there is no error, or the error is different, the test will fail. If you want to ensure that an error occurs on the client side, use clientError annotation instead. Do not check for a particular wording of error message, it may change in the future, and the test will needlessly break. Check only the error code. If the existing error code is not precise enough for your needs, consider adding a new one. "},{"title":"Testing a Distributed Query​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#testing-a-distributed-query","content":"If you want to use distributed queries in functional tests, you can leverage remote table function with 127.0.0.{1..2} addresses for the server to query itself; or you can use predefined test clusters in server configuration file like test_shard_localhost. Remember to add the words shard or distributed to the test name, so that it is run in CI in correct configurations, where the server is configured to support distributed queries. "},{"title":"Known Bugs​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#known-bugs","content":"If we know some bugs that can be easily reproduced by functional tests, we place prepared functional tests in tests/queries/bugs directory. These tests will be moved to tests/queries/0_stateless when bugs are fixed. "},{"title":"Integration Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#integration-tests","content":"Integration tests allow testing ClickHouse in clustered configuration and ClickHouse interaction with other servers like MySQL, Postgres, MongoDB. They are useful to emulate network splits, packet drops, etc. These tests are run under Docker and create multiple containers with various software. See tests/integration/README.md on how to run these tests. Note that integration of ClickHouse with third-party drivers is not tested. Also, we currently do not have integration tests with our JDBC and ODBC drivers. "},{"title":"Unit Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#unit-tests","content":"Unit tests are useful when you want to test not the ClickHouse as a whole, but a single isolated library or class. You can enable or disable build of tests with ENABLE_TESTS CMake option. Unit tests (and other test programs) are located in tests subdirectories across the code. To run unit tests, type ninja test. Some tests use gtest, but some are just programs that return non-zero exit code on test failure. It’s not necessary to have unit tests if the code is already covered by functional tests (and functional tests are usually much more simple to use). You can run individual gtest checks by calling the executable directly, for example: $ ./src/unit_tests_dbms --gtest_filter=LocalAddress*  "},{"title":"Performance Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#performance-tests","content":"Performance tests allow to measure and compare performance of some isolated part of ClickHouse on synthetic queries. Tests are located at tests/performance. Each test is represented by .xml file with description of test case. Tests are run with docker/tests/performance-comparison tool . See the readme file for invocation. Each test run one or multiple queries (possibly with combinations of parameters) in a loop. If you want to improve performance of ClickHouse in some scenario, and if improvements can be observed on simple queries, it is highly recommended to write a performance test. It always makes sense to use perf top or other perf tools during your tests. "},{"title":"Test Tools and Scripts​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#test-tools-and-scripts","content":"Some programs in tests directory are not prepared tests, but are test tools. For example, for Lexer there is a tool src/Parsers/tests/lexer that just do tokenization of stdin and writes colorized result to stdout. You can use these kind of tools as a code examples and for exploration and manual testing. "},{"title":"Miscellaneous Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#miscellaneous-tests","content":"There are tests for machine learned models in tests/external_models. These tests are not updated and must be transferred to integration tests. There is separate test for quorum inserts. This test run ClickHouse cluster on separate servers and emulate various failure cases: network split, packet drop (between ClickHouse nodes, between ClickHouse and ZooKeeper, between ClickHouse server and client, etc.), kill -9, kill -STOP and kill -CONT , like Jepsen. Then the test checks that all acknowledged inserts was written and all rejected inserts was not. Quorum test was written by separate team before ClickHouse was open-sourced. This team no longer work with ClickHouse. Test was accidentally written in Java. For these reasons, quorum test must be rewritten and moved to integration tests. "},{"title":"Manual Testing​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#manual-testing","content":"When you develop a new feature, it is reasonable to also test it manually. You can do it with the following steps: Build ClickHouse. Run ClickHouse from the terminal: change directory to programs/clickhouse-server and run it with ./clickhouse-server. It will use configuration (config.xml, users.xml and files within config.d and users.d directories) from the current directory by default. To connect to ClickHouse server, run programs/clickhouse-client/clickhouse-client. Note that all clickhouse tools (server, client, etc) are just symlinks to a single binary named clickhouse. You can find this binary at programs/clickhouse. All tools can also be invoked as clickhouse tool instead of clickhouse-tool. Alternatively you can install ClickHouse package: either stable release from ClickHouse repository or you can build package for yourself with ./release in ClickHouse sources root. Then start the server with sudo clickhouse start (or stop to stop the server). Look for logs at /etc/clickhouse-server/clickhouse-server.log. When ClickHouse is already installed on your system, you can build a new clickhouse binary and replace the existing binary: $ sudo clickhouse stop $ sudo cp ./clickhouse /usr/bin/ $ sudo clickhouse start  Also you can stop system clickhouse-server and run your own with the same configuration but with logging to terminal: $ sudo clickhouse stop $ sudo -u clickhouse /usr/bin/clickhouse server --config-file /etc/clickhouse-server/config.xml  Example with gdb: $ sudo -u clickhouse gdb --args /usr/bin/clickhouse server --config-file /etc/clickhouse-server/config.xml  If the system clickhouse-server is already running and you do not want to stop it, you can change port numbers in your config.xml (or override them in a file in config.d directory), provide appropriate data path, and run it. clickhouse binary has almost no dependencies and works across wide range of Linux distributions. To quick and dirty test your changes on a server, you can simply scp your fresh built clickhouse binary to your server and then run it as in examples above. "},{"title":"Build Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#build-tests","content":"Build tests allow to check that build is not broken on various alternative configurations and on some foreign systems. These tests are automated as well. Examples: cross-compile for Darwin x86_64 (Mac OS X)cross-compile for FreeBSD x86_64cross-compile for Linux AArch64build on Ubuntu with libraries from system packages (discouraged)build with shared linking of libraries (discouraged) For example, build with system packages is bad practice, because we cannot guarantee what exact version of packages a system will have. But this is really needed by Debian maintainers. For this reason we at least have to support this variant of build. Another example: shared linking is a common source of trouble, but it is needed for some enthusiasts. Though we cannot run all tests on all variant of builds, we want to check at least that various build variants are not broken. For this purpose we use build tests. We also test that there are no translation units that are too long to compile or require too much RAM. We also test that there are no too large stack frames. "},{"title":"Testing for Protocol Compatibility​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#testing-for-protocol-compatibility","content":"When we extend ClickHouse network protocol, we test manually that old clickhouse-client works with new clickhouse-server and new clickhouse-client works with old clickhouse-server (simply by running binaries from corresponding packages). We also test some cases automatically with integrational tests: if data written by old version of ClickHouse can be successfully read by the new version;do distributed queries work in a cluster with different ClickHouse versions. "},{"title":"Help from the Compiler​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#help-from-the-compiler","content":"Main ClickHouse code (that is located in dbms directory) is built with -Wall -Wextra -Werror and with some additional enabled warnings. Although these options are not enabled for third-party libraries. Clang has even more useful warnings - you can look for them with -Weverything and pick something to default build. For production builds, clang is used, but we also test make gcc builds. For development, clang is usually more convenient to use. You can build on your own machine with debug mode (to save battery of your laptop), but please note that compiler is able to generate more warnings with -O3 due to better control flow and inter-procedure analysis. When building with clang in debug mode, debug version of libc++ is used that allows to catch more errors at runtime. "},{"title":"Sanitizers​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#sanitizers","content":""},{"title":"Address sanitizer​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#address-sanitizer","content":"We run functional, integration, stress and unit tests under ASan on per-commit basis. "},{"title":"Thread sanitizer​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#thread-sanitizer","content":"We run functional, integration, stress and unit tests under TSan on per-commit basis. "},{"title":"Memory sanitizer​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#memory-sanitizer","content":"We run functional, integration, stress and unit tests under MSan on per-commit basis. "},{"title":"Undefined behaviour sanitizer​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#undefined-behaviour-sanitizer","content":"We run functional, integration, stress and unit tests under UBSan on per-commit basis. The code of some third-party libraries is not sanitized for UB. "},{"title":"Valgrind (Memcheck)​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#valgrind-memcheck","content":"We used to run functional tests under Valgrind overnight, but don't do it anymore. It takes multiple hours. Currently there is one known false positive in re2 library, see this article. "},{"title":"Fuzzing​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#fuzzing","content":"ClickHouse fuzzing is implemented both using libFuzzer and random SQL queries. All the fuzz testing should be performed with sanitizers (Address and Undefined). LibFuzzer is used for isolated fuzz testing of library code. Fuzzers are implemented as part of test code and have “_fuzzer” name postfixes. Fuzzer example can be found at src/Parsers/tests/lexer_fuzzer.cpp. LibFuzzer-specific configs, dictionaries and corpus are stored at tests/fuzz. We encourage you to write fuzz tests for every functionality that handles user input. Fuzzers are not built by default. To build fuzzers both -DENABLE_FUZZING=1 and -DENABLE_TESTS=1 options should be set. We recommend to disable Jemalloc while building fuzzers. Configuration used to integrate ClickHouse fuzzing to Google OSS-Fuzz can be found at docker/fuzz. We also use simple fuzz test to generate random SQL queries and to check that the server does not die executing them. You can find it in 00746_sql_fuzzy.pl. This test should be run continuously (overnight and longer). We also use sophisticated AST-based query fuzzer that is able to find huge amount of corner cases. It does random permutations and substitutions in queries AST. It remembers AST nodes from previous tests to use them for fuzzing of subsequent tests while processing them in random order. You can learn more about this fuzzer in this blog article. "},{"title":"Stress test​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#stress-test","content":"Stress tests are another case of fuzzing. It runs all functional tests in parallel in random order with a single server. Results of the tests are not checked. It is checked that: server does not crash, no debug or sanitizer traps are triggered;there are no deadlocks;the database structure is consistent;server can successfully stop after the test and start again without exceptions. There are five variants (Debug, ASan, TSan, MSan, UBSan). "},{"title":"Thread Fuzzer​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#thread-fuzzer","content":"Thread Fuzzer (please don't mix up with Thread Sanitizer) is another kind of fuzzing that allows to randomize thread order of execution. It helps to find even more special cases. "},{"title":"Security Audit​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#security-audit","content":"Our Security Team did some basic overview of ClickHouse capabilities from the security standpoint. "},{"title":"Static Analyzers​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#static-analyzers","content":"We run clang-tidy on per-commit basis. clang-static-analyzer checks are also enabled. clang-tidy is also used for some style checks. We have evaluated clang-tidy, Coverity, cppcheck, PVS-Studio, tscancode, CodeQL. You will find instructions for usage in tests/instructions/ directory. If you use CLion as an IDE, you can leverage some clang-tidy checks out of the box. We also use shellcheck for static analysis of shell scripts. "},{"title":"Hardening​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#hardening","content":"In debug build we are using custom allocator that does ASLR of user-level allocations. We also manually protect memory regions that are expected to be readonly after allocation. In debug build we also involve a customization of libc that ensures that no &quot;harmful&quot; (obsolete, insecure, not thread-safe) functions are called. Debug assertions are used extensively. In debug build, if exception with &quot;logical error&quot; code (implies a bug) is being thrown, the program is terminated prematurally. It allows to use exceptions in release build but make it an assertion in debug build. Debug version of jemalloc is used for debug builds. Debug version of libc++ is used for debug builds. "},{"title":"Runtime Integrity Checks​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#runtime-integrity-checks","content":"Data stored on disk is checksummed. Data in MergeTree tables is checksummed in three ways simultaneously* (compressed data blocks, uncompressed data blocks, the total checksum across blocks). Data transferred over network between client and server or between servers is also checksummed. Replication ensures bit-identical data on replicas. It is required to protect from faulty hardware (bit rot on storage media, bit flips in RAM on server, bit flips in RAM of network controller, bit flips in RAM of network switch, bit flips in RAM of client, bit flips on the wire). Note that bit flips are common and likely to occur even for ECC RAM and in presense of TCP checksums (if you manage to run thousands of servers processing petabytes of data each day). See the video (russian). ClickHouse provides diagnostics that will help ops engineers to find faulty hardware. * and it is not slow. "},{"title":"Code Style​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#code-style","content":"Code style rules are described here. To check for some common style violations, you can use utils/check-style script. To force proper style of your code, you can use clang-format. File .clang-format is located at the sources root. It mostly corresponding with our actual code style. But it’s not recommended to apply clang-format to existing files because it makes formatting worse. You can use clang-format-diff tool that you can find in clang source repository. Alternatively you can try uncrustify tool to reformat your code. Configuration is in uncrustify.cfg in the sources root. It is less tested than clang-format. CLion has its own code formatter that has to be tuned for our code style. We also use codespell to find typos in code. It is automated as well. "},{"title":"Test Coverage​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#test-coverage","content":"We also track test coverage but only for functional tests and only for clickhouse-server. It is performed on daily basis. "},{"title":"Tests for Tests​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#tests-for-tests","content":"There is automated check for flaky tests. It runs all new tests 100 times (for functional tests) or 10 times (for integration tests). If at least single time the test failed, it is considered flaky. "},{"title":"Testflows​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#testflows","content":"Testflows is an enterprise-grade open-source testing framework, which is used to test a subset of ClickHouse. "},{"title":"Test Automation​","type":1,"pageTitle":"ClickHouse Testing","url":"en/development/tests#test-automation","content":"We run tests with GitHub Actions. Build jobs and tests are run in Sandbox on per commit basis. Resulting packages and test results are published in GitHub and can be downloaded by direct links. Artifacts are stored for several months. When you send a pull request on GitHub, we tag it as “can be tested” and our CI system will build ClickHouse packages (release, debug, with address sanitizer, etc) for you. We do not use Travis CI due to the limit on time and computational power. We do not use Jenkins. It was used before and now we are happy we are not using Jenkins. Original article "},{"title":"Table Engines","type":0,"sectionRef":"#","url":"en/engines/table-engines/","content":"","keywords":""},{"title":"Engine Families​","type":1,"pageTitle":"Table Engines","url":"en/engines/table-engines/#engine-families","content":""},{"title":"MergeTree​","type":1,"pageTitle":"Table Engines","url":"en/engines/table-engines/#mergetree","content":"The most universal and functional table engines for high-load tasks. The property shared by these engines is quick data insertion with subsequent background data processing. MergeTree family engines support data replication (with Replicated* versions of engines), partitioning, secondary data-skipping indexes, and other features not supported in other engines. Engines in the family: MergeTreeReplacingMergeTreeSummingMergeTreeAggregatingMergeTreeCollapsingMergeTreeVersionedCollapsingMergeTreeGraphiteMergeTree "},{"title":"Log​","type":1,"pageTitle":"Table Engines","url":"en/engines/table-engines/#log","content":"Lightweight engines with minimum functionality. They’re the most effective when you need to quickly write many small tables (up to approximately 1 million rows) and read them later as a whole. Engines in the family: TinyLogStripeLogLog "},{"title":"Integration Engines​","type":1,"pageTitle":"Table Engines","url":"en/engines/table-engines/#integration-engines","content":"Engines for communicating with other data storage and processing systems. Engines in the family: ODBCJDBCMySQLMongoDBHDFSS3KafkaEmbeddedRocksDBRabbitMQPostgreSQL "},{"title":"Special Engines​","type":1,"pageTitle":"Table Engines","url":"en/engines/table-engines/#special-engines","content":"Engines in the family: DistributedMaterializedViewDictionaryMergeFileNullSetJoinURLViewMemoryBuffer "},{"title":"Virtual Columns​","type":1,"pageTitle":"Table Engines","url":"en/engines/table-engines/#table_engines-virtual_columns","content":"Virtual column is an integral table engine attribute that is defined in the engine source code. You shouldn’t specify virtual columns in the CREATE TABLE query and you can’t see them in SHOW CREATE TABLE and DESCRIBE TABLE query results. Virtual columns are also read-only, so you can’t insert data into virtual columns. To select data from a virtual column, you must specify its name in the SELECT query. SELECT * does not return values from virtual columns. If you create a table with a column that has the same name as one of the table virtual columns, the virtual column becomes inaccessible. We do not recommend doing this. To help avoid conflicts, virtual column names are usually prefixed with an underscore. Original article "},{"title":"[experimental] MaterializedPostgreSQL","type":0,"sectionRef":"#","url":"en/engines/database-engines/materialized-postgresql","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"en/engines/database-engines/materialized-postgresql#creating-a-database","content":"CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster] ENGINE = MaterializedPostgreSQL('host:port', 'database', 'user', 'password') [SETTINGS ...]  Engine Parameters host:port — PostgreSQL server endpoint.database — PostgreSQL database name.user — PostgreSQL user.password — User password. "},{"title":"Example of Use​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"en/engines/database-engines/materialized-postgresql#example-of-use","content":"CREATE DATABASE postgres_db ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgres_user', 'postgres_password'); SHOW TABLES FROM postgres_db; ┌─name───┐ │ table1 │ └────────┘ SELECT * FROM postgresql_db.postgres_table;  "},{"title":"Dynamically adding new tables to replication​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"en/engines/database-engines/materialized-postgresql#dynamically-adding-table-to-replication","content":"After MaterializedPostgreSQL database is created, it does not automatically detect new tables in according PostgreSQL database. Such tables can be added manually: ATTACH TABLE postgres_database.new_table;  warning Before version 22.1, adding a table to replication left an unremoved temporary replication slot (named {db_name}_ch_replication_slot_tmp). If attaching tables in ClickHouse version before 22.1, make sure to delete it manually (SELECT pg_drop_replication_slot('{db_name}_ch_replication_slot_tmp')). Otherwise disk usage will grow. This issue is fixed in 22.1. "},{"title":"Dynamically removing tables from replication​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"en/engines/database-engines/materialized-postgresql#dynamically-removing-table-from-replication","content":"It is possible to remove specific tables from replication: DETACH TABLE postgres_database.table_to_remove;  "},{"title":"PostgreSQL schema​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"en/engines/database-engines/materialized-postgresql#schema","content":"PostgreSQL schema can be configured in 3 ways (starting from version 21.12). One schema for one MaterializedPostgreSQL database engine. Requires to use setting materialized_postgresql_schema. Tables are accessed via table name only: CREATE DATABASE postgres_database ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgres_user', 'postgres_password') SETTINGS materialized_postgresql_schema = 'postgres_schema'; SELECT * FROM postgres_database.table1;  Any number of schemas with specified set of tables for one MaterializedPostgreSQL database engine. Requires to use setting materialized_postgresql_tables_list. Each table is written along with its schema. Tables are accessed via schema name and table name at the same time: CREATE DATABASE database1 ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgres_user', 'postgres_password') SETTINGS materialized_postgresql_tables_list = 'schema1.table1,schema2.table2,schema1.table3', materialized_postgresql_tables_list_with_schema = 1; SELECT * FROM database1.`schema1.table1`; SELECT * FROM database1.`schema2.table2`;  But in this case all tables in materialized_postgresql_tables_list must be written with its schema name. Requires materialized_postgresql_tables_list_with_schema = 1. Warning: for this case dots in table name are not allowed. Any number of schemas with full set of tables for one MaterializedPostgreSQL database engine. Requires to use setting materialized_postgresql_schema_list. CREATE DATABASE database1 ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgres_user', 'postgres_password') SETTINGS materialized_postgresql_schema_list = 'schema1,schema2,schema3'; SELECT * FROM database1.`schema1.table1`; SELECT * FROM database1.`schema1.table2`; SELECT * FROM database1.`schema2.table2`;  Warning: for this case dots in table name are not allowed. "},{"title":"Requirements​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"en/engines/database-engines/materialized-postgresql#requirements","content":"The wal_level setting must have a value logical and max_replication_slots parameter must have a value at least 2 in the PostgreSQL config file. Each replicated table must have one of the following replica identity: primary key (by default) index postgres# CREATE TABLE postgres_table (a Integer NOT NULL, b Integer, c Integer NOT NULL, d Integer, e Integer NOT NULL); postgres# CREATE unique INDEX postgres_table_index on postgres_table(a, c, e); postgres# ALTER TABLE postgres_table REPLICA IDENTITY USING INDEX postgres_table_index;  The primary key is always checked first. If it is absent, then the index, defined as replica identity index, is checked. If the index is used as a replica identity, there has to be only one such index in a table. You can check what type is used for a specific table with the following command: postgres# SELECT CASE relreplident WHEN 'd' THEN 'default' WHEN 'n' THEN 'nothing' WHEN 'f' THEN 'full' WHEN 'i' THEN 'index' END AS replica_identity FROM pg_class WHERE oid = 'postgres_table'::regclass;  warning Replication of TOAST values is not supported. The default value for the data type will be used. "},{"title":"Settings​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"en/engines/database-engines/materialized-postgresql#settings","content":"materialized_postgresql_tables_list {#materialized-postgresql-tables-list} Sets a comma-separated list of PostgreSQL database tables, which will be replicated via MaterializedPostgreSQL database engine. Default value: empty list — means whole PostgreSQL database will be replicated. materialized_postgresql_schema {#materialized-postgresql-schema} Default value: empty string. (Default schema is used) materialized_postgresql_schema_list {#materialized-postgresql-schema-list} Default value: empty list. (Default schema is used) materialized_postgresql_allow_automatic_update {#materialized-postgresql-allow-automatic-update} Do not use this setting before 22.1 version. Allows reloading table in the background, when schema changes are detected. DDL queries on the PostgreSQL side are not replicated via ClickHouse MaterializedPostgreSQL engine, because it is not allowed with PostgreSQL logical replication protocol, but the fact of DDL changes is detected transactionally. In this case, the default behaviour is to stop replicating those tables once DDL is detected. However, if this setting is enabled, then, instead of stopping the replication of those tables, they will be reloaded in the background via database snapshot without data losses and replication will continue for them. Possible values: 0 — The table is not automatically updated in the background, when schema changes are detected.1 — The table is automatically updated in the background, when schema changes are detected. Default value: 0. materialized_postgresql_max_block_size {#materialized-postgresql-max-block-size} Sets the number of rows collected in memory before flushing data into PostgreSQL database table. Possible values: Positive integer. Default value: 65536. materialized_postgresql_replication_slot {#materialized-postgresql-replication-slot} A user-created replication slot. Must be used together with materialized_postgresql_snapshot. materialized_postgresql_snapshot {#materialized-postgresql-snapshot} A text string identifying a snapshot, from which initial dump of PostgreSQL tables will be performed. Must be used together with materialized_postgresql_replication_slot. CREATE DATABASE database1 ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgres_user', 'postgres_password') SETTINGS materialized_postgresql_tables_list = 'table1,table2,table3'; SELECT * FROM database1.table1; The settings can be changed, if necessary, using a DDL query. But it is impossible to change the setting materialized_postgresql_tables_list. To update the list of tables in this setting use the ATTACH TABLE query. ALTER DATABASE postgres_database MODIFY SETTING materialized_postgresql_max_block_size = &lt;new_size&gt;;  "},{"title":"Notes​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"en/engines/database-engines/materialized-postgresql#notes","content":""},{"title":"Failover of the logical replication slot​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"en/engines/database-engines/materialized-postgresql#logical-replication-slot-failover","content":"Logical Replication Slots which exist on the primary are not available on standby replicas. So if there is a failover, new primary (the old physical standby) won’t be aware of any slots which were existing with old primary. This will lead to a broken replication from PostgreSQL. A solution to this is to manage replication slots yourself and define a permanent replication slot (some information can be found here). You'll need to pass slot name via materialized_postgresql_replication_slot setting, and it has to be exported with EXPORT SNAPSHOT option. The snapshot identifier needs to be passed via materialized_postgresql_snapshot setting. Please note that this should be used only if it is actually needed. If there is no real need for that or full understanding why, then it is better to allow the table engine to create and manage its own replication slot. Example (from @bchrobot) Configure replication slot in PostgreSQL. apiVersion: &quot;acid.zalan.do/v1&quot; kind: postgresql metadata: name: acid-demo-cluster spec: numberOfInstances: 2 postgresql: parameters: wal_level: logical patroni: slots: clickhouse_sync: type: logical database: demodb plugin: pgoutput Wait for replication slot to be ready, then begin a transaction and export the transaction snapshot identifier: BEGIN; SELECT pg_export_snapshot(); In ClickHouse create database: CREATE DATABASE demodb ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgres_user', 'postgres_password') SETTINGS materialized_postgresql_replication_slot = 'clickhouse_sync', materialized_postgresql_snapshot = '0000000A-0000023F-3', materialized_postgresql_tables_list = 'table1,table2,table3'; End the PostgreSQL transaction once replication to ClickHouse DB is confirmed. Verify that replication continues after failover: kubectl exec acid-demo-cluster-0 -c postgres -- su postgres -c 'patronictl failover --candidate acid-demo-cluster-1 --force'  "},{"title":"Required permissions​","type":1,"pageTitle":"[experimental] MaterializedPostgreSQL","url":"en/engines/database-engines/materialized-postgresql#required-permissions","content":"CREATE PUBLICATION -- create query privilege. CREATE_REPLICATION_SLOT -- replication privelege. pg_drop_replication_slot -- replication privilege or superuser. DROP PUBLICATION -- owner of publication (username in MaterializedPostgreSQL engine itself). It is possible to avoid executing 2 and 3 commands and having those permissions. Use settings materialized_postgresql_replication_slot and materialized_postgresql_snapshot. But with much care. Access to tables: pg_publication pg_replication_slots pg_publication_tables "},{"title":"Getting Started Guide for Building ClickHouse","type":0,"sectionRef":"#","url":"en/development/developer-instruction","content":"","keywords":""},{"title":"Creating a Repository on GitHub​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"en/development/developer-instruction#creating-a-repository-on-github","content":"To start working with ClickHouse repository you will need a GitHub account. You probably already have one, but if you do not, please register at https://github.com. In case you do not have SSH keys, you should generate them and then upload them on GitHub. It is required for sending over your patches. It is also possible to use the same SSH keys that you use with any other SSH servers - probably you already have those. Create a fork of ClickHouse repository. To do that please click on the “fork” button in the upper right corner at https://github.com/ClickHouse/ClickHouse. It will fork your own copy of ClickHouse/ClickHouse to your account. The development process consists of first committing the intended changes into your fork of ClickHouse and then creating a “pull request” for these changes to be accepted into the main repository (ClickHouse/ClickHouse). To work with git repositories, please install git. To do that in Ubuntu you would run in the command line terminal: sudo apt update sudo apt install git  A brief manual on using Git can be found here: https://education.github.com/git-cheat-sheet-education.pdf. For a detailed manual on Git see https://git-scm.com/book/en/v2. "},{"title":"Cloning a Repository to Your Development Machine​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"en/development/developer-instruction#cloning-a-repository-to-your-development-machine","content":"Next, you need to download the source files onto your working machine. This is called “to clone a repository” because it creates a local copy of the repository on your working machine. In the command line terminal run: git clone --recursive git@github.com:your_github_username/ClickHouse.git cd ClickHouse  Note: please, substitute your_github_username with what is appropriate! This command will create a directory ClickHouse containing the working copy of the project. It is important that the path to the working directory contains no whitespaces as it may lead to problems with running the build system. Please note that ClickHouse repository uses submodules. That is what the references to additional repositories are called (i.e. external libraries on which the project depends). It means that when cloning the repository you need to specify the --recursive flag as in the example above. If the repository has been cloned without submodules, to download them you need to run the following: git submodule init git submodule update  You can check the status with the command: git submodule status. If you get the following error message: Permission denied (publickey). fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists.  It generally means that the SSH keys for connecting to GitHub are missing. These keys are normally located in ~/.ssh. For SSH keys to be accepted you need to upload them in the settings section of GitHub UI. You can also clone the repository via https protocol: git clone --recursive https://github.com/ClickHouse/ClickHouse.git  This, however, will not let you send your changes to the server. You can still use it temporarily and add the SSH keys later replacing the remote address of the repository with git remote command. You can also add original ClickHouse repo’s address to your local repository to pull updates from there: git remote add upstream git@github.com:ClickHouse/ClickHouse.git  After successfully running this command you will be able to pull updates from the main ClickHouse repo by running git pull upstream master. "},{"title":"Working with Submodules​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"en/development/developer-instruction#working-with-submodules","content":"Working with submodules in git could be painful. Next commands will help to manage it: # ! each command accepts # Update remote URLs for submodules. Barely rare case git submodule sync # Add new submodules git submodule init # Update existing submodules to the current state git submodule update # Two last commands could be merged together git submodule update --init  The next commands would help you to reset all submodules to the initial state (!WARNING! - any changes inside will be deleted): # Synchronizes submodules' remote URL with .gitmodules git submodule sync # Update the registered submodules with initialize not yet initialized git submodule update --init # Reset all changes done after HEAD git submodule foreach git reset --hard # Clean files from .gitignore git submodule foreach git clean -xfd # Repeat last 4 commands for all submodule git submodule foreach git submodule sync git submodule foreach git submodule update --init git submodule foreach git submodule foreach git reset --hard git submodule foreach git submodule foreach git clean -xfd  "},{"title":"Build System​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"en/development/developer-instruction#build-system","content":"ClickHouse uses CMake and Ninja for building. CMake - a meta-build system that can generate Ninja files (build tasks). Ninja - a smaller build system with a focus on the speed used to execute those cmake generated tasks. To install on Ubuntu, Debian or Mint run sudo apt install cmake ninja-build. On CentOS, RedHat run sudo yum install cmake ninja-build. If you use Arch or Gentoo, you probably know it yourself how to install CMake. For installing CMake and Ninja on Mac OS X first install Homebrew and then install everything else via brew: /usr/bin/ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; brew install cmake ninja  Next, check the version of CMake: cmake --version. If it is below 3.12, you should install a newer version from the website: https://cmake.org/download/. "},{"title":"C++ Compiler​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"en/development/developer-instruction#c-compiler","content":"Compilers Clang starting from version 11 is supported for building ClickHouse. Clang should be used instead of gcc. Though, our continuous integration (CI) platform runs checks for about a dozen of build combinations. On Ubuntu/Debian you can use the automatic installation script (check official webpage) sudo bash -c &quot;$(wget -O - https://apt.llvm.org/llvm.sh)&quot;  Mac OS X build is also supported. Just run brew install llvm "},{"title":"The Building Process​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"en/development/developer-instruction#the-building-process","content":"Now that you are ready to build ClickHouse we recommend you to create a separate directory build inside ClickHouse that will contain all of the build artefacts: mkdir build cd build  You can have several different directories (build_release, build_debug, etc.) for different types of build. While inside the build directory, configure your build by running CMake. Before the first run, you need to define environment variables that specify compiler. export CC=clang CXX=clang++ cmake ..  If you installed clang using the automatic installation script above, also specify the version of clang installed in the first command, e.g. export CC=clang-13 CXX=clang++-13. The clang version will be in the script output. The CC variable specifies the compiler for C (short for C Compiler), and CXX variable instructs which C++ compiler is to be used for building. For a faster build, you can resort to the debug build type - a build with no optimizations. For that supply the following parameter -D CMAKE_BUILD_TYPE=Debug: cmake -D CMAKE_BUILD_TYPE=Debug ..  You can change the type of build by running this command in the build directory. Run ninja to build: ninja clickhouse-server clickhouse-client  Only the required binaries are going to be built in this example. If you require to build all the binaries (utilities and tests), you should run ninja with no parameters: ninja  Full build requires about 30GB of free disk space or 15GB to build the main binaries. When a large amount of RAM is available on build machine you should limit the number of build tasks run in parallel with -j param: ninja -j 1 clickhouse-server clickhouse-client  On machines with 4GB of RAM, it is recommended to specify 1, for 8GB of RAM -j 2 is recommended. If you get the message: ninja: error: loading 'build.ninja': No such file or directory, it means that generating a build configuration has failed and you need to inspect the message above. Upon the successful start of the building process, you’ll see the build progress - the number of processed tasks and the total number of tasks. While building messages about protobuf files in libhdfs2 library like libprotobuf WARNING may show up. They affect nothing and are safe to be ignored. Upon successful build you get an executable file ClickHouse/&lt;build_dir&gt;/programs/clickhouse: ls -l programs/clickhouse  "},{"title":"Running the Built Executable of ClickHouse​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"en/development/developer-instruction#running-the-built-executable-of-clickhouse","content":"To run the server under the current user you need to navigate to ClickHouse/programs/server/ (located outside of build) and run: ../../build/programs/clickhouse server  In this case, ClickHouse will use config files located in the current directory. You can run clickhouse server from any directory specifying the path to a config file as a command-line parameter --config-file. To connect to ClickHouse with clickhouse-client in another terminal navigate to ClickHouse/build/programs/ and run ./clickhouse client. If you get Connection refused message on Mac OS X or FreeBSD, try specifying host address 127.0.0.1: clickhouse client --host 127.0.0.1  You can replace the production version of ClickHouse binary installed in your system with your custom-built ClickHouse binary. To do that install ClickHouse on your machine following the instructions from the official website. Next, run the following: sudo service clickhouse-server stop sudo cp ClickHouse/build/programs/clickhouse /usr/bin/ sudo service clickhouse-server start  Note that clickhouse-client, clickhouse-server and others are symlinks to the commonly shared clickhouse binary. You can also run your custom-built ClickHouse binary with the config file from the ClickHouse package installed on your system: sudo service clickhouse-server stop sudo -u clickhouse ClickHouse/build/programs/clickhouse server --config-file /etc/clickhouse-server/config.xml  "},{"title":"IDE (Integrated Development Environment)​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"en/development/developer-instruction#ide-integrated-development-environment","content":"If you do not know which IDE to use, we recommend that you use CLion. CLion is commercial software, but it offers 30 days free trial period. It is also free of charge for students. CLion can be used both on Linux and on Mac OS X. KDevelop and QTCreator are other great alternatives of an IDE for developing ClickHouse. KDevelop comes in as a very handy IDE although unstable. If KDevelop crashes after a while upon opening project, you should click “Stop All” button as soon as it has opened the list of project’s files. After doing so KDevelop should be fine to work with. As simple code editors, you can use Sublime Text or Visual Studio Code, or Kate (all of which are available on Linux). Just in case, it is worth mentioning that CLion creates build path on its own, it also on its own selects debug for build type, for configuration it uses a version of CMake that is defined in CLion and not the one installed by you, and finally, CLion will use make to run build tasks instead of ninja. This is normal behaviour, just keep that in mind to avoid confusion. "},{"title":"Writing Code​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"en/development/developer-instruction#writing-code","content":"The description of ClickHouse architecture can be found here: https://clickhouse.com/docs/en/development/architecture/ The Code Style Guide: https://clickhouse.com/docs/en/development/style/ Adding third-party libraries: https://clickhouse.com/docs/en/development/contrib/#adding-third-party-libraries Writing tests: https://clickhouse.com/docs/en/development/tests/ List of tasks: https://github.com/ClickHouse/ClickHouse/issues?q=is%3Aopen+is%3Aissue+label%3Ahacktoberfest "},{"title":"Test Data​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"en/development/developer-instruction#test-data","content":"Developing ClickHouse often requires loading realistic datasets. It is particularly important for performance testing. We have a specially prepared set of anonymized data of web analytics. It requires additionally some 3GB of free disk space. Note that this data is not required to accomplish most of the development tasks. sudo apt install wget xz-utils wget https://datasets.clickhouse.com/hits/tsv/hits_v1.tsv.xz wget https://datasets.clickhouse.com/visits/tsv/visits_v1.tsv.xz xz -v -d hits_v1.tsv.xz xz -v -d visits_v1.tsv.xz clickhouse-client CREATE DATABASE IF NOT EXISTS test CREATE TABLE test.hits ( WatchID UInt64, JavaEnable UInt8, Title String, GoodEvent Int16, EventTime DateTime, EventDate Date, CounterID UInt32, ClientIP UInt32, ClientIP6 FixedString(16), RegionID UInt32, UserID UInt64, CounterClass Int8, OS UInt8, UserAgent UInt8, URL String, Referer String, URLDomain String, RefererDomain String, Refresh UInt8, IsRobot UInt8, RefererCategories Array(UInt16), URLCategories Array(UInt16), URLRegions Array(UInt32), RefererRegions Array(UInt32), ResolutionWidth UInt16, ResolutionHeight UInt16, ResolutionDepth UInt8, FlashMajor UInt8, FlashMinor UInt8, FlashMinor2 String, NetMajor UInt8, NetMinor UInt8, UserAgentMajor UInt16, UserAgentMinor FixedString(2), CookieEnable UInt8, JavascriptEnable UInt8, IsMobile UInt8, MobilePhone UInt8, MobilePhoneModel String, Params String, IPNetworkID UInt32, TraficSourceID Int8, SearchEngineID UInt16, SearchPhrase String, AdvEngineID UInt8, IsArtifical UInt8, WindowClientWidth UInt16, WindowClientHeight UInt16, ClientTimeZone Int16, ClientEventTime DateTime, SilverlightVersion1 UInt8, SilverlightVersion2 UInt8, SilverlightVersion3 UInt32, SilverlightVersion4 UInt16, PageCharset String, CodeVersion UInt32, IsLink UInt8, IsDownload UInt8, IsNotBounce UInt8, FUniqID UInt64, HID UInt32, IsOldCounter UInt8, IsEvent UInt8, IsParameter UInt8, DontCountHits UInt8, WithHash UInt8, HitColor FixedString(1), UTCEventTime DateTime, Age UInt8, Sex UInt8, Income UInt8, Interests UInt16, Robotness UInt8, GeneralInterests Array(UInt16), RemoteIP UInt32, RemoteIP6 FixedString(16), WindowName Int32, OpenerName Int32, HistoryLength Int16, BrowserLanguage FixedString(2), BrowserCountry FixedString(2), SocialNetwork String, SocialAction String, HTTPError UInt16, SendTiming Int32, DNSTiming Int32, ConnectTiming Int32, ResponseStartTiming Int32, ResponseEndTiming Int32, FetchTiming Int32, RedirectTiming Int32, DOMInteractiveTiming Int32, DOMContentLoadedTiming Int32, DOMCompleteTiming Int32, LoadEventStartTiming Int32, LoadEventEndTiming Int32, NSToDOMContentLoadedTiming Int32, FirstPaintTiming Int32, RedirectCount Int8, SocialSourceNetworkID UInt8, SocialSourcePage String, ParamPrice Int64, ParamOrderID String, ParamCurrency FixedString(3), ParamCurrencyID UInt16, GoalsReached Array(UInt32), OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID UInt8, RefererHash UInt64, URLHash UInt64, CLID UInt32, YCLID UInt64, ShareService String, ShareURL String, ShareTitle String, `ParsedParams.Key1` Array(String), `ParsedParams.Key2` Array(String), `ParsedParams.Key3` Array(String), `ParsedParams.Key4` Array(String), `ParsedParams.Key5` Array(String), `ParsedParams.ValueDouble` Array(Float64), IslandID FixedString(16), RequestNum UInt32, RequestTry UInt8) ENGINE = MergeTree PARTITION BY toYYYYMM(EventDate) SAMPLE BY intHash32(UserID) ORDER BY (CounterID, EventDate, intHash32(UserID), EventTime); CREATE TABLE test.visits ( CounterID UInt32, StartDate Date, Sign Int8, IsNew UInt8, VisitID UInt64, UserID UInt64, StartTime DateTime, Duration UInt32, UTCStartTime DateTime, PageViews Int32, Hits Int32, IsBounce UInt8, Referer String, StartURL String, RefererDomain String, StartURLDomain String, EndURL String, LinkURL String, IsDownload UInt8, TraficSourceID Int8, SearchEngineID UInt16, SearchPhrase String, AdvEngineID UInt8, PlaceID Int32, RefererCategories Array(UInt16), URLCategories Array(UInt16), URLRegions Array(UInt32), RefererRegions Array(UInt32), IsYandex UInt8, GoalReachesDepth Int32, GoalReachesURL Int32, GoalReachesAny Int32, SocialSourceNetworkID UInt8, SocialSourcePage String, MobilePhoneModel String, ClientEventTime DateTime, RegionID UInt32, ClientIP UInt32, ClientIP6 FixedString(16), RemoteIP UInt32, RemoteIP6 FixedString(16), IPNetworkID UInt32, SilverlightVersion3 UInt32, CodeVersion UInt32, ResolutionWidth UInt16, ResolutionHeight UInt16, UserAgentMajor UInt16, UserAgentMinor UInt16, WindowClientWidth UInt16, WindowClientHeight UInt16, SilverlightVersion2 UInt8, SilverlightVersion4 UInt16, FlashVersion3 UInt16, FlashVersion4 UInt16, ClientTimeZone Int16, OS UInt8, UserAgent UInt8, ResolutionDepth UInt8, FlashMajor UInt8, FlashMinor UInt8, NetMajor UInt8, NetMinor UInt8, MobilePhone UInt8, SilverlightVersion1 UInt8, Age UInt8, Sex UInt8, Income UInt8, JavaEnable UInt8, CookieEnable UInt8, JavascriptEnable UInt8, IsMobile UInt8, BrowserLanguage UInt16, BrowserCountry UInt16, Interests UInt16, Robotness UInt8, GeneralInterests Array(UInt16), Params Array(String), `Goals.ID` Array(UInt32), `Goals.Serial` Array(UInt32), `Goals.EventTime` Array(DateTime), `Goals.Price` Array(Int64), `Goals.OrderID` Array(String), `Goals.CurrencyID` Array(UInt32), WatchIDs Array(UInt64), ParamSumPrice Int64, ParamCurrency FixedString(3), ParamCurrencyID UInt16, ClickLogID UInt64, ClickEventID Int32, ClickGoodEvent Int32, ClickEventTime DateTime, ClickPriorityID Int32, ClickPhraseID Int32, ClickPageID Int32, ClickPlaceID Int32, ClickTypeID Int32, ClickResourceID Int32, ClickCost UInt32, ClickClientIP UInt32, ClickDomainID UInt32, ClickURL String, ClickAttempt UInt8, ClickOrderID UInt32, ClickBannerID UInt32, ClickMarketCategoryID UInt32, ClickMarketPP UInt32, ClickMarketCategoryName String, ClickMarketPPName String, ClickAWAPSCampaignName String, ClickPageName String, ClickTargetType UInt16, ClickTargetPhraseID UInt64, ClickContextType UInt8, ClickSelectType Int8, ClickOptions String, ClickGroupBannerID Int32, OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID UInt8, FirstVisit DateTime, PredLastVisit Date, LastVisit Date, TotalVisits UInt32, `TraficSource.ID` Array(Int8), `TraficSource.SearchEngineID` Array(UInt16), `TraficSource.AdvEngineID` Array(UInt8), `TraficSource.PlaceID` Array(UInt16), `TraficSource.SocialSourceNetworkID` Array(UInt8), `TraficSource.Domain` Array(String), `TraficSource.SearchPhrase` Array(String), `TraficSource.SocialSourcePage` Array(String), Attendance FixedString(16), CLID UInt32, YCLID UInt64, NormalizedRefererHash UInt64, SearchPhraseHash UInt64, RefererDomainHash UInt64, NormalizedStartURLHash UInt64, StartURLDomainHash UInt64, NormalizedEndURLHash UInt64, TopLevelDomain UInt64, URLScheme UInt64, OpenstatServiceNameHash UInt64, OpenstatCampaignIDHash UInt64, OpenstatAdIDHash UInt64, OpenstatSourceIDHash UInt64, UTMSourceHash UInt64, UTMMediumHash UInt64, UTMCampaignHash UInt64, UTMContentHash UInt64, UTMTermHash UInt64, FromHash UInt64, WebVisorEnabled UInt8, WebVisorActivity UInt32, `ParsedParams.Key1` Array(String), `ParsedParams.Key2` Array(String), `ParsedParams.Key3` Array(String), `ParsedParams.Key4` Array(String), `ParsedParams.Key5` Array(String), `ParsedParams.ValueDouble` Array(Float64), `Market.Type` Array(UInt8), `Market.GoalID` Array(UInt32), `Market.OrderID` Array(String), `Market.OrderPrice` Array(Int64), `Market.PP` Array(UInt32), `Market.DirectPlaceID` Array(UInt32), `Market.DirectOrderID` Array(UInt32), `Market.DirectBannerID` Array(UInt32), `Market.GoodID` Array(String), `Market.GoodName` Array(String), `Market.GoodQuantity` Array(Int32), `Market.GoodPrice` Array(Int64), IslandID FixedString(16)) ENGINE = CollapsingMergeTree(Sign) PARTITION BY toYYYYMM(StartDate) SAMPLE BY intHash32(UserID) ORDER BY (CounterID, StartDate, intHash32(UserID), VisitID); clickhouse-client --max_insert_block_size 100000 --query &quot;INSERT INTO test.hits FORMAT TSV&quot; &lt; hits_v1.tsv clickhouse-client --max_insert_block_size 100000 --query &quot;INSERT INTO test.visits FORMAT TSV&quot; &lt; visits_v1.tsv  "},{"title":"Creating Pull Request​","type":1,"pageTitle":"Getting Started Guide for Building ClickHouse","url":"en/development/developer-instruction#creating-pull-request","content":"Navigate to your fork repository in GitHub’s UI. If you have been developing in a branch, you need to select that branch. There will be a “Pull request” button located on the screen. In essence, this means “create a request for accepting my changes into the main repository”. A pull request can be created even if the work is not completed yet. In this case please put the word “WIP” (work in progress) at the beginning of the title, it can be changed later. This is useful for cooperative reviewing and discussion of changes as well as for running all of the available tests. It is important that you provide a brief description of your changes, it will later be used for generating release changelogs. Testing will commence as soon as ClickHouse employees label your PR with a tag “can be tested”. The results of some first checks (e.g. code style) will come in within several minutes. Build check results will arrive within half an hour. And the main set of tests will report itself within an hour. The system will prepare ClickHouse binary builds for your pull request individually. To retrieve these builds click the “Details” link next to “ClickHouse build check” entry in the list of checks. There you will find direct links to the built .deb packages of ClickHouse which you can deploy even on your production servers (if you have no fear). Most probably some of the builds will fail at first times. This is due to the fact that we check builds both with gcc as well as with clang, with almost all of existing warnings (always with the -Werror flag) enabled for clang. On that same page, you can find all of the build logs so that you do not have to build ClickHouse in all of the possible ways. "},{"title":"ExternalDistributed","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/ExternalDistributed","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"ExternalDistributed","url":"en/engines/table-engines/integrations/ExternalDistributed#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ... ) ENGINE = ExternalDistributed('engine', 'host:port', 'database', 'table', 'user', 'password');  See a detailed description of the CREATE TABLE query. The table structure can differ from the original table structure: Column names should be the same as in the original table, but you can use just some of these columns and in any order.Column types may differ from those in the original table. ClickHouse tries to cast values to the ClickHouse data types. Engine Parameters engine — The table engine MySQL or PostgreSQL.host:port — MySQL or PostgreSQL server address.database — Remote database name.table — Remote table name.user — User name.password — User password. "},{"title":"Implementation Details​","type":1,"pageTitle":"ExternalDistributed","url":"en/engines/table-engines/integrations/ExternalDistributed#implementation-details","content":"Supports multiple replicas that must be listed by | and shards must be listed by ,. For example: CREATE TABLE test_shards (id UInt32, name String, age UInt32, money UInt32) ENGINE = ExternalDistributed('MySQL', `mysql{1|2}:3306,mysql{3|4}:3306`, 'clickhouse', 'test_replicas', 'root', 'clickhouse');  When specifying replicas, one of the available replicas is selected for each of the shards when reading. If the connection fails, the next replica is selected, and so on for all the replicas. If the connection attempt fails for all the replicas, the attempt is repeated the same way several times. You can specify any number of shards and any number of replicas for each shard. See Also MySQL table enginePostgreSQL table engineDistributed table engine Original article "},{"title":"HDFS","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/hdfs","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"HDFS","url":"en/engines/table-engines/integrations/hdfs#usage","content":"ENGINE = HDFS(URI, format)  Engine Parameters URI - whole file URI in HDFS. The path part of URI may contain globs. In this case the table would be readonly.format - specifies one of the available file formats. To performSELECT queries, the format must be supported for input, and to performINSERT queries – for output. The available formats are listed in theFormats section. Example: 1. Set up the hdfs_engine_table table: CREATE TABLE hdfs_engine_table (name String, value UInt32) ENGINE=HDFS('hdfs://hdfs1:9000/other_storage', 'TSV')  2. Fill file: INSERT INTO hdfs_engine_table VALUES ('one', 1), ('two', 2), ('three', 3)  3. Query the data: SELECT * FROM hdfs_engine_table LIMIT 2  ┌─name─┬─value─┐ │ one │ 1 │ │ two │ 2 │ └──────┴───────┘  "},{"title":"Implementation Details​","type":1,"pageTitle":"HDFS","url":"en/engines/table-engines/integrations/hdfs#implementation-details","content":"Reads and writes can be parallel.Zero-copy replication is supported. Not supported: ALTER and SELECT...SAMPLE operations.Indexes. Globs in path Multiple path components can have globs. For being processed file should exists and matches to the whole path pattern. Listing of files determines during SELECT (not at CREATE moment). * — Substitutes any number of any characters except / including empty string.? — Substitutes any single character.{some_string,another_string,yet_another_one} — Substitutes any of strings 'some_string', 'another_string', 'yet_another_one'.{N..M} — Substitutes any number in range from N to M including both borders. Constructions with {} are similar to the remote table function. Example Suppose we have several files in TSV format with the following URIs on HDFS: 'hdfs://hdfs1:9000/some_dir/some_file_1''hdfs://hdfs1:9000/some_dir/some_file_2''hdfs://hdfs1:9000/some_dir/some_file_3''hdfs://hdfs1:9000/another_dir/some_file_1''hdfs://hdfs1:9000/another_dir/some_file_2''hdfs://hdfs1:9000/another_dir/some_file_3' There are several ways to make a table consisting of all six files: CREATE TABLE table_with_range (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/some_file_{1..3}', 'TSV')  Another way: CREATE TABLE table_with_question_mark (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/some_file_?', 'TSV')  Table consists of all the files in both directories (all files should satisfy format and schema described in query): CREATE TABLE table_with_asterisk (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/{some,another}_dir/*', 'TSV')  warning If the listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?. Example Create table with files named file000, file001, … , file999: CREATE TABLE big_table (name String, value UInt32) ENGINE = HDFS('hdfs://hdfs1:9000/big_dir/file{0..9}{0..9}{0..9}', 'CSV')  "},{"title":"Configuration​","type":1,"pageTitle":"HDFS","url":"en/engines/table-engines/integrations/hdfs#configuration","content":"Similar to GraphiteMergeTree, the HDFS engine supports extended configuration using the ClickHouse config file. There are two configuration keys that you can use: global (hdfs) and user-level (hdfs_*). The global configuration is applied first, and then the user-level configuration is applied (if it exists).  &lt;!-- Global configuration options for HDFS engine type --&gt; &lt;hdfs&gt; &lt;hadoop_kerberos_keytab&gt;/tmp/keytab/clickhouse.keytab&lt;/hadoop_kerberos_keytab&gt; &lt;hadoop_kerberos_principal&gt;clickuser@TEST.CLICKHOUSE.TECH&lt;/hadoop_kerberos_principal&gt; &lt;hadoop_security_authentication&gt;kerberos&lt;/hadoop_security_authentication&gt; &lt;/hdfs&gt; &lt;!-- Configuration specific for user &quot;root&quot; --&gt; &lt;hdfs_root&gt; &lt;hadoop_kerberos_principal&gt;root@TEST.CLICKHOUSE.TECH&lt;/hadoop_kerberos_principal&gt; &lt;/hdfs_root&gt;  "},{"title":"Configuration Options​","type":1,"pageTitle":"HDFS","url":"en/engines/table-engines/integrations/hdfs#configuration-options","content":"Supported by libhdfs3​ parameter\tdefault valuerpc_client_connect_tcpnodelay\ttrue dfs_client_read_shortcircuit\ttrue output_replace-datanode-on-failure\ttrue input_notretry-another-node\tfalse input_localread_mappedfile\ttrue dfs_client_use_legacy_blockreader_local\tfalse rpc_client_ping_interval\t10 * 1000 rpc_client_connect_timeout\t600 * 1000 rpc_client_read_timeout\t3600 * 1000 rpc_client_write_timeout\t3600 * 1000 rpc_client_socekt_linger_timeout\t-1 rpc_client_connect_retry\t10 rpc_client_timeout\t3600 * 1000 dfs_default_replica\t3 input_connect_timeout\t600 * 1000 input_read_timeout\t3600 * 1000 input_write_timeout\t3600 * 1000 input_localread_default_buffersize\t1 1024 1024 dfs_prefetchsize\t10 input_read_getblockinfo_retry\t3 input_localread_blockinfo_cachesize\t1000 input_read_max_retry\t60 output_default_chunksize\t512 output_default_packetsize\t64 * 1024 output_default_write_retry\t10 output_connect_timeout\t600 * 1000 output_read_timeout\t3600 * 1000 output_write_timeout\t3600 * 1000 output_close_timeout\t3600 * 1000 output_packetpool_size\t1024 output_heeartbeat_interval\t10 * 1000 dfs_client_failover_max_attempts\t15 dfs_client_read_shortcircuit_streams_cache_size\t256 dfs_client_socketcache_expiryMsec\t3000 dfs_client_socketcache_capacity\t16 dfs_default_blocksize\t64 1024 1024 dfs_default_uri\t&quot;hdfs://localhost:9000&quot; hadoop_security_authentication\t&quot;simple&quot; hadoop_security_kerberos_ticket_cache_path\t&quot;&quot; dfs_client_log_severity\t&quot;INFO&quot; dfs_domain_socket_path\t&quot;&quot; HDFS Configuration Reference might explain some parameters. ClickHouse extras​ parameter\tdefault valuehadoop_kerberos_keytab\t&quot;&quot; hadoop_kerberos_principal\t&quot;&quot; hadoop_kerberos_kinit_command\tkinit libhdfs3_conf\t&quot;&quot; "},{"title":"Limitations​","type":1,"pageTitle":"HDFS","url":"en/engines/table-engines/integrations/hdfs#limitations","content":"hadoop_security_kerberos_ticket_cache_path and libhdfs3_conf can be global only, not user specific "},{"title":"Kerberos support​","type":1,"pageTitle":"HDFS","url":"en/engines/table-engines/integrations/hdfs#kerberos-support","content":"If the hadoop_security_authentication parameter has the value kerberos, ClickHouse authenticates via Kerberos. Parameters are here and hadoop_security_kerberos_ticket_cache_path may be of help. Note that due to libhdfs3 limitations only old-fashioned approach is supported, datanode communications are not secured by SASL (HADOOP_SECURE_DN_USER is a reliable indicator of such security approach). Use tests/integration/test_storage_kerberized_hdfs/hdfs_configs/bootstrap.sh for reference. If hadoop_kerberos_keytab, hadoop_kerberos_principal or hadoop_kerberos_kinit_command is specified, kinit will be invoked. hadoop_kerberos_keytab and hadoop_kerberos_principal are mandatory in this case. kinit tool and krb5 configuration files are required. "},{"title":"HDFS Namenode HA support​","type":1,"pageTitle":"HDFS","url":"en/engines/table-engines/integrations/hdfs#namenode-ha","content":"libhdfs3 support HDFS namenode HA. Copy hdfs-site.xml from an HDFS node to /etc/clickhouse-server/.Add following piece to ClickHouse config file:  &lt;hdfs&gt; &lt;libhdfs3_conf&gt;/etc/clickhouse-server/hdfs-site.xml&lt;/libhdfs3_conf&gt; &lt;/hdfs&gt;  Then use dfs.nameservices tag value of hdfs-site.xml as the namenode address in the HDFS URI. For example, replace hdfs://appadmin@192.168.101.11:8020/abc/ with hdfs://appadmin@my_nameservice/abc/. "},{"title":"Virtual Columns​","type":1,"pageTitle":"HDFS","url":"en/engines/table-engines/integrations/hdfs#virtual-columns","content":"_path — Path to the file._file — Name of the file. See Also Virtual columns Original article "},{"title":"ClickHouse Adopters","type":0,"sectionRef":"#","url":"en/about-us/adopters","content":"ClickHouse Adopters Disclaimer The following list of companies using ClickHouse and their success stories is assembled from public sources, thus might differ from current reality. We’d appreciate it if you share the story of adopting ClickHouse in your company and add it to the list, but please make sure you won’t have any NDA issues by doing so. Providing updates with publications from other companies is also useful. Company\tIndustry\tUsecase\tCluster Size\t(Un)Compressed Data Size*\tReference2gis\tMaps\tMonitoring\t—\t—\tTalk in Russian, July 2019 Adapty\tSubscription Analytics\tMain product\t—\t—\tTweet, November 2021 Admiral\tMartech\tEngagement Management\t—\t—\tWebinar Slides, June 2020 AdScribe\tAds\tTV Analytics\t—\t—\tA quote from CTO Ahrefs\tSEO\tAnalytics\t—\t—\tJob listing Alibaba Cloud\tCloud\tManaged Service\t—\t—\tOfficial Website Alibaba Cloud\tCloud\tE-MapReduce\t—\t—\tOfficial Website Aloha Browser\tMobile App\tBrowser backend\t—\t—\tSlides in Russian, May 2019 Altinity\tCloud, SaaS\tMain product\t—\t—\tOfficial Website Amadeus\tTravel\tAnalytics\t—\t—\tPress Release, April 2018 ApiRoad\tAPI marketplace\tAnalytics\t—\t—\tBlog post, November 2018, March 2020 Appsflyer\tMobile analytics\tMain product\t—\t—\tTalk in Russian, July 2019 ArenaData\tData Platform\tMain product\t—\t—\tSlides in Russian, December 2019 Argedor\tClickHouse support\t—\t—\t—\tOfficial website Avito\tClassifieds\tMonitoring\t—\t—\tMeetup, April 2020 Badoo\tDating\tTimeseries\t—\t1.6 mln events/sec (2018)\tSlides in Russian, December 2019 Beeline\tTelecom\tData Platform\t—\t—\tBlog post, July 2021 Benocs\tNetwork Telemetry and Analytics\tMain Product\t—\t—\tSlides in English, October 2017 Better Stack\tCloud, SaaS\tLog Management\t-\t-\tOfficial Website BIGO\tVideo\tComputing Platform\t—\t—\tBlog Article, August 2020 BiliBili\tVideo sharing\t—\t—\t—\tBlog post, June 2021 Bloomberg\tFinance, Media\tMonitoring\t—\t—\tJob opening, September 2021, slides, May 2018 Bloxy\tBlockchain\tAnalytics\t—\t—\tSlides in Russian, August 2018 Bytedance\tSocial platforms\t—\t—\t—\tThe ClickHouse Meetup East, October 2020 CardsMobile\tFinance\tAnalytics\t—\t—\tVC.ru CARTO\tBusiness Intelligence\tGeo analytics\t—\t—\tGeospatial processing with ClickHouse CERN\tResearch\tExperiment\t—\t—\tPress release, April 2012 Checkly\tSoftware Development\tAnalytics\t—\t—\tTweet, October 2021 ChelPipe Group\tAnalytics\t—\t—\t—\tBlog post, June 2021 Cisco\tNetworking\tTraffic analysis\t—\t—\tLightning talk, October 2019 Citadel Securities\tFinance\t—\t—\t—\tContribution, March 2019 Citymobil\tTaxi\tAnalytics\t—\t—\tBlog Post in Russian, March 2020 Cloudflare\tCDN\tTraffic analysis\t36 servers\t—\tBlog post, May 2017, Blog post, March 2018 Comcast\tMedia\tCDN Traffic Analysis\t—\t—\tApacheCon 2019 Talk Contentsquare\tWeb analytics\tMain product\t—\t—\tBlog post in French, November 2018 Corunet\tAnalytics\tMain product\t—\t—\tSlides in English, April 2019 CraiditX 氪信\tFinance AI\tAnalysis\t—\t—\tSlides in English, November 2019 Crazypanda\tGames —\t—\tLive session on ClickHouse meetup Criteo\tRetail\tMain product\t—\t—\tSlides in English, October 2018 Cryptology\tDigital Assets Trading Platform\t—\t—\t—\tJob advertisement, March 2021 Datafold\tData Reliability Platform\t—\t—\t—\tJob advertisement, April 2022 Dataliance for China Telecom\tTelecom\tAnalytics\t—\t—\tSlides in Chinese, January 2018 Deutsche Bank\tFinance\tBI Analytics\t—\t—\tSlides in English, October 2019 Deepl\tMachine Learning\t—\t—\t—\tVideo, October 2021 Deeplay\tGaming Analytics\t—\t—\t—\tJob advertisement, 2020 Diva-e\tDigital consulting\tMain Product\t—\t—\tSlides in English, September 2019 Ecommpay\tPayment Processing\tLogs\t—\t—\tVideo, Nov 2019 Ecwid\tE-commerce SaaS\tMetrics, Logging\t—\t—\tSlides in Russian, April 2019 eBay\tE-commerce\tLogs, Metrics and Events\t—\t—\tOfficial website, Sep 2020 Exness\tTrading\tMetrics, Logging\t—\t—\tTalk in Russian, May 2019 EventBunker.io\tServerless Data Processing\t—\t—\t—\tTweet, April 2021 FastNetMon\tDDoS Protection\tMain Product —\tOfficial website Firebolt\tAnalytics\tMain product\t-\t-\tYouTube Tech Talk Flipkart\te-Commerce\t—\t—\t—\tTalk in English, July 2020 FunCorp\tGames —\t14 bn records/day as of Jan 2021\tArticle Futurra Group\tAnalytics\t—\t—\t—\tArticle in Russian, December 2021 Geniee\tAd network\tMain product\t—\t—\tBlog post in Japanese, July 2017 Genotek\tBioinformatics\tMain product\t—\t—\tVideo, August 2020 Gigapipe\tManaged ClickHouse\tMain product\t—\t—\tOfficial website Gigasheet\tAnalytics\tMain product\t—\t—\tDirect Reference, February 2022 Glaber\tMonitoring\tMain product\t—\t—\tWebsite GraphCDN\tCDN\tTraffic Analytics\t—\t—\tBlog Post in English, August 2021 Grouparoo\tData Warehouse Integrations\tMain product\t—\t—\tOfficial Website, November 2021 HUYA\tVideo Streaming\tAnalytics\t—\t—\tSlides in Chinese, October 2018 Hydrolix\tCloud data platform\tMain product\t—\t—\tDocumentation Hystax\tCloud Operations\tObservability Analytics\t-\t-\tBlog ICA\tFinTech\tRisk Management\t—\t—\tBlog Post in English, Sep 2020 Idealista\tReal Estate\tAnalytics\t—\t—\tBlog Post in English, April 2019 Improvado\tRevenue Operations\tData Stack\t—\t—\tBlog Post, December 2021 Infobaleen\tAI markting tool\tAnalytics\t—\t—\tOfficial site Infovista\tNetworks\tAnalytics\t—\t—\tSlides in English, October 2019 InnoGames\tGames\tMetrics, Logging\t—\t—\tSlides in Russian, September 2019 Instabug\tAPM Platform\tMain product\t—\t—\tA quote from Co-Founder Instana\tAPM Platform\tMain product\t—\t—\tTwitter post Integros\tPlatform for video services\tAnalytics\t—\t—\tSlides in Russian, May 2019 Ippon Technologies\tTechnology Consulting\t—\t—\t—\tTalk in English, July 2020 Ivi\tOnline Cinema\tAnalytics, Monitoring\t—\t—\tArticle in Russian, Jan 2018 Jinshuju 金数据\tBI Analytics\tMain product\t—\t—\tSlides in Chinese, October 2019 Jitsu\tCloud Software\tData Pipeline\t—\t—\tDocumentation, Hacker News post JuiceFS\tStorage\tShopping Cart\t-\t-\tBlog June\tProduct analytics\tMain product\t-\t-\tJob post kakaocorp\tInternet company\t—\t—\t—\tif(kakao)2020, if(kakao)2021 Kaiko\tDigital Assets Data Provider\t—\t—\t—\tJob advertisement, April 2022 Kodiak Data\tClouds\tMain product\t—\t—\tSlides in Engish, April 2018 Kontur\tSoftware Development\tMetrics\t—\t—\tTalk in Russian, November 2018 Kuaishou\tVideo\t—\t—\t—\tClickHouse Meetup, October 2018 KGK Global\tVehicle monitoring\t—\t—\t—\tPress release, June 2021 LANCOM Systems\tNetwork Solutions\tTraffic analysis\t-\t-\tClickHouse Operator for Kubernetes, [Hacker News post] (https://news.ycombinator.com/item?id=29413660) Lawrence Berkeley National Laboratory\tResearch\tTraffic analysis\t5 servers\t55 TiB\tSlides in English, April 2019 Lever\tTalent Management\tRecruiting\t-\t-\tHacker News post LifeStreet\tAd network\tMain product\t75 servers (3 replicas)\t5.27 PiB\tBlog post in Russian, February 2017 Lookforsale\tE-Commerce\t—\t—\t—\tJob Posting, December 2021 Luabase\tSoftware\tAnalytics\t—\t—\tHacker News, April 2022 Mail.ru Cloud Solutions\tCloud services\tMain product\t—\t—\tArticle in Russian Marfeel\tMobile SaaS\t—\t—\t—\t[Job offer, Apr 2022] MAXILECT\tAd Tech, Blockchain, ML, AI\t—\t—\t—\tJob advertisement, 2021 Marilyn\tAdvertising\tStatistics\t—\t—\tTalk in Russian, June 2017 Mello\tMarketing\tAnalytics\t1 server\t—\tArticle, October 2020 MessageBird\tTelecommunications\tStatistics\t—\t—\tSlides in English, November 2018 Microsoft\tWeb Analytics\tClarity (Main Product)\t—\t—\tA question on GitHub MindsDB\tMachine Learning\tMain Product\t—\t—\tOfficial Website MUX\tOnline Video\tVideo Analytics\t—\t—\tTalk in English, August 2019 MGID\tAd network\tWeb-analytics\t—\t—\tBlog post in Russian, April 2020 Muse Group\tMusic Software\tPerformance Monitoring\t—\t—\tBlog post in Russian, January 2021 Netskope\tNetwork Security\t—\t—\t—\tJob advertisement, March 2021 NIC Labs\tNetwork Monitoring\tRaTA-DNS\t—\t—\tBlog post, March 2021 NLMK\tSteel\tMonitoring\t—\t—\tArticle in Russian, Jan 2022 NOC Project\tNetwork Monitoring\tAnalytics\tMain Product\t—\tOfficial Website Noction\tNetwork Technology\tMain Product\t—\t—\tOfficial Website ntop\tNetwork Monitoning\tMonitoring\t—\t—\tOfficial website, January 2022 Nuna Inc.\tHealth Data Analytics\t—\t—\t—\tTalk in English, July 2020 Ok.ru\tSocial Network\t—\t72 servers\t810 TB compressed, 50bn rows/day, 1.5 TB/day\tSmartData conference, October 2021 Omnicomm\tTransportation Monitoring\t—\t—\t—\tFacebook post, October 2021 OneAPM\tMonitoring and Data Analysis\tMain product\t—\t—\tSlides in Chinese, October 2018 Opensee\tFinancial Analytics\tMain product\t-\t-\tBlog Open Targets\tGenome Research\tGenome Search\t—\t—\tTweet, October 2021, Blog OZON\tE-commerce\t—\t—\t—\tOfficial website Panelbear\tAnalytics\tMonitoring and Analytics\t—\t—\tTech Stack, November 2020 Percent 百分点\tAnalytics\tMain Product\t—\t—\tSlides in Chinese, June 2019 Percona\tPerformance analysis\tPercona Monitoring and Management\t—\t—\tOfficial website, Mar 2020 PingCAP\tAnalytics\tReal-Time Transactional and Analytical Processing\t-\t-\tGitHub, TiFlash/TiDB Piwik PRO\tWeb Analytics\t—\t—\t—\tOfficial website, Dec 2018 Plausible\tAnalytics\tMain Product\t—\t—\tBlog post, June 2020 PostHog\tProduct Analytics\tMain Product\t—\t—\tRelease Notes, October 2020, Blog, November 2021 Postmates\tDelivery\t—\t—\t—\tTalk in English, July 2020 Pragma Innovation\tTelemetry and Big Data Analysis\tMain product\t—\t—\tSlides in English, October 2018 PRANA\tIndustrial predictive analytics\tMain product\t—\t—\tNews (russian), Feb 2021 QINGCLOUD\tCloud services\tMain product\t—\t—\tSlides in Chinese, October 2018 Qrator\tDDoS protection\tMain product\t—\t—\tBlog Post, March 2019 R-Vision\tInformation Security\t—\t—\t—\tArticle in Russian, December 2021 Raiffeisenbank\tBanking\tAnalytics\t—\t—\tLecture in Russian, December 2020 Rambler\tInternet services\tAnalytics\t—\t—\tTalk in Russian, April 2018 Replica\tUrban Planning\tAnalytics\t—\t—\tJob advertisement Retell\tSpeech synthesis\tAnalytics\t—\t—\tBlog Article, August 2020 Rollbar\tSoftware Development\tMain Product\t—\t—\tOfficial Website Rspamd\tAntispam\tAnalytics\t—\t—\tOfficial Website RuSIEM\tSIEM\tMain Product\t—\t—\tOfficial Website S7 Airlines\tAirlines\tMetrics, Logging\t—\t—\tTalk in Russian, March 2019 Sber\tBanking, Fintech, Retail, Cloud, Media\t—\t128 servers\t&gt;1 PB\tJob advertisement, March 2021 scireum GmbH\te-Commerce\tMain product\t—\t—\tTalk in German, February 2020 Segment\tData processing\tMain product\t9 * i3en.3xlarge nodes 7.5TB NVME SSDs, 96GB Memory, 12 vCPUs\t—\tSlides, 2019 sembot.io\tShopping Ads\t—\t—\t—\tA comment on LinkedIn, 2020 SEMrush\tMarketing\tMain product\t—\t—\tSlides in Russian, August 2018 Sentry\tSoftware Development\tMain product\t—\t—\tBlog Post in English, May 2019 seo.do\tAnalytics\tMain product\t—\t—\tSlides in English, November 2019 SGK\tGovernment Social Security\tAnalytics\t—\t—\tSlides in English, November 2019 SigNoz\tObservability Platform\tMain Product\t—\t—\tSource code Sina\tNews\t—\t—\t—\tSlides in Chinese, October 2018 Sipfront\tSoftware Development\tAnalytics\t—\t—\tTweet, October 2021 SMI2\tNews\tAnalytics\t—\t—\tBlog Post in Russian, November 2017 Spark New Zealand\tTelecommunications\tSecurity Operations\t—\t—\tBlog Post, Feb 2020 Splitbee\tAnalytics\tMain Product\t—\t—\tBlog Post, Mai 2021 Splunk\tBusiness Analytics\tMain product\t—\t—\tSlides in English, January 2018 Spotify\tMusic\tExperimentation\t—\t—\tSlides, July 2018 Staffcop\tInformation Security\tMain Product\t—\t—\tOfficial website, Documentation Suning\tE-Commerce\tUser behaviour analytics\t—\t—\tBlog article Superwall\tMonetization Tooling\tMain product\t—\t—\tWord of mouth, Jan 2022 Swetrix\tAnalytics\tMain Product\t—\t—\tSource code Synpse\tApplication Management\tMain Product\t-\t-\tTweet, January 2022 Teralytics\tMobility\tAnalytics\t—\t—\tTech blog Tencent\tBig Data\tData processing\t—\t—\tSlides in Chinese, October 2018 Tencent\tMessaging\tLogging\t—\t—\tTalk in Chinese, November 2019 Tencent Music Entertainment (TME)\tBigData\tData processing\t—\t—\tBlog in Chinese, June 2020 Tesla\tElectric vehicle and clean energy company\t—\t—\t—\tVacancy description, March 2021 Timeflow\tSoftware\tAnalytics\t—\t—\tBlog Tinybird\tReal-time Data Products\tData processing\t—\t—\tOfficial website Traffic Stars\tAD network\t—\t300 servers in Europe/US\t1.8 PiB, 700 000 insert rps (as of 2021)\tSlides in Russian, May 2018 Uber\tTaxi\tLogging\t—\t—\tSlides, February 2020 Uptrace\tSoftware\tTracing Solution\t—\t—\tOfficial website, March 2021 UseTech\tSoftware Development\t—\t—\t—\tJob Posting, December 2021 UTMSTAT\tAnalytics\tMain product\t—\t—\tBlog post, June 2020 Vercel\tTraffic and Performance Analytics\t—\t—\t—\tDirect reference, October 2021 VKontakte\tSocial Network\tStatistics, Logging\t—\t—\tSlides in Russian, August 2018 VKontech\tDistributed Systems\tMigrating from MongoDB\t-\t-\tBlog, January 2022 VMware\tCloud\tVeloCloud, SDN\t—\t—\tProduct documentation Walmart Labs\tInternet, Retail\t—\t—\t—\tTalk in English, July 2020 Wargaming\tGames —\t—\tInterview Wildberries\tE-commerce —\t—\tOfficial website Wisebits\tIT Solutions\tAnalytics\t—\t—\tSlides in Russian, May 2019 Workato\tAutomation Software\t—\t—\t—\tTalk in English, July 2020 Xenoss\tMarketing, Advertising\t—\t—\t—\tInstagram, March 2021 Xiaoxin Tech\tEducation\tCommon purpose\t—\t—\tSlides in English, November 2019 Ximalaya\tAudio sharing\tOLAP\t—\t—\tSlides in English, November 2019 Yandex Cloud\tPublic Cloud\tMain product\t—\t—\tTalk in Russian, December 2019 Yandex DataLens\tBusiness Intelligence\tMain product\t—\t—\tSlides in Russian, December 2019 Yandex Market\te-Commerce\tMetrics, Logging\t—\t—\tTalk in Russian, January 2019 Yandex Metrica\tWeb analytics\tMain product\t630 servers in one cluster, 360 servers in another cluster, 1862 servers in one department\t133 PiB / 8.31 PiB / 120 trillion records\tSlides, February 2020 Yellowfin\tAnalytics\tMain product\t-\t-\tIntegration Yotascale\tCloud\tData pipeline\t—\t2 bn records/day\tLinkedIn (Accomplishments) Your Analytics\tProduct Analytics\tMain Product\t—\t-\tTweet, November 2021 Zagrava Trading\t—\t—\t—\t—\tJob offer, May 2021 ЦВТ\tSoftware Development\tMetrics, Logging\t—\t—\tBlog Post, March 2019, in Russian МКБ\tBank\tWeb-system monitoring\t—\t—\tSlides in Russian, September 2019 ЦФТ\tBanking, Financial products, Payments\t—\t—\t—\tMeetup in Russian, April 2020 Цифровой Рабочий\tIndustrial IoT, Analytics\t—\t—\t—\tBlog post in Russian, March 2021 ООО «МПЗ Богородский»\tAgriculture\t—\t—\t—\tArticle in Russian, November 2020 ДомКлик\tReal Estate\t—\t—\t—\tArticle in Russian, October 2021 АС &quot;Стрела&quot;\tTransportation\t—\t—\t—\tJob posting, Jan 2022 Deepglint 格灵深瞳\tAI, Computer Vision\tOLAP\t—\t—\tOfficial Website","keywords":""},{"title":"EmbeddedRocksDB Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/embedded-rocksdb","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"EmbeddedRocksDB Engine","url":"en/engines/table-engines/integrations/embedded-rocksdb#table_engine-EmbeddedRocksDB-creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = EmbeddedRocksDB PRIMARY KEY(primary_key_name)  Required parameters: primary_key_name – any column name in the column list.primary key must be specified, it supports only one column in the primary key. The primary key will be serialized in binary as a rocksdb key.columns other than the primary key will be serialized in binary as rocksdb value in corresponding order.queries with key equals or in filtering will be optimized to multi keys lookup from rocksdb. Example: CREATE TABLE test ( `key` String, `v1` UInt32, `v2` String, `v3` Float32 ) ENGINE = EmbeddedRocksDB PRIMARY KEY key  "},{"title":"Metrics​","type":1,"pageTitle":"EmbeddedRocksDB Engine","url":"en/engines/table-engines/integrations/embedded-rocksdb#metrics","content":"There is also system.rocksdb table, that expose rocksdb statistics: SELECT name, value FROM system.rocksdb ┌─name──────────────────────┬─value─┐ │ no.file.opens │ 1 │ │ number.block.decompressed │ 1 │ └───────────────────────────┴───────┘  "},{"title":"Configuration​","type":1,"pageTitle":"EmbeddedRocksDB Engine","url":"en/engines/table-engines/integrations/embedded-rocksdb#configuration","content":"You can also change any rocksdb options using config: &lt;rocksdb&gt; &lt;options&gt; &lt;max_background_jobs&gt;8&lt;/max_background_jobs&gt; &lt;/options&gt; &lt;column_family_options&gt; &lt;num_levels&gt;2&lt;/num_levels&gt; &lt;/column_family_options&gt; &lt;tables&gt; &lt;table&gt; &lt;name&gt;TABLE&lt;/name&gt; &lt;options&gt; &lt;max_background_jobs&gt;8&lt;/max_background_jobs&gt; &lt;/options&gt; &lt;column_family_options&gt; &lt;num_levels&gt;2&lt;/num_levels&gt; &lt;/column_family_options&gt; &lt;/table&gt; &lt;/tables&gt; &lt;/rocksdb&gt;  Original article "},{"title":"JDBC","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/jdbc","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"JDBC","url":"en/engines/table-engines/integrations/jdbc#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name ( columns list... ) ENGINE = JDBC(datasource_uri, external_database, external_table)  Engine Parameters datasource_uri — URI or name of an external DBMS. URI Format: jdbc:&lt;driver_name&gt;://&lt;host_name&gt;:&lt;port&gt;/?user=&lt;username&gt;&amp;password=&lt;password&gt;. Example for MySQL: jdbc:mysql://localhost:3306/?user=root&amp;password=root. external_database — Database in an external DBMS. external_table — Name of the table in external_database or a select query like select * from table1 where column1=1. "},{"title":"Usage Example​","type":1,"pageTitle":"JDBC","url":"en/engines/table-engines/integrations/jdbc#usage-example","content":"Creating a table in MySQL server by connecting directly with it’s console client: mysql&gt; CREATE TABLE `test`.`test` ( -&gt; `int_id` INT NOT NULL AUTO_INCREMENT, -&gt; `int_nullable` INT NULL DEFAULT NULL, -&gt; `float` FLOAT NOT NULL, -&gt; `float_nullable` FLOAT NULL DEFAULT NULL, -&gt; PRIMARY KEY (`int_id`)); Query OK, 0 rows affected (0,09 sec) mysql&gt; insert into test (`int_id`, `float`) VALUES (1,2); Query OK, 1 row affected (0,00 sec) mysql&gt; select * from test; +------+----------+-----+----------+ | int_id | int_nullable | float | float_nullable | +------+----------+-----+----------+ | 1 | NULL | 2 | NULL | +------+----------+-----+----------+ 1 row in set (0,00 sec)  Creating a table in ClickHouse server and selecting data from it: CREATE TABLE jdbc_table ( `int_id` Int32, `int_nullable` Nullable(Int32), `float` Float32, `float_nullable` Nullable(Float32) ) ENGINE JDBC('jdbc:mysql://localhost:3306/?user=root&amp;password=root', 'test', 'test')  SELECT * FROM jdbc_table  ┌─int_id─┬─int_nullable─┬─float─┬─float_nullable─┐ │ 1 │ ᴺᵁᴸᴸ │ 2 │ ᴺᵁᴸᴸ │ └────────┴──────────────┴───────┴────────────────┘  INSERT INTO jdbc_table(`int_id`, `float`) SELECT toInt32(number), toFloat32(number * 1.0) FROM system.numbers  "},{"title":"See Also​","type":1,"pageTitle":"JDBC","url":"en/engines/table-engines/integrations/jdbc#see-also","content":"JDBC table function. Original article "},{"title":"[experimental] MaterializedMySQL","type":0,"sectionRef":"#","url":"en/engines/database-engines/materialized-mysql","content":"","keywords":""},{"title":"Creating a Database​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"en/engines/database-engines/materialized-mysql#creating-a-database","content":"CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster] ENGINE = MaterializedMySQL('host:port', ['database' | database], 'user', 'password') [SETTINGS ...] [TABLE OVERRIDE table1 (...), TABLE OVERRIDE table2 (...)]  Engine Parameters host:port — MySQL server endpoint.database — MySQL database name.user — MySQL user.password — User password. Engine Settings max_rows_in_buffer — Maximum number of rows that data is allowed to cache in memory (for single table and the cache data unable to query). When this number is exceeded, the data will be materialized. Default: 65 505.max_bytes_in_buffer — Maximum number of bytes that data is allowed to cache in memory (for single table and the cache data unable to query). When this number is exceeded, the data will be materialized. Default: 1 048 576.max_flush_data_time — Maximum number of milliseconds that data is allowed to cache in memory (for database and the cache data unable to query). When this time is exceeded, the data will be materialized. Default: 1000.max_wait_time_when_mysql_unavailable — Retry interval when MySQL is not available (milliseconds). Negative value disables retry. Default: 1000.allows_query_when_mysql_lost — Allows to query a materialized table when MySQL is lost. Default: 0 (false).materialized_mysql_tables_list — a comma-separated list of mysql database tables, which will be replicated by MaterializedMySQL database engine. Default value: empty list — means whole tables will be replicated. CREATE DATABASE mysql ENGINE = MaterializedMySQL('localhost:3306', 'db', 'user', '***') SETTINGS allows_query_when_mysql_lost=true, max_wait_time_when_mysql_unavailable=10000;  Settings on MySQL-server Side For the correct work of MaterializedMySQL, there are few mandatory MySQL-side configuration settings that must be set: default_authentication_plugin = mysql_native_password since MaterializedMySQL can only authorize with this method.gtid_mode = on since GTID based logging is a mandatory for providing correct MaterializedMySQL replication. note While turning on gtid_mode you should also specify enforce_gtid_consistency = on. "},{"title":"Virtual Columns​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"en/engines/database-engines/materialized-mysql#virtual-columns","content":"When working with the MaterializedMySQL database engine, ReplacingMergeTree tables are used with virtual _sign and _version columns. _version — Transaction counter. Type UInt64._sign — Deletion mark. Type Int8. Possible values: 1 — Row is not deleted,-1 — Row is deleted. "},{"title":"Data Types Support​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"en/engines/database-engines/materialized-mysql#data_types-support","content":"MySQL\tClickHouseTINY\tInt8 SHORT\tInt16 INT24\tInt32 LONG\tUInt32 LONGLONG\tUInt64 FLOAT\tFloat32 DOUBLE\tFloat64 DECIMAL, NEWDECIMAL\tDecimal DATE, NEWDATE\tDate DATETIME, TIMESTAMP\tDateTime DATETIME2, TIMESTAMP2\tDateTime64 YEAR\tUInt16 TIME\tInt64 ENUM\tEnum STRING\tString VARCHAR, VAR_STRING\tString BLOB\tString GEOMETRY\tString BINARY\tFixedString BIT\tUInt64 SET\tUInt64 Nullable is supported. The data of TIME type in MySQL is converted to microseconds in ClickHouse. Other types are not supported. If MySQL table contains a column of such type, ClickHouse throws exception &quot;Unhandled data type&quot; and stops replication. "},{"title":"Specifics and Recommendations​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"en/engines/database-engines/materialized-mysql#specifics-and-recommendations","content":""},{"title":"Compatibility Restrictions​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"en/engines/database-engines/materialized-mysql#compatibility-restrictions","content":"Apart of the data types limitations there are few restrictions comparing to MySQL databases, that should be resolved before replication will be possible: Each table in MySQL should contain PRIMARY KEY. Replication for tables, those are containing rows with ENUM field values out of range (specified in ENUM signature) will not work. "},{"title":"DDL Queries​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"en/engines/database-engines/materialized-mysql#ddl-queries","content":"MySQL DDL queries are converted into the corresponding ClickHouse DDL queries (ALTER, CREATE, DROP, RENAME). If ClickHouse cannot parse some DDL query, the query is ignored. "},{"title":"Data Replication​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"en/engines/database-engines/materialized-mysql#data-replication","content":"MaterializedMySQL does not support direct INSERT, DELETE and UPDATE queries. However, they are supported in terms of data replication: MySQL INSERT query is converted into INSERT with _sign=1. MySQL DELETE query is converted into INSERT with _sign=-1. MySQL UPDATE query is converted into INSERT with _sign=-1 and INSERT with _sign=1 if the primary key has been changed, orINSERT with _sign=1 if not. "},{"title":"Selecting from MaterializedMySQL Tables​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"en/engines/database-engines/materialized-mysql#select","content":"SELECT query from MaterializedMySQL tables has some specifics: If _version is not specified in the SELECT query, theFINAL modifier is used, so only rows withMAX(_version) are returned for each primary key value. If _sign is not specified in the SELECT query, WHERE _sign=1 is used by default. So the deleted rows are not included into the result set. The result includes columns comments in case they exist in MySQL database tables. "},{"title":"Index Conversion​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"en/engines/database-engines/materialized-mysql#index-conversion","content":"MySQL PRIMARY KEY and INDEX clauses are converted into ORDER BY tuples in ClickHouse tables. ClickHouse has only one physical order, which is determined by ORDER BY clause. To create a new physical order, usematerialized views. Notes Rows with _sign=-1 are not deleted physically from the tables.Cascade UPDATE/DELETE queries are not supported by the MaterializedMySQL engine, as they are not visible in the MySQL binlog.Replication can be easily broken.Manual operations on database and tables are forbidden.MaterializedMySQL is affected by the optimize_on_insertsetting. Data is merged in the corresponding table in the MaterializedMySQL database when a table in the MySQL server changes. "},{"title":"Table Overrides​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"en/engines/database-engines/materialized-mysql#table-overrides","content":"Table overrides can be used to customize the ClickHouse DDL queries, allowing you to make schema optimizations for your application. This is especially useful for controlling partitioning, which is important for the overall performance of MaterializedMySQL. These are the schema conversion manipulations you can do with table overrides for MaterializedMySQL: Modify column type. Must be compatible with the original type, or replication will fail. For example, you can modify a UInt32 column to UInt64, but you can not modify a String column to Array(String).Modify column TTL.Modify column compression codec.Add ALIAS columns.Add skipping indexesAdd projections. Note that projection optimizations are disabled when using SELECT ... FINAL (which MaterializedMySQL does by default), so their utility is limited here.INDEX ... TYPE hypothesis as [described in the v21.12 blog post]](https://clickhouse.com/blog/en/2021/clickhouse-v21.12-released/) may be more useful in this case.Modify PARTITION BYModify ORDER BYModify PRIMARY KEYAdd SAMPLE BYAdd table TTL CREATE DATABASE db_name ENGINE = MaterializedMySQL(...) [SETTINGS ...] [TABLE OVERRIDE table_name ( [COLUMNS ( [col_name [datatype] [ALIAS expr] [CODEC(...)] [TTL expr], ...] [INDEX index_name expr TYPE indextype[(...)] GRANULARITY val, ...] [PROJECTION projection_name (SELECT &lt;COLUMN LIST EXPR&gt; [GROUP BY] [ORDER BY]), ...] )] [ORDER BY expr] [PRIMARY KEY expr] [PARTITION BY expr] [SAMPLE BY expr] [TTL expr] ), ...]  Example: CREATE DATABASE db_name ENGINE = MaterializedMySQL(...) TABLE OVERRIDE table1 ( COLUMNS ( userid UUID, category LowCardinality(String), timestamp DateTime CODEC(Delta, Default) ) PARTITION BY toYear(timestamp) ), TABLE OVERRIDE table2 ( COLUMNS ( client_ip String TTL created + INTERVAL 72 HOUR ) SAMPLE BY ip_hash )  The COLUMNS list is sparse; existing columns are modified as specified, extra ALIAS columns are added. It is not possible to add ordinary or MATERIALIZED columns. Modified columns with a different type must be assignable from the original type. There is currently no validation of this or similar issues when the CREATE DATABASE query executes, so extra care needs to be taken. You may specify overrides for tables that do not exist yet. warning It is easy to break replication with table overrides if not used with care. For example: If an ALIAS column is added with a table override, and a column with the same name is later added to the source MySQL table, the converted ALTER TABLE query in ClickHouse will fail and replication stops.It is currently possible to add overrides that reference nullable columns where not-nullable are required, such as inORDER BY or PARTITION BY. This will cause CREATE TABLE queries that will fail, also causing replication to stop. "},{"title":"Examples of Use​","type":1,"pageTitle":"[experimental] MaterializedMySQL","url":"en/engines/database-engines/materialized-mysql#examples-of-use","content":"Queries in MySQL: mysql&gt; CREATE DATABASE db; mysql&gt; CREATE TABLE db.test (a INT PRIMARY KEY, b INT); mysql&gt; INSERT INTO db.test VALUES (1, 11), (2, 22); mysql&gt; DELETE FROM db.test WHERE a=1; mysql&gt; ALTER TABLE db.test ADD COLUMN c VARCHAR(16); mysql&gt; UPDATE db.test SET c='Wow!', b=222; mysql&gt; SELECT * FROM test;  ┌─a─┬───b─┬─c────┐ │ 2 │ 222 │ Wow! │ └───┴─────┴──────┘  Database in ClickHouse, exchanging data with the MySQL server: The database and the table created: CREATE DATABASE mysql ENGINE = MaterializedMySQL('localhost:3306', 'db', 'user', '***'); SHOW TABLES FROM mysql;  ┌─name─┐ │ test │ └──────┘  After inserting data: SELECT * FROM mysql.test;  ┌─a─┬──b─┐ │ 1 │ 11 │ │ 2 │ 22 │ └───┴────┘  After deleting data, adding the column and updating: SELECT * FROM mysql.test;  ┌─a─┬───b─┬─c────┐ │ 2 │ 222 │ Wow! │ └───┴─────┴──────┘  Original article "},{"title":"Kafka","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/kafka","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"Kafka","url":"en/engines/table-engines/integrations/kafka#table_engine-kafka-creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = Kafka() SETTINGS kafka_broker_list = 'host:port', kafka_topic_list = 'topic1,topic2,...', kafka_group_name = 'group_name', kafka_format = 'data_format'[,] [kafka_row_delimiter = 'delimiter_symbol',] [kafka_schema = '',] [kafka_num_consumers = N,] [kafka_max_block_size = 0,] [kafka_skip_broken_messages = N,] [kafka_commit_every_batch = 0,] [kafka_thread_per_consumer = 0]  Required parameters: kafka_broker_list — A comma-separated list of brokers (for example, localhost:9092).kafka_topic_list — A list of Kafka topics.kafka_group_name — A group of Kafka consumers. Reading margins are tracked for each group separately. If you do not want messages to be duplicated in the cluster, use the same group name everywhere.kafka_format — Message format. Uses the same notation as the SQL FORMAT function, such as JSONEachRow. For more information, see the Formats section. Optional parameters: kafka_row_delimiter — Delimiter character, which ends the message.kafka_schema — Parameter that must be used if the format requires a schema definition. For example, Cap’n Proto requires the path to the schema file and the name of the root schema.capnp:Message object.kafka_num_consumers — The number of consumers per table. Default: 1. Specify more consumers if the throughput of one consumer is insufficient. The total number of consumers should not exceed the number of partitions in the topic, since only one consumer can be assigned per partition, and must not be greater than the number of physical cores on the server where ClickHouse is deployed.kafka_max_block_size — The maximum batch size (in messages) for poll (default: max_block_size).kafka_skip_broken_messages — Kafka message parser tolerance to schema-incompatible messages per block. Default: 0. If kafka_skip_broken_messages = N then the engine skips N Kafka messages that cannot be parsed (a message equals a row of data).kafka_commit_every_batch — Commit every consumed and handled batch instead of a single commit after writing a whole block (default: 0).kafka_thread_per_consumer — Provide independent thread for each consumer (default: 0). When enabled, every consumer flush the data independently, in parallel (otherwise — rows from several consumers squashed to form one block). Examples:  CREATE TABLE queue ( timestamp UInt64, level String, message String ) ENGINE = Kafka('localhost:9092', 'topic', 'group1', 'JSONEachRow'); SELECT * FROM queue LIMIT 5; CREATE TABLE queue2 ( timestamp UInt64, level String, message String ) ENGINE = Kafka SETTINGS kafka_broker_list = 'localhost:9092', kafka_topic_list = 'topic', kafka_group_name = 'group1', kafka_format = 'JSONEachRow', kafka_num_consumers = 4; CREATE TABLE queue3 ( timestamp UInt64, level String, message String ) ENGINE = Kafka('localhost:9092', 'topic', 'group1') SETTINGS kafka_format = 'JSONEachRow', kafka_num_consumers = 4;  Deprecated Method for Creating a Table warning Do not use this method in new projects. If possible, switch old projects to the method described above. Kafka(kafka_broker_list, kafka_topic_list, kafka_group_name, kafka_format [, kafka_row_delimiter, kafka_schema, kafka_num_consumers, kafka_skip_broken_messages])  "},{"title":"Description​","type":1,"pageTitle":"Kafka","url":"en/engines/table-engines/integrations/kafka#description","content":"The delivered messages are tracked automatically, so each message in a group is only counted once. If you want to get the data twice, then create a copy of the table with another group name. Groups are flexible and synced on the cluster. For instance, if you have 10 topics and 5 copies of a table in a cluster, then each copy gets 2 topics. If the number of copies changes, the topics are redistributed across the copies automatically. Read more about this at http://kafka.apache.org/intro. SELECT is not particularly useful for reading messages (except for debugging), because each message can be read only once. It is more practical to create real-time threads using materialized views. To do this: Use the engine to create a Kafka consumer and consider it a data stream.Create a table with the desired structure.Create a materialized view that converts data from the engine and puts it into a previously created table. When the MATERIALIZED VIEW joins the engine, it starts collecting data in the background. This allows you to continually receive messages from Kafka and convert them to the required format using SELECT. One kafka table can have as many materialized views as you like, they do not read data from the kafka table directly, but receive new records (in blocks), this way you can write to several tables with different detail level (with grouping - aggregation and without). Example:  CREATE TABLE queue ( timestamp UInt64, level String, message String ) ENGINE = Kafka('localhost:9092', 'topic', 'group1', 'JSONEachRow'); CREATE TABLE daily ( day Date, level String, total UInt64 ) ENGINE = SummingMergeTree(day, (day, level), 8192); CREATE MATERIALIZED VIEW consumer TO daily AS SELECT toDate(toDateTime(timestamp)) AS day, level, count() as total FROM queue GROUP BY day, level; SELECT level, sum(total) FROM daily GROUP BY level;  To improve performance, received messages are grouped into blocks the size of max_insert_block_size. If the block wasn’t formed within stream_flush_interval_ms milliseconds, the data will be flushed to the table regardless of the completeness of the block. To stop receiving topic data or to change the conversion logic, detach the materialized view:  DETACH TABLE consumer; ATTACH TABLE consumer;  If you want to change the target table by using ALTER, we recommend disabling the material view to avoid discrepancies between the target table and the data from the view. "},{"title":"Configuration​","type":1,"pageTitle":"Kafka","url":"en/engines/table-engines/integrations/kafka#configuration","content":"Similar to GraphiteMergeTree, the Kafka engine supports extended configuration using the ClickHouse config file. There are two configuration keys that you can use: global (kafka) and topic-level (kafka_*). The global configuration is applied first, and then the topic-level configuration is applied (if it exists).  &lt;!-- Global configuration options for all tables of Kafka engine type --&gt; &lt;kafka&gt; &lt;debug&gt;cgrp&lt;/debug&gt; &lt;auto_offset_reset&gt;smallest&lt;/auto_offset_reset&gt; &lt;/kafka&gt; &lt;!-- Configuration specific for topic &quot;logs&quot; --&gt; &lt;kafka_logs&gt; &lt;retry_backoff_ms&gt;250&lt;/retry_backoff_ms&gt; &lt;fetch_min_bytes&gt;100000&lt;/fetch_min_bytes&gt; &lt;/kafka_logs&gt;  For a list of possible configuration options, see the librdkafka configuration reference. Use the underscore (_) instead of a dot in the ClickHouse configuration. For example, check.crcs=true will be &lt;check_crcs&gt;true&lt;/check_crcs&gt;. "},{"title":"Kerberos support​","type":1,"pageTitle":"Kafka","url":"en/engines/table-engines/integrations/kafka#kafka-kerberos-support","content":"To deal with Kerberos-aware Kafka, add security_protocol child element with sasl_plaintext value. It is enough if Kerberos ticket-granting ticket is obtained and cached by OS facilities. ClickHouse is able to maintain Kerberos credentials using a keytab file. Consider sasl_kerberos_service_name, sasl_kerberos_keytab, sasl_kerberos_principal and sasl.kerberos.kinit.cmd child elements. Example:  &lt;!-- Kerberos-aware Kafka --&gt; &lt;kafka&gt; &lt;security_protocol&gt;SASL_PLAINTEXT&lt;/security_protocol&gt; &lt;sasl_kerberos_keytab&gt;/home/kafkauser/kafkauser.keytab&lt;/sasl_kerberos_keytab&gt; &lt;sasl_kerberos_principal&gt;kafkauser/kafkahost@EXAMPLE.COM&lt;/sasl_kerberos_principal&gt; &lt;/kafka&gt;  "},{"title":"Virtual Columns​","type":1,"pageTitle":"Kafka","url":"en/engines/table-engines/integrations/kafka#virtual-columns","content":"_topic — Kafka topic._key — Key of the message._offset — Offset of the message._timestamp — Timestamp of the message._timestamp_ms — Timestamp in milliseconds of the message._partition — Partition of Kafka topic. See Also Virtual columnsbackground_message_broker_schedule_pool_size Original article "},{"title":"MaterializedPostgreSQL","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/materialized-postgresql","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"MaterializedPostgreSQL","url":"en/engines/table-engines/integrations/materialized-postgresql#creating-a-table","content":"CREATE TABLE postgresql_db.postgresql_replica (key UInt64, value UInt64) ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgresql_replica', 'postgres_user', 'postgres_password') PRIMARY KEY key;  Engine Parameters host:port — PostgreSQL server address.database — Remote database name.table — Remote table name.user — PostgreSQL user.password — User password. "},{"title":"Requirements​","type":1,"pageTitle":"MaterializedPostgreSQL","url":"en/engines/table-engines/integrations/materialized-postgresql#requirements","content":"The wal_level setting must have a value logical and max_replication_slots parameter must have a value at least 2 in the PostgreSQL config file. A table with MaterializedPostgreSQL engine must have a primary key — the same as a replica identity index (by default: primary key) of a PostgreSQL table (see details on replica identity index). Only database Atomic is allowed. "},{"title":"Virtual columns​","type":1,"pageTitle":"MaterializedPostgreSQL","url":"en/engines/table-engines/integrations/materialized-postgresql#virtual-columns","content":"_version — Transaction counter. Type: UInt64. _sign — Deletion mark. Type: Int8. Possible values: 1 — Row is not deleted,-1 — Row is deleted. These columns do not need to be added when a table is created. They are always accessible in SELECT query._version column equals LSN position in WAL, so it might be used to check how up-to-date replication is. CREATE TABLE postgresql_db.postgresql_replica (key UInt64, value UInt64) ENGINE = MaterializedPostgreSQL('postgres1:5432', 'postgres_database', 'postgresql_replica', 'postgres_user', 'postgres_password') PRIMARY KEY key; SELECT key, value, _version FROM postgresql_db.postgresql_replica;  warning Replication of TOAST values is not supported. The default value for the data type will be used. Original article "},{"title":"MongoDB","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/mongodb","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"MongoDB","url":"en/engines/table-engines/integrations/mongodb#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name ( name1 [type1], name2 [type2], ... ) ENGINE = MongoDB(host:port, database, collection, user, password [, options]);  Engine Parameters host:port — MongoDB server address. database — Remote database name. collection — Remote collection name. user — MongoDB user. password — User password. options — MongoDB connection string options (optional parameter). "},{"title":"Usage Example​","type":1,"pageTitle":"MongoDB","url":"en/engines/table-engines/integrations/mongodb#usage-example","content":"Create a table in ClickHouse which allows to read data from MongoDB collection: CREATE TABLE mongo_table ( key UInt64, data String ) ENGINE = MongoDB('mongo1:27017', 'test', 'simple_table', 'testuser', 'clickhouse');  To read from an SSL secured MongoDB server: CREATE TABLE mongo_table_ssl ( key UInt64, data String ) ENGINE = MongoDB('mongo2:27017', 'test', 'simple_table', 'testuser', 'clickhouse', 'ssl=true');  Query: SELECT COUNT() FROM mongo_table;  ┌─count()─┐ │ 4 │ └─────────┘  You can also adjust connection timeout: CREATE TABLE mongo_table ( key UInt64, data String ) ENGINE = MongoDB('mongo2:27017', 'test', 'simple_table', 'testuser', 'clickhouse', 'connectTimeoutMS=100000');  Original article "},{"title":"MySQL","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/mysql","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"MySQL","url":"en/engines/table-engines/integrations/mysql#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ... ) ENGINE = MySQL('host:port', 'database', 'table', 'user', 'password'[, replace_query, 'on_duplicate_clause']) SETTINGS [connection_pool_size=16, ] [connection_max_tries=3, ] [connection_wait_timeout=5, ] /* 0 -- do not wait */ [connection_auto_close=true ] ;  See a detailed description of the CREATE TABLE query. The table structure can differ from the original MySQL table structure: Column names should be the same as in the original MySQL table, but you can use just some of these columns and in any order.Column types may differ from those in the original MySQL table. ClickHouse tries to cast values to the ClickHouse data types.The external_table_functions_use_nulls setting defines how to handle Nullable columns. Default value: 1. If 0, the table function does not make Nullable columns and inserts default values instead of nulls. This is also applicable for NULL values inside arrays. Engine Parameters host:port — MySQL server address. database — Remote database name. table — Remote table name. user — MySQL user. password — User password. replace_query — Flag that converts INSERT INTO queries to REPLACE INTO. If replace_query=1, the query is substituted. on_duplicate_clause — The ON DUPLICATE KEY on_duplicate_clause expression that is added to the INSERT query. Example: INSERT INTO t (c1,c2) VALUES ('a', 2) ON DUPLICATE KEY UPDATE c2 = c2 + 1, where on_duplicate_clause is UPDATE c2 = c2 + 1. See the MySQL documentation to find which on_duplicate_clause you can use with the ON DUPLICATE KEY clause. To specify on_duplicate_clause you need to pass 0 to the replace_query parameter. If you simultaneously pass replace_query = 1 and on_duplicate_clause, ClickHouse generates an exception. Simple WHERE clauses such as =, !=, &gt;, &gt;=, &lt;, &lt;= are executed on the MySQL server. The rest of the conditions and the LIMIT sampling constraint are executed in ClickHouse only after the query to MySQL finishes. Supports multiple replicas that must be listed by |. For example: CREATE TABLE test_replicas (id UInt32, name String, age UInt32, money UInt32) ENGINE = MySQL(`mysql{2|3|4}:3306`, 'clickhouse', 'test_replicas', 'root', 'clickhouse');  "},{"title":"Usage Example​","type":1,"pageTitle":"MySQL","url":"en/engines/table-engines/integrations/mysql#usage-example","content":"Table in MySQL: mysql&gt; CREATE TABLE `test`.`test` ( -&gt; `int_id` INT NOT NULL AUTO_INCREMENT, -&gt; `int_nullable` INT NULL DEFAULT NULL, -&gt; `float` FLOAT NOT NULL, -&gt; `float_nullable` FLOAT NULL DEFAULT NULL, -&gt; PRIMARY KEY (`int_id`)); Query OK, 0 rows affected (0,09 sec) mysql&gt; insert into test (`int_id`, `float`) VALUES (1,2); Query OK, 1 row affected (0,00 sec) mysql&gt; select * from test; +------+----------+-----+----------+ | int_id | int_nullable | float | float_nullable | +------+----------+-----+----------+ | 1 | NULL | 2 | NULL | +------+----------+-----+----------+ 1 row in set (0,00 sec)  Table in ClickHouse, retrieving data from the MySQL table created above: CREATE TABLE mysql_table ( `float_nullable` Nullable(Float32), `int_id` Int32 ) ENGINE = MySQL('localhost:3306', 'test', 'test', 'bayonet', '123')  SELECT * FROM mysql_table  ┌─float_nullable─┬─int_id─┐ │ ᴺᵁᴸᴸ │ 1 │ └────────────────┴────────┘  "},{"title":"Settings​","type":1,"pageTitle":"MySQL","url":"en/engines/table-engines/integrations/mysql#mysql-settings","content":"Default settings are not very efficient, since they do not even reuse connections. These settings allow you to increase the number of queries run by the server per second. "},{"title":"connection_auto_close​","type":1,"pageTitle":"MySQL","url":"en/engines/table-engines/integrations/mysql#connection-auto-close","content":"Allows to automatically close the connection after query execution, i.e. disable connection reuse. Possible values: 1 — Auto-close connection is allowed, so the connection reuse is disabled0 — Auto-close connection is not allowed, so the connection reuse is enabled Default value: 1. "},{"title":"connection_max_tries​","type":1,"pageTitle":"MySQL","url":"en/engines/table-engines/integrations/mysql#connection-max-tries","content":"Sets the number of retries for pool with failover. Possible values: Positive integer.0 — There are no retries for pool with failover. Default value: 3. "},{"title":"connection_pool_size​","type":1,"pageTitle":"MySQL","url":"en/engines/table-engines/integrations/mysql#connection-pool-size","content":"Size of connection pool (if all connections are in use, the query will wait until some connection will be freed). Possible values: Positive integer. Default value: 16. "},{"title":"See Also​","type":1,"pageTitle":"MySQL","url":"en/engines/table-engines/integrations/mysql#see-also","content":"The mysql table functionUsing MySQL as a source of external dictionary Original article "},{"title":"ODBC","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/odbc","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"ODBC","url":"en/engines/table-engines/integrations/odbc#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1], name2 [type2], ... ) ENGINE = ODBC(connection_settings, external_database, external_table)  See a detailed description of the CREATE TABLE query. The table structure can differ from the source table structure: Column names should be the same as in the source table, but you can use just some of these columns and in any order.Column types may differ from those in the source table. ClickHouse tries to cast values to the ClickHouse data types.The external_table_functions_use_nulls setting defines how to handle Nullable columns. Default value: 1. If 0, the table function does not make Nullable columns and inserts default values instead of nulls. This is also applicable for NULL values inside arrays. Engine Parameters connection_settings — Name of the section with connection settings in the odbc.ini file.external_database — Name of a database in an external DBMS.external_table — Name of a table in the external_database. "},{"title":"Usage Example​","type":1,"pageTitle":"ODBC","url":"en/engines/table-engines/integrations/odbc#usage-example","content":"Retrieving data from the local MySQL installation via ODBC This example is checked for Ubuntu Linux 18.04 and MySQL server 5.7. Ensure that unixODBC and MySQL Connector are installed. By default (if installed from packages), ClickHouse starts as user clickhouse. Thus, you need to create and configure this user in the MySQL server. $ sudo mysql  mysql&gt; CREATE USER 'clickhouse'@'localhost' IDENTIFIED BY 'clickhouse'; mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'clickhouse'@'clickhouse' WITH GRANT OPTION;  Then configure the connection in /etc/odbc.ini. $ cat /etc/odbc.ini [mysqlconn] DRIVER = /usr/local/lib/libmyodbc5w.so SERVER = 127.0.0.1 PORT = 3306 DATABASE = test USERNAME = clickhouse PASSWORD = clickhouse  You can check the connection using the isql utility from the unixODBC installation. $ isql -v mysqlconn +-------------------------+ | Connected! | | | ...  Table in MySQL: mysql&gt; CREATE TABLE `test`.`test` ( -&gt; `int_id` INT NOT NULL AUTO_INCREMENT, -&gt; `int_nullable` INT NULL DEFAULT NULL, -&gt; `float` FLOAT NOT NULL, -&gt; `float_nullable` FLOAT NULL DEFAULT NULL, -&gt; PRIMARY KEY (`int_id`)); Query OK, 0 rows affected (0,09 sec) mysql&gt; insert into test (`int_id`, `float`) VALUES (1,2); Query OK, 1 row affected (0,00 sec) mysql&gt; select * from test; +------+----------+-----+----------+ | int_id | int_nullable | float | float_nullable | +------+----------+-----+----------+ | 1 | NULL | 2 | NULL | +------+----------+-----+----------+ 1 row in set (0,00 sec)  Table in ClickHouse, retrieving data from the MySQL table: CREATE TABLE odbc_t ( `int_id` Int32, `float_nullable` Nullable(Float32) ) ENGINE = ODBC('DSN=mysqlconn', 'test', 'test')  SELECT * FROM odbc_t  ┌─int_id─┬─float_nullable─┐ │ 1 │ ᴺᵁᴸᴸ │ └────────┴────────────────┘  "},{"title":"See Also​","type":1,"pageTitle":"ODBC","url":"en/engines/table-engines/integrations/odbc#see-also","content":"ODBC external dictionariesODBC table function Original article "},{"title":"PostgreSQL","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/postgresql","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"PostgreSQL","url":"en/engines/table-engines/integrations/postgresql#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ... ) ENGINE = PostgreSQL('host:port', 'database', 'table', 'user', 'password'[, `schema`]);  See a detailed description of the CREATE TABLE query. The table structure can differ from the original PostgreSQL table structure: Column names should be the same as in the original PostgreSQL table, but you can use just some of these columns and in any order.Column types may differ from those in the original PostgreSQL table. ClickHouse tries to cast values to the ClickHouse data types.The external_table_functions_use_nulls setting defines how to handle Nullable columns. Default value: 1. If 0, the table function does not make Nullable columns and inserts default values instead of nulls. This is also applicable for NULL values inside arrays. Engine Parameters host:port — PostgreSQL server address.database — Remote database name.table — Remote table name.user — PostgreSQL user.password — User password.schema — Non-default table schema. Optional.on conflict ... — example: ON CONFLICT DO NOTHING. Optional. Note: adding this option will make insertion less efficient. or via config (since version 21.11): &lt;named_collections&gt; &lt;postgres1&gt; &lt;host&gt;&lt;/host&gt; &lt;port&gt;&lt;/port&gt; &lt;user&gt;&lt;/user&gt; &lt;password&gt;&lt;/password&gt; &lt;table&gt;&lt;/table&gt; &lt;/postgres1&gt; &lt;postgres2&gt; &lt;host&gt;&lt;/host&gt; &lt;port&gt;&lt;/port&gt; &lt;user&gt;&lt;/user&gt; &lt;password&gt;&lt;/password&gt; &lt;/postgres2&gt; &lt;/named_collections&gt;  Some parameters can be overriden by key value arguments: SELECT * FROM postgresql(postgres1, schema='schema1', table='table1');  "},{"title":"Implementation Details​","type":1,"pageTitle":"PostgreSQL","url":"en/engines/table-engines/integrations/postgresql#implementation-details","content":"SELECT queries on PostgreSQL side run as COPY (SELECT ...) TO STDOUT inside read-only PostgreSQL transaction with commit after each SELECT query. Simple WHERE clauses such as =, !=, &gt;, &gt;=, &lt;, &lt;=, and IN are executed on the PostgreSQL server. All joins, aggregations, sorting, IN [ array ] conditions and the LIMIT sampling constraint are executed in ClickHouse only after the query to PostgreSQL finishes. INSERT queries on PostgreSQL side run as COPY &quot;table_name&quot; (field1, field2, ... fieldN) FROM STDIN inside PostgreSQL transaction with auto-commit after each INSERT statement. PostgreSQL Array types are converted into ClickHouse arrays. warning Be careful - in PostgreSQL an array data, created like a type_name[], may contain multi-dimensional arrays of different dimensions in different table rows in same column. But in ClickHouse it is only allowed to have multidimensional arrays of the same count of dimensions in all table rows in same column. Supports multiple replicas that must be listed by |. For example: CREATE TABLE test_replicas (id UInt32, name String) ENGINE = PostgreSQL(`postgres{2|3|4}:5432`, 'clickhouse', 'test_replicas', 'postgres', 'mysecretpassword');  Replicas priority for PostgreSQL dictionary source is supported. The bigger the number in map, the less the priority. The highest priority is 0. In the example below replica example01-1 has the highest priority: &lt;postgresql&gt; &lt;port&gt;5432&lt;/port&gt; &lt;user&gt;clickhouse&lt;/user&gt; &lt;password&gt;qwerty&lt;/password&gt; &lt;replica&gt; &lt;host&gt;example01-1&lt;/host&gt; &lt;priority&gt;1&lt;/priority&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;example01-2&lt;/host&gt; &lt;priority&gt;2&lt;/priority&gt; &lt;/replica&gt; &lt;db&gt;db_name&lt;/db&gt; &lt;table&gt;table_name&lt;/table&gt; &lt;where&gt;id=10&lt;/where&gt; &lt;invalidate_query&gt;SQL_QUERY&lt;/invalidate_query&gt; &lt;/postgresql&gt; &lt;/source&gt;  "},{"title":"Usage Example​","type":1,"pageTitle":"PostgreSQL","url":"en/engines/table-engines/integrations/postgresql#usage-example","content":"Table in PostgreSQL: postgres=# CREATE TABLE &quot;public&quot;.&quot;test&quot; ( &quot;int_id&quot; SERIAL, &quot;int_nullable&quot; INT NULL DEFAULT NULL, &quot;float&quot; FLOAT NOT NULL, &quot;str&quot; VARCHAR(100) NOT NULL DEFAULT '', &quot;float_nullable&quot; FLOAT NULL DEFAULT NULL, PRIMARY KEY (int_id)); CREATE TABLE postgres=# INSERT INTO test (int_id, str, &quot;float&quot;) VALUES (1,'test',2); INSERT 0 1 postgresql&gt; SELECT * FROM test; int_id | int_nullable | float | str | float_nullable --------+--------------+-------+------+---------------- 1 | | 2 | test | (1 row)  Table in ClickHouse, retrieving data from the PostgreSQL table created above: CREATE TABLE default.postgresql_table ( `float_nullable` Nullable(Float32), `str` String, `int_id` Int32 ) ENGINE = PostgreSQL('localhost:5432', 'public', 'test', 'postges_user', 'postgres_password');  SELECT * FROM postgresql_table WHERE str IN ('test');  ┌─float_nullable─┬─str──┬─int_id─┐ │ ᴺᵁᴸᴸ │ test │ 1 │ └────────────────┴──────┴────────┘  Using Non-default Schema: postgres=# CREATE SCHEMA &quot;nice.schema&quot;; postgres=# CREATE TABLE &quot;nice.schema&quot;.&quot;nice.table&quot; (a integer); postgres=# INSERT INTO &quot;nice.schema&quot;.&quot;nice.table&quot; SELECT i FROM generate_series(0, 99) as t(i)  CREATE TABLE pg_table_schema_with_dots (a UInt32) ENGINE PostgreSQL('localhost:5432', 'clickhouse', 'nice.table', 'postgrsql_user', 'password', 'nice.schema');  See Also The postgresql table functionUsing PostgreSQL as a source of external dictionary Original article "},{"title":"RabbitMQ Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/rabbitmq","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"RabbitMQ Engine","url":"en/engines/table-engines/integrations/rabbitmq#table_engine-rabbitmq-creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = RabbitMQ SETTINGS rabbitmq_host_port = 'host:port' [or rabbitmq_address = 'amqp(s)://guest:guest@localhost/vhost'], rabbitmq_exchange_name = 'exchange_name', rabbitmq_format = 'data_format'[,] [rabbitmq_exchange_type = 'exchange_type',] [rabbitmq_routing_key_list = 'key1,key2,...',] [rabbitmq_secure = 0,] [rabbitmq_row_delimiter = 'delimiter_symbol',] [rabbitmq_schema = '',] [rabbitmq_num_consumers = N,] [rabbitmq_num_queues = N,] [rabbitmq_queue_base = 'queue',] [rabbitmq_deadletter_exchange = 'dl-exchange',] [rabbitmq_persistent = 0,] [rabbitmq_skip_broken_messages = N,] [rabbitmq_max_block_size = N,] [rabbitmq_flush_interval_ms = N] [rabbitmq_queue_settings_list = 'x-dead-letter-exchange=my-dlx,x-max-length=10,x-overflow=reject-publish']  Required parameters: rabbitmq_host_port – host:port (for example, localhost:5672).rabbitmq_exchange_name – RabbitMQ exchange name.rabbitmq_format – Message format. Uses the same notation as the SQL FORMAT function, such as JSONEachRow. For more information, see the Formats section. Optional parameters: rabbitmq_exchange_type – The type of RabbitMQ exchange: direct, fanout, topic, headers, consistent_hash. Default: fanout.rabbitmq_routing_key_list – A comma-separated list of routing keys.rabbitmq_row_delimiter – Delimiter character, which ends the message.rabbitmq_schema – Parameter that must be used if the format requires a schema definition. For example, Cap’n Proto requires the path to the schema file and the name of the root schema.capnp:Message object.rabbitmq_num_consumers – The number of consumers per table. Default: 1. Specify more consumers if the throughput of one consumer is insufficient.rabbitmq_num_queues – Total number of queues. Default: 1. Increasing this number can significantly improve performance.rabbitmq_queue_base - Specify a hint for queue names. Use cases of this setting are described below.rabbitmq_deadletter_exchange - Specify name for a dead letter exchange. You can create another table with this exchange name and collect messages in cases when they are republished to dead letter exchange. By default dead letter exchange is not specified.rabbitmq_persistent - If set to 1 (true), in insert query delivery mode will be set to 2 (marks messages as 'persistent'). Default: 0.rabbitmq_skip_broken_messages – RabbitMQ message parser tolerance to schema-incompatible messages per block. Default: 0. If rabbitmq_skip_broken_messages = N then the engine skips N RabbitMQ messages that cannot be parsed (a message equals a row of data).rabbitmq_max_block_sizerabbitmq_flush_interval_msrabbitmq_queue_settings_list - allows to set RabbitMQ settings when creating a queue. Available settings: x-max-length, x-max-length-bytes, x-message-ttl, x-expires, x-priority, x-max-priority, x-overflow, x-dead-letter-exchange, x-queue-type. The durable setting is enabled automatically for the queue. SSL connection: Use either rabbitmq_secure = 1 or amqps in connection address: rabbitmq_address = 'amqps://guest:guest@localhost/vhost'. The default behaviour of the used library is not to check if the created TLS connection is sufficiently secure. Whether the certificate is expired, self-signed, missing or invalid: the connection is simply permitted. More strict checking of certificates can possibly be implemented in the future. Also format settings can be added along with rabbitmq-related settings. Example:  CREATE TABLE queue ( key UInt64, value UInt64, date DateTime ) ENGINE = RabbitMQ SETTINGS rabbitmq_host_port = 'localhost:5672', rabbitmq_exchange_name = 'exchange1', rabbitmq_format = 'JSONEachRow', rabbitmq_num_consumers = 5, date_time_input_format = 'best_effort';  The RabbitMQ server configuration should be added using the ClickHouse config file. Required configuration:  &lt;rabbitmq&gt; &lt;username&gt;root&lt;/username&gt; &lt;password&gt;clickhouse&lt;/password&gt; &lt;/rabbitmq&gt;  Additional configuration:  &lt;rabbitmq&gt; &lt;vhost&gt;clickhouse&lt;/vhost&gt; &lt;/rabbitmq&gt;  "},{"title":"Description​","type":1,"pageTitle":"RabbitMQ Engine","url":"en/engines/table-engines/integrations/rabbitmq#description","content":"SELECT is not particularly useful for reading messages (except for debugging), because each message can be read only once. It is more practical to create real-time threads using materialized views. To do this: Use the engine to create a RabbitMQ consumer and consider it a data stream.Create a table with the desired structure.Create a materialized view that converts data from the engine and puts it into a previously created table. When the MATERIALIZED VIEW joins the engine, it starts collecting data in the background. This allows you to continually receive messages from RabbitMQ and convert them to the required format using SELECT. One RabbitMQ table can have as many materialized views as you like. Data can be channeled based on rabbitmq_exchange_type and the specified rabbitmq_routing_key_list. There can be no more than one exchange per table. One exchange can be shared between multiple tables - it enables routing into multiple tables at the same time. Exchange type options: direct - Routing is based on the exact matching of keys. Example table key list: key1,key2,key3,key4,key5, message key can equal any of them.fanout - Routing to all tables (where exchange name is the same) regardless of the keys.topic - Routing is based on patterns with dot-separated keys. Examples: *.logs, records.*.*.2020, *.2018,*.2019,*.2020.headers - Routing is based on key=value matches with a setting x-match=all or x-match=any. Example table key list: x-match=all,format=logs,type=report,year=2020.consistent_hash - Data is evenly distributed between all bound tables (where the exchange name is the same). Note that this exchange type must be enabled with RabbitMQ plugin: rabbitmq-plugins enable rabbitmq_consistent_hash_exchange. Setting rabbitmq_queue_base may be used for the following cases: to let different tables share queues, so that multiple consumers could be registered for the same queues, which makes a better performance. If using rabbitmq_num_consumers and/or rabbitmq_num_queues settings, the exact match of queues is achieved in case these parameters are the same.to be able to restore reading from certain durable queues when not all messages were successfully consumed. To resume consumption from one specific queue - set its name in rabbitmq_queue_base setting and do not specify rabbitmq_num_consumers and rabbitmq_num_queues (defaults to 1). To resume consumption from all queues, which were declared for a specific table - just specify the same settings: rabbitmq_queue_base, rabbitmq_num_consumers, rabbitmq_num_queues. By default, queue names will be unique to tables.to reuse queues as they are declared durable and not auto-deleted. (Can be deleted via any of RabbitMQ CLI tools.) To improve performance, received messages are grouped into blocks the size of max_insert_block_size. If the block wasn’t formed within stream_flush_interval_ms milliseconds, the data will be flushed to the table regardless of the completeness of the block. If rabbitmq_num_consumers and/or rabbitmq_num_queues settings are specified along with rabbitmq_exchange_type, then: rabbitmq-consistent-hash-exchange plugin must be enabled.message_id property of the published messages must be specified (unique for each message/batch). For insert query there is message metadata, which is added for each published message: messageID and republished flag (true, if published more than once) - can be accessed via message headers. Do not use the same table for inserts and materialized views. Example:  CREATE TABLE queue ( key UInt64, value UInt64 ) ENGINE = RabbitMQ SETTINGS rabbitmq_host_port = 'localhost:5672', rabbitmq_exchange_name = 'exchange1', rabbitmq_exchange_type = 'headers', rabbitmq_routing_key_list = 'format=logs,type=report,year=2020', rabbitmq_format = 'JSONEachRow', rabbitmq_num_consumers = 5; CREATE TABLE daily (key UInt64, value UInt64) ENGINE = MergeTree() ORDER BY key; CREATE MATERIALIZED VIEW consumer TO daily AS SELECT key, value FROM queue; SELECT key, value FROM daily ORDER BY key;  "},{"title":"Virtual Columns​","type":1,"pageTitle":"RabbitMQ Engine","url":"en/engines/table-engines/integrations/rabbitmq#virtual-columns","content":"_exchange_name - RabbitMQ exchange name._channel_id - ChannelID, on which consumer, who received the message, was declared._delivery_tag - DeliveryTag of the received message. Scoped per channel._redelivered - redelivered flag of the message._message_id - messageID of the received message; non-empty if was set, when message was published._timestamp - timestamp of the received message; non-empty if was set, when message was published. Original article "},{"title":"S3 Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/s3","content":"","keywords":""},{"title":"Create Table​","type":1,"pageTitle":"S3 Table Engine","url":"en/engines/table-engines/integrations/s3#creating-a-table","content":"CREATE TABLE s3_engine_table (name String, value UInt32) ENGINE = S3(path, [aws_access_key_id, aws_secret_access_key,] format, [compression]) [SETTINGS ...]  Engine parameters path — Bucket url with path to file. Supports following wildcards in readonly mode: *, ?, {abc,def} and {N..M} where N, M — numbers, 'abc', 'def' — strings. For more information see below.format — The format of the file.aws_access_key_id, aws_secret_access_key - Long-term credentials for the AWS account user. You can use these to authenticate your requests. Parameter is optional. If credentials are not specified, they are used from the configuration file. For more information see Using S3 for Data Storage.compression — Compression type. Supported values: none, gzip/gz, brotli/br, xz/LZMA, zstd/zst. Parameter is optional. By default, it will autodetect compression by file extension. Example CREATE TABLE s3_engine_table (name String, value UInt32) ENGINE=S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'gzip') SETTINGS input_format_with_names_use_header = 0; INSERT INTO s3_engine_table VALUES ('one', 1), ('two', 2), ('three', 3); SELECT * FROM s3_engine_table LIMIT 2;  ┌─name─┬─value─┐ │ one │ 1 │ │ two │ 2 │ └──────┴───────┘  "},{"title":"Virtual columns​","type":1,"pageTitle":"S3 Table Engine","url":"en/engines/table-engines/integrations/s3#virtual-columns","content":"_path — Path to the file._file — Name of the file. For more information about virtual columns see here. "},{"title":"Implementation Details​","type":1,"pageTitle":"S3 Table Engine","url":"en/engines/table-engines/integrations/s3#implementation-details","content":"Reads and writes can be parallelZero-copy replication is supported. Not supported: ALTER and SELECT...SAMPLE operations.Indexes. "},{"title":"Wildcards In Path​","type":1,"pageTitle":"S3 Table Engine","url":"en/engines/table-engines/integrations/s3#wildcards-in-path","content":"path argument can specify multiple files using bash-like wildcards. For being processed file should exist and match to the whole path pattern. Listing of files is determined during SELECT (not at CREATE moment). * — Substitutes any number of any characters except / including empty string.? — Substitutes any single character.{some_string,another_string,yet_another_one} — Substitutes any of strings 'some_string', 'another_string', 'yet_another_one'.{N..M} — Substitutes any number in range from N to M including both borders. N and M can have leading zeroes e.g. 000..078. Constructions with {} are similar to the remote table function. warning If the listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?. Example with wildcards 1 Create table with files named file-000.csv, file-001.csv, … , file-999.csv: CREATE TABLE big_table (name String, value UInt32) ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/my_folder/file-{000..999}.csv', 'CSV');  Example with wildcards 2 Suppose we have several files in CSV format with the following URIs on S3: 'https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_1.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_2.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/some_folder/some_file_3.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_1.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_2.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/another_folder/some_file_3.csv' There are several ways to make a table consisting of all six files: Specify the range of file postfixes: CREATE TABLE table_with_range (name String, value UInt32) ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_{1..3}', 'CSV');  Take all files with some_file_ prefix (there should be no extra files with such prefix in both folders): CREATE TABLE table_with_question_mark (name String, value UInt32) ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/some_file_?', 'CSV');  Take all the files in both folders (all files should satisfy format and schema described in query): CREATE TABLE table_with_asterisk (name String, value UInt32) ENGINE = S3('https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/{some,another}_folder/*', 'CSV');  "},{"title":"S3-related Settings​","type":1,"pageTitle":"S3 Table Engine","url":"en/engines/table-engines/integrations/s3#settings","content":"The following settings can be set before query execution or placed into configuration file. s3_max_single_part_upload_size — The maximum size of object to upload using singlepart upload to S3. Default value is 64Mb.s3_min_upload_part_size — The minimum size of part to upload during multipart upload to S3 Multipart upload. Default value is 512Mb.s3_max_redirects — Max number of S3 redirects hops allowed. Default value is 10.s3_single_read_retries — The maximum number of attempts during single read. Default value is 4. Security consideration: if malicious user can specify arbitrary S3 URLs, s3_max_redirects must be set to zero to avoid SSRF attacks; or alternatively, remote_host_filter must be specified in server configuration. "},{"title":"Endpoint-based Settings​","type":1,"pageTitle":"S3 Table Engine","url":"en/engines/table-engines/integrations/s3#endpoint-settings","content":"The following settings can be specified in configuration file for given endpoint (which will be matched by exact prefix of a URL): endpoint — Specifies prefix of an endpoint. Mandatory.access_key_id and secret_access_key — Specifies credentials to use with given endpoint. Optional.use_environment_credentials — If set to true, S3 client will try to obtain credentials from environment variables and Amazon EC2 metadata for given endpoint. Optional, default value is false.region — Specifies S3 region name. Optional.use_insecure_imds_request — If set to true, S3 client will use insecure IMDS request while obtaining credentials from Amazon EC2 metadata. Optional, default value is false.header — Adds specified HTTP header to a request to given endpoint. Optional, can be speficied multiple times.server_side_encryption_customer_key_base64 — If specified, required headers for accessing S3 objects with SSE-C encryption will be set. Optional.max_single_read_retries — The maximum number of attempts during single read. Default value is 4. Optional. Example: &lt;s3&gt; &lt;endpoint-name&gt; &lt;endpoint&gt;https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/&lt;/endpoint&gt; &lt;!-- &lt;access_key_id&gt;ACCESS_KEY_ID&lt;/access_key_id&gt; --&gt; &lt;!-- &lt;secret_access_key&gt;SECRET_ACCESS_KEY&lt;/secret_access_key&gt; --&gt; &lt;!-- &lt;region&gt;us-west-1&lt;/region&gt; --&gt; &lt;!-- &lt;use_environment_credentials&gt;false&lt;/use_environment_credentials&gt; --&gt; &lt;!-- &lt;use_insecure_imds_request&gt;false&lt;/use_insecure_imds_request&gt; --&gt; &lt;!-- &lt;header&gt;Authorization: Bearer SOME-TOKEN&lt;/header&gt; --&gt; &lt;!-- &lt;server_side_encryption_customer_key_base64&gt;BASE64-ENCODED-KEY&lt;/server_side_encryption_customer_key_base64&gt; --&gt; &lt;!-- &lt;max_single_read_retries&gt;4&lt;/max_single_read_retries&gt; --&gt; &lt;/endpoint-name&gt; &lt;/s3&gt;  "},{"title":"See also​","type":1,"pageTitle":"S3 Table Engine","url":"en/engines/table-engines/integrations/s3#see-also","content":"s3 table function Original article "},{"title":"SQLite","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/sqlite","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"SQLite","url":"en/engines/table-engines/integrations/sqlite#creating-a-table","content":" CREATE TABLE [IF NOT EXISTS] [db.]table_name ( name1 [type1], name2 [type2], ... ) ENGINE = SQLite('db_path', 'table')  Engine Parameters db_path — Path to SQLite file with a database.table — Name of a table in the SQLite database. "},{"title":"Usage Example​","type":1,"pageTitle":"SQLite","url":"en/engines/table-engines/integrations/sqlite#usage-example","content":"Shows a query creating the SQLite table: SHOW CREATE TABLE sqlite_db.table2;  CREATE TABLE SQLite.table2 ( `col1` Nullable(Int32), `col2` Nullable(String) ) ENGINE = SQLite('sqlite.db','table2');  Returns the data from the table: SELECT * FROM sqlite_db.table2 ORDER BY col1;  ┌─col1─┬─col2──┐ │ 1 │ text1 │ │ 2 │ text2 │ │ 3 │ text3 │ └──────┴───────┘  See Also SQLite enginesqlite table function Original article "},{"title":"Log Engine Family","type":0,"sectionRef":"#","url":"en/engines/table-engines/log-family/","content":"","keywords":""},{"title":"Common Properties​","type":1,"pageTitle":"Log Engine Family","url":"en/engines/table-engines/log-family/#common-properties","content":"Engines: Store data on a disk. Append data to the end of file when writing. Support locks for concurrent data access. During INSERT queries, the table is locked, and other queries for reading and writing data both wait for the table to unlock. If there are no data writing queries, any number of data reading queries can be performed concurrently. Do not support mutations. Do not support indexes. This means that SELECT queries for ranges of data are not efficient. Do not write data atomically. You can get a table with corrupted data if something breaks the write operation, for example, abnormal server shutdown. "},{"title":"Differences​","type":1,"pageTitle":"Log Engine Family","url":"en/engines/table-engines/log-family/#differences","content":"The TinyLog engine is the simplest in the family and provides the poorest functionality and lowest efficiency. The TinyLog engine does not support parallel data reading by several threads in a single query. It reads data slower than other engines in the family that support parallel reading from a single query and it uses almost as many file descriptors as the Log engine because it stores each column in a separate file. Use it only in simple scenarios. The Log and StripeLog engines support parallel data reading. When reading data, ClickHouse uses multiple threads. Each thread processes a separate data block. The Log engine uses a separate file for each column of the table. StripeLog stores all the data in one file. As a result, the StripeLog engine uses fewer file descriptors, but the Log engine provides higher efficiency when reading data. Original article "},{"title":"Log","type":0,"sectionRef":"#","url":"en/engines/table-engines/log-family/log","content":"Log The engine belongs to the family of Log engines. See the common properties of Log engines and their differences in the Log Engine Family article. Log differs from TinyLog in that a small file of &quot;marks&quot; resides with the column files. These marks are written on every data block and contain offsets that indicate where to start reading the file in order to skip the specified number of rows. This makes it possible to read table data in multiple threads. For concurrent data access, the read operations can be performed simultaneously, while write operations block reads and each other. The Log engine does not support indexes. Similarly, if writing to a table failed, the table is broken, and reading from it returns an error. The Log engine is appropriate for temporary data, write-once tables, and for testing or demonstration purposes. Original article","keywords":""},{"title":"How to Write C++ Code","type":0,"sectionRef":"#","url":"en/development/style","content":"","keywords":""},{"title":"General Recommendations​","type":1,"pageTitle":"How to Write C++ Code","url":"en/development/style#general-recommendations","content":"1. The following are recommendations, not requirements. 2. If you are editing code, it makes sense to follow the formatting of the existing code. 3. Code style is needed for consistency. Consistency makes it easier to read the code, and it also makes it easier to search the code. 4. Many of the rules do not have logical reasons; they are dictated by established practices. "},{"title":"Formatting​","type":1,"pageTitle":"How to Write C++ Code","url":"en/development/style#formatting","content":"1. Most of the formatting will be done automatically by clang-format. 2. Indents are 4 spaces. Configure your development environment so that a tab adds four spaces. 3. Opening and closing curly brackets must be on a separate line. inline void readBoolText(bool &amp; x, ReadBuffer &amp; buf) { char tmp = '0'; readChar(tmp, buf); x = tmp != '0'; }  4. If the entire function body is a single statement, it can be placed on a single line. Place spaces around curly braces (besides the space at the end of the line). inline size_t mask() const { return buf_size() - 1; } inline size_t place(HashValue x) const { return x &amp; mask(); }  5. For functions. Don’t put spaces around brackets. void reinsert(const Value &amp; x)  memcpy(&amp;buf[place_value], &amp;x, sizeof(x));  6. In if, for, while and other expressions, a space is inserted in front of the opening bracket (as opposed to function calls). for (size_t i = 0; i &lt; rows; i += storage.index_granularity)  7. Add spaces around binary operators (+, -, *, /, %, …) and the ternary operator ?:. UInt16 year = (s[0] - '0') * 1000 + (s[1] - '0') * 100 + (s[2] - '0') * 10 + (s[3] - '0'); UInt8 month = (s[5] - '0') * 10 + (s[6] - '0'); UInt8 day = (s[8] - '0') * 10 + (s[9] - '0');  8. If a line feed is entered, put the operator on a new line and increase the indent before it. if (elapsed_ns) message &lt;&lt; &quot; (&quot; &lt;&lt; rows_read_on_server * 1000000000 / elapsed_ns &lt;&lt; &quot; rows/s., &quot; &lt;&lt; bytes_read_on_server * 1000.0 / elapsed_ns &lt;&lt; &quot; MB/s.) &quot;;  9. You can use spaces for alignment within a line, if desired. dst.ClickLogID = click.LogID; dst.ClickEventID = click.EventID; dst.ClickGoodEvent = click.GoodEvent;  10. Don’t use spaces around the operators ., -&gt;. If necessary, the operator can be wrapped to the next line. In this case, the offset in front of it is increased. 11. Do not use a space to separate unary operators (--, ++, *, &amp;, …) from the argument. 12. Put a space after a comma, but not before it. The same rule goes for a semicolon inside a for expression. 13. Do not use spaces to separate the [] operator. 14. In a template &lt;...&gt; expression, use a space between template and &lt;; no spaces after &lt; or before &gt;. template &lt;typename TKey, typename TValue&gt; struct AggregatedStatElement {}  15. In classes and structures, write public, private, and protected on the same level as class/struct, and indent the rest of the code. template &lt;typename T&gt; class MultiVersion { public: /// Version of object for usage. shared_ptr manage lifetime of version. using Version = std::shared_ptr&lt;const T&gt;; ... }  16. If the same namespace is used for the entire file, and there isn’t anything else significant, an offset is not necessary inside namespace. 17. If the block for an if, for, while, or other expression consists of a single statement, the curly brackets are optional. Place the statement on a separate line, instead. This rule is also valid for nested if, for, while, … But if the inner statement contains curly brackets or else, the external block should be written in curly brackets. /// Finish write. for (auto &amp; stream : streams) stream.second-&gt;finalize();  18. There shouldn’t be any spaces at the ends of lines. 19. Source files are UTF-8 encoded. 20. Non-ASCII characters can be used in string literals. &lt;&lt; &quot;, &quot; &lt;&lt; (timer.elapsed() / chunks_stats.hits) &lt;&lt; &quot; μsec/hit.&quot;;  21. Do not write multiple expressions in a single line. 22. Group sections of code inside functions and separate them with no more than one empty line. 23. Separate functions, classes, and so on with one or two empty lines. 24. A const (related to a value) must be written before the type name. //correct const char * pos const std::string &amp; s //incorrect char const * pos  25. When declaring a pointer or reference, the * and &amp; symbols should be separated by spaces on both sides. //correct const char * pos //incorrect const char* pos const char *pos  26. When using template types, alias them with the using keyword (except in the simplest cases). In other words, the template parameters are specified only in using and aren’t repeated in the code. using can be declared locally, such as inside a function. //correct using FileStreams = std::map&lt;std::string, std::shared_ptr&lt;Stream&gt;&gt;; FileStreams streams; //incorrect std::map&lt;std::string, std::shared_ptr&lt;Stream&gt;&gt; streams;  27. Do not declare several variables of different types in one statement. //incorrect int x, *y;  28. Do not use C-style casts. //incorrect std::cerr &lt;&lt; (int)c &lt;&lt;; std::endl; //correct std::cerr &lt;&lt; static_cast&lt;int&gt;(c) &lt;&lt; std::endl;  29. In classes and structs, group members and functions separately inside each visibility scope. 30. For small classes and structs, it is not necessary to separate the method declaration from the implementation. The same is true for small methods in any classes or structs. For templated classes and structs, do not separate the method declarations from the implementation (because otherwise they must be defined in the same translation unit). 31. You can wrap lines at 140 characters, instead of 80. 32. Always use the prefix increment/decrement operators if postfix is not required. for (Names::const_iterator it = column_names.begin(); it != column_names.end(); ++it)  "},{"title":"Comments​","type":1,"pageTitle":"How to Write C++ Code","url":"en/development/style#comments","content":"1. Be sure to add comments for all non-trivial parts of code. This is very important. Writing the comment might help you realize that the code isn’t necessary, or that it is designed wrong. /** Part of piece of memory, that can be used. * For example, if internal_buffer is 1MB, and there was only 10 bytes loaded to buffer from file for reading, * then working_buffer will have size of only 10 bytes * (working_buffer.end() will point to position right after those 10 bytes available for read). */  2. Comments can be as detailed as necessary. 3. Place comments before the code they describe. In rare cases, comments can come after the code, on the same line. /** Parses and executes the query. */ void executeQuery( ReadBuffer &amp; istr, /// Where to read the query from (and data for INSERT, if applicable) WriteBuffer &amp; ostr, /// Where to write the result Context &amp; context, /// DB, tables, data types, engines, functions, aggregate functions... BlockInputStreamPtr &amp; query_plan, /// Here could be written the description on how query was executed QueryProcessingStage::Enum stage = QueryProcessingStage::Complete /// Up to which stage process the SELECT query )  4. Comments should be written in English only. 5. If you are writing a library, include detailed comments explaining it in the main header file. 6. Do not add comments that do not provide additional information. In particular, do not leave empty comments like this: /* * Procedure Name: * Original procedure name: * Author: * Date of creation: * Dates of modification: * Modification authors: * Original file name: * Purpose: * Intent: * Designation: * Classes used: * Constants: * Local variables: * Parameters: * Date of creation: * Purpose: */  The example is borrowed from the resource http://home.tamk.fi/~jaalto/course/coding-style/doc/unmaintainable-code/. 7. Do not write garbage comments (author, creation date ..) at the beginning of each file. 8. Single-line comments begin with three slashes: /// and multi-line comments begin with /**. These comments are considered “documentation”. Note: You can use Doxygen to generate documentation from these comments. But Doxygen is not generally used because it is more convenient to navigate the code in the IDE. 9. Multi-line comments must not have empty lines at the beginning and end (except the line that closes a multi-line comment). 10. For commenting out code, use basic comments, not “documenting” comments. 11. Delete the commented out parts of the code before committing. 12. Do not use profanity in comments or code. 13. Do not use uppercase letters. Do not use excessive punctuation. /// WHAT THE FAIL???  14. Do not use comments to make delimeters. ///******************************************************  15. Do not start discussions in comments. /// Why did you do this stuff?  16. There’s no need to write a comment at the end of a block describing what it was about. /// for  "},{"title":"Names​","type":1,"pageTitle":"How to Write C++ Code","url":"en/development/style#names","content":"1. Use lowercase letters with underscores in the names of variables and class members. size_t max_block_size;  2. For the names of functions (methods), use camelCase beginning with a lowercase letter. std::string getName() const override { return &quot;Memory&quot;; }  3. For the names of classes (structs), use CamelCase beginning with an uppercase letter. Prefixes other than I are not used for interfaces. class StorageMemory : public IStorage  4. using are named the same way as classes. 5. Names of template type arguments: in simple cases, use T; T, U; T1, T2. For more complex cases, either follow the rules for class names, or add the prefix T. template &lt;typename TKey, typename TValue&gt; struct AggregatedStatElement  6. Names of template constant arguments: either follow the rules for variable names, or use N in simple cases. template &lt;bool without_www&gt; struct ExtractDomain  7. For abstract classes (interfaces) you can add the I prefix. class IBlockInputStream  8. If you use a variable locally, you can use the short name. In all other cases, use a name that describes the meaning. bool info_successfully_loaded = false;  9. Names of defines and global constants use ALL_CAPS with underscores. #define MAX_SRC_TABLE_NAMES_TO_STORE 1000  10. File names should use the same style as their contents. If a file contains a single class, name the file the same way as the class (CamelCase). If the file contains a single function, name the file the same way as the function (camelCase). 11. If the name contains an abbreviation, then: For variable names, the abbreviation should use lowercase letters mysql_connection (not mySQL_connection).For names of classes and functions, keep the uppercase letters in the abbreviationMySQLConnection (not MySqlConnection). 12. Constructor arguments that are used just to initialize the class members should be named the same way as the class members, but with an underscore at the end. FileQueueProcessor( const std::string &amp; path_, const std::string &amp; prefix_, std::shared_ptr&lt;FileHandler&gt; handler_) : path(path_), prefix(prefix_), handler(handler_), log(&amp;Logger::get(&quot;FileQueueProcessor&quot;)) { }  The underscore suffix can be omitted if the argument is not used in the constructor body. 13. There is no difference in the names of local variables and class members (no prefixes required). timer (not m_timer)  14. For the constants in an enum, use CamelCase with a capital letter. ALL_CAPS is also acceptable. If the enum is non-local, use an enum class. enum class CompressionMethod { QuickLZ = 0, LZ4 = 1, };  15. All names must be in English. Transliteration of Hebrew words is not allowed. not T_PAAMAYIM_NEKUDOTAYIM  16. Abbreviations are acceptable if they are well known (when you can easily find the meaning of the abbreviation in Wikipedia or in a search engine). `AST`, `SQL`. Not `NVDH` (some random letters)  Incomplete words are acceptable if the shortened version is common use. You can also use an abbreviation if the full name is included next to it in the comments. 17. File names with C++ source code must have the .cpp extension. Header files must have the .h extension. "},{"title":"How to Write Code​","type":1,"pageTitle":"How to Write C++ Code","url":"en/development/style#how-to-write-code","content":"1. Memory management. Manual memory deallocation (delete) can only be used in library code. In library code, the delete operator can only be used in destructors. In application code, memory must be freed by the object that owns it. Examples: The easiest way is to place an object on the stack, or make it a member of another class.For a large number of small objects, use containers.For automatic deallocation of a small number of objects that reside in the heap, use shared_ptr/unique_ptr. 2. Resource management. Use RAII and see above. 3. Error handling. Use exceptions. In most cases, you only need to throw an exception, and do not need to catch it (because of RAII). In offline data processing applications, it’s often acceptable to not catch exceptions. In servers that handle user requests, it’s usually enough to catch exceptions at the top level of the connection handler. In thread functions, you should catch and keep all exceptions to rethrow them in the main thread after join. /// If there weren't any calculations yet, calculate the first block synchronously if (!started) { calculate(); started = true; } else /// If calculations are already in progress, wait for the result pool.wait(); if (exception) exception-&gt;rethrow();  Never hide exceptions without handling. Never just blindly put all exceptions to log. //Not correct catch (...) {}  If you need to ignore some exceptions, do so only for specific ones and rethrow the rest. catch (const DB::Exception &amp; e) { if (e.code() == ErrorCodes::UNKNOWN_AGGREGATE_FUNCTION) return nullptr; else throw; }  When using functions with response codes or errno, always check the result and throw an exception in case of error. if (0 != close(fd)) throwFromErrno(&quot;Cannot close file &quot; + file_name, ErrorCodes::CANNOT_CLOSE_FILE);  You can use assert to check invariants in code. 4. Exception types. There is no need to use complex exception hierarchy in application code. The exception text should be understandable to a system administrator. 5. Throwing exceptions from destructors. This is not recommended, but it is allowed. Use the following options: Create a function (done() or finalize()) that will do all the work in advance that might lead to an exception. If that function was called, there should be no exceptions in the destructor later.Tasks that are too complex (such as sending messages over the network) can be put in separate method that the class user will have to call before destruction.If there is an exception in the destructor, it’s better to log it than to hide it (if the logger is available).In simple applications, it is acceptable to rely on std::terminate (for cases of noexcept by default in C++11) to handle exceptions. 6. Anonymous code blocks. You can create a separate code block inside a single function in order to make certain variables local, so that the destructors are called when exiting the block. Block block = data.in-&gt;read(); { std::lock_guard&lt;std::mutex&gt; lock(mutex); data.ready = true; data.block = block; } ready_any.set();  7. Multithreading. In offline data processing programs: Try to get the best possible performance on a single CPU core. You can then parallelize your code if necessary. In server applications: Use the thread pool to process requests. At this point, we haven’t had any tasks that required userspace context switching. Fork is not used for parallelization. 8. Syncing threads. Often it is possible to make different threads use different memory cells (even better: different cache lines,) and to not use any thread synchronization (except joinAll). If synchronization is required, in most cases, it is sufficient to use mutex under lock_guard. In other cases use system synchronization primitives. Do not use busy wait. Atomic operations should be used only in the simplest cases. Do not try to implement lock-free data structures unless it is your primary area of expertise. 9. Pointers vs references. In most cases, prefer references. 10. const. Use constant references, pointers to constants, const_iterator, and const methods. Consider const to be default and use non-const only when necessary. When passing variables by value, using const usually does not make sense. 11. unsigned. Use unsigned if necessary. 12. Numeric types. Use the types UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, and Int64, as well as size_t, ssize_t, and ptrdiff_t. Don’t use these types for numbers: signed/unsigned long, long long, short, signed/unsigned char, char. 13. Passing arguments. Pass complex values by value if they are going to be moved and use std::move; pass by reference if you want to update value in a loop. If a function captures ownership of an object created in the heap, make the argument type shared_ptr or unique_ptr. 14. Return values. In most cases, just use return. Do not write return std::move(res). If the function allocates an object on heap and returns it, use shared_ptr or unique_ptr. In rare cases (updating a value in a loop) you might need to return the value via an argument. In this case, the argument should be a reference. using AggregateFunctionPtr = std::shared_ptr&lt;IAggregateFunction&gt;; /** Allows creating an aggregate function by its name. */ class AggregateFunctionFactory { public: AggregateFunctionFactory(); AggregateFunctionPtr get(const String &amp; name, const DataTypes &amp; argument_types) const;  15. namespace. There is no need to use a separate namespace for application code. Small libraries do not need this, either. For medium to large libraries, put everything in a namespace. In the library’s .h file, you can use namespace detail to hide implementation details not needed for the application code. In a .cpp file, you can use a static or anonymous namespace to hide symbols. Also, a namespace can be used for an enum to prevent the corresponding names from falling into an external namespace (but it’s better to use an enum class). 16. Deferred initialization. If arguments are required for initialization, then you normally shouldn’t write a default constructor. If later you’ll need to delay initialization, you can add a default constructor that will create an invalid object. Or, for a small number of objects, you can use shared_ptr/unique_ptr. Loader(DB::Connection * connection_, const std::string &amp; query, size_t max_block_size_); /// For deferred initialization Loader() {}  17. Virtual functions. If the class is not intended for polymorphic use, you do not need to make functions virtual. This also applies to the destructor. 18. Encodings. Use UTF-8 everywhere. Use std::string and char *. Do not use std::wstring and wchar_t. 19. Logging. See the examples everywhere in the code. Before committing, delete all meaningless and debug logging, and any other types of debug output. Logging in cycles should be avoided, even on the Trace level. Logs must be readable at any logging level. Logging should only be used in application code, for the most part. Log messages must be written in English. The log should preferably be understandable for the system administrator. Do not use profanity in the log. Use UTF-8 encoding in the log. In rare cases you can use non-ASCII characters in the log. 20. Input-output. Don’t use iostreams in internal cycles that are critical for application performance (and never use stringstream). Use the DB/IO library instead. 21. Date and time. See the DateLUT library. 22. include. Always use #pragma once instead of include guards. 23. using. using namespace is not used. You can use using with something specific. But make it local inside a class or function. 24. Do not use trailing return type for functions unless necessary. auto f() -&gt; void  25. Declaration and initialization of variables. //right way std::string s = &quot;Hello&quot;; std::string s{&quot;Hello&quot;}; //wrong way auto s = std::string{&quot;Hello&quot;};  26. For virtual functions, write virtual in the base class, but write override instead of virtual in descendent classes. "},{"title":"Unused Features of C++​","type":1,"pageTitle":"How to Write C++ Code","url":"en/development/style#unused-features-of-c","content":"1. Virtual inheritance is not used. 2. Exception specifiers from C++03 are not used. 3. Constructs which have convenient syntactic sugar in modern C++, e.g. // Traditional way without syntactic sugar template &lt;typename G, typename = std::enable_if_t&lt;std::is_same&lt;G, F&gt;::value, void&gt;&gt; // SFINAE via std::enable_if, usage of ::value std::pair&lt;int, int&gt; func(const E&lt;G&gt; &amp; e) // explicitly specified return type { if (elements.count(e)) // .count() membership test { // ... } elements.erase( std::remove_if( elements.begin(), elements.end(), [&amp;](const auto x){ return x == 1; }), elements.end()); // remove-erase idiom return std::make_pair(1, 2); // create pair via make_pair() } // With syntactic sugar (C++14/17/20) template &lt;typename G&gt; requires std::same_v&lt;G, F&gt; // SFINAE via C++20 concept, usage of C++14 template alias auto func(const E&lt;G&gt; &amp; e) // auto return type (C++14) { if (elements.contains(e)) // C++20 .contains membership test { // ... } elements.erase_if( elements, [&amp;](const auto x){ return x == 1; }); // C++20 std::erase_if return {1, 2}; // or: return std::pair(1, 2); // create pair via initialization list or value initialization (C++17) }  "},{"title":"Platform​","type":1,"pageTitle":"How to Write C++ Code","url":"en/development/style#platform","content":"1. We write code for a specific platform. But other things being equal, cross-platform or portable code is preferred. 2. Language: C++20 (see the list of available C++20 features). 3. Compiler: clang. At this time (April 2021), the code is compiled using clang version 11. (It can also be compiled using gcc version 10, but it's untested and not suitable for production usage). The standard library is used (libc++). 4.OS: Linux Ubuntu, not older than Precise. 5.Code is written for x86_64 CPU architecture. The CPU instruction set is the minimum supported set among our servers. Currently, it is SSE 4.2. 6. Use -Wall -Wextra -Werror compilation flags. Also -Weverything is used with few exceptions. 7. Use static linking with all libraries except those that are difficult to connect to statically (see the output of the ldd command). 8. Code is developed and debugged with release settings. "},{"title":"Tools​","type":1,"pageTitle":"How to Write C++ Code","url":"en/development/style#tools","content":"1. KDevelop is a good IDE. 2. For debugging, use gdb, valgrind (memcheck), strace, -fsanitize=..., or tcmalloc_minimal_debug. 3. For profiling, use Linux Perf, valgrind (callgrind), or strace -cf. 4. Sources are in Git. 5. Assembly uses CMake. 6. Programs are released using deb packages. 7. Commits to master must not break the build. Though only selected revisions are considered workable. 8. Make commits as often as possible, even if the code is only partially ready. Use branches for this purpose. If your code in the master branch is not buildable yet, exclude it from the build before the push. You’ll need to finish it or remove it within a few days. 9. For non-trivial changes, use branches and publish them on the server. 10. Unused code is removed from the repository. "},{"title":"Libraries​","type":1,"pageTitle":"How to Write C++ Code","url":"en/development/style#libraries","content":"1. The C++20 standard library is used (experimental extensions are allowed), as well as boost and Poco frameworks. 2. It is not allowed to use libraries from OS packages. It is also not allowed to use pre-installed libraries. All libraries should be placed in form of source code in contrib directory and built with ClickHouse. See Guidelines for adding new third-party libraries for details. 3. Preference is always given to libraries that are already in use. "},{"title":"General Recommendations​","type":1,"pageTitle":"How to Write C++ Code","url":"en/development/style#general-recommendations-1","content":"1. Write as little code as possible. 2. Try the simplest solution. 3. Don’t write code until you know how it’s going to work and how the inner loop will function. 4. In the simplest cases, use using instead of classes or structs. 5. If possible, do not write copy constructors, assignment operators, destructors (other than a virtual one, if the class contains at least one virtual function), move constructors or move assignment operators. In other words, the compiler-generated functions must work correctly. You can use default. 6. Code simplification is encouraged. Reduce the size of your code where possible. "},{"title":"Additional Recommendations​","type":1,"pageTitle":"How to Write C++ Code","url":"en/development/style#additional-recommendations","content":"1. Explicitly specifying std:: for types from stddef.h is not recommended. In other words, we recommend writing size_t instead std::size_t, because it’s shorter. It is acceptable to add std::. 2. Explicitly specifying std:: for functions from the standard C library is not recommended. In other words, write memcpy instead of std::memcpy. The reason is that there are similar non-standard functions, such as memmem. We do use these functions on occasion. These functions do not exist in namespace std. If you write std::memcpy instead of memcpy everywhere, then memmem without std:: will look strange. Nevertheless, you can still use std:: if you prefer it. 3. Using functions from C when the same ones are available in the standard C++ library. This is acceptable if it is more efficient. For example, use memcpy instead of std::copy for copying large chunks of memory. 4. Multiline function arguments. Any of the following wrapping styles are allowed: function( T1 x1, T2 x2)  function( size_t left, size_t right, const &amp; RangesInDataParts ranges, size_t limit)  function(size_t left, size_t right, const &amp; RangesInDataParts ranges, size_t limit)  function(size_t left, size_t right, const &amp; RangesInDataParts ranges, size_t limit)  function( size_t left, size_t right, const &amp; RangesInDataParts ranges, size_t limit)  Original article "},{"title":"Stripelog","type":0,"sectionRef":"#","url":"en/engines/table-engines/log-family/stripelog","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"Stripelog","url":"en/engines/table-engines/log-family/stripelog#table_engines-stripelog-creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( column1_name [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], column2_name [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = StripeLog  See the detailed description of the CREATE TABLE query. "},{"title":"Writing the Data​","type":1,"pageTitle":"Stripelog","url":"en/engines/table-engines/log-family/stripelog#table_engines-stripelog-writing-the-data","content":"The StripeLog engine stores all the columns in one file. For each INSERT query, ClickHouse appends the data block to the end of a table file, writing columns one by one. For each table ClickHouse writes the files: data.bin — Data file.index.mrk — File with marks. Marks contain offsets for each column of each data block inserted. The StripeLog engine does not support the ALTER UPDATE and ALTER DELETE operations. "},{"title":"Reading the Data​","type":1,"pageTitle":"Stripelog","url":"en/engines/table-engines/log-family/stripelog#table_engines-stripelog-reading-the-data","content":"The file with marks allows ClickHouse to parallelize the reading of data. This means that a SELECT query returns rows in an unpredictable order. Use the ORDER BY clause to sort rows. "},{"title":"Example of Use​","type":1,"pageTitle":"Stripelog","url":"en/engines/table-engines/log-family/stripelog#table_engines-stripelog-example-of-use","content":"Creating a table: CREATE TABLE stripe_log_table ( timestamp DateTime, message_type String, message String ) ENGINE = StripeLog  Inserting data: INSERT INTO stripe_log_table VALUES (now(),'REGULAR','The first regular message') INSERT INTO stripe_log_table VALUES (now(),'REGULAR','The second regular message'),(now(),'WARNING','The first warning message')  We used two INSERT queries to create two data blocks inside the data.bin file. ClickHouse uses multiple threads when selecting data. Each thread reads a separate data block and returns resulting rows independently as it finishes. As a result, the order of blocks of rows in the output does not match the order of the same blocks in the input in most cases. For example: SELECT * FROM stripe_log_table  ┌───────────timestamp─┬─message_type─┬─message────────────────────┐ │ 2019-01-18 14:27:32 │ REGULAR │ The second regular message │ │ 2019-01-18 14:34:53 │ WARNING │ The first warning message │ └─────────────────────┴──────────────┴────────────────────────────┘ ┌───────────timestamp─┬─message_type─┬─message───────────────────┐ │ 2019-01-18 14:23:43 │ REGULAR │ The first regular message │ └─────────────────────┴──────────────┴───────────────────────────┘  Sorting the results (ascending order by default): SELECT * FROM stripe_log_table ORDER BY timestamp  ┌───────────timestamp─┬─message_type─┬─message────────────────────┐ │ 2019-01-18 14:23:43 │ REGULAR │ The first regular message │ │ 2019-01-18 14:27:32 │ REGULAR │ The second regular message │ │ 2019-01-18 14:34:53 │ WARNING │ The first warning message │ └─────────────────────┴──────────────┴────────────────────────────┘  Original article "},{"title":"Hive","type":0,"sectionRef":"#","url":"en/engines/table-engines/integrations/hive","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"Hive","url":"en/engines/table-engines/integrations/hive#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [ALIAS expr1], name2 [type2] [ALIAS expr2], ... ) ENGINE = Hive('thrift://host:port', 'database', 'table'); PARTITION BY expr  See a detailed description of the CREATE TABLE query. The table structure can differ from the original Hive table structure: Column names should be the same as in the original Hive table, but you can use just some of these columns and in any order, also you can use some alias columns calculated from other columns.Column types should be the same from those in the original Hive table.Partition by expression should be consistent with the original Hive table, and columns in partition by expression should be in the table structure. Engine Parameters thrift://host:port — Hive Metastore address database — Remote database name. table — Remote table name. "},{"title":"Usage Example​","type":1,"pageTitle":"Hive","url":"en/engines/table-engines/integrations/hive#usage-example","content":""},{"title":"How to Use Local Cache for HDFS Filesystem​","type":1,"pageTitle":"Hive","url":"en/engines/table-engines/integrations/hive#how-to-use-local-cache-for-hdfs-filesystem","content":"We strongly advice you to enable local cache for remote filesystems. Benchmark shows that its almost 2x faster with cache. Before using cache, add it to config.xml &lt;local_cache_for_remote_fs&gt; &lt;enable&gt;true&lt;/enable&gt; &lt;root_dir&gt;local_cache&lt;/root_dir&gt; &lt;limit_size&gt;559096952&lt;/limit_size&gt; &lt;bytes_read_before_flush&gt;1048576&lt;/bytes_read_before_flush&gt; &lt;/local_cache_for_remote_fs&gt;  enable: ClickHouse will maintain local cache for remote filesystem(HDFS) after startup if true.root_dir: Required. The root directory to store local cache files for remote filesystem.limit_size: Required. The maximum size(in bytes) of local cache files.bytes_read_before_flush: Control bytes before flush to local filesystem when downloading file from remote filesystem. The default value is 1MB. When ClickHouse is started up with local cache for remote filesystem enabled, users can still choose not to use cache with settings use_local_cache_for_remote_fs = 0 in their query. use_local_cache_for_remote_fs is false in default. "},{"title":"Query Hive Table with ORC Input Format​","type":1,"pageTitle":"Hive","url":"en/engines/table-engines/integrations/hive#query-hive-table-with-orc-input-format","content":"Create Table in Hive​ hive &gt; CREATE TABLE `test`.`test_orc`( `f_tinyint` tinyint, `f_smallint` smallint, `f_int` int, `f_integer` int, `f_bigint` bigint, `f_float` float, `f_double` double, `f_decimal` decimal(10,0), `f_timestamp` timestamp, `f_date` date, `f_string` string, `f_varchar` varchar(100), `f_bool` boolean, `f_binary` binary, `f_array_int` array&lt;int&gt;, `f_array_string` array&lt;string&gt;, `f_array_float` array&lt;float&gt;, `f_array_array_int` array&lt;array&lt;int&gt;&gt;, `f_array_array_string` array&lt;array&lt;string&gt;&gt;, `f_array_array_float` array&lt;array&lt;float&gt;&gt;) PARTITIONED BY ( `day` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat' LOCATION 'hdfs://testcluster/data/hive/test.db/test_orc' OK Time taken: 0.51 seconds hive &gt; insert into test.test_orc partition(day='2021-09-18') select 1, 2, 3, 4, 5, 6.11, 7.22, 8.333, current_timestamp(), current_date(), 'hello world', 'hello world', 'hello world', true, 'hello world', array(1, 2, 3), array('hello world', 'hello world'), array(float(1.1), float(1.2)), array(array(1, 2), array(3, 4)), array(array('a', 'b'), array('c', 'd')), array(array(float(1.11), float(2.22)), array(float(3.33), float(4.44))); OK Time taken: 36.025 seconds hive &gt; select * from test.test_orc; OK 1 2 3 4 5 6.11 7.22 8 2021-11-05 12:38:16.314 2021-11-05 hello world hello world hello world true hello world [1,2,3] [&quot;hello world&quot;,&quot;hello world&quot;] [1.1,1.2] [[1,2],[3,4]] [[&quot;a&quot;,&quot;b&quot;],[&quot;c&quot;,&quot;d&quot;]] [[1.11,2.22],[3.33,4.44]] 2021-09-18 Time taken: 0.295 seconds, Fetched: 1 row(s)  Create Table in ClickHouse​ Table in ClickHouse, retrieving data from the Hive table created above: CREATE TABLE test.test_orc ( `f_tinyint` Int8, `f_smallint` Int16, `f_int` Int32, `f_integer` Int32, `f_bigint` Int64, `f_float` Float32, `f_double` Float64, `f_decimal` Float64, `f_timestamp` DateTime, `f_date` Date, `f_string` String, `f_varchar` String, `f_bool` Bool, `f_binary` String, `f_array_int` Array(Int32), `f_array_string` Array(String), `f_array_float` Array(Float32), `f_array_array_int` Array(Array(Int32)), `f_array_array_string` Array(Array(String)), `f_array_array_float` Array(Array(Float32)), `day` String ) ENGINE = Hive('thrift://202.168.117.26:9083', 'test', 'test_orc') PARTITION BY day  SELECT * FROM test.test_orc settings input_format_orc_allow_missing_columns = 1\\G  SELECT * FROM test.test_orc SETTINGS input_format_orc_allow_missing_columns = 1 Query id: c3eaffdc-78ab-43cd-96a4-4acc5b480658 Row 1: ────── f_tinyint: 1 f_smallint: 2 f_int: 3 f_integer: 4 f_bigint: 5 f_float: 6.11 f_double: 7.22 f_decimal: 8 f_timestamp: 2021-12-04 04:00:44 f_date: 2021-12-03 f_string: hello world f_varchar: hello world f_bool: true f_binary: hello world f_array_int: [1,2,3] f_array_string: ['hello world','hello world'] f_array_float: [1.1,1.2] f_array_array_int: [[1,2],[3,4]] f_array_array_string: [['a','b'],['c','d']] f_array_array_float: [[1.11,2.22],[3.33,4.44]] day: 2021-09-18 1 rows in set. Elapsed: 0.078 sec.  "},{"title":"Query Hive Table with Parquet Input Format​","type":1,"pageTitle":"Hive","url":"en/engines/table-engines/integrations/hive#query-hive-table-with-parquet-input-format","content":"Create Table in Hive​ hive &gt; CREATE TABLE `test`.`test_parquet`( `f_tinyint` tinyint, `f_smallint` smallint, `f_int` int, `f_integer` int, `f_bigint` bigint, `f_float` float, `f_double` double, `f_decimal` decimal(10,0), `f_timestamp` timestamp, `f_date` date, `f_string` string, `f_varchar` varchar(100), `f_char` char(100), `f_bool` boolean, `f_binary` binary, `f_array_int` array&lt;int&gt;, `f_array_string` array&lt;string&gt;, `f_array_float` array&lt;float&gt;, `f_array_array_int` array&lt;array&lt;int&gt;&gt;, `f_array_array_string` array&lt;array&lt;string&gt;&gt;, `f_array_array_float` array&lt;array&lt;float&gt;&gt;) PARTITIONED BY ( `day` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat' LOCATION 'hdfs://testcluster/data/hive/test.db/test_parquet' OK Time taken: 0.51 seconds hive &gt; insert into test.test_parquet partition(day='2021-09-18') select 1, 2, 3, 4, 5, 6.11, 7.22, 8.333, current_timestamp(), current_date(), 'hello world', 'hello world', 'hello world', true, 'hello world', array(1, 2, 3), array('hello world', 'hello world'), array(float(1.1), float(1.2)), array(array(1, 2), array(3, 4)), array(array('a', 'b'), array('c', 'd')), array(array(float(1.11), float(2.22)), array(float(3.33), float(4.44))); OK Time taken: 36.025 seconds hive &gt; select * from test.test_parquet; OK 1 2 3 4 5 6.11 7.22 8 2021-12-14 17:54:56.743 2021-12-14 hello world hello world hello world true hello world [1,2,3] [&quot;hello world&quot;,&quot;hello world&quot;] [1.1,1.2] [[1,2],[3,4]] [[&quot;a&quot;,&quot;b&quot;],[&quot;c&quot;,&quot;d&quot;]] [[1.11,2.22],[3.33,4.44]] 2021-09-18 Time taken: 0.766 seconds, Fetched: 1 row(s)  Create Table in ClickHouse​ Table in ClickHouse, retrieving data from the Hive table created above: CREATE TABLE test.test_parquet ( `f_tinyint` Int8, `f_smallint` Int16, `f_int` Int32, `f_integer` Int32, `f_bigint` Int64, `f_float` Float32, `f_double` Float64, `f_decimal` Float64, `f_timestamp` DateTime, `f_date` Date, `f_string` String, `f_varchar` String, `f_char` String, `f_bool` Bool, `f_binary` String, `f_array_int` Array(Int32), `f_array_string` Array(String), `f_array_float` Array(Float32), `f_array_array_int` Array(Array(Int32)), `f_array_array_string` Array(Array(String)), `f_array_array_float` Array(Array(Float32)), `day` String ) ENGINE = Hive('thrift://localhost:9083', 'test', 'test_parquet') PARTITION BY day  SELECT * FROM test.test_parquet settings input_format_parquet_allow_missing_columns = 1\\G  SELECT * FROM test_parquet SETTINGS input_format_parquet_allow_missing_columns = 1 Query id: 4e35cf02-c7b2-430d-9b81-16f438e5fca9 Row 1: ────── f_tinyint: 1 f_smallint: 2 f_int: 3 f_integer: 4 f_bigint: 5 f_float: 6.11 f_double: 7.22 f_decimal: 8 f_timestamp: 2021-12-14 17:54:56 f_date: 2021-12-14 f_string: hello world f_varchar: hello world f_char: hello world f_bool: true f_binary: hello world f_array_int: [1,2,3] f_array_string: ['hello world','hello world'] f_array_float: [1.1,1.2] f_array_array_int: [[1,2],[3,4]] f_array_array_string: [['a','b'],['c','d']] f_array_array_float: [[1.11,2.22],[3.33,4.44]] day: 2021-09-18 1 rows in set. Elapsed: 0.357 sec.  "},{"title":"Query Hive Table with Text Input Format​","type":1,"pageTitle":"Hive","url":"en/engines/table-engines/integrations/hive#query-hive-table-with-text-input-format","content":"Create Table in Hive​ hive &gt; CREATE TABLE `test`.`test_text`( `f_tinyint` tinyint, `f_smallint` smallint, `f_int` int, `f_integer` int, `f_bigint` bigint, `f_float` float, `f_double` double, `f_decimal` decimal(10,0), `f_timestamp` timestamp, `f_date` date, `f_string` string, `f_varchar` varchar(100), `f_char` char(100), `f_bool` boolean, `f_binary` binary, `f_array_int` array&lt;int&gt;, `f_array_string` array&lt;string&gt;, `f_array_float` array&lt;float&gt;, `f_array_array_int` array&lt;array&lt;int&gt;&gt;, `f_array_array_string` array&lt;array&lt;string&gt;&gt;, `f_array_array_float` array&lt;array&lt;float&gt;&gt;) PARTITIONED BY ( `day` string) ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat' LOCATION 'hdfs://testcluster/data/hive/test.db/test_text' Time taken: 0.1 seconds, Fetched: 34 row(s) hive &gt; insert into test.test_text partition(day='2021-09-18') select 1, 2, 3, 4, 5, 6.11, 7.22, 8.333, current_timestamp(), current_date(), 'hello world', 'hello world', 'hello world', true, 'hello world', array(1, 2, 3), array('hello world', 'hello world'), array(float(1.1), float(1.2)), array(array(1, 2), array(3, 4)), array(array('a', 'b'), array('c', 'd')), array(array(float(1.11), float(2.22)), array(float(3.33), float(4.44))); OK Time taken: 36.025 seconds hive &gt; select * from test.test_text; OK 1 2 3 4 5 6.11 7.22 8 2021-12-14 18:11:17.239 2021-12-14 hello world hello world hello world true hello world [1,2,3] [&quot;hello world&quot;,&quot;hello world&quot;] [1.1,1.2] [[1,2],[3,4]] [[&quot;a&quot;,&quot;b&quot;],[&quot;c&quot;,&quot;d&quot;]] [[1.11,2.22],[3.33,4.44]] 2021-09-18 Time taken: 0.624 seconds, Fetched: 1 row(s)  Create Table in ClickHouse​ Table in ClickHouse, retrieving data from the Hive table created above: CREATE TABLE test.test_text ( `f_tinyint` Int8, `f_smallint` Int16, `f_int` Int32, `f_integer` Int32, `f_bigint` Int64, `f_float` Float32, `f_double` Float64, `f_decimal` Float64, `f_timestamp` DateTime, `f_date` Date, `f_string` String, `f_varchar` String, `f_char` String, `f_bool` Bool, `day` String ) ENGINE = Hive('thrift://localhost:9083', 'test', 'test_text') PARTITION BY day  SELECT * FROM test.test_text settings input_format_skip_unknown_fields = 1, input_format_with_names_use_header = 1, date_time_input_format = 'best_effort'\\G  SELECT * FROM test.test_text SETTINGS input_format_skip_unknown_fields = 1, input_format_with_names_use_header = 1, date_time_input_format = 'best_effort' Query id: 55b79d35-56de-45b9-8be6-57282fbf1f44 Row 1: ────── f_tinyint: 1 f_smallint: 2 f_int: 3 f_integer: 4 f_bigint: 5 f_float: 6.11 f_double: 7.22 f_decimal: 8 f_timestamp: 2021-12-14 18:11:17 f_date: 2021-12-14 f_string: hello world f_varchar: hello world f_char: hello world f_bool: true day: 2021-09-18  Original article "},{"title":"TinyLog","type":0,"sectionRef":"#","url":"en/engines/table-engines/log-family/tinylog","content":"TinyLog The engine belongs to the log engine family. See Log Engine Family for common properties of log engines and their differences. This table engine is typically used with the write-once method: write data one time, then read it as many times as necessary. For example, you can use TinyLog-type tables for intermediary data that is processed in small batches. Note that storing data in a large number of small tables is inefficient. Queries are executed in a single stream. In other words, this engine is intended for relatively small tables (up to about 1,000,000 rows). It makes sense to use this table engine if you have many small tables, since it’s simpler than the Log engine (fewer files need to be opened). Original article","keywords":""},{"title":"MergeTree Engine Family","type":0,"sectionRef":"#","url":"en/engines/table-engines/mergetree-family/","content":"MergeTree Engine Family Table engines from the MergeTree family are the core of ClickHouse data storage capabilities. They provide most features for resilience and high-performance data retrieval: columnar storage, custom partitioning, sparse primary index, secondary data-skipping indexes, etc. Base MergeTree table engine can be considered the default table engine for single-node ClickHouse instances because it is versatile and practical for a wide range of use cases. For production usage ReplicatedMergeTree is the way to go, because it adds high-availability to all features of regular MergeTree engine. A bonus is automatic data deduplication on data ingestion, so the software can safely retry if there was some network issue during insert. All other engines of MergeTree family add extra functionality for some specific use cases. Usually, it’s implemented as additional data manipulation in background. The main downside of MergeTree engines is that they are rather heavy-weight. So the typical pattern is to have not so many of them. If you need many small tables, for example for temporary data, consider Log engine family.","keywords":""},{"title":"AggregatingMergeTree","type":0,"sectionRef":"#","url":"en/engines/table-engines/mergetree-family/aggregatingmergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"AggregatingMergeTree","url":"en/engines/table-engines/mergetree-family/aggregatingmergetree#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = AggregatingMergeTree() [PARTITION BY expr] [ORDER BY expr] [SAMPLE BY expr] [TTL expr] [SETTINGS name=value, ...]  For a description of request parameters, see request description. Query clauses When creating a AggregatingMergeTree table the same clauses are required, as when creating a MergeTree table. Deprecated Method for Creating a Table warning Do not use this method in new projects and, if possible, switch the old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE [=] AggregatingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity) All of the parameters have the same meaning as in MergeTree. "},{"title":"SELECT and INSERT​","type":1,"pageTitle":"AggregatingMergeTree","url":"en/engines/table-engines/mergetree-family/aggregatingmergetree#select-and-insert","content":"To insert data, use INSERT SELECT query with aggregate -State- functions. When selecting data from AggregatingMergeTree table, use GROUP BY clause and the same aggregate functions as when inserting data, but using -Merge suffix. In the results of SELECT query, the values of AggregateFunction type have implementation-specific binary representation for all of the ClickHouse output formats. If dump data into, for example, TabSeparated format with SELECT query then this dump can be loaded back using INSERT query. "},{"title":"Example of an Aggregated Materialized View​","type":1,"pageTitle":"AggregatingMergeTree","url":"en/engines/table-engines/mergetree-family/aggregatingmergetree#example-of-an-aggregated-materialized-view","content":"AggregatingMergeTree materialized view that watches the test.visits table: CREATE MATERIALIZED VIEW test.basic ENGINE = AggregatingMergeTree() PARTITION BY toYYYYMM(StartDate) ORDER BY (CounterID, StartDate) AS SELECT CounterID, StartDate, sumState(Sign) AS Visits, uniqState(UserID) AS Users FROM test.visits GROUP BY CounterID, StartDate;  Inserting data into the test.visits table. INSERT INTO test.visits ...  The data are inserted in both the table and view test.basic that will perform the aggregation. To get the aggregated data, we need to execute a query such as SELECT ... GROUP BY ... from the view test.basic: SELECT StartDate, sumMerge(Visits) AS Visits, uniqMerge(Users) AS Users FROM test.basic GROUP BY StartDate ORDER BY StartDate;  Original article "},{"title":"Custom Partitioning Key","type":0,"sectionRef":"#","url":"en/engines/table-engines/mergetree-family/custom-partitioning-key","content":"Custom Partitioning Key warning In most cases you do not need a partition key, and in most other cases you do not need a partition key more granular than by months. Partitioning does not speed up queries (in contrast to the ORDER BY expression). You should never use too granular of partitioning. Don't partition your data by client identifiers or names. Instead, make a client identifier or name the first column in the ORDER BY expression. Partitioning is available for the MergeTree family tables (including replicated tables). Materialized views based on MergeTree tables support partitioning, as well. A partition is a logical combination of records in a table by a specified criterion. You can set a partition by an arbitrary criterion, such as by month, by day, or by event type. Each partition is stored separately to simplify manipulations of this data. When accessing the data, ClickHouse uses the smallest subset of partitions possible. The partition is specified in the PARTITION BY expr clause when creating a table. The partition key can be any expression from the table columns. For example, to specify partitioning by month, use the expression toYYYYMM(date_column): CREATE TABLE visits ( VisitDate Date, Hour UInt8, ClientID UUID ) ENGINE = MergeTree() PARTITION BY toYYYYMM(VisitDate) ORDER BY Hour; The partition key can also be a tuple of expressions (similar to the primary key). For example: ENGINE = ReplicatedCollapsingMergeTree('/clickhouse/tables/name', 'replica1', Sign) PARTITION BY (toMonday(StartDate), EventType) ORDER BY (CounterID, StartDate, intHash32(UserID)); In this example, we set partitioning by the event types that occurred during the current week. By default, the floating-point partition key is not supported. To use it enable the setting allow_floating_point_partition_key. When inserting new data to a table, this data is stored as a separate part (chunk) sorted by the primary key. In 10-15 minutes after inserting, the parts of the same partition are merged into the entire part. info A merge only works for data parts that have the same value for the partitioning expression. This means you shouldn’t make overly granular partitions (more than about a thousand partitions). Otherwise, the SELECT query performs poorly because of an unreasonably large number of files in the file system and open file descriptors. Use the system.parts table to view the table parts and partitions. For example, let’s assume that we have a visits table with partitioning by month. Let’s perform the SELECT query for the system.parts table: SELECT partition, name, active FROM system.parts WHERE table = 'visits' ┌─partition─┬─name──────────────┬─active─┐ │ 201901 │ 201901_1_3_1 │ 0 │ │ 201901 │ 201901_1_9_2_11 │ 1 │ │ 201901 │ 201901_8_8_0 │ 0 │ │ 201901 │ 201901_9_9_0 │ 0 │ │ 201902 │ 201902_4_6_1_11 │ 1 │ │ 201902 │ 201902_10_10_0_11 │ 1 │ │ 201902 │ 201902_11_11_0_11 │ 1 │ └───────────┴───────────────────┴────────┘ The partition column contains the names of the partitions. There are two partitions in this example: 201901 and 201902. You can use this column value to specify the partition name in ALTER … PARTITION queries. The name column contains the names of the partition data parts. You can use this column to specify the name of the part in the ALTER ATTACH PART query. Let’s break down the name of the part: 201901_1_9_2_11: 201901 is the partition name.1 is the minimum number of the data block.9 is the maximum number of the data block.2 is the chunk level (the depth of the merge tree it is formed from).11 is the mutation version (if a part mutated) info The parts of old-type tables have the name: 20190117_20190123_2_2_0 (minimum date - maximum date - minimum block number - maximum block number - level). The active column shows the status of the part. 1 is active; 0 is inactive. The inactive parts are, for example, source parts remaining after merging to a larger part. The corrupted data parts are also indicated as inactive. As you can see in the example, there are several separated parts of the same partition (for example, 201901_1_3_1 and 201901_1_9_2). This means that these parts are not merged yet. ClickHouse merges the inserted parts of data periodically, approximately 15 minutes after inserting. In addition, you can perform a non-scheduled merge using the OPTIMIZE query. Example: OPTIMIZE TABLE visits PARTITION 201902; ┌─partition─┬─name─────────────┬─active─┐ │ 201901 │ 201901_1_3_1 │ 0 │ │ 201901 │ 201901_1_9_2_11 │ 1 │ │ 201901 │ 201901_8_8_0 │ 0 │ │ 201901 │ 201901_9_9_0 │ 0 │ │ 201902 │ 201902_4_6_1 │ 0 │ │ 201902 │ 201902_4_11_2_11 │ 1 │ │ 201902 │ 201902_10_10_0 │ 0 │ │ 201902 │ 201902_11_11_0 │ 0 │ └───────────┴──────────────────┴────────┘ Inactive parts will be deleted approximately 10 minutes after merging. Another way to view a set of parts and partitions is to go into the directory of the table: /var/lib/clickhouse/data/&lt;database&gt;/&lt;table&gt;/. For example: /var/lib/clickhouse/data/default/visits$ ls -l total 40 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 1 16:48 201901_1_3_1 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 16:17 201901_1_9_2_11 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 15:52 201901_8_8_0 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 15:52 201901_9_9_0 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 16:17 201902_10_10_0 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 16:17 201902_11_11_0 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 16:19 201902_4_11_2_11 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 5 12:09 201902_4_6_1 drwxr-xr-x 2 clickhouse clickhouse 4096 Feb 1 16:48 detached The folders ‘201901_1_1_0’, ‘201901_1_7_1’ and so on are the directories of the parts. Each part relates to a corresponding partition and contains data just for a certain month (the table in this example has partitioning by month). The detached directory contains parts that were detached from the table using the DETACH query. The corrupted parts are also moved to this directory, instead of being deleted. The server does not use the parts from the detached directory. You can add, delete, or modify the data in this directory at any time – the server will not know about this until you run the ATTACH query. Note that on the operating server, you cannot manually change the set of parts or their data on the file system, since the server will not know about it. For non-replicated tables, you can do this when the server is stopped, but it isn’t recommended. For replicated tables, the set of parts cannot be changed in any case. ClickHouse allows you to perform operations with the partitions: delete them, copy from one table to another, or create a backup. See the list of all operations in the section Manipulations With Partitions and Parts. Original article","keywords":""},{"title":"ReplacingMergeTree","type":0,"sectionRef":"#","url":"en/engines/table-engines/mergetree-family/replacingmergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"ReplacingMergeTree","url":"en/engines/table-engines/mergetree-family/replacingmergetree#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = ReplacingMergeTree([ver]) [PARTITION BY expr] [ORDER BY expr] [PRIMARY KEY expr] [SAMPLE BY expr] [SETTINGS name=value, ...]  For a description of request parameters, see statement description. warning Uniqueness of rows is determined by the ORDER BY table section, not PRIMARY KEY. ReplacingMergeTree Parameters ver — column with the version number. Type UInt*, Date, DateTime or DateTime64. Optional parameter. When merging, ReplacingMergeTree from all the rows with the same sorting key leaves only one: The last in the selection, if ver not set. A selection is a set of rows in a set of parts participating in the merge. The most recently created part (the last insert) will be the last one in the selection. Thus, after deduplication, the very last row from the most recent insert will remain for each unique sorting key.With the maximum version, if ver specified. Query clauses When creating a ReplacingMergeTree table the same clauses are required, as when creating a MergeTree table. Deprecated Method for Creating a Table warning Do not use this method in new projects and, if possible, switch old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE [=] ReplacingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, [ver]) All of the parameters excepting ver have the same meaning as in MergeTree. ver - column with the version. Optional parameter. For a description, see the text above. "},{"title":"SummingMergeTree","type":0,"sectionRef":"#","url":"en/engines/table-engines/mergetree-family/summingmergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"SummingMergeTree","url":"en/engines/table-engines/mergetree-family/summingmergetree#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = SummingMergeTree([columns]) [PARTITION BY expr] [ORDER BY expr] [SAMPLE BY expr] [SETTINGS name=value, ...]  For a description of request parameters, see request description. Parameters of SummingMergeTree columns - a tuple with the names of columns where values will be summarized. Optional parameter. The columns must be of a numeric type and must not be in the primary key. If columns not specified, ClickHouse summarizes the values in all columns with a numeric data type that are not in the primary key. Query clauses When creating a SummingMergeTree table the same clauses are required, as when creating a MergeTree table. Deprecated Method for Creating a Table warning Do not use this method in new projects and, if possible, switch the old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE [=] SummingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, [columns]) All of the parameters excepting columns have the same meaning as in MergeTree. columns — tuple with names of columns values of which will be summarized. Optional parameter. For a description, see the text above. "},{"title":"Usage Example​","type":1,"pageTitle":"SummingMergeTree","url":"en/engines/table-engines/mergetree-family/summingmergetree#usage-example","content":"Consider the following table: CREATE TABLE summtt ( key UInt32, value UInt32 ) ENGINE = SummingMergeTree() ORDER BY key  Insert data to it: INSERT INTO summtt Values(1,1),(1,2),(2,1)  ClickHouse may sum all the rows not completely (see below), so we use an aggregate function sum and GROUP BY clause in the query. SELECT key, sum(value) FROM summtt GROUP BY key  ┌─key─┬─sum(value)─┐ │ 2 │ 1 │ │ 1 │ 3 │ └─────┴────────────┘  "},{"title":"Data Processing​","type":1,"pageTitle":"SummingMergeTree","url":"en/engines/table-engines/mergetree-family/summingmergetree#data-processing","content":"When data are inserted into a table, they are saved as-is. ClickHouse merges the inserted parts of data periodically and this is when rows with the same primary key are summed and replaced with one for each resulting part of data. ClickHouse can merge the data parts so that different resulting parts of data can consist rows with the same primary key, i.e. the summation will be incomplete. Therefore (SELECT) an aggregate function sum() and GROUP BY clause should be used in a query as described in the example above. "},{"title":"Common Rules for Summation​","type":1,"pageTitle":"SummingMergeTree","url":"en/engines/table-engines/mergetree-family/summingmergetree#common-rules-for-summation","content":"The values in the columns with the numeric data type are summarized. The set of columns is defined by the parameter columns. If the values were 0 in all of the columns for summation, the row is deleted. If column is not in the primary key and is not summarized, an arbitrary value is selected from the existing ones. The values are not summarized for columns in the primary key. "},{"title":"The Summation in the Aggregatefunction Columns​","type":1,"pageTitle":"SummingMergeTree","url":"en/engines/table-engines/mergetree-family/summingmergetree#the-summation-in-the-aggregatefunction-columns","content":"For columns of AggregateFunction type ClickHouse behaves as AggregatingMergeTree engine aggregating according to the function. "},{"title":"Nested Structures​","type":1,"pageTitle":"SummingMergeTree","url":"en/engines/table-engines/mergetree-family/summingmergetree#nested-structures","content":"Table can have nested data structures that are processed in a special way. If the name of a nested table ends with Map and it contains at least two columns that meet the following criteria: the first column is numeric (*Int*, Date, DateTime) or a string (String, FixedString), let’s call it key,the other columns are arithmetic (*Int*, Float32/64), let’s call it (values...), then this nested table is interpreted as a mapping of key =&gt; (values...), and when merging its rows, the elements of two data sets are merged by key with a summation of the corresponding (values...). Examples: [(1, 100)] + [(2, 150)] -&gt; [(1, 100), (2, 150)] [(1, 100)] + [(1, 150)] -&gt; [(1, 250)] [(1, 100)] + [(1, 150), (2, 150)] -&gt; [(1, 250), (2, 150)] [(1, 100), (2, 150)] + [(1, -100)] -&gt; [(2, 150)]  When requesting data, use the sumMap(key, value) function for aggregation of Map. For nested data structure, you do not need to specify its columns in the tuple of columns for summation. Original article "},{"title":"VersionedCollapsingMergeTree","type":0,"sectionRef":"#","url":"en/engines/table-engines/mergetree-family/versionedcollapsingmergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"VersionedCollapsingMergeTree","url":"en/engines/table-engines/mergetree-family/versionedcollapsingmergetree#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = VersionedCollapsingMergeTree(sign, version) [PARTITION BY expr] [ORDER BY expr] [SAMPLE BY expr] [SETTINGS name=value, ...]  For a description of query parameters, see the query description. Engine Parameters VersionedCollapsingMergeTree(sign, version)  sign — Name of the column with the type of row: 1 is a “state” row, -1 is a “cancel” row. The column data type should be Int8. version — Name of the column with the version of the object state. The column data type should be UInt*. Query Clauses When creating a VersionedCollapsingMergeTree table, the same clauses are required as when creating a MergeTree table. Deprecated Method for Creating a Table warning Do not use this method in new projects. If possible, switch old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE [=] VersionedCollapsingMergeTree(date-column [, samp#table_engines_versionedcollapsingmergetreeling_expression], (primary, key), index_granularity, sign, version) All of the parameters except sign and version have the same meaning as in MergeTree. sign — Name of the column with the type of row: 1 is a “state” row, -1 is a “cancel” row. Column Data Type — Int8. version — Name of the column with the version of the object state. The column data type should be UInt*. "},{"title":"Collapsing​","type":1,"pageTitle":"VersionedCollapsingMergeTree","url":"en/engines/table-engines/mergetree-family/versionedcollapsingmergetree#table_engines_versionedcollapsingmergetree","content":""},{"title":"Data​","type":1,"pageTitle":"VersionedCollapsingMergeTree","url":"en/engines/table-engines/mergetree-family/versionedcollapsingmergetree#data","content":"Consider a situation where you need to save continually changing data for some object. It is reasonable to have one row for an object and update the row whenever there are changes. However, the update operation is expensive and slow for a DBMS because it requires rewriting the data in the storage. Update is not acceptable if you need to write data quickly, but you can write the changes to an object sequentially as follows. Use the Sign column when writing the row. If Sign = 1 it means that the row is a state of an object (let’s call it the “state” row). If Sign = -1 it indicates the cancellation of the state of an object with the same attributes (let’s call it the “cancel” row). Also use the Version column, which should identify each state of an object with a separate number. For example, we want to calculate how many pages users visited on some site and how long they were there. At some point in time we write the following row with the state of user activity: ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ 1 | └─────────────────────┴───────────┴──────────┴──────┴─────────┘  At some point later we register the change of user activity and write it with the following two rows. ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 5 │ 146 │ -1 │ 1 | │ 4324182021466249494 │ 6 │ 185 │ 1 │ 2 | └─────────────────────┴───────────┴──────────┴──────┴─────────┘  The first row cancels the previous state of the object (user). It should copy all of the fields of the canceled state except Sign. The second row contains the current state. Because we need only the last state of user activity, the rows ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ 1 | │ 4324182021466249494 │ 5 │ 146 │ -1 │ 1 | └─────────────────────┴───────────┴──────────┴──────┴─────────┘  can be deleted, collapsing the invalid (old) state of the object. VersionedCollapsingMergeTree does this while merging the data parts. To find out why we need two rows for each change, see Algorithm. Notes on Usage The program that writes the data should remember the state of an object to be able to cancel it. “Cancel” string should contain copies of the primary key fields and the version of the “state” string and the opposite Sign. It increases the initial size of storage but allows to write the data quickly.Long growing arrays in columns reduce the efficiency of the engine due to the load for writing. The more straightforward the data, the better the efficiency.SELECT results depend strongly on the consistency of the history of object changes. Be accurate when preparing data for inserting. You can get unpredictable results with inconsistent data, such as negative values for non-negative metrics like session depth. "},{"title":"Algorithm​","type":1,"pageTitle":"VersionedCollapsingMergeTree","url":"en/engines/table-engines/mergetree-family/versionedcollapsingmergetree#table_engines-versionedcollapsingmergetree-algorithm","content":"When ClickHouse merges data parts, it deletes each pair of rows that have the same primary key and version and different Sign. The order of rows does not matter. When ClickHouse inserts data, it orders rows by the primary key. If the Version column is not in the primary key, ClickHouse adds it to the primary key implicitly as the last field and uses it for ordering. "},{"title":"Selecting Data​","type":1,"pageTitle":"VersionedCollapsingMergeTree","url":"en/engines/table-engines/mergetree-family/versionedcollapsingmergetree#selecting-data","content":"ClickHouse does not guarantee that all of the rows with the same primary key will be in the same resulting data part or even on the same physical server. This is true both for writing the data and for subsequent merging of the data parts. In addition, ClickHouse processes SELECT queries with multiple threads, and it cannot predict the order of rows in the result. This means that aggregation is required if there is a need to get completely “collapsed” data from a VersionedCollapsingMergeTree table. To finalize collapsing, write a query with a GROUP BY clause and aggregate functions that account for the sign. For example, to calculate quantity, use sum(Sign) instead of count(). To calculate the sum of something, use sum(Sign * x) instead of sum(x), and add HAVING sum(Sign) &gt; 0. The aggregates count, sum and avg can be calculated this way. The aggregate uniq can be calculated if an object has at least one non-collapsed state. The aggregates min and max can’t be calculated because VersionedCollapsingMergeTree does not save the history of values of collapsed states. If you need to extract the data with “collapsing” but without aggregation (for example, to check whether rows are present whose newest values match certain conditions), you can use the FINAL modifier for the FROM clause. This approach is inefficient and should not be used with large tables. "},{"title":"Example of Use​","type":1,"pageTitle":"VersionedCollapsingMergeTree","url":"en/engines/table-engines/mergetree-family/versionedcollapsingmergetree#example-of-use","content":"Example data: ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ 1 | │ 4324182021466249494 │ 5 │ 146 │ -1 │ 1 | │ 4324182021466249494 │ 6 │ 185 │ 1 │ 2 | └─────────────────────┴───────────┴──────────┴──────┴─────────┘  Creating the table: CREATE TABLE UAct ( UserID UInt64, PageViews UInt8, Duration UInt8, Sign Int8, Version UInt8 ) ENGINE = VersionedCollapsingMergeTree(Sign, Version) ORDER BY UserID  Inserting the data: INSERT INTO UAct VALUES (4324182021466249494, 5, 146, 1, 1)  INSERT INTO UAct VALUES (4324182021466249494, 5, 146, -1, 1),(4324182021466249494, 6, 185, 1, 2)  We use two INSERT queries to create two different data parts. If we insert the data with a single query, ClickHouse creates one data part and will never perform any merge. Getting the data: SELECT * FROM UAct  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┴─────────┘ ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 5 │ 146 │ -1 │ 1 │ │ 4324182021466249494 │ 6 │ 185 │ 1 │ 2 │ └─────────────────────┴───────────┴──────────┴──────┴─────────┘  What do we see here and where are the collapsed parts? We created two data parts using two INSERT queries. The SELECT query was performed in two threads, and the result is a random order of rows. Collapsing did not occur because the data parts have not been merged yet. ClickHouse merges data parts at an unknown point in time which we cannot predict. This is why we need aggregation: SELECT UserID, sum(PageViews * Sign) AS PageViews, sum(Duration * Sign) AS Duration, Version FROM UAct GROUP BY UserID, Version HAVING sum(Sign) &gt; 0  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Version─┐ │ 4324182021466249494 │ 6 │ 185 │ 2 │ └─────────────────────┴───────────┴──────────┴─────────┘  If we do not need aggregation and want to force collapsing, we can use the FINAL modifier for the FROM clause. SELECT * FROM UAct FINAL  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┬─Version─┐ │ 4324182021466249494 │ 6 │ 185 │ 1 │ 2 │ └─────────────────────┴───────────┴──────────┴──────┴─────────┘  This is a very inefficient way to select data. Don’t use it for large tables. Original article "},{"title":"Special Table Engines","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/","content":"Special Table Engines There are three main categories of table engines: MergeTree engine family for main production use.Log engine family for small temporary data.Table engines for integrations. The remaining engines are unique in their purpose and are not grouped into families yet, thus they are placed in this “special” category.","keywords":""},{"title":"Buffer Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/buffer","content":"Buffer Table Engine Buffers the data to write in RAM, periodically flushing it to another table. During the read operation, data is read from the buffer and the other table simultaneously. Buffer(database, table, num_layers, min_time, max_time, min_rows, max_rows, min_bytes, max_bytes) Engine parameters: database – Database name. You can use currentDatabase() or another constant expression that returns a string.table – Table to flush data to.num_layers – Parallelism layer. Physically, the table will be represented as num_layers of independent buffers. Recommended value: 16.min_time, max_time, min_rows, max_rows, min_bytes, and max_bytes – Conditions for flushing data from the buffer. Optional engine parameters: flush_time, flush_rows, flush_bytes – Conditions for flushing data from the buffer, that will happen only in background (omitted or zero means no flush* parameters). Data is flushed from the buffer and written to the destination table if all the min* conditions or at least one max* condition are met. Also, if at least one flush* condition are met flush initiated in background, this is different from max*, since flush* allows you to configure background flushes separately to avoid adding latency for INSERT (into Buffer) queries. min_time, max_time, flush_time – Condition for the time in seconds from the moment of the first write to the buffer.min_rows, max_rows, flush_rows – Condition for the number of rows in the buffer.min_bytes, max_bytes, flush_bytes – Condition for the number of bytes in the buffer. During the write operation, data is inserted to a num_layers number of random buffers. Or, if the data part to insert is large enough (greater than max_rows or max_bytes), it is written directly to the destination table, omitting the buffer. The conditions for flushing the data are calculated separately for each of the num_layers buffers. For example, if num_layers = 16 and max_bytes = 100000000, the maximum RAM consumption is 1.6 GB. Example: CREATE TABLE merge.hits_buffer AS merge.hits ENGINE = Buffer(merge, hits, 16, 10, 100, 10000, 1000000, 10000000, 100000000) Creating a merge.hits_buffer table with the same structure as merge.hits and using the Buffer engine. When writing to this table, data is buffered in RAM and later written to the ‘merge.hits’ table. 16 buffers are created. The data in each of them is flushed if either 100 seconds have passed, or one million rows have been written, or 100 MB of data have been written; or if simultaneously 10 seconds have passed and 10,000 rows and 10 MB of data have been written. For example, if just one row has been written, after 100 seconds it will be flushed, no matter what. But if many rows have been written, the data will be flushed sooner. When the server is stopped, with DROP TABLE or DETACH TABLE, buffer data is also flushed to the destination table. You can set empty strings in single quotation marks for the database and table name. This indicates the absence of a destination table. In this case, when the data flush conditions are reached, the buffer is simply cleared. This may be useful for keeping a window of data in memory. When reading from a Buffer table, data is processed both from the buffer and from the destination table (if there is one). Note that the Buffer tables does not support an index. In other words, data in the buffer is fully scanned, which might be slow for large buffers. (For data in a subordinate table, the index that it supports will be used.) If the set of columns in the Buffer table does not match the set of columns in a subordinate table, a subset of columns that exist in both tables is inserted. If the types do not match for one of the columns in the Buffer table and a subordinate table, an error message is entered in the server log, and the buffer is cleared. The same thing happens if the subordinate table does not exist when the buffer is flushed. warning Running ALTER on the Buffer table in releases made before 26 Oct 2021 will cause a Block structure mismatch error (see #15117 and #30565), so deleting the Buffer table and then recreating is the only option. It is advisable to check that this error is fixed in your release before trying to run ALTER on the Buffer table. If the server is restarted abnormally, the data in the buffer is lost. FINAL and SAMPLE do not work correctly for Buffer tables. These conditions are passed to the destination table, but are not used for processing data in the buffer. If these features are required we recommend only using the Buffer table for writing, while reading from the destination table. When adding data to a Buffer, one of the buffers is locked. This causes delays if a read operation is simultaneously being performed from the table. Data that is inserted to a Buffer table may end up in the subordinate table in a different order and in different blocks. Because of this, a Buffer table is difficult to use for writing to a CollapsingMergeTree correctly. To avoid problems, you can set num_layers to 1. If the destination table is replicated, some expected characteristics of replicated tables are lost when writing to a Buffer table. The random changes to the order of rows and sizes of data parts cause data deduplication to quit working, which means it is not possible to have a reliable ‘exactly once’ write to replicated tables. Due to these disadvantages, we can only recommend using a Buffer table in rare cases. A Buffer table is used when too many INSERTs are received from a large number of servers over a unit of time and data can’t be buffered before insertion, which means the INSERTs can’t run fast enough. Note that it does not make sense to insert data one row at a time, even for Buffer tables. This will only produce a speed of a few thousand rows per second, while inserting larger blocks of data can produce over a million rows per second (see the section “Performance”). Original article","keywords":""},{"title":"Dictionary Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/dictionary","content":"","keywords":""},{"title":"Example​","type":1,"pageTitle":"Dictionary Table Engine","url":"en/engines/table-engines/special/dictionary#example","content":"As an example, consider a dictionary of products with the following configuration: &lt;dictionaries&gt; &lt;dictionary&gt; &lt;name&gt;products&lt;/name&gt; &lt;source&gt; &lt;odbc&gt; &lt;table&gt;products&lt;/table&gt; &lt;connection_string&gt;DSN=some-db-server&lt;/connection_string&gt; &lt;/odbc&gt; &lt;/source&gt; &lt;lifetime&gt; &lt;min&gt;300&lt;/min&gt; &lt;max&gt;360&lt;/max&gt; &lt;/lifetime&gt; &lt;layout&gt; &lt;flat/&gt; &lt;/layout&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;product_id&lt;/name&gt; &lt;/id&gt; &lt;attribute&gt; &lt;name&gt;title&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;/dictionary&gt; &lt;/dictionaries&gt;  Query the dictionary data: SELECT name, type, key, attribute.names, attribute.types, bytes_allocated, element_count, source FROM system.dictionaries WHERE name = 'products'  ┌─name─────┬─type─┬─key────┬─attribute.names─┬─attribute.types─┬─bytes_allocated─┬─element_count─┬─source──────────┐ │ products │ Flat │ UInt64 │ ['title'] │ ['String'] │ 23065376 │ 175032 │ ODBC: .products │ └──────────┴──────┴────────┴─────────────────┴─────────────────┴─────────────────┴───────────────┴─────────────────┘  You can use the dictGet* function to get the dictionary data in this format. This view isn’t helpful when you need to get raw data, or when performing a JOIN operation. For these cases, you can use the Dictionary engine, which displays the dictionary data in a table. Syntax: CREATE TABLE %table_name% (%fields%) engine = Dictionary(%dictionary_name%)`  Usage example: create table products (product_id UInt64, title String) Engine = Dictionary(products);   Ok  Take a look at what’s in the table. select * from products limit 1;  ┌────product_id─┬─title───────────┐ │ 152689 │ Some item │ └───────────────┴─────────────────┘  See Also Dictionary function Original article "},{"title":"External Data for Query Processing","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/external-data","content":"External Data for Query Processing ClickHouse allows sending a server the data that is needed for processing a query, together with a SELECT query. This data is put in a temporary table (see the section “Temporary tables”) and can be used in the query (for example, in IN operators). For example, if you have a text file with important user identifiers, you can upload it to the server along with a query that uses filtration by this list. If you need to run more than one query with a large volume of external data, do not use this feature. It is better to upload the data to the DB ahead of time. External data can be uploaded using the command-line client (in non-interactive mode), or using the HTTP interface. In the command-line client, you can specify a parameters section in the format --external --file=... [--name=...] [--format=...] [--types=...|--structure=...] You may have multiple sections like this, for the number of tables being transmitted. –external – Marks the beginning of a clause.–file – Path to the file with the table dump, or -, which refers to stdin. Only a single table can be retrieved from stdin. The following parameters are optional: –name– Name of the table. If omitted, _data is used.–format – Data format in the file. If omitted, TabSeparated is used. One of the following parameters is required:–types – A list of comma-separated column types. For example: UInt64,String. The columns will be named _1, _2, …–structure– The table structure in the formatUserID UInt64, URL String. Defines the column names and types. The files specified in ‘file’ will be parsed by the format specified in ‘format’, using the data types specified in ‘types’ or ‘structure’. The table will be uploaded to the server and accessible there as a temporary table with the name in ‘name’. Examples: $ echo -ne &quot;1\\n2\\n3\\n&quot; | clickhouse-client --query=&quot;SELECT count() FROM test.visits WHERE TraficSourceID IN _data&quot; --external --file=- --types=Int8 849897 $ cat /etc/passwd | sed 's/:/\\t/g' | clickhouse-client --query=&quot;SELECT shell, count() AS c FROM passwd GROUP BY shell ORDER BY c DESC&quot; --external --file=- --name=passwd --structure='login String, unused String, uid UInt16, gid UInt16, comment String, home String, shell String' /bin/sh 20 /bin/false 5 /bin/bash 4 /usr/sbin/nologin 1 /bin/sync 1 When using the HTTP interface, external data is passed in the multipart/form-data format. Each table is transmitted as a separate file. The table name is taken from the file name. The query_string is passed the parameters name_format, name_types, and name_structure, where name is the name of the table that these parameters correspond to. The meaning of the parameters is the same as when using the command-line client. Example: $ cat /etc/passwd | sed 's/:/\\t/g' &gt; passwd.tsv $ curl -F 'passwd=@passwd.tsv;' 'http://localhost:8123/?query=SELECT+shell,+count()+AS+c+FROM+passwd+GROUP+BY+shell+ORDER+BY+c+DESC&amp;passwd_structure=login+String,+unused+String,+uid+UInt16,+gid+UInt16,+comment+String,+home+String,+shell+String' /bin/sh 20 /bin/false 5 /bin/bash 4 /usr/sbin/nologin 1 /bin/sync 1 For distributed query processing, the temporary tables are sent to all the remote servers.","keywords":""},{"title":"File Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/file","content":"","keywords":""},{"title":"Usage in ClickHouse Server​","type":1,"pageTitle":"File Table Engine","url":"en/engines/table-engines/special/file#usage-in-clickhouse-server","content":"File(Format)  The Format parameter specifies one of the available file formats. To performSELECT queries, the format must be supported for input, and to performINSERT queries – for output. The available formats are listed in theFormats section. ClickHouse does not allow specifying filesystem path for File. It will use folder defined by path setting in server configuration. When creating table using File(Format) it creates empty subdirectory in that folder. When data is written to that table, it’s put into data.Format file in that subdirectory. You may manually create this subfolder and file in server filesystem and then ATTACH it to table information with matching name, so you can query data from that file. warning Be careful with this functionality, because ClickHouse does not keep track of external changes to such files. The result of simultaneous writes via ClickHouse and outside of ClickHouse is undefined. "},{"title":"Example​","type":1,"pageTitle":"File Table Engine","url":"en/engines/table-engines/special/file#example","content":"1. Set up the file_engine_table table: CREATE TABLE file_engine_table (name String, value UInt32) ENGINE=File(TabSeparated)  By default ClickHouse will create folder /var/lib/clickhouse/data/default/file_engine_table. 2. Manually create /var/lib/clickhouse/data/default/file_engine_table/data.TabSeparated containing: $ cat data.TabSeparated one 1 two 2  3. Query the data: SELECT * FROM file_engine_table  ┌─name─┬─value─┐ │ one │ 1 │ │ two │ 2 │ └──────┴───────┘  "},{"title":"Usage in ClickHouse-local​","type":1,"pageTitle":"File Table Engine","url":"en/engines/table-engines/special/file#usage-in-clickhouse-local","content":"In clickhouse-local File engine accepts file path in addition to Format. Default input/output streams can be specified using numeric or human-readable names like 0 or stdin, 1 or stdout. It is possible to read and write compressed files based on an additional engine parameter or file extension (gz, br or xz). Example: $ echo -e &quot;1,2\\n3,4&quot; | clickhouse-local -q &quot;CREATE TABLE table (a Int64, b Int64) ENGINE = File(CSV, stdin); SELECT a, b FROM table; DROP TABLE table&quot;  "},{"title":"Details of Implementation​","type":1,"pageTitle":"File Table Engine","url":"en/engines/table-engines/special/file#details-of-implementation","content":"Multiple SELECT queries can be performed concurrently, but INSERT queries will wait each other.Supported creating new file by INSERT query.If file exists, INSERT would append new values in it.Not supported: ALTERSELECT ... SAMPLEIndicesReplication Original article "},{"title":"GenerateRandom Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/generate","content":"","keywords":""},{"title":"Usage in ClickHouse Server​","type":1,"pageTitle":"GenerateRandom Table Engine","url":"en/engines/table-engines/special/generate#usage-in-clickhouse-server","content":"ENGINE = GenerateRandom(random_seed, max_string_length, max_array_length)  The max_array_length and max_string_length parameters specify maximum length of all array columns and strings correspondingly in generated data. Generate table engine supports only SELECT queries. It supports all DataTypes that can be stored in a table except LowCardinality and AggregateFunction. "},{"title":"Example​","type":1,"pageTitle":"GenerateRandom Table Engine","url":"en/engines/table-engines/special/generate#example","content":"1. Set up the generate_engine_table table: CREATE TABLE generate_engine_table (name String, value UInt32) ENGINE = GenerateRandom(1, 5, 3)  2. Query the data: SELECT * FROM generate_engine_table LIMIT 3  ┌─name─┬──────value─┐ │ c4xJ │ 1412771199 │ │ r │ 1791099446 │ │ 7#$ │ 124312908 │ └──────┴────────────┘  "},{"title":"Details of Implementation​","type":1,"pageTitle":"GenerateRandom Table Engine","url":"en/engines/table-engines/special/generate#details-of-implementation","content":"Not supported: ALTERSELECT ... SAMPLEINSERTIndicesReplication Original article "},{"title":"CollapsingMergeTree","type":0,"sectionRef":"#","url":"en/engines/table-engines/mergetree-family/collapsingmergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"CollapsingMergeTree","url":"en/engines/table-engines/mergetree-family/collapsingmergetree#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = CollapsingMergeTree(sign) [PARTITION BY expr] [ORDER BY expr] [SAMPLE BY expr] [SETTINGS name=value, ...]  For a description of query parameters, see query description. CollapsingMergeTree Parameters sign — Name of the column with the type of row: 1 is a “state” row, -1 is a “cancel” row. Column data type — Int8. Query clauses When creating a CollapsingMergeTree table, the same query clauses are required, as when creating a MergeTree table. Deprecated Method for Creating a Table warning Do not use this method in new projects and, if possible, switch old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE [=] CollapsingMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, sign) All of the parameters excepting sign have the same meaning as in MergeTree. sign — Name of the column with the type of row: 1 — “state” row, -1 — “cancel” row. Column Data Type — Int8. "},{"title":"Collapsing​","type":1,"pageTitle":"CollapsingMergeTree","url":"en/engines/table-engines/mergetree-family/collapsingmergetree#table_engine-collapsingmergetree-collapsing","content":""},{"title":"Data​","type":1,"pageTitle":"CollapsingMergeTree","url":"en/engines/table-engines/mergetree-family/collapsingmergetree#data","content":"Consider the situation where you need to save continually changing data for some object. It sounds logical to have one row for an object and update it at any change, but update operation is expensive and slow for DBMS because it requires rewriting of the data in the storage. If you need to write data quickly, update not acceptable, but you can write the changes of an object sequentially as follows. Use the particular column Sign. If Sign = 1 it means that the row is a state of an object, let’s call it “state” row. If Sign = -1 it means the cancellation of the state of an object with the same attributes, let’s call it “cancel” row. For example, we want to calculate how much pages users checked at some site and how long they were there. At some moment we write the following row with the state of user activity: ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  At some moment later we register the change of user activity and write it with the following two rows. ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ -1 │ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  The first row cancels the previous state of the object (user). It should copy the sorting key fields of the cancelled state excepting Sign. The second row contains the current state. As we need only the last state of user activity, the rows ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ │ 4324182021466249494 │ 5 │ 146 │ -1 │ └─────────────────────┴───────────┴──────────┴──────┘  can be deleted collapsing the invalid (old) state of an object. CollapsingMergeTree does this while merging of the data parts. Why we need 2 rows for each change read in the Algorithm paragraph. Peculiar properties of such approach The program that writes the data should remember the state of an object to be able to cancel it. “Cancel” string should contain copies of the sorting key fields of the “state” string and the opposite Sign. It increases the initial size of storage but allows to write the data quickly.Long growing arrays in columns reduce the efficiency of the engine due to load for writing. The more straightforward data, the higher the efficiency.The SELECT results depend strongly on the consistency of object changes history. Be accurate when preparing data for inserting. You can get unpredictable results in inconsistent data, for example, negative values for non-negative metrics such as session depth. "},{"title":"Algorithm​","type":1,"pageTitle":"CollapsingMergeTree","url":"en/engines/table-engines/mergetree-family/collapsingmergetree#table_engine-collapsingmergetree-collapsing-algorithm","content":"When ClickHouse merges data parts, each group of consecutive rows with the same sorting key (ORDER BY) is reduced to not more than two rows, one with Sign = 1 (“state” row) and another with Sign = -1 (“cancel” row). In other words, entries collapse. For each resulting data part ClickHouse saves: The first “cancel” and the last “state” rows, if the number of “state” and “cancel” rows matches and the last row is a “state” row.The last “state” row, if there are more “state” rows than “cancel” rows.The first “cancel” row, if there are more “cancel” rows than “state” rows.None of the rows, in all other cases. Also when there are at least 2 more “state” rows than “cancel” rows, or at least 2 more “cancel” rows then “state” rows, the merge continues, but ClickHouse treats this situation as a logical error and records it in the server log. This error can occur if the same data were inserted more than once. Thus, collapsing should not change the results of calculating statistics. Changes gradually collapsed so that in the end only the last state of almost every object left. The Sign is required because the merging algorithm does not guarantee that all of the rows with the same sorting key will be in the same resulting data part and even on the same physical server. ClickHouse process SELECT queries with multiple threads, and it can not predict the order of rows in the result. The aggregation is required if there is a need to get completely “collapsed” data from CollapsingMergeTree table. To finalize collapsing, write a query with GROUP BY clause and aggregate functions that account for the sign. For example, to calculate quantity, use sum(Sign) instead of count(). To calculate the sum of something, use sum(Sign * x) instead of sum(x), and so on, and also add HAVING sum(Sign) &gt; 0. The aggregates count, sum and avg could be calculated this way. The aggregate uniq could be calculated if an object has at least one state not collapsed. The aggregates min and max could not be calculated because CollapsingMergeTree does not save the values history of the collapsed states. If you need to extract data without aggregation (for example, to check whether rows are present whose newest values match certain conditions), you can use the FINAL modifier for the FROM clause. This approach is significantly less efficient. "},{"title":"Example of Use​","type":1,"pageTitle":"CollapsingMergeTree","url":"en/engines/table-engines/mergetree-family/collapsingmergetree#example-of-use","content":"Example data: ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ │ 4324182021466249494 │ 5 │ 146 │ -1 │ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  Creation of the table: CREATE TABLE UAct ( UserID UInt64, PageViews UInt8, Duration UInt8, Sign Int8 ) ENGINE = CollapsingMergeTree(Sign) ORDER BY UserID  Insertion of the data: INSERT INTO UAct VALUES (4324182021466249494, 5, 146, 1)  INSERT INTO UAct VALUES (4324182021466249494, 5, 146, -1),(4324182021466249494, 6, 185, 1)  We use two INSERT queries to create two different data parts. If we insert the data with one query ClickHouse creates one data part and will not perform any merge ever. Getting the data: SELECT * FROM UAct  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ -1 │ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘ ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  What do we see and where is collapsing? With two INSERT queries, we created 2 data parts. The SELECT query was performed in 2 threads, and we got a random order of rows. Collapsing not occurred because there was no merge of the data parts yet. ClickHouse merges data part in an unknown moment which we can not predict. Thus we need aggregation: SELECT UserID, sum(PageViews * Sign) AS PageViews, sum(Duration * Sign) AS Duration FROM UAct GROUP BY UserID HAVING sum(Sign) &gt; 0  ┌──────────────UserID─┬─PageViews─┬─Duration─┐ │ 4324182021466249494 │ 6 │ 185 │ └─────────────────────┴───────────┴──────────┘  If we do not need aggregation and want to force collapsing, we can use FINAL modifier for FROM clause. SELECT * FROM UAct FINAL  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  This way of selecting the data is very inefficient. Don’t use it for big tables. "},{"title":"Example of Another Approach​","type":1,"pageTitle":"CollapsingMergeTree","url":"en/engines/table-engines/mergetree-family/collapsingmergetree#example-of-another-approach","content":"Example data: ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ 1 │ │ 4324182021466249494 │ -5 │ -146 │ -1 │ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  The idea is that merges take into account only key fields. And in the “Cancel” line we can specify negative values that equalize the previous version of the row when summing without using the Sign column. For this approach, it is necessary to change the data type PageViews,Duration to store negative values of UInt8 -&gt; Int16. CREATE TABLE UAct ( UserID UInt64, PageViews Int16, Duration Int16, Sign Int8 ) ENGINE = CollapsingMergeTree(Sign) ORDER BY UserID  Let’s test the approach: insert into UAct values(4324182021466249494, 5, 146, 1); insert into UAct values(4324182021466249494, -5, -146, -1); insert into UAct values(4324182021466249494, 6, 185, 1); select * from UAct final; // avoid using final in production (just for a test or small tables)  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  SELECT UserID, sum(PageViews) AS PageViews, sum(Duration) AS Duration FROM UAct GROUP BY UserID  ┌──────────────UserID─┬─PageViews─┬─Duration─┐ │ 4324182021466249494 │ 6 │ 185 │ └─────────────────────┴───────────┴──────────┘  select count() FROM UAct  ┌─count()─┐ │ 3 │ └─────────┘  optimize table UAct final; select * FROM UAct  ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  Original article "},{"title":"MaterializedView Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/materializedview","content":"MaterializedView Table Engine Used for implementing materialized views (for more information, see CREATE VIEW). For storing data, it uses a different engine that was specified when creating the view. When reading from a table, it just uses that engine. Original article","keywords":""},{"title":"GraphiteMergeTree","type":0,"sectionRef":"#","url":"en/engines/table-engines/mergetree-family/graphitemergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"GraphiteMergeTree","url":"en/engines/table-engines/mergetree-family/graphitemergetree#creating-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( Path String, Time DateTime, Value &lt;Numeric_type&gt;, Version &lt;Numeric_type&gt; ... ) ENGINE = GraphiteMergeTree(config_section) [PARTITION BY expr] [ORDER BY expr] [SAMPLE BY expr] [SETTINGS name=value, ...]  See a detailed description of the CREATE TABLE query. A table for the Graphite data should have the following columns for the following data: Metric name (Graphite sensor). Data type: String. Time of measuring the metric. Data type: DateTime. Value of the metric. Data type: any numeric. Version of the metric. Data type: any numeric (ClickHouse saves the rows with the highest version or the last written if versions are the same. Other rows are deleted during the merge of data parts). The names of these columns should be set in the rollup configuration. GraphiteMergeTree parameters config_section — Name of the section in the configuration file, where are the rules of rollup set. Query clauses When creating a GraphiteMergeTree table, the same clauses are required, as when creating a MergeTree table. Deprecated Method for Creating a Table warning Do not use this method in new projects and, if possible, switch old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( EventDate Date, Path String, Time DateTime, Value &lt;Numeric_type&gt;, Version &lt;Numeric_type&gt; ... ) ENGINE [=] GraphiteMergeTree(date-column [, sampling_expression], (primary, key), index_granularity, config_section) All of the parameters excepting config_section have the same meaning as in MergeTree. config_section — Name of the section in the configuration file, where are the rules of rollup set. "},{"title":"Rollup Configuration​","type":1,"pageTitle":"GraphiteMergeTree","url":"en/engines/table-engines/mergetree-family/graphitemergetree#rollup-configuration","content":"The settings for rollup are defined by the graphite_rollup parameter in the server configuration. The name of the parameter could be any. You can create several configurations and use them for different tables. Rollup configuration structure:  required-columns patterns  "},{"title":"Required Columns​","type":1,"pageTitle":"GraphiteMergeTree","url":"en/engines/table-engines/mergetree-family/graphitemergetree#required-columns","content":"path_column_name — The name of the column storing the metric name (Graphite sensor). Default value: Path.time_column_name — The name of the column storing the time of measuring the metric. Default value: Time.value_column_name — The name of the column storing the value of the metric at the time set in time_column_name. Default value: Value.version_column_name — The name of the column storing the version of the metric. Default value: Timestamp. "},{"title":"Patterns​","type":1,"pageTitle":"GraphiteMergeTree","url":"en/engines/table-engines/mergetree-family/graphitemergetree#patterns","content":"Structure of the patterns section: pattern rule_type regexp function pattern rule_type regexp age + precision ... pattern rule_type regexp function age + precision ... pattern ... default function age + precision ...  warning Patterns must be strictly ordered: Patterns without function or retention.Patterns with both function and retention.Pattern default. When processing a row, ClickHouse checks the rules in the pattern sections. Each of pattern (including default) sections can contain function parameter for aggregation, retention parameters or both. If the metric name matches the regexp, the rules from the pattern section (or sections) are applied; otherwise, the rules from the default section are used. Fields for pattern and default sections: rule_type - a rule's type. It's applied only to a particular metrics. The engine use it to separate plain and tagged metrics. Optional parameter. Default value: all. It's unnecessary when performance is not critical, or only one metrics type is used, e.g. plain metrics. By default only one type of rules set is created. Otherwise, if any of special types is defined, two different sets are created. One for plain metrics (root.branch.leaf) and one for tagged metrics (root.branch.leaf;tag1=value1). The default rules are ended up in both sets. Valid values: - `all` (default) - a universal rule, used when `rule_type` is omitted. - `plain` - a rule for plain metrics. The field `regexp` is processed as regular expression. - `tagged` - a rule for tagged metrics (metrics are stored in DB in the format of `someName?tag1=value1&amp;tag2=value2&amp;tag3=value3`). Regular expression must be sorted by tags' names, first tag must be `__name__` if exists. The field `regexp` is processed as regular expression. - `tag_list` - a rule for tagged matrics, a simple DSL for easier metric description in graphite format `someName;tag1=value1;tag2=value2`, `someName`, or `tag1=value1;tag2=value2`. The field `regexp` is translated into a `tagged` rule. The sorting by tags' names is unnecessary, ti will be done automatically. A tag's value (but not a name) can be set as a regular expression, e.g. `env=(dev|staging)`. regexp – A pattern for the metric name (a regular or DSL).age – The minimum age of the data in seconds.precision– How precisely to define the age of the data in seconds. Should be a divisor for 86400 (seconds in a day).function – The name of the aggregating function to apply to data whose age falls within the range [age, age + precision]. Accepted functions: min / max / any / avg. The average is calculated imprecisely, like the average of the averages.  "},{"title":"Configuration Example without rules types​","type":1,"pageTitle":"GraphiteMergeTree","url":"en/engines/table-engines/mergetree-family/graphitemergetree#configuration-example","content":"&lt;graphite_rollup&gt; &lt;version_column_name&gt;Version&lt;/version_column_name&gt; &lt;pattern&gt; &lt;regexp&gt;click_cost&lt;/regexp&gt; &lt;function&gt;any&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;5&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;/pattern&gt; &lt;default&gt; &lt;function&gt;max&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;3600&lt;/age&gt; &lt;precision&gt;300&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;3600&lt;/precision&gt; &lt;/retention&gt; &lt;/default&gt; &lt;/graphite_rollup&gt;  "},{"title":"Configuration Example with rules types​","type":1,"pageTitle":"GraphiteMergeTree","url":"en/engines/table-engines/mergetree-family/graphitemergetree#configuration-typed-example","content":"&lt;graphite_rollup&gt; &lt;version_column_name&gt;Version&lt;/version_column_name&gt; &lt;pattern&gt; &lt;rule_type&gt;plain&lt;/rule_type&gt; &lt;regexp&gt;click_cost&lt;/regexp&gt; &lt;function&gt;any&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;5&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;/pattern&gt; &lt;pattern&gt; &lt;rule_type&gt;tagged&lt;/rule_type&gt; &lt;regexp&gt;^((.*)|.)min\\?&lt;/regexp&gt; &lt;function&gt;min&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;5&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;/pattern&gt; &lt;pattern&gt; &lt;rule_type&gt;tagged&lt;/rule_type&gt; &lt;regexp&gt;&lt;![CDATA[^someName\\?(.*&amp;)*tag1=value1(&amp;|$)]]&gt;&lt;/regexp&gt; &lt;function&gt;min&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;5&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;/pattern&gt; &lt;pattern&gt; &lt;rule_type&gt;tag_list&lt;/rule_type&gt; &lt;regexp&gt;someName;tag2=value2&lt;/regexp&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;5&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;/pattern&gt; &lt;default&gt; &lt;function&gt;max&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;3600&lt;/age&gt; &lt;precision&gt;300&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;3600&lt;/precision&gt; &lt;/retention&gt; &lt;/default&gt; &lt;/graphite_rollup&gt;  warning Data rollup is performed during merges. Usually, for old partitions, merges are not started, so for rollup it is necessary to trigger an unscheduled merge using optimize. Or use additional tools, for example graphite-ch-optimizer. "},{"title":"Memory Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/memory","content":"Memory Table Engine The Memory engine stores data in RAM, in uncompressed form. Data is stored in exactly the same form as it is received when read. In other words, reading from this table is completely free. Concurrent data access is synchronized. Locks are short: read and write operations do not block each other. Indexes are not supported. Reading is parallelized. Maximal productivity (over 10 GB/sec) is reached on simple queries, because there is no reading from the disk, decompressing, or deserializing data. (We should note that in many cases, the productivity of the MergeTree engine is almost as high.) When restarting a server, data disappears from the table and the table becomes empty. Normally, using this table engine is not justified. However, it can be used for tests, and for tasks where maximum speed is required on a relatively small number of rows (up to approximately 100,000,000). The Memory engine is used by the system for temporary tables with external query data (see the section “External data for processing a query”), and for implementing GLOBAL IN (see the section “IN operators”). Original article","keywords":""},{"title":"Data Replication","type":0,"sectionRef":"#","url":"en/engines/table-engines/mergetree-family/replication","content":"","keywords":""},{"title":"Creating Replicated Tables​","type":1,"pageTitle":"Data Replication","url":"en/engines/table-engines/mergetree-family/replication#creating-replicated-tables","content":"The Replicated prefix is added to the table engine name. For example:ReplicatedMergeTree. Replicated*MergeTree parameters zoo_path — The path to the table in ZooKeeper.replica_name — The replica name in ZooKeeper.other_parameters — Parameters of an engine which is used for creating the replicated version, for example, version in ReplacingMergeTree. Example: CREATE TABLE table_name ( EventDate DateTime, CounterID UInt32, UserID UInt32, ver UInt16 ) ENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{layer}-{shard}/table_name', '{replica}', ver) PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID);  Example in deprecated syntax CREATE TABLE table_name ( EventDate DateTime, CounterID UInt32, UserID UInt32 ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/table_name', '{replica}', EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID), EventTime), 8192);  As the example shows, these parameters can contain substitutions in curly brackets. The substituted values are taken from the macros section of the configuration file. Example: &lt;macros&gt; &lt;layer&gt;05&lt;/layer&gt; &lt;shard&gt;02&lt;/shard&gt; &lt;replica&gt;example05-02-1&lt;/replica&gt; &lt;/macros&gt;  The path to the table in ZooKeeper should be unique for each replicated table. Tables on different shards should have different paths. In this case, the path consists of the following parts: /clickhouse/tables/ is the common prefix. We recommend using exactly this one. {layer}-{shard} is the shard identifier. In this example it consists of two parts, since the example cluster uses bi-level sharding. For most tasks, you can leave just the {shard} substitution, which will be expanded to the shard identifier. table_name is the name of the node for the table in ZooKeeper. It is a good idea to make it the same as the table name. It is defined explicitly, because in contrast to the table name, it does not change after a RENAME query.HINT: you could add a database name in front of table_name as well. E.g. db_name.table_name The two built-in substitutions {database} and {table} can be used, they expand into the table name and the database name respectively (unless these macros are defined in the macros section). So the zookeeper path can be specified as '/clickhouse/tables/{layer}-{shard}/{database}/{table}'. Be careful with table renames when using these built-in substitutions. The path in Zookeeper cannot be changed, and when the table is renamed, the macros will expand into a different path, the table will refer to a path that does not exist in Zookeeper, and will go into read-only mode. The replica name identifies different replicas of the same table. You can use the server name for this, as in the example. The name only needs to be unique within each shard. You can define the parameters explicitly instead of using substitutions. This might be convenient for testing and for configuring small clusters. However, you can’t use distributed DDL queries (ON CLUSTER) in this case. When working with large clusters, we recommend using substitutions because they reduce the probability of error. You can specify default arguments for Replicated table engine in the server configuration file. For instance: &lt;default_replica_path&gt;/clickhouse/tables/{shard}/{database}/{table}&lt;/default_replica_path&gt; &lt;default_replica_name&gt;{replica}&lt;/default_replica_name&gt;  In this case, you can omit arguments when creating tables: CREATE TABLE table_name ( x UInt32 ) ENGINE = ReplicatedMergeTree ORDER BY x;  It is equivalent to: CREATE TABLE table_name ( x UInt32 ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/{database}/table_name', '{replica}') ORDER BY x;  Run the CREATE TABLE query on each replica. This query creates a new replicated table, or adds a new replica to an existing one. If you add a new replica after the table already contains some data on other replicas, the data will be copied from the other replicas to the new one after running the query. In other words, the new replica syncs itself with the others. To delete a replica, run DROP TABLE. However, only one replica is deleted – the one that resides on the server where you run the query. "},{"title":"Recovery After Failures​","type":1,"pageTitle":"Data Replication","url":"en/engines/table-engines/mergetree-family/replication#recovery-after-failures","content":"If ZooKeeper is unavailable when a server starts, replicated tables switch to read-only mode. The system periodically attempts to connect to ZooKeeper. If ZooKeeper is unavailable during an INSERT, or an error occurs when interacting with ZooKeeper, an exception is thrown. After connecting to ZooKeeper, the system checks whether the set of data in the local file system matches the expected set of data (ZooKeeper stores this information). If there are minor inconsistencies, the system resolves them by syncing data with the replicas. If the system detects broken data parts (with the wrong size of files) or unrecognized parts (parts written to the file system but not recorded in ZooKeeper), it moves them to the detached subdirectory (they are not deleted). Any missing parts are copied from the replicas. Note that ClickHouse does not perform any destructive actions such as automatically deleting a large amount of data. When the server starts (or establishes a new session with ZooKeeper), it only checks the quantity and sizes of all files. If the file sizes match but bytes have been changed somewhere in the middle, this is not detected immediately, but only when attempting to read the data for a SELECT query. The query throws an exception about a non-matching checksum or size of a compressed block. In this case, data parts are added to the verification queue and copied from the replicas if necessary. If the local set of data differs too much from the expected one, a safety mechanism is triggered. The server enters this in the log and refuses to launch. The reason for this is that this case may indicate a configuration error, such as if a replica on a shard was accidentally configured like a replica on a different shard. However, the thresholds for this mechanism are set fairly low, and this situation might occur during normal failure recovery. In this case, data is restored semi-automatically - by “pushing a button”. To start recovery, create the node /path_to_table/replica_name/flags/force_restore_data in ZooKeeper with any content, or run the command to restore all replicated tables: sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data  Then restart the server. On start, the server deletes these flags and starts recovery. "},{"title":"Recovery After Complete Data Loss​","type":1,"pageTitle":"Data Replication","url":"en/engines/table-engines/mergetree-family/replication#recovery-after-complete-data-loss","content":"If all data and metadata disappeared from one of the servers, follow these steps for recovery: Install ClickHouse on the server. Define substitutions correctly in the config file that contains the shard identifier and replicas, if you use them.If you had unreplicated tables that must be manually duplicated on the servers, copy their data from a replica (in the directory /var/lib/clickhouse/data/db_name/table_name/).Copy table definitions located in /var/lib/clickhouse/metadata/ from a replica. If a shard or replica identifier is defined explicitly in the table definitions, correct it so that it corresponds to this replica. (Alternatively, start the server and make all the ATTACH TABLE queries that should have been in the .sql files in /var/lib/clickhouse/metadata/.)To start recovery, create the ZooKeeper node /path_to_table/replica_name/flags/force_restore_data with any content, or run the command to restore all replicated tables: sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data Then start the server (restart, if it is already running). Data will be downloaded from replicas. An alternative recovery option is to delete information about the lost replica from ZooKeeper (/path_to_table/replica_name), then create the replica again as described in “Creating replicated tables”. There is no restriction on network bandwidth during recovery. Keep this in mind if you are restoring many replicas at once. "},{"title":"Converting from MergeTree to ReplicatedMergeTree​","type":1,"pageTitle":"Data Replication","url":"en/engines/table-engines/mergetree-family/replication#converting-from-mergetree-to-replicatedmergetree","content":"We use the term MergeTree to refer to all table engines in the MergeTree family, the same as for ReplicatedMergeTree. If you had a MergeTree table that was manually replicated, you can convert it to a replicated table. You might need to do this if you have already collected a large amount of data in a MergeTree table and now you want to enable replication. If the data differs on various replicas, first sync it, or delete this data on all the replicas except one. Rename the existing MergeTree table, then create a ReplicatedMergeTree table with the old name. Move the data from the old table to the detached subdirectory inside the directory with the new table data (/var/lib/clickhouse/data/db_name/table_name/). Then run ALTER TABLE ATTACH PARTITION on one of the replicas to add these data parts to the working set. "},{"title":"Converting from ReplicatedMergeTree to MergeTree​","type":1,"pageTitle":"Data Replication","url":"en/engines/table-engines/mergetree-family/replication#converting-from-replicatedmergetree-to-mergetree","content":"Create a MergeTree table with a different name. Move all the data from the directory with the ReplicatedMergeTree table data to the new table’s data directory. Then delete the ReplicatedMergeTree table and restart the server. If you want to get rid of a ReplicatedMergeTree table without launching the server: Delete the corresponding .sql file in the metadata directory (/var/lib/clickhouse/metadata/).Delete the corresponding path in ZooKeeper (/path_to_table/replica_name). After this, you can launch the server, create a MergeTree table, move the data to its directory, and then restart the server. "},{"title":"Recovery When Metadata in the Zookeeper Cluster Is Lost or Damaged​","type":1,"pageTitle":"Data Replication","url":"en/engines/table-engines/mergetree-family/replication#recovery-when-metadata-in-the-zookeeper-cluster-is-lost-or-damaged","content":"If the data in ZooKeeper was lost or damaged, you can save data by moving it to an unreplicated table as described above. See Also background_schedule_pool_sizebackground_fetches_pool_sizeexecute_merges_on_single_replica_time_thresholdmax_replicated_fetches_network_bandwidthmax_replicated_sends_network_bandwidth Original article "},{"title":"Merge Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/merge","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"Merge Table Engine","url":"en/engines/table-engines/special/merge#creating-a-table","content":"CREATE TABLE ... Engine=Merge(db_name, tables_regexp)  Engine Parameters db_name — Possible values: database name, constant expression that returns a string with a database name, for example, currentDatabase(),REGEXP(expression), where expression is a regular expression to match the DB names. tables_regexp — A regular expression to match the table names in the specified DB or DBs. Regular expressions — re2 (supports a subset of PCRE), case-sensitive. See the notes about escaping symbols in regular expressions in the &quot;match&quot; section. "},{"title":"Usage​","type":1,"pageTitle":"Merge Table Engine","url":"en/engines/table-engines/special/merge#usage","content":"When selecting tables to read, the Merge table itself is not selected, even if it matches the regex. This is to avoid loops. It is possible to create two Merge tables that will endlessly try to read each others' data, but this is not a good idea. The typical way to use the Merge engine is for working with a large number of TinyLog tables as if with a single table. "},{"title":"Examples​","type":1,"pageTitle":"Merge Table Engine","url":"en/engines/table-engines/special/merge#examples","content":"Example 1 Consider two databases ABC_corporate_site and ABC_store. The all_visitors table will contain IDs from the tables visitors in both databases. CREATE TABLE all_visitors (id UInt32) ENGINE=Merge(REGEXP('ABC_*'), 'visitors');  Example 2 Let's say you have an old table WatchLog_old and decided to change partitioning without moving data to a new table WatchLog_new, and you need to see data from both tables. CREATE TABLE WatchLog_old(date Date, UserId Int64, EventType String, Cnt UInt64) ENGINE=MergeTree(date, (UserId, EventType), 8192); INSERT INTO WatchLog_old VALUES ('2018-01-01', 1, 'hit', 3); CREATE TABLE WatchLog_new(date Date, UserId Int64, EventType String, Cnt UInt64) ENGINE=MergeTree PARTITION BY date ORDER BY (UserId, EventType) SETTINGS index_granularity=8192; INSERT INTO WatchLog_new VALUES ('2018-01-02', 2, 'hit', 3); CREATE TABLE WatchLog as WatchLog_old ENGINE=Merge(currentDatabase(), '^WatchLog'); SELECT * FROM WatchLog;  ┌───────date─┬─UserId─┬─EventType─┬─Cnt─┐ │ 2018-01-01 │ 1 │ hit │ 3 │ └────────────┴────────┴───────────┴─────┘ ┌───────date─┬─UserId─┬─EventType─┬─Cnt─┐ │ 2018-01-02 │ 2 │ hit │ 3 │ └────────────┴────────┴───────────┴─────┘  "},{"title":"Virtual Columns​","type":1,"pageTitle":"Merge Table Engine","url":"en/engines/table-engines/special/merge#virtual-columns","content":"_table — Contains the name of the table from which data was read. Type: String. You can set the constant conditions on _table in the WHERE/PREWHERE clause (for example, WHERE _table='xyz'). In this case the read operation is performed only for that tables where the condition on _table is satisfied, so the _table column acts as an index. See Also Virtual columnsmerge table function Original article "},{"title":"Null Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/null","content":"Null Table Engine When writing to a Null table, data is ignored. When reading from a Null table, the response is empty. note If you are wondering why this is useful, note that you can create a materialized view on a Null table. So the data written to the table will end up affecting the view, but original raw data will still be discarded. Original article","keywords":""},{"title":"Set Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/set","content":"","keywords":""},{"title":"Limitations and Settings​","type":1,"pageTitle":"Set Table Engine","url":"en/engines/table-engines/special/set#join-limitations-and-settings","content":"When creating a table, the following settings are applied: persistent Original article "},{"title":"URL Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/url","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"URL Table Engine","url":"en/engines/table-engines/special/url#using-the-engine-in-the-clickhouse-server","content":"INSERT and SELECT queries are transformed to POST and GET requests, respectively. For processing POST requests, the remote server must supportChunked transfer encoding. You can limit the maximum number of HTTP GET redirect hops using the max_http_get_redirects setting. "},{"title":"Example​","type":1,"pageTitle":"URL Table Engine","url":"en/engines/table-engines/special/url#example","content":"1. Create a url_engine_table table on the server : CREATE TABLE url_engine_table (word String, value UInt64) ENGINE=URL('http://127.0.0.1:12345/', CSV)  2. Create a basic HTTP server using the standard Python 3 tools and start it: from http.server import BaseHTTPRequestHandler, HTTPServer class CSVHTTPServer(BaseHTTPRequestHandler): def do_GET(self): self.send_response(200) self.send_header('Content-type', 'text/csv') self.end_headers() self.wfile.write(bytes('Hello,1\\nWorld,2\\n', &quot;utf-8&quot;)) if __name__ == &quot;__main__&quot;: server_address = ('127.0.0.1', 12345) HTTPServer(server_address, CSVHTTPServer).serve_forever()  $ python3 server.py  3. Request data: SELECT * FROM url_engine_table  ┌─word──┬─value─┐ │ Hello │ 1 │ │ World │ 2 │ └───────┴───────┘  "},{"title":"Details of Implementation​","type":1,"pageTitle":"URL Table Engine","url":"en/engines/table-engines/special/url#details-of-implementation","content":"Reads and writes can be parallelNot supported: ALTER and SELECT...SAMPLE operations.Indexes.Replication. Original article "},{"title":"View Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/view","content":"View Table Engine Used for implementing views (for more information, see the CREATE VIEW query). It does not store data, but only stores the specified SELECT query. When reading from a table, it runs this query (and deletes all unnecessary columns from the query). Original article","keywords":""},{"title":"Distributed Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/distributed","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"Distributed Table Engine","url":"en/engines/table-engines/special/distributed#distributed-creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE = Distributed(cluster, database, table[, sharding_key[, policy_name]]) [SETTINGS name=value, ...]  "},{"title":"From a Table​","type":1,"pageTitle":"Distributed Table Engine","url":"en/engines/table-engines/special/distributed#distributed-from-a-table","content":"When the Distributed table is pointing to a table on the current server you can adopt that table's schema: CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] AS [db2.]name2 ENGINE = Distributed(cluster, database, table[, sharding_key[, policy_name]]) [SETTINGS name=value, ...]  Distributed Parameters cluster - the cluster name in the server’s config file database - the name of a remote database table - the name of a remote table sharding_key - (optionally) sharding key policy_name - (optionally) policy name, it will be used to store temporary files for async send See Also insert_distributed_sync settingMergeTree for the examples Distributed Settings fsync_after_insert - do the fsync for the file data after asynchronous insert to Distributed. Guarantees that the OS flushed the whole inserted data to a file on the initiator node disk. fsync_directories - do the fsync for directories. Guarantees that the OS refreshed directory metadata after operations related to asynchronous inserts on Distributed table (after insert, after sending the data to shard, etc). bytes_to_throw_insert - if more than this number of compressed bytes will be pending for async INSERT, an exception will be thrown. 0 - do not throw. Default 0. bytes_to_delay_insert - if more than this number of compressed bytes will be pending for async INSERT, the query will be delayed. 0 - do not delay. Default 0. max_delay_to_insert - max delay of inserting data into Distributed table in seconds, if there are a lot of pending bytes for async send. Default 60. monitor_batch_inserts - same as distributed_directory_monitor_batch_inserts monitor_split_batch_on_failure - same as distributed_directory_monitor_split_batch_on_failure monitor_sleep_time_ms - same as distributed_directory_monitor_sleep_time_ms monitor_max_sleep_time_ms - same as distributed_directory_monitor_max_sleep_time_ms note Durability settings (fsync_...): Affect only asynchronous INSERTs (i.e. insert_distributed_sync=false) when data first stored on the initiator node disk and later asynchronously send to shards.May significantly decrease the inserts' performanceAffect writing the data stored inside Distributed table folder into the node which accepted your insert. If you need to have guarantees of writing data to underlying MergeTree tables - see durability settings (...fsync...) in system.merge_tree_settings For Insert limit settings (..._insert) see also: insert_distributed_sync settingprefer_localhost_replica settingbytes_to_throw_insert handled before bytes_to_delay_insert, so you should not set it to the value less then bytes_to_delay_insert Example CREATE TABLE hits_all AS hits ENGINE = Distributed(logs, default, hits[, sharding_key[, policy_name]]) SETTINGS fsync_after_insert=0, fsync_directories=0;  Data will be read from all servers in the logs cluster, from the default.hits table located on every server in the cluster. Data is not only read but is partially processed on the remote servers (to the extent that this is possible). For example, for a query with GROUP BY, data will be aggregated on remote servers, and the intermediate states of aggregate functions will be sent to the requestor server. Then data will be further aggregated. Instead of the database name, you can use a constant expression that returns a string. For example: currentDatabase(). "},{"title":"Clusters​","type":1,"pageTitle":"Distributed Table Engine","url":"en/engines/table-engines/special/distributed#distributed-clusters","content":"Clusters are configured in the server configuration file: &lt;remote_servers&gt; &lt;logs&gt; &lt;!-- Inter-server per-cluster secret for Distributed queries default: no secret (no authentication will be performed) If set, then Distributed queries will be validated on shards, so at least: - such cluster should exist on the shard, - such cluster should have the same secret. And also (and which is more important), the initial_user will be used as current user for the query. --&gt; &lt;!-- &lt;secret&gt;&lt;/secret&gt; --&gt; &lt;shard&gt; &lt;!-- Optional. Shard weight when writing data. Default: 1. --&gt; &lt;weight&gt;1&lt;/weight&gt; &lt;!-- Optional. Whether to write data to just one of the replicas. Default: false (write data to all replicas). --&gt; &lt;internal_replication&gt;false&lt;/internal_replication&gt; &lt;replica&gt; &lt;!-- Optional. Priority of the replica for load balancing (see also load_balancing setting). Default: 1 (less value has more priority). --&gt; &lt;priority&gt;1&lt;/priority&gt; &lt;host&gt;example01-01-1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;example01-01-2&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;weight&gt;2&lt;/weight&gt; &lt;internal_replication&gt;false&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;example01-02-1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;example01-02-2&lt;/host&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;port&gt;9440&lt;/port&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/logs&gt; &lt;/remote_servers&gt;  Here a cluster is defined with the name logs that consists of two shards, each of which contains two replicas. Shards refer to the servers that contain different parts of the data (in order to read all the data, you must access all the shards). Replicas are duplicating servers (in order to read all the data, you can access the data on any one of the replicas). Cluster names must not contain dots. The parameters host, port, and optionally user, password, secure, compression are specified for each server: host – The address of the remote server. You can use either the domain or the IPv4 or IPv6 address. If you specify the domain, the server makes a DNS request when it starts, and the result is stored as long as the server is running. If the DNS request fails, the server does not start. If you change the DNS record, restart the server.port – The TCP port for messenger activity (tcp_port in the config, usually set to 9000). Not to be confused with http_port.user – Name of the user for connecting to a remote server. Default value is the default user. This user must have access to connect to the specified server. Access is configured in the users.xml file. For more information, see the section Access rights.password – The password for connecting to a remote server (not masked). Default value: empty string.secure - Whether to use a secure SSL/TLS connection. Usually also requires specifying the port (the default secure port is 9440). The server should listen on &lt;tcp_port_secure&gt;9440&lt;/tcp_port_secure&gt; and be configured with correct certificates.compression - Use data compression. Default value: true. When specifying replicas, one of the available replicas will be selected for each of the shards when reading. You can configure the algorithm for load balancing (the preference for which replica to access) – see the load_balancing setting. If the connection with the server is not established, there will be an attempt to connect with a short timeout. If the connection failed, the next replica will be selected, and so on for all the replicas. If the connection attempt failed for all the replicas, the attempt will be repeated the same way, several times. This works in favour of resiliency, but does not provide complete fault tolerance: a remote server might accept the connection, but might not work, or work poorly. You can specify just one of the shards (in this case, query processing should be called remote, rather than distributed) or up to any number of shards. In each shard, you can specify from one to any number of replicas. You can specify a different number of replicas for each shard. You can specify as many clusters as you wish in the configuration. To view your clusters, use the system.clusters table. The Distributed engine allows working with a cluster like a local server. However, the cluster's configuration cannot be specified dynamically, it has to be configured in the server config file. Usually, all servers in a cluster will have the same cluster config (though this is not required). Clusters from the config file are updated on the fly, without restarting the server. If you need to send a query to an unknown set of shards and replicas each time, you do not need to create a Distributed table – use the remote table function instead. See the section Table functions. "},{"title":"Writing data​","type":1,"pageTitle":"Distributed Table Engine","url":"en/engines/table-engines/special/distributed#distributed-writing-data","content":"There are two methods for writing data to a cluster: First, you can define which servers to write which data to and perform the write directly on each shard. In other words, perform direct INSERT statements on the remote tables in the cluster that the Distributed table is pointing to. This is the most flexible solution as you can use any sharding scheme, even one that is non-trivial due to the requirements of the subject area. This is also the most optimal solution since data can be written to different shards completely independently. Second, you can perform INSERT statements on a Distributed table. In this case, the table will distribute the inserted data across the servers itself. In order to write to a Distributed table, it must have the sharding_key parameter configured (except if there is only one shard). Each shard can have a &lt;weight&gt; defined in the config file. By default, the weight is 1. Data is distributed across shards in the amount proportional to the shard weight. All shard weights are summed up, then each shard's weight is divided by the total to determine each shard's proportion. For example, if there are two shards and the first has a weight of 1 while the second has a weight of 2, the first will be sent one third (1 / 3) of inserted rows and the second will be sent two thirds (2 / 3). Each shard can have the internal_replication parameter defined in the config file. If this parameter is set to true, the write operation selects the first healthy replica and writes data to it. Use this if the tables underlying the Distributed table are replicated tables (e.g. any of the Replicated*MergeTree table engines). One of the table replicas will receive the write and it will be replicated to the other replicas automatically. If internal_replication is set to false (the default), data is written to all replicas. In this case, the Distributed table replicates data itself. This is worse than using replicated tables because the consistency of replicas is not checked and, over time, they will contain slightly different data. To select the shard that a row of data is sent to, the sharding expression is analyzed, and its remainder is taken from dividing it by the total weight of the shards. The row is sent to the shard that corresponds to the half-interval of the remainders from prev_weights to prev_weights + weight, where prev_weights is the total weight of the shards with the smallest number, and weight is the weight of this shard. For example, if there are two shards, and the first has a weight of 9 while the second has a weight of 10, the row will be sent to the first shard for the remainders from the range [0, 9), and to the second for the remainders from the range [9, 19). The sharding expression can be any expression from constants and table columns that returns an integer. For example, you can use the expression rand() for random distribution of data, or UserID for distribution by the remainder from dividing the user’s ID (then the data of a single user will reside on a single shard, which simplifies running IN and JOIN by users). If one of the columns is not distributed evenly enough, you can wrap it in a hash function e.g. intHash64(UserID). A simple remainder from the division is a limited solution for sharding and isn’t always appropriate. It works for medium and large volumes of data (dozens of servers), but not for very large volumes of data (hundreds of servers or more). In the latter case, use the sharding scheme required by the subject area rather than using entries in Distributed tables. You should be concerned about the sharding scheme in the following cases: Queries are used that require joining data (IN or JOIN) by a specific key. If data is sharded by this key, you can use local IN or JOIN instead of GLOBAL IN or GLOBAL JOIN, which is much more efficient.A large number of servers is used (hundreds or more) with a large number of small queries, for example, queries for data of individual clients (e.g. websites, advertisers, or partners). In order for the small queries to not affect the entire cluster, it makes sense to locate data for a single client on a single shard. Alternatively, you can set up bi-level sharding: divide the entire cluster into “layers”, where a layer may consist of multiple shards. Data for a single client is located on a single layer, but shards can be added to a layer as necessary, and data is randomly distributed within them. Distributed tables are created for each layer, and a single shared distributed table is created for global queries. Data is written asynchronously. When inserted in the table, the data block is just written to the local file system. The data is sent to the remote servers in the background as soon as possible. The periodicity for sending data is managed by the distributed_directory_monitor_sleep_time_ms and distributed_directory_monitor_max_sleep_time_ms settings. The Distributed engine sends each file with inserted data separately, but you can enable batch sending of files with the distributed_directory_monitor_batch_inserts setting. This setting improves cluster performance by better utilizing local server and network resources. You should check whether data is sent successfully by checking the list of files (data waiting to be sent) in the table directory: /var/lib/clickhouse/data/database/table/. The number of threads performing background tasks can be set by background_distributed_schedule_pool_size setting. If the server ceased to exist or had a rough restart (for example, due to a hardware failure) after an INSERT to a Distributed table, the inserted data might be lost. If a damaged data part is detected in the table directory, it is transferred to the broken subdirectory and no longer used. "},{"title":"Reading data​","type":1,"pageTitle":"Distributed Table Engine","url":"en/engines/table-engines/special/distributed#distributed-reading-data","content":"When querying a Distributed table, SELECT queries are sent to all shards and work regardless of how data is distributed across the shards (they can be distributed completely randomly). When you add a new shard, you do not have to transfer old data into it. Instead, you can write new data to it by using a heavier weight – the data will be distributed slightly unevenly, but queries will work correctly and efficiently. When the max_parallel_replicas option is enabled, query processing is parallelized across all replicas within a single shard. For more information, see the section max_parallel_replicas. To learn more about how distibuted in and global in queries are processed, refer to this documentation. "},{"title":"Virtual Columns​","type":1,"pageTitle":"Distributed Table Engine","url":"en/engines/table-engines/special/distributed#virtual-columns","content":"_shard_num — Contains the shard_num value from the table system.clusters. Type: UInt32. note Since remote and cluster table functions internally create temporary Distributed table, _shard_num is available there too. See Also Virtual columns descriptionbackground_distributed_schedule_pool_size settingshardNum() and shardCount() functions Original article "},{"title":"General Questions About ClickHouse","type":0,"sectionRef":"#","url":"en/faq/general/","content":"General Questions About ClickHouse What is ClickHouse?Why ClickHouse is so fast?Who is using ClickHouse?What does “ClickHouse” mean?What does “Не тормозит” mean?What is OLAP?What is a columnar database?How do I choose a primary key?Why not use something like MapReduce?How do I contribute code to ClickHouse? Don’t see what you're looking for? Check out our other FAQ categories and also browse the many helpful articles found here in the documentation.","keywords":"clickhouse faq questions what is"},{"title":"What Is a Columnar Database?","type":0,"sectionRef":"#","url":"en/faq/general/columnar-database","content":"What Is a Columnar Database? A columnar database stores data of each column independently. This allows to read data from disks only for those columns that are used in any given query. The cost is that operations that affect whole rows become proportionally more expensive. The synonym for a columnar database is a column-oriented database management system. ClickHouse is a typical example of such a system. Key columnar database advantages are: Queries that use only a few columns out of many.Aggregating queries against large volumes of data.Column-wise data compression. Here is the illustration of the difference between traditional row-oriented systems and columnar databases when building reports: Traditional row-oriented Columnar A columnar database is a preferred choice for analytical applications because it allows to have many columns in a table just in case, but do not pay the cost for unused columns on read query execution time. Column-oriented databases are designed for big data processing because and data warehousing, they often natively scale using distributed clusters of low-cost hardware to increase throughput. ClickHouse does it with combination of distributed and replicated tables.","keywords":""},{"title":"Join Table Engine","type":0,"sectionRef":"#","url":"en/engines/table-engines/special/join","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"Join Table Engine","url":"en/engines/table-engines/special/join#creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ) ENGINE = Join(join_strictness, join_type, k1[, k2, ...])  See the detailed description of the CREATE TABLE query. Engine Parameters join_strictness – JOIN strictness.join_type – JOIN type.k1[, k2, ...] – Key columns from the USING clause that the JOIN operation is made with. Enter join_strictness and join_type parameters without quotes, for example, Join(ANY, LEFT, col1). They must match the JOIN operation that the table will be used for. If the parameters do not match, ClickHouse does not throw an exception and may return incorrect data. "},{"title":"Specifics and Recommendations​","type":1,"pageTitle":"Join Table Engine","url":"en/engines/table-engines/special/join#specifics-and-recommendations","content":""},{"title":"Data Storage​","type":1,"pageTitle":"Join Table Engine","url":"en/engines/table-engines/special/join#data-storage","content":"Join table data is always located in the RAM. When inserting rows into a table, ClickHouse writes data blocks to the directory on the disk so that they can be restored when the server restarts. If the server restarts incorrectly, the data block on the disk might get lost or damaged. In this case, you may need to manually delete the file with damaged data. "},{"title":"Selecting and Inserting Data​","type":1,"pageTitle":"Join Table Engine","url":"en/engines/table-engines/special/join#selecting-and-inserting-data","content":"You can use INSERT queries to add data to the Join-engine tables. If the table was created with the ANY strictness, data for duplicate keys are ignored. With the ALL strictness, all rows are added. Main use-cases for Join-engine tables are following: Place the table to the right side in a JOIN clause.Call the joinGet function, which lets you extract data from the table the same way as from a dictionary. "},{"title":"Deleting Data​","type":1,"pageTitle":"Join Table Engine","url":"en/engines/table-engines/special/join#deleting-data","content":"ALTER DELETE queries for Join-engine tables are implemented as mutations. DELETE mutation reads filtered data and overwrites data of memory and disk. "},{"title":"Limitations and Settings​","type":1,"pageTitle":"Join Table Engine","url":"en/engines/table-engines/special/join#join-limitations-and-settings","content":"When creating a table, the following settings are applied: join_use_nullsmax_rows_in_joinmax_bytes_in_joinjoin_overflow_modejoin_any_take_last_rowpersistent The Join-engine tables can’t be used in GLOBAL JOIN operations. The Join-engine allows to specify join_use_nulls setting in the CREATE TABLE statement. SELECT query should have the same join_use_nulls value. "},{"title":"Usage Examples​","type":1,"pageTitle":"Join Table Engine","url":"en/engines/table-engines/special/join#example","content":"Creating the left-side table: CREATE TABLE id_val(`id` UInt32, `val` UInt32) ENGINE = TinyLog;  INSERT INTO id_val VALUES (1,11)(2,12)(3,13);  Creating the right-side Join table: CREATE TABLE id_val_join(`id` UInt32, `val` UInt8) ENGINE = Join(ANY, LEFT, id);  INSERT INTO id_val_join VALUES (1,21)(1,22)(3,23);  Joining the tables: SELECT * FROM id_val ANY LEFT JOIN id_val_join USING (id);  ┌─id─┬─val─┬─id_val_join.val─┐ │ 1 │ 11 │ 21 │ │ 2 │ 12 │ 0 │ │ 3 │ 13 │ 23 │ └────┴─────┴─────────────────┘  As an alternative, you can retrieve data from the Join table, specifying the join key value: SELECT joinGet('id_val_join', 'val', toUInt32(1));  ┌─joinGet('id_val_join', 'val', toUInt32(1))─┐ │ 21 │ └────────────────────────────────────────────┘  Deleting a row from the Join table: ALTER TABLE id_val_join DELETE WHERE id = 3;  ┌─id─┬─val─┐ │ 1 │ 21 │ └────┴─────┘  Original article "},{"title":"What Does “ClickHouse” Mean?","type":0,"sectionRef":"#","url":"en/faq/general/dbms-naming","content":"What Does “ClickHouse” Mean? It’s a combination of “Clickstream” and “Data wareHouse”. It comes from the original use case at Yandex.Metrica, where ClickHouse was supposed to keep records of all clicks by people from all over the Internet, and it still does the job. You can read more about this use case on ClickHouse history page. This two-part meaning has two consequences: The only correct way to write ClickHouse is with capital H.If you need to abbreviate it, use CH. For some historical reasons, abbreviating as CK is also popular in China, mostly because one of the first talks about ClickHouse in Chinese used this form. info Many years after ClickHouse got its name, this approach of combining two words that are meaningful on their own has been highlighted as the best way to name a database in a research by Andy Pavlo, an Associate Professor of Databases at Carnegie Mellon University. ClickHouse shared his “best database name of all time” award with Postgres.","keywords":""},{"title":"How do I contribute code to ClickHouse?","type":0,"sectionRef":"#","url":"en/faq/general/how-do-i-contribute-code-to-clickhouse","content":"How do I contribute code to ClickHouse? ClickHouse is an open-source project developed on GitHub. As customary, contribution instructions are published in CONTRIBUTING.md file in the root of the source code repository. If you want to suggest a substantial change to ClickHouse, consider opening a GitHub issue explaining what you want to do, to discuss it with maintainers and community first. Examples of such RFC issues. If your contributions are security related, please check out our security policy too.","keywords":""},{"title":"Why Not Use Something Like MapReduce?","type":0,"sectionRef":"#","url":"en/faq/general/mapreduce","content":"Why Not Use Something Like MapReduce? We can refer to systems like MapReduce as distributed computing systems in which the reduce operation is based on distributed sorting. The most common open-source solution in this class is Apache Hadoop. These systems aren’t appropriate for online queries due to their high latency. In other words, they can’t be used as the back-end for a web interface. These types of systems aren’t useful for real-time data updates. Distributed sorting isn’t the best way to perform reduce operations if the result of the operation and all the intermediate results (if there are any) are located in the RAM of a single server, which is usually the case for online queries. In such a case, a hash table is an optimal way to perform reduce operations. A common approach to optimizing map-reduce tasks is pre-aggregation (partial reduce) using a hash table in RAM. The user performs this optimization manually. Distributed sorting is one of the main causes of reduced performance when running simple map-reduce tasks. Most MapReduce implementations allow you to execute arbitrary code on a cluster. But a declarative query language is better suited to OLAP to run experiments quickly. For example, Hadoop has Hive and Pig. Also consider Cloudera Impala or Shark (outdated) for Spark, as well as Spark SQL, Presto, and Apache Drill. Performance when running such tasks is highly sub-optimal compared to specialized systems, but relatively high latency makes it unrealistic to use these systems as the backend for a web interface.","keywords":""},{"title":"What Does “Не тормозит” Mean?","type":0,"sectionRef":"#","url":"en/faq/general/ne-tormozit","content":"What Does “Не тормозит” Mean? This question usually arises when people see official ClickHouse t-shirts. They have large words “ClickHouse не тормозит” on the front. Before ClickHouse became open-source, it has been developed as an in-house storage system by the largest Russian IT company, Yandex. That’s why it initially got its slogan in Russian, which is “не тормозит” (pronounced as “ne tormozit”). After the open-source release we first produced some of those t-shirts for events in Russia and it was a no-brainer to use the slogan as-is. One of the following batches of those t-shirts was supposed to be given away on events outside of Russia and we tried to make the English version of the slogan. Unfortunately, the Russian language is kind of elegant in terms of expressing stuff and there was a restriction of limited space on a t-shirt, so we failed to come up with good enough translation (most options appeared to be either long or inaccurate) and decided to keep the slogan in Russian even on t-shirts produced for international events. It appeared to be a great decision because people all over the world get positively surprised and curious when they see it. So, what does it mean? Here are some ways to translate “не тормозит”: If you translate it literally, it’d be something like “ClickHouse does not press the brake pedal”.If you’d want to express it as close to how it sounds to a Russian person with IT background, it’d be something like “If your larger system lags, it’s not because it uses ClickHouse”.Shorter, but not so precise versions could be “ClickHouse is not slow”, “ClickHouse does not lag” or just “ClickHouse is fast”. If you haven’t seen one of those t-shirts in person, you can check them out online in many ClickHouse-related videos. For example, this one: P.S. These t-shirts are not for sale, they are given away for free on most ClickHouse Meetups, usually for best questions or other forms of active participation.","keywords":""},{"title":"What Is OLAP?","type":0,"sectionRef":"#","url":"en/faq/general/olap","content":"","keywords":""},{"title":"OLAP from the Business Perspective​","type":1,"pageTitle":"What Is OLAP?","url":"en/faq/general/olap#olap-from-the-business-perspective","content":"In recent years, business people started to realize the value of data. Companies who make their decisions blindly, more often than not fail to keep up with the competition. The data-driven approach of successful companies forces them to collect all data that might be remotely useful for making business decisions and need mechanisms to timely analyze them. Here’s where OLAP database management systems (DBMS) come in. In a business sense, OLAP allows companies to continuously plan, analyze, and report operational activities, thus maximizing efficiency, reducing expenses, and ultimately conquering the market share. It could be done either in an in-house system or outsourced to SaaS providers like web/mobile analytics services, CRM services, etc. OLAP is the technology behind many BI applications (Business Intelligence). ClickHouse is an OLAP database management system that is pretty often used as a backend for those SaaS solutions for analyzing domain-specific data. However, some businesses are still reluctant to share their data with third-party providers and an in-house data warehouse scenario is also viable. "},{"title":"OLAP from the Technical Perspective​","type":1,"pageTitle":"What Is OLAP?","url":"en/faq/general/olap#olap-from-the-technical-perspective","content":"All database management systems could be classified into two groups: OLAP (Online Analytical Processing) and OLTP (Online Transactional Processing). Former focuses on building reports, each based on large volumes of historical data, but doing it not so frequently. While the latter usually handle a continuous stream of transactions, constantly modifying the current state of data. In practice OLAP and OLTP are not categories, it’s more like a spectrum. Most real systems usually focus on one of them but provide some solutions or workarounds if the opposite kind of workload is also desired. This situation often forces businesses to operate multiple storage systems integrated, which might be not so big deal but having more systems make it more expensive to maintain. So the trend of recent years is HTAP (Hybrid Transactional/Analytical Processing) when both kinds of the workload are handled equally well by a single database management system. Even if a DBMS started as a pure OLAP or pure OLTP, they are forced to move towards that HTAP direction to keep up with their competition. And ClickHouse is no exception, initially, it has been designed as fast-as-possible OLAP system and it still does not have full-fledged transaction support, but some features like consistent read/writes and mutations for updating/deleting data had to be added. The fundamental trade-off between OLAP and OLTP systems remains: To build analytical reports efficiently it’s crucial to be able to read columns separately, thus most OLAP databases are columnar,While storing columns separately increases costs of operations on rows, like append or in-place modification, proportionally to the number of columns (which can be huge if the systems try to collect all details of an event just in case). Thus, most OLTP systems store data arranged by rows. "},{"title":"Who Is Using ClickHouse?","type":0,"sectionRef":"#","url":"en/faq/general/who-is-using-clickhouse","content":"Who Is Using ClickHouse? Being an open-source product makes this question not so straightforward to answer. You do not have to tell anyone if you want to start using ClickHouse, you just go grab source code or pre-compiled packages. There’s no contract to sign and the Apache 2.0 license allows for unconstrained software distribution. Also, the technology stack is often in a grey zone of what’s covered by an NDA. Some companies consider technologies they use as a competitive advantage even if they are open-source and do not allow employees to share any details publicly. Some see some PR risks and allow employees to share implementation details only with their PR department approval. So how to tell who is using ClickHouse? One way is to ask around. If it’s not in writing, people are much more willing to share what technologies are used in their companies, what the use cases are, what kind of hardware is used, data volumes, etc. We’re talking with users regularly on ClickHouse Meetups all over the world and have heard stories about 1000+ companies that use ClickHouse. Unfortunately, that’s not reproducible and we try to treat such stories as if they were told under NDA to avoid any potential troubles. But you can come to any of our future meetups and talk with other users on your own. There are multiple ways how meetups are announced, for example, you can subscribe to our Twitter. The second way is to look for companies publicly saying that they use ClickHouse. It’s more substantial because there’s usually some hard evidence like a blog post, talk video recording, slide deck, etc. We collect the collection of links to such evidence on our Adopters page. Feel free to contribute the story of your employer or just some links you’ve stumbled upon (but try not to violate your NDA in the process). You can find names of very large companies in the adopters list, like Bloomberg, Cisco, China Telecom, Tencent, or Uber, but with the first approach, we found that there are many more. For example, if you take the list of largest IT companies by Forbes (2020) over half of them are using ClickHouse in some way. Also, it would be unfair not to mention Yandex, the company which initially open-sourced ClickHouse in 2016 and happens to be one of the largest IT companies in Europe.","keywords":""},{"title":"Why ClickHouse Is So Fast?","type":0,"sectionRef":"#","url":"en/faq/general/why-clickhouse-is-so-fast","content":"Why ClickHouse Is So Fast? It was designed to be fast. Query execution performance has always been a top priority during the development process, but other important characteristics like user-friendliness, scalability, and security were also considered so ClickHouse could become a real production system. ClickHouse was initially built as a prototype to do just a single task well: to filter and aggregate data as fast as possible. That’s what needs to be done to build a typical analytical report and that’s what a typical GROUP BY query does. ClickHouse team has made several high-level decisions that combined made achieving this task possible: Column-oriented storage : Source data often contain hundreds or even thousands of columns, while a report can use just a few of them. The system needs to avoid reading unnecessary columns, or most expensive disk read operations would be wasted. Indexes : ClickHouse keeps data structures in memory that allows reading not only used columns but only necessary row ranges of those columns. Data compression : Storing different values of the same column together often leads to better compression ratios (compared to row-oriented systems) because in real data column often has the same or not so many different values for neighboring rows. In addition to general-purpose compression, ClickHouse supports specialized codecs that can make data even more compact. Vectorized query execution : ClickHouse not only stores data in columns but also processes data in columns. It leads to better CPU cache utilization and allows for SIMD CPU instructions usage. Scalability : ClickHouse can leverage all available CPU cores and disks to execute even a single query. Not only on a single server but all CPU cores and disks of a cluster as well. But many other database management systems use similar techniques. What really makes ClickHouse stand out is attention to low-level details. Most programming languages provide implementations for most common algorithms and data structures, but they tend to be too generic to be effective. Every task can be considered as a landscape with various characteristics, instead of just throwing in random implementation. For example, if you need a hash table, here are some key questions to consider: Which hash function to choose?Collision resolution algorithm: open addressing vs chaining?Memory layout: one array for keys and values or separate arrays? Will it store small or large values?Fill factor: when and how to resize? How to move values around on resize?Will values be removed and which algorithm will work better if they will?Will we need fast probing with bitmaps, inline placement of string keys, support for non-movable values, prefetch, and batching? Hash table is a key data structure for GROUP BY implementation and ClickHouse automatically chooses one of 30+ variations for each specific query. The same goes for algorithms, for example, in sorting you might consider: What will be sorted: an array of numbers, tuples, strings, or structures?Is all data available completely in RAM?Do we need a stable sort?Do we need a full sort? Maybe partial sort or n-th element will suffice?How to implement comparisons?Are we sorting data that has already been partially sorted? Algorithms that they rely on characteristics of data they are working with can often do better than their generic counterparts. If it is not really known in advance, the system can try various implementations and choose the one that works best in runtime. For example, see an article on how LZ4 decompression is implemented in ClickHouse. Last but not least, the ClickHouse team always monitors the Internet on people claiming that they came up with the best implementation, algorithm, or data structure to do something and tries it out. Those claims mostly appear to be false, but from time to time you’ll indeed find a gem. Tips for building your own high-performance software Keep in mind low-level details when designing your system.Design based on hardware capabilities.Choose data structures and abstractions based on the needs of the task.Provide specializations for special cases.Try new, “best” algorithms, that you read about yesterday.Choose an algorithm in runtime based on statistics.Benchmark on real datasets.Test for performance regressions in CI.Measure and observe everything.","keywords":""},{"title":"Questions About Integrating ClickHouse and Other Systems","type":0,"sectionRef":"#","url":"en/faq/integration/","content":"Questions About Integrating ClickHouse and Other Systems How do I export data from ClickHouse to a file?How to import JSON into ClickHouse?How do I connect Kafka to ClickHouse?Can I connect my Java application to ClickHouse?Can ClickHouse read tables from MySQL?Can ClickHouse read tables from PostgreSQLWhat if I have a problem with encodings when connecting to Oracle via ODBC? Don’t see what you're looking for? Check out our other FAQ categories and also browse the many helpful articles found here in the documentation.","keywords":"clickhouse faq questions integrations"},{"title":"How Do I Export Data from ClickHouse to a File?","type":0,"sectionRef":"#","url":"en/faq/integration/file-export","content":"","keywords":""},{"title":"Using INTO OUTFILE Clause​","type":1,"pageTitle":"How Do I Export Data from ClickHouse to a File?","url":"en/faq/integration/file-export#using-into-outfile-clause","content":"Add an INTO OUTFILE clause to your query. For example: SELECT * FROM table INTO OUTFILE 'file'  By default, ClickHouse uses the TabSeparated format for output data. To select the data format, use the FORMAT clause. For example: SELECT * FROM table INTO OUTFILE 'file' FORMAT CSV  "},{"title":"Using a File-Engine Table​","type":1,"pageTitle":"How Do I Export Data from ClickHouse to a File?","url":"en/faq/integration/file-export#using-a-file-engine-table","content":"See File table engine. "},{"title":"Using Command-Line Redirection​","type":1,"pageTitle":"How Do I Export Data from ClickHouse to a File?","url":"en/faq/integration/file-export#using-command-line-redirection","content":"$ clickhouse-client --query &quot;SELECT * from table&quot; --format FormatName &gt; result.txt  See clickhouse-client. "},{"title":"How to Import JSON Into ClickHouse?","type":0,"sectionRef":"#","url":"en/faq/integration/json-import","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"How to Import JSON Into ClickHouse?","url":"en/faq/integration/json-import#examples","content":"Using HTTP interface: $ echo '{&quot;foo&quot;:&quot;bar&quot;}' | curl 'http://localhost:8123/?query=INSERT%20INTO%20test%20FORMAT%20JSONEachRow' --data-binary @-  Using CLI interface: $ echo '{&quot;foo&quot;:&quot;bar&quot;}' | clickhouse-client --query=&quot;INSERT INTO test FORMAT JSONEachRow&quot;  Instead of inserting data manually, you might consider to use one of client libraries instead. "},{"title":"Useful Settings​","type":1,"pageTitle":"How to Import JSON Into ClickHouse?","url":"en/faq/integration/json-import#useful-settings","content":"input_format_skip_unknown_fields allows to insert JSON even if there were additional fields not present in table schema (by discarding them).input_format_import_nested_json allows to insert nested JSON objects into columns of Nested type. note Settings are specified as GET parameters for the HTTP interface or as additional command-line arguments prefixed with -- for the CLI interface. "},{"title":"What If I Have a Problem with Encodings When Using Oracle Via ODBC?","type":0,"sectionRef":"#","url":"en/faq/integration/oracle-odbc","content":"What If I Have a Problem with Encodings When Using Oracle Via ODBC? If you use Oracle as a source of ClickHouse external dictionaries via Oracle ODBC driver, you need to set the correct value for the NLS_LANG environment variable in /etc/default/clickhouse. For more information, see the Oracle NLS_LANG FAQ. Example NLS_LANG=RUSSIAN_RUSSIA.UTF8 ","keywords":""},{"title":"Question About Operating ClickHouse Servers and Clusters","type":0,"sectionRef":"#","url":"en/faq/operations/","content":"Question About Operating ClickHouse Servers and Clusters Which ClickHouse version should I use in production?Is it possible to delete old records from a ClickHouse table?How do I configure ClickHouse Keeper?Can ClickHouse integrate with LDAP?How do I configure users, roles and permissions in ClickHouse?Can you update or delete rows in ClickHouse?Does ClickHouse support multi-region replication? Don’t see what you're looking for? Check out our other FAQ categories and also browse the many helpful articles found here in the documentation.","keywords":""},{"title":"Is It Possible to Delete Old Records from a ClickHouse Table?","type":0,"sectionRef":"#","url":"en/faq/operations/delete-old-data","content":"","keywords":""},{"title":"TTL​","type":1,"pageTitle":"Is It Possible to Delete Old Records from a ClickHouse Table?","url":"en/faq/operations/delete-old-data#ttl","content":"ClickHouse allows to automatically drop values when some condition happens. This condition is configured as an expression based on any columns, usually just static offset for any timestamp column. The key advantage of this approach is that it does not need any external system to trigger, once TTL is configured, data removal happens automatically in background. note TTL can also be used to move data not only to /dev/null, but also between different storage systems, like from SSD to HDD. More details on configuring TTL. "},{"title":"ALTER DELETE​","type":1,"pageTitle":"Is It Possible to Delete Old Records from a ClickHouse Table?","url":"en/faq/operations/delete-old-data#alter-delete","content":"ClickHouse does not have real-time point deletes like in OLTP databases. The closest thing to them are mutations. They are issued as ALTER ... DELETE or ALTER ... UPDATE queries to distinguish from normal DELETE or UPDATE as they are asynchronous batch operations, not immediate modifications. The rest of syntax after ALTER TABLE prefix is similar. ALTER DELETE can be issued to flexibly remove old data. If you need to do it regularly, the main downside will be the need to have an external system to submit the query. There are also some performance considerations since mutation rewrite complete parts even there’s only a single row to be deleted. This is the most common approach to make your system based on ClickHouse GDPR-compliant. More details on mutations. "},{"title":"DROP PARTITION​","type":1,"pageTitle":"Is It Possible to Delete Old Records from a ClickHouse Table?","url":"en/faq/operations/delete-old-data#drop-partition","content":"ALTER TABLE ... DROP PARTITION provides a cost-efficient way to drop a whole partition. It’s not that flexible and needs proper partitioning scheme configured on table creation, but still covers most common cases. Like mutations need to be executed from an external system for regular use. More details on manipulating partitions. "},{"title":"TRUNCATE​","type":1,"pageTitle":"Is It Possible to Delete Old Records from a ClickHouse Table?","url":"en/faq/operations/delete-old-data#truncate","content":"It’s rather radical to drop all data from a table, but in some cases it might be exactly what you need. More details on table truncation. "},{"title":"Does ClickHouse support multi-region replication?","type":0,"sectionRef":"#","url":"en/faq/operations/multi-region-replication","content":"Does ClickHouse support multi-region replication? The short answer is &quot;yes&quot;. However, we recommend keeping latency between all regions/datacenters in two-digit range, otherwise write performance will suffer as it goes through distributed consensus protocol. For example, replication between US coasts will likely work fine, but between the US and Europe won't. Configuration-wise there's no difference compared to single-region replication, simply use hosts that are located in different locations for replicas. For more information, see full article on data replication.","keywords":""},{"title":"Which ClickHouse Version to Use in Production?","type":0,"sectionRef":"#","url":"en/faq/operations/production","content":"","keywords":""},{"title":"Which ClickHouse Version Do You Recommend?​","type":1,"pageTitle":"Which ClickHouse Version to Use in Production?","url":"en/faq/operations/production#which-clickhouse-version-do-you-recommend","content":"It’s tempting to hire consultants or trust some known experts to get rid of responsibility for your production environment. You install some specific ClickHouse version that someone else recommended, now if there’s some issue with it - it’s not your fault, it’s someone else’s. This line of reasoning is a big trap. No external person knows better what’s going on in your company’s production environment. So how to properly choose which ClickHouse version to upgrade to? Or how to choose your first ClickHouse version? First of all, you need to invest in setting up a realistic pre-production environment. In an ideal world, it could be a completely identical shadow copy, but that’s usually expensive. Here’re some key points to get reasonable fidelity in a pre-production environment with not so high costs: Pre-production environment needs to run an as close set of queries as you intend to run in production: Don’t make it read-only with some frozen data.Don’t make it write-only with just copying data without building some typical reports.Don’t wipe it clean instead of applying schema migrations. Use a sample of real production data and queries. Try to choose a sample that’s still representative and makes SELECT queries return reasonable results. Use obfuscation if your data is sensitive and internal policies do not allow it to leave the production environment.Make sure that pre-production is covered by your monitoring and alerting software the same way as your production environment does.If your production spans across multiple datacenters or regions, make your pre-production does the same.If your production uses complex features like replication, distributed table, cascading materialize views, make sure they are configured similarly in pre-production.There’s a trade-off on using the roughly same number of servers or VMs in pre-production as in production, but of smaller size, or much less of them, but of the same size. The first option might catch extra network-related issues, while the latter is easier to manage. The second area to invest in is automated testing infrastructure. Don’t assume that if some kind of query has executed successfully once, it’ll continue to do so forever. It’s ok to have some unit tests where ClickHouse is mocked but make sure your product has a reasonable set of automated tests that are run against real ClickHouse and check that all important use cases are still working as expected. Extra step forward could be contributing those automated tests to ClickHouse’s open-source test infrastructure that’s continuously used in its day-to-day development. It definitely will take some additional time and effort to learn how to run it and then how to adapt your tests to this framework, but it’ll pay off by ensuring that ClickHouse releases are already tested against them when they are announced stable, instead of repeatedly losing time on reporting the issue after the fact and then waiting for a bugfix to be implemented, backported and released. Some companies even have such test contributions to infrastructure by its use as an internal policy, most notably it’s called Beyonce’s Rule at Google. When you have your pre-production environment and testing infrastructure in place, choosing the best version is straightforward: Routinely run your automated tests against new ClickHouse releases. You can do it even for ClickHouse releases that are marked as testing, but going forward to the next steps with them is not recommended.Deploy the ClickHouse release that passed the tests to pre-production and check that all processes are running as expected.Report any issues you discovered to ClickHouse GitHub Issues.If there were no major issues, it should be safe to start deploying ClickHouse release to your production environment. Investing in gradual release automation that implements an approach similar to canary releases or green-blue deployments might further reduce the risk of issues in production. As you might have noticed, there’s nothing specific to ClickHouse in the approach described above, people do that for any piece of infrastructure they rely on if they take their production environment seriously. "},{"title":"How to Choose Between ClickHouse Releases?​","type":1,"pageTitle":"Which ClickHouse Version to Use in Production?","url":"en/faq/operations/production#how-to-choose-between-clickhouse-releases","content":"If you look into contents of ClickHouse package repository, you’ll see four kinds of packages: testingprestablestablelts (long-term support) As was mentioned earlier, testing is good mostly to notice issues early, running them in production is not recommended because each of them is not tested as thoroughly as other kinds of packages. prestable is a release candidate which generally looks promising and is likely to become announced as stable soon. You can try them out in pre-production and report issues if you see any. For production use, there are two key options: stable and lts. Here is some guidance on how to choose between them: stable is the kind of package we recommend by default. They are released roughly monthly (and thus provide new features with reasonable delay) and three latest stable releases are supported in terms of diagnostics and backporting of bugfixes.lts are released twice a year and are supported for a year after their initial release. You might prefer them over stable in the following cases: Your company has some internal policies that do not allow for frequent upgrades or using non-LTS software.You are using ClickHouse in some secondary products that either does not require any complex ClickHouse features and do not have enough resources to keep it updated. Many teams who initially thought that lts is the way to go, often switch to stable anyway because of some recent feature that’s important for their product. warning One more thing to keep in mind when upgrading ClickHouse: we’re always keeping eye on compatibility across releases, but sometimes it’s not reasonable to keep and some minor details might change. So make sure you check the changelog before upgrading to see if there are any notes about backward-incompatible changes. "},{"title":"Questions About ClickHouse Use Cases","type":0,"sectionRef":"#","url":"en/faq/use-cases/","content":"Questions About ClickHouse Use Cases Can I use ClickHouse as a time-series database?Can I use ClickHouse as a key-value storage? Don’t see what you're looking for? Check out our other FAQ categories and also browse the many helpful articles found here in the documentation.","keywords":""},{"title":"Can I Use ClickHouse As a Key-Value Storage?","type":0,"sectionRef":"#","url":"en/faq/use-cases/key-value","content":"Can I Use ClickHouse As a Key-Value Storage? The short answer is “no”. The key-value workload is among top positions in the list of cases when NOT{.text-danger} to use ClickHouse. It’s an OLAP system after all, while there are many excellent key-value storage systems out there. However, there might be situations where it still makes sense to use ClickHouse for key-value-like queries. Usually, it’s some low-budget products where the main workload is analytical in nature and fits ClickHouse well, but there’s also some secondary process that needs a key-value pattern with not so high request throughput and without strict latency requirements. If you had an unlimited budget, you would have installed a secondary key-value database for thus secondary workload, but in reality, there’s an additional cost of maintaining one more storage system (monitoring, backups, etc.) which might be desirable to avoid. If you decide to go against recommendations and run some key-value-like queries against ClickHouse, here’re some tips: The key reason why point queries are expensive in ClickHouse is its sparse primary index of main MergeTree table engine family. This index can’t point to each specific row of data, instead, it points to each N-th and the system has to scan from the neighboring N-th row to the desired one, reading excessive data along the way. In a key-value scenario, it might be useful to reduce the value of N with the index_granularity setting.ClickHouse keeps each column in a separate set of files, so to assemble one complete row it needs to go through each of those files. Their count increases linearly with the number of columns, so in the key-value scenario, it might be worth to avoid using many columns and put all your payload in a single String column encoded in some serialization format like JSON, Protobuf or whatever makes sense.There’s an alternative approach that uses Join table engine instead of normal MergeTree tables and joinGet function to retrieve the data. It can provide better query performance but might have some usability and reliability issues. Here’s an usage example.","keywords":""},{"title":"Can I Use ClickHouse As a Time-Series Database?","type":0,"sectionRef":"#","url":"en/faq/use-cases/time-series","content":"Can I Use ClickHouse As a Time-Series Database? ClickHouse is a generic data storage solution for OLAP workloads, while there are many specialized time-series database management systems. Nevertheless, ClickHouse’s focus on query execution speed allows it to outperform specialized systems in many cases. There are many independent benchmarks on this topic out there, so we’re not going to conduct one here. Instead, let’s focus on ClickHouse features that are important to use if that’s your use case. First of all, there are specialized codecs which make typical time-series. Either common algorithms like DoubleDelta and Gorilla or specific to ClickHouse like T64. Second, time-series queries often hit only recent data, like one day or one week old. It makes sense to use servers that have both fast nVME/SSD drives and high-capacity HDD drives. ClickHouse TTL feature allows to configure keeping fresh hot data on fast drives and gradually move it to slower drives as it ages. Rollup or removal of even older data is also possible if your requirements demand it. Even though it’s against ClickHouse philosophy of storing and processing raw data, you can use materialized views to fit into even tighter latency or costs requirements.","keywords":""},{"title":"GitHub Events Dataset","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/github-events","content":"GitHub Events Dataset Dataset contains all events on GitHub from 2011 to Dec 6 2020, the size is 3.1 billion records. Download size is 75 GB and it will require up to 200 GB space on disk if stored in a table with lz4 compression. Full dataset description, insights, download instruction and interactive queries are posted here.","keywords":""},{"title":"Cell Towers","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/cell-towers","content":"","keywords":""},{"title":"Get the Dataset​","type":1,"pageTitle":"Cell Towers","url":"en/getting-started/example-datasets/cell-towers#get-the-dataset","content":"Download the snapshot of the dataset from February 2021: [https://datasets.clickhouse.com/cell_towers.csv.xz] (729 MB). Validate the integrity (optional step): md5sum cell_towers.csv.xz 8cf986f4a0d9f12c6f384a0e9192c908 cell_towers.csv.xz  Decompress it with the following command: xz -d cell_towers.csv.xz  Create a table: CREATE TABLE cell_towers ( radio Enum8('' = 0, 'CDMA' = 1, 'GSM' = 2, 'LTE' = 3, 'NR' = 4, 'UMTS' = 5), mcc UInt16, net UInt16, area UInt16, cell UInt64, unit Int16, lon Float64, lat Float64, range UInt32, samples UInt32, changeable UInt8, created DateTime, updated DateTime, averageSignal UInt8 ) ENGINE = MergeTree ORDER BY (radio, mcc, net, created);  Insert the dataset: clickhouse-client --query &quot;INSERT INTO cell_towers FORMAT CSVWithNames&quot; &lt; cell_towers.csv  "},{"title":"Examples​","type":1,"pageTitle":"Cell Towers","url":"en/getting-started/example-datasets/cell-towers#examples","content":"A number of cell towers by type: SELECT radio, count() AS c FROM cell_towers GROUP BY radio ORDER BY c DESC ┌─radio─┬────────c─┐ │ UMTS │ 20686487 │ │ LTE │ 12101148 │ │ GSM │ 9931312 │ │ CDMA │ 556344 │ │ NR │ 867 │ └───────┴──────────┘ 5 rows in set. Elapsed: 0.011 sec. Processed 43.28 million rows, 43.28 MB (3.83 billion rows/s., 3.83 GB/s.)  Cell towers by mobile country code (MCC): SELECT mcc, count() FROM cell_towers GROUP BY mcc ORDER BY count() DESC LIMIT 10 ┌─mcc─┬─count()─┐ │ 310 │ 5024650 │ │ 262 │ 2622423 │ │ 250 │ 1953176 │ │ 208 │ 1891187 │ │ 724 │ 1836150 │ │ 404 │ 1729151 │ │ 234 │ 1618924 │ │ 510 │ 1353998 │ │ 440 │ 1343355 │ │ 311 │ 1332798 │ └─────┴─────────┘ 10 rows in set. Elapsed: 0.019 sec. Processed 43.28 million rows, 86.55 MB (2.33 billion rows/s., 4.65 GB/s.)  So, the top countries are: the USA, Germany, and Russia. You may want to create an External Dictionary in ClickHouse to decode these values. "},{"title":"Use case​","type":1,"pageTitle":"Cell Towers","url":"en/getting-started/example-datasets/cell-towers#use-case","content":"Using pointInPolygon function. Create a table where we will store polygons: CREATE TEMPORARY TABLE moscow (polygon Array(Tuple(Float64, Float64)));  This is a rough shape of Moscow (without &quot;new Moscow&quot;): INSERT INTO moscow VALUES ([(37.84172564285271, 55.78000432402266), (37.8381207618713, 55.775874525970494), (37.83979446823122, 55.775626746008065), (37.84243326983639, 55.77446586811748), (37.84262672750849, 55.771974101091104), (37.84153238623039, 55.77114545193181), (37.841124690460184, 55.76722010265554), (37.84239076983644, 55.76654891107098), (37.842283558197025, 55.76258709833121), (37.8421759312134, 55.758073999993734), (37.84198330422974, 55.75381499999371), (37.8416827275085, 55.749277102484484), (37.84157576190186, 55.74794544108413), (37.83897929098507, 55.74525257875241), (37.83739676451868, 55.74404373042019), (37.838732481460525, 55.74298009816793), (37.841183997352545, 55.743060321833575), (37.84097476190185, 55.73938799999373), (37.84048155819702, 55.73570799999372), (37.840095812164286, 55.73228210777237), (37.83983814285274, 55.73080491981639), (37.83846476321406, 55.729799917464675), (37.83835745269769, 55.72919751082619), (37.838636380279524, 55.72859509486539), (37.8395161005249, 55.727705075632784), (37.83897964285276, 55.722727886185154), (37.83862557539366, 55.72034817326636), (37.83559735744853, 55.71944437307499), (37.835370708803126, 55.71831419154461), (37.83738169402022, 55.71765218986692), (37.83823396494291, 55.71691750159089), (37.838056931213345, 55.71547311301385), (37.836812846557606, 55.71221445615604), (37.83522525396725, 55.709331054395555), (37.83269301586908, 55.70953687463627), (37.829667367706236, 55.70903403789297), (37.83311126588435, 55.70552351822608), (37.83058993121339, 55.70041317726053), (37.82983872750851, 55.69883771404813), (37.82934501586913, 55.69718947487017), (37.828926414016685, 55.69504441658371), (37.82876530422971, 55.69287499999378), (37.82894754100031, 55.690759754047335), (37.827697554878185, 55.68951421135665), (37.82447346292115, 55.68965045405069), (37.83136543914793, 55.68322046195302), (37.833554015869154, 55.67814012759211), (37.83544184655761, 55.67295011628339), (37.837480388885474, 55.6672498719639), (37.838960677246064, 55.66316274139358), (37.83926093121332, 55.66046999999383), (37.839025050262435, 55.65869897264431), (37.83670784390257, 55.65794084879904), (37.835656529083245, 55.65694309303843), (37.83704060449217, 55.65689306460552), (37.83696819873806, 55.65550363526252), (37.83760389616388, 55.65487847246661), (37.83687972750851, 55.65356745541324), (37.83515216004943, 55.65155951234079), (37.83312418518067, 55.64979413590619), (37.82801726983639, 55.64640836412121), (37.820614174591, 55.64164525405531), (37.818908190475426, 55.6421883258084), (37.81717543386075, 55.64112490388471), (37.81690987037274, 55.63916106913107), (37.815099354492155, 55.637925371757085), (37.808769150787356, 55.633798276884455), (37.80100123544311, 55.62873670012244), (37.79598013491824, 55.62554336109055), (37.78634567724606, 55.62033499605651), (37.78334147619623, 55.618768681480326), (37.77746201055901, 55.619855533402706), (37.77527329626457, 55.61909966711279), (37.77801986242668, 55.618770300976294), (37.778212973541216, 55.617257701952106), (37.77784818518065, 55.61574504433011), (37.77016867724609, 55.61148576294007), (37.760191219573976, 55.60599579539028), (37.75338926983641, 55.60227892751446), (37.746329965606634, 55.59920577639331), (37.73939925396728, 55.59631430313617), (37.73273665739439, 55.5935318803559), (37.7299954450912, 55.59350760316188), (37.7268679946899, 55.59469840523759), (37.72626726983634, 55.59229549697373), (37.7262673598022, 55.59081598950582), (37.71897193121335, 55.5877595845419), (37.70871550793456, 55.58393177431724), (37.700497489410374, 55.580917323756644), (37.69204305026244, 55.57778089778455), (37.68544477378839, 55.57815154690915), (37.68391050793454, 55.57472945079756), (37.678803592590306, 55.57328235936491), (37.6743402539673, 55.57255251445782), (37.66813862698363, 55.57216388774464), (37.617927457672096, 55.57505691895805), (37.60443099999999, 55.5757737568051), (37.599683515869145, 55.57749105910326), (37.59754177842709, 55.57796291823627), (37.59625834786988, 55.57906686095235), (37.59501783265684, 55.57746616444403), (37.593090671936025, 55.57671634534502), (37.587018007904, 55.577944600233785), (37.578692203704804, 55.57982895000019), (37.57327546607398, 55.58116294118248), (37.57385012109279, 55.581550362779), (37.57399562266922, 55.5820107079112), (37.5735356072979, 55.58226289171689), (37.57290393054962, 55.582393529795155), (37.57037722355653, 55.581919415056234), (37.5592298306885, 55.584471614867844), (37.54189249206543, 55.58867650795186), (37.5297256269836, 55.59158133551745), (37.517837865081766, 55.59443656218868), (37.51200186508174, 55.59635625174229), (37.506808949737554, 55.59907823904434), (37.49820432275389, 55.6062944994944), (37.494406071441674, 55.60967103463367), (37.494760001358024, 55.61066689753365), (37.49397137107085, 55.61220931698269), (37.49016528606031, 55.613417718449064), (37.48773249206542, 55.61530616333343), (37.47921386508177, 55.622640129112334), (37.470652153442394, 55.62993723476164), (37.46273446298218, 55.6368075123157), (37.46350692265317, 55.64068225239439), (37.46050283203121, 55.640794546982576), (37.457627470916734, 55.64118904154646), (37.450718034393326, 55.64690488145138), (37.44239252645875, 55.65397824729769), (37.434587576721185, 55.66053543155961), (37.43582144975277, 55.661693766520735), (37.43576786245721, 55.662755031737014), (37.430982915344174, 55.664610641628116), (37.428547447097685, 55.66778515273695), (37.42945134592044, 55.668633314343566), (37.42859571562949, 55.66948145750025), (37.4262836402282, 55.670813882451405), (37.418709037048295, 55.6811141674414), (37.41922139651101, 55.68235377885389), (37.419218771842885, 55.68359335082235), (37.417196501327446, 55.684375235224735), (37.41607020370478, 55.68540557585352), (37.415640857147146, 55.68686637150793), (37.414632153442334, 55.68903015131686), (37.413344899475064, 55.690896881757396), (37.41171432275391, 55.69264232162232), (37.40948282275393, 55.69455101638112), (37.40703674603271, 55.69638690385348), (37.39607169577025, 55.70451821283731), (37.38952706878662, 55.70942491932811), (37.387778313491815, 55.71149057784176), (37.39049275399779, 55.71419814298992), (37.385557272491454, 55.7155489617061), (37.38388335714726, 55.71849856042102), (37.378368238098155, 55.7292763261685), (37.37763597123337, 55.730845879211614), (37.37890062088197, 55.73167906388319), (37.37750451918789, 55.734703664681774), (37.375610832015965, 55.734851959522246), (37.3723813571472, 55.74105626086403), (37.37014935714723, 55.746115620904355), (37.36944173016362, 55.750883999993725), (37.36975304365541, 55.76335905525834), (37.37244070571134, 55.76432079697595), (37.3724259757175, 55.76636979670426), (37.369922155757884, 55.76735417953104), (37.369892695770275, 55.76823419316575), (37.370214730163575, 55.782312184391266), (37.370493611114505, 55.78436801120489), (37.37120164550783, 55.78596427165359), (37.37284851456452, 55.7874378183096), (37.37608325135799, 55.7886695054807), (37.3764587460632, 55.78947647305964), (37.37530000265506, 55.79146512926804), (37.38235915344241, 55.79899647809345), (37.384344043655396, 55.80113596939471), (37.38594269577028, 55.80322699999366), (37.38711208598329, 55.804919036911976), (37.3880239841309, 55.806610999993666), (37.38928977249147, 55.81001864976979), (37.39038389947512, 55.81348641242801), (37.39235781481933, 55.81983538336746), (37.393709457672124, 55.82417822811877), (37.394685720901464, 55.82792275755836), (37.39557615344238, 55.830447148154136), (37.39844478226658, 55.83167107969975), (37.40019761214057, 55.83151823557964), (37.400398790382326, 55.83264967594742), (37.39659544313046, 55.83322180909622), (37.39667059524539, 55.83402792148566), (37.39682089947515, 55.83638877400216), (37.39643489154053, 55.83861656112751), (37.3955338994751, 55.84072348043264), (37.392680272491454, 55.84502158126453), (37.39241188227847, 55.84659117913199), (37.392529730163616, 55.84816071336481), (37.39486835714723, 55.85288092980303), (37.39873052645878, 55.859893456073635), (37.40272161111449, 55.86441833633205), (37.40697072750854, 55.867579567544375), (37.410007082016016, 55.868369880337), (37.4120992989502, 55.86920843741314), (37.412668021163924, 55.87055369615854), (37.41482461111453, 55.87170587948249), (37.41862266137694, 55.873183961039565), (37.42413732540892, 55.874879126654704), (37.4312182698669, 55.875614937236705), (37.43111093783558, 55.8762723478417), (37.43332105622856, 55.87706546369396), (37.43385747619623, 55.87790681284802), (37.441303050262405, 55.88027084462084), (37.44747234260555, 55.87942070143253), (37.44716141796871, 55.88072960917233), (37.44769797085568, 55.88121221323979), (37.45204320500181, 55.882080694420715), (37.45673176190186, 55.882346110794586), (37.463383999999984, 55.88252729504517), (37.46682797486874, 55.88294937719063), (37.470014457672086, 55.88361266759345), (37.47751410450743, 55.88546991372396), (37.47860317658232, 55.88534929207307), (37.48165826025772, 55.882563306475106), (37.48316434442331, 55.8815803226785), (37.483831555817645, 55.882427612793315), (37.483182967125686, 55.88372791409729), (37.483092277908824, 55.88495581062434), (37.4855716508179, 55.8875561994203), (37.486440636245746, 55.887827444039566), (37.49014203439328, 55.88897899871799), (37.493210285705544, 55.890208937135604), (37.497512451065035, 55.891342397444696), (37.49780744510645, 55.89174030252967), (37.49940333499519, 55.89239745507079), (37.50018383334346, 55.89339220941865), (37.52421672750851, 55.903869074155224), (37.52977457672118, 55.90564076517974), (37.53503220370484, 55.90661661218259), (37.54042858064267, 55.90714113744566), (37.54320461007303, 55.905645048442985), (37.545686966066306, 55.906608607018505), (37.54743976120755, 55.90788552162358), (37.55796999999999, 55.90901557907218), (37.572711542327866, 55.91059395704873), (37.57942799999998, 55.91073854155573), (37.58502865872187, 55.91009969268444), (37.58739968913264, 55.90794809960554), (37.59131567193598, 55.908713267595054), (37.612687423278814, 55.902866854295375), (37.62348079629517, 55.90041967242986), (37.635797880950896, 55.898141151686396), (37.649487626983664, 55.89639275532968), (37.65619302513125, 55.89572360207488), (37.66294133862307, 55.895295577183965), (37.66874564418033, 55.89505457604897), (37.67375601586915, 55.89254677027454), (37.67744661901856, 55.8947775867987), (37.688347, 55.89450045676125), (37.69480554232789, 55.89422926332761), (37.70107096560668, 55.89322256101114), (37.705962965606716, 55.891763491662616), (37.711885134918205, 55.889110234998974), (37.71682005026245, 55.886577568759876), (37.7199315476074, 55.88458159806678), (37.72234560316464, 55.882281005794134), (37.72364385977171, 55.8809452036196), (37.725371142837474, 55.8809722706006), (37.727870902099546, 55.88037213862385), (37.73394330422971, 55.877941504088696), (37.745339592590376, 55.87208120378722), (37.75525267724611, 55.86703807949492), (37.76919976190188, 55.859821640197474), (37.827835219574, 55.82962968399116), (37.83341438888553, 55.82575289922351), (37.83652584655761, 55.82188784027888), (37.83809213491821, 55.81612575504693), (37.83605359521481, 55.81460347077685), (37.83632178569025, 55.81276696067908), (37.838623105812026, 55.811486181656385), (37.83912198147584, 55.807329380532785), (37.839079078033414, 55.80510270463816), (37.83965844708251, 55.79940712529036), (37.840581150787344, 55.79131399999368), (37.84172564285271, 55.78000432402266)]);  Check how many cell towers are in Moscow: SELECT count() FROM cell_towers WHERE pointInPolygon((lon, lat), (SELECT * FROM moscow)) ┌─count()─┐ │ 310463 │ └─────────┘ 1 rows in set. Elapsed: 0.067 sec. Processed 43.28 million rows, 692.42 MB (645.83 million rows/s., 10.33 GB/s.)  The data is also available for interactive queries in the Playground, example. Although you cannot create temporary tables there. "},{"title":"ClickHouse Playground","type":0,"sectionRef":"#","url":"en/getting-started/playground","content":"","keywords":"clickhouse playground getting started docs"},{"title":"Credentials​","type":1,"pageTitle":"ClickHouse Playground","url":"en/getting-started/playground#credentials","content":"Parameter\tValueHTTPS endpoint\thttps://play.clickhouse.com:443/ Native TCP endpoint\tplay.clickhouse.com:9440 User\texplorer or play Password\t(empty) "},{"title":"Limitations​","type":1,"pageTitle":"ClickHouse Playground","url":"en/getting-started/playground#limitations","content":"The queries are executed as a read-only user. It implies some limitations: DDL queries are not allowedINSERT queries are not allowed The service also have quotas on its usage. "},{"title":"Examples​","type":1,"pageTitle":"ClickHouse Playground","url":"en/getting-started/playground#examples","content":"HTTPS endpoint example with curl: curl &quot;https://play.clickhouse.com/?user=explorer&quot; --data-binary &quot;SELECT 'Play ClickHouse'&quot;  TCP endpoint example with CLI: clickhouse client --secure --host play.clickhouse.com --user explorer  "},{"title":"Anonymized Web Analytics Data","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/metrica","content":"","keywords":""},{"title":"Obtaining Tables from Prepared Partitions​","type":1,"pageTitle":"Anonymized Web Analytics Data","url":"en/getting-started/example-datasets/metrica#obtaining-tables-from-prepared-partitions","content":"Download and import hits table: curl -O https://datasets.clickhouse.com/hits/partitions/hits_v1.tar tar xvf hits_v1.tar -C /var/lib/clickhouse # path to ClickHouse data directory # check permissions on unpacked data, fix if required sudo service clickhouse-server restart clickhouse-client --query &quot;SELECT COUNT(*) FROM datasets.hits_v1&quot;  Download and import visits: curl -O https://datasets.clickhouse.com/visits/partitions/visits_v1.tar tar xvf visits_v1.tar -C /var/lib/clickhouse # path to ClickHouse data directory # check permissions on unpacked data, fix if required sudo service clickhouse-server restart clickhouse-client --query &quot;SELECT COUNT(*) FROM datasets.visits_v1&quot;  "},{"title":"Obtaining Tables from Compressed TSV File​","type":1,"pageTitle":"Anonymized Web Analytics Data","url":"en/getting-started/example-datasets/metrica#obtaining-tables-from-compressed-tsv-file","content":"Download and import hits from compressed TSV file: curl https://datasets.clickhouse.com/hits/tsv/hits_v1.tsv.xz | unxz --threads=`nproc` &gt; hits_v1.tsv # Validate the checksum md5sum hits_v1.tsv # Checksum should be equal to: f3631b6295bf06989c1437491f7592cb # now create table clickhouse-client --query &quot;CREATE DATABASE IF NOT EXISTS datasets&quot; # for hits_v1 clickhouse-client --query &quot;CREATE TABLE datasets.hits_v1 ( WatchID UInt64, JavaEnable UInt8, Title String, GoodEvent Int16, EventTime DateTime, EventDate Date, CounterID UInt32, ClientIP UInt32, ClientIP6 FixedString(16), RegionID UInt32, UserID UInt64, CounterClass Int8, OS UInt8, UserAgent UInt8, URL String, Referer String, URLDomain String, RefererDomain String, Refresh UInt8, IsRobot UInt8, RefererCategories Array(UInt16), URLCategories Array(UInt16), URLRegions Array(UInt32), RefererRegions Array(UInt32), ResolutionWidth UInt16, ResolutionHeight UInt16, ResolutionDepth UInt8, FlashMajor UInt8, FlashMinor UInt8, FlashMinor2 String, NetMajor UInt8, NetMinor UInt8, UserAgentMajor UInt16, UserAgentMinor FixedString(2), CookieEnable UInt8, JavascriptEnable UInt8, IsMobile UInt8, MobilePhone UInt8, MobilePhoneModel String, Params String, IPNetworkID UInt32, TraficSourceID Int8, SearchEngineID UInt16, SearchPhrase String, AdvEngineID UInt8, IsArtifical UInt8, WindowClientWidth UInt16, WindowClientHeight UInt16, ClientTimeZone Int16, ClientEventTime DateTime, SilverlightVersion1 UInt8, SilverlightVersion2 UInt8, SilverlightVersion3 UInt32, SilverlightVersion4 UInt16, PageCharset String, CodeVersion UInt32, IsLink UInt8, IsDownload UInt8, IsNotBounce UInt8, FUniqID UInt64, HID UInt32, IsOldCounter UInt8, IsEvent UInt8, IsParameter UInt8, DontCountHits UInt8, WithHash UInt8, HitColor FixedString(1), UTCEventTime DateTime, Age UInt8, Sex UInt8, Income UInt8, Interests UInt16, Robotness UInt8, GeneralInterests Array(UInt16), RemoteIP UInt32, RemoteIP6 FixedString(16), WindowName Int32, OpenerName Int32, HistoryLength Int16, BrowserLanguage FixedString(2), BrowserCountry FixedString(2), SocialNetwork String, SocialAction String, HTTPError UInt16, SendTiming Int32, DNSTiming Int32, ConnectTiming Int32, ResponseStartTiming Int32, ResponseEndTiming Int32, FetchTiming Int32, RedirectTiming Int32, DOMInteractiveTiming Int32, DOMContentLoadedTiming Int32, DOMCompleteTiming Int32, LoadEventStartTiming Int32, LoadEventEndTiming Int32, NSToDOMContentLoadedTiming Int32, FirstPaintTiming Int32, RedirectCount Int8, SocialSourceNetworkID UInt8, SocialSourcePage String, ParamPrice Int64, ParamOrderID String, ParamCurrency FixedString(3), ParamCurrencyID UInt16, GoalsReached Array(UInt32), OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID UInt8, RefererHash UInt64, URLHash UInt64, CLID UInt32, YCLID UInt64, ShareService String, ShareURL String, ShareTitle String, ParsedParams Nested(Key1 String, Key2 String, Key3 String, Key4 String, Key5 String, ValueDouble Float64), IslandID FixedString(16), RequestNum UInt32, RequestTry UInt8) ENGINE = MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID) SETTINGS index_granularity = 8192&quot; # for hits_100m_obfuscated clickhouse-client --query=&quot;CREATE TABLE default.hits_100m_obfuscated (WatchID UInt64, JavaEnable UInt8, Title String, GoodEvent Int16, EventTime DateTime, EventDate Date, CounterID UInt32, ClientIP UInt32, RegionID UInt32, UserID UInt64, CounterClass Int8, OS UInt8, UserAgent UInt8, URL String, Referer String, Refresh UInt8, RefererCategoryID UInt16, RefererRegionID UInt32, URLCategoryID UInt16, URLRegionID UInt32, ResolutionWidth UInt16, ResolutionHeight UInt16, ResolutionDepth UInt8, FlashMajor UInt8, FlashMinor UInt8, FlashMinor2 String, NetMajor UInt8, NetMinor UInt8, UserAgentMajor UInt16, UserAgentMinor FixedString(2), CookieEnable UInt8, JavascriptEnable UInt8, IsMobile UInt8, MobilePhone UInt8, MobilePhoneModel String, Params String, IPNetworkID UInt32, TraficSourceID Int8, SearchEngineID UInt16, SearchPhrase String, AdvEngineID UInt8, IsArtifical UInt8, WindowClientWidth UInt16, WindowClientHeight UInt16, ClientTimeZone Int16, ClientEventTime DateTime, SilverlightVersion1 UInt8, SilverlightVersion2 UInt8, SilverlightVersion3 UInt32, SilverlightVersion4 UInt16, PageCharset String, CodeVersion UInt32, IsLink UInt8, IsDownload UInt8, IsNotBounce UInt8, FUniqID UInt64, OriginalURL String, HID UInt32, IsOldCounter UInt8, IsEvent UInt8, IsParameter UInt8, DontCountHits UInt8, WithHash UInt8, HitColor FixedString(1), LocalEventTime DateTime, Age UInt8, Sex UInt8, Income UInt8, Interests UInt16, Robotness UInt8, RemoteIP UInt32, WindowName Int32, OpenerName Int32, HistoryLength Int16, BrowserLanguage FixedString(2), BrowserCountry FixedString(2), SocialNetwork String, SocialAction String, HTTPError UInt16, SendTiming UInt32, DNSTiming UInt32, ConnectTiming UInt32, ResponseStartTiming UInt32, ResponseEndTiming UInt32, FetchTiming UInt32, SocialSourceNetworkID UInt8, SocialSourcePage String, ParamPrice Int64, ParamOrderID String, ParamCurrency FixedString(3), ParamCurrencyID UInt16, OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID UInt8, RefererHash UInt64, URLHash UInt64, CLID UInt32) ENGINE = MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID) SETTINGS index_granularity = 8192&quot; # import data cat hits_v1.tsv | clickhouse-client --query &quot;INSERT INTO datasets.hits_v1 FORMAT TSV&quot; --max_insert_block_size=100000 # optionally you can optimize table clickhouse-client --query &quot;OPTIMIZE TABLE datasets.hits_v1 FINAL&quot; clickhouse-client --query &quot;SELECT COUNT(*) FROM datasets.hits_v1&quot;  Download and import visits from compressed tsv-file: curl https://datasets.clickhouse.com/visits/tsv/visits_v1.tsv.xz | unxz --threads=`nproc` &gt; visits_v1.tsv # Validate the checksum md5sum visits_v1.tsv # Checksum should be equal to: 6dafe1a0f24e59e3fc2d0fed85601de6 # now create table clickhouse-client --query &quot;CREATE DATABASE IF NOT EXISTS datasets&quot; clickhouse-client --query &quot;CREATE TABLE datasets.visits_v1 ( CounterID UInt32, StartDate Date, Sign Int8, IsNew UInt8, VisitID UInt64, UserID UInt64, StartTime DateTime, Duration UInt32, UTCStartTime DateTime, PageViews Int32, Hits Int32, IsBounce UInt8, Referer String, StartURL String, RefererDomain String, StartURLDomain String, EndURL String, LinkURL String, IsDownload UInt8, TraficSourceID Int8, SearchEngineID UInt16, SearchPhrase String, AdvEngineID UInt8, PlaceID Int32, RefererCategories Array(UInt16), URLCategories Array(UInt16), URLRegions Array(UInt32), RefererRegions Array(UInt32), IsYandex UInt8, GoalReachesDepth Int32, GoalReachesURL Int32, GoalReachesAny Int32, SocialSourceNetworkID UInt8, SocialSourcePage String, MobilePhoneModel String, ClientEventTime DateTime, RegionID UInt32, ClientIP UInt32, ClientIP6 FixedString(16), RemoteIP UInt32, RemoteIP6 FixedString(16), IPNetworkID UInt32, SilverlightVersion3 UInt32, CodeVersion UInt32, ResolutionWidth UInt16, ResolutionHeight UInt16, UserAgentMajor UInt16, UserAgentMinor UInt16, WindowClientWidth UInt16, WindowClientHeight UInt16, SilverlightVersion2 UInt8, SilverlightVersion4 UInt16, FlashVersion3 UInt16, FlashVersion4 UInt16, ClientTimeZone Int16, OS UInt8, UserAgent UInt8, ResolutionDepth UInt8, FlashMajor UInt8, FlashMinor UInt8, NetMajor UInt8, NetMinor UInt8, MobilePhone UInt8, SilverlightVersion1 UInt8, Age UInt8, Sex UInt8, Income UInt8, JavaEnable UInt8, CookieEnable UInt8, JavascriptEnable UInt8, IsMobile UInt8, BrowserLanguage UInt16, BrowserCountry UInt16, Interests UInt16, Robotness UInt8, GeneralInterests Array(UInt16), Params Array(String), Goals Nested(ID UInt32, Serial UInt32, EventTime DateTime, Price Int64, OrderID String, CurrencyID UInt32), WatchIDs Array(UInt64), ParamSumPrice Int64, ParamCurrency FixedString(3), ParamCurrencyID UInt16, ClickLogID UInt64, ClickEventID Int32, ClickGoodEvent Int32, ClickEventTime DateTime, ClickPriorityID Int32, ClickPhraseID Int32, ClickPageID Int32, ClickPlaceID Int32, ClickTypeID Int32, ClickResourceID Int32, ClickCost UInt32, ClickClientIP UInt32, ClickDomainID UInt32, ClickURL String, ClickAttempt UInt8, ClickOrderID UInt32, ClickBannerID UInt32, ClickMarketCategoryID UInt32, ClickMarketPP UInt32, ClickMarketCategoryName String, ClickMarketPPName String, ClickAWAPSCampaignName String, ClickPageName String, ClickTargetType UInt16, ClickTargetPhraseID UInt64, ClickContextType UInt8, ClickSelectType Int8, ClickOptions String, ClickGroupBannerID Int32, OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID UInt8, FirstVisit DateTime, PredLastVisit Date, LastVisit Date, TotalVisits UInt32, TraficSource Nested(ID Int8, SearchEngineID UInt16, AdvEngineID UInt8, PlaceID UInt16, SocialSourceNetworkID UInt8, Domain String, SearchPhrase String, SocialSourcePage String), Attendance FixedString(16), CLID UInt32, YCLID UInt64, NormalizedRefererHash UInt64, SearchPhraseHash UInt64, RefererDomainHash UInt64, NormalizedStartURLHash UInt64, StartURLDomainHash UInt64, NormalizedEndURLHash UInt64, TopLevelDomain UInt64, URLScheme UInt64, OpenstatServiceNameHash UInt64, OpenstatCampaignIDHash UInt64, OpenstatAdIDHash UInt64, OpenstatSourceIDHash UInt64, UTMSourceHash UInt64, UTMMediumHash UInt64, UTMCampaignHash UInt64, UTMContentHash UInt64, UTMTermHash UInt64, FromHash UInt64, WebVisorEnabled UInt8, WebVisorActivity UInt32, ParsedParams Nested(Key1 String, Key2 String, Key3 String, Key4 String, Key5 String, ValueDouble Float64), Market Nested(Type UInt8, GoalID UInt32, OrderID String, OrderPrice Int64, PP UInt32, DirectPlaceID UInt32, DirectOrderID UInt32, DirectBannerID UInt32, GoodID String, GoodName String, GoodQuantity Int32, GoodPrice Int64), IslandID FixedString(16)) ENGINE = CollapsingMergeTree(Sign) PARTITION BY toYYYYMM(StartDate) ORDER BY (CounterID, StartDate, intHash32(UserID), VisitID) SAMPLE BY intHash32(UserID) SETTINGS index_granularity = 8192&quot; # import data cat visits_v1.tsv | clickhouse-client --query &quot;INSERT INTO datasets.visits_v1 FORMAT TSV&quot; --max_insert_block_size=100000 # optionally you can optimize table clickhouse-client --query &quot;OPTIMIZE TABLE datasets.visits_v1 FINAL&quot; clickhouse-client --query &quot;SELECT COUNT(*) FROM datasets.visits_v1&quot;  "},{"title":"Example Queries​","type":1,"pageTitle":"Anonymized Web Analytics Data","url":"en/getting-started/example-datasets/metrica#example-queries","content":"The ClickHouse tutorial is based on this web analytics dataset, and the recommended way to get started with this dataset is to go through the tutorial. Additional examples of queries to these tables can be found among stateful tests of ClickHouse (they are named test.hits and test.visits there). "},{"title":"WikiStat","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/wikistat","content":"WikiStat See http://dumps.wikimedia.org/other/pagecounts-raw/ for details. Creating a table: CREATE TABLE wikistat ( date Date, time DateTime, project String, subproject String, path String, hits UInt64, size UInt64 ) ENGINE = MergeTree(date, (path, time), 8192); Loading data: $ for i in {2007..2016}; do for j in {01..12}; do echo $i-$j &gt;&amp;2; curl -sSL &quot;http://dumps.wikimedia.org/other/pagecounts-raw/$i/$i-$j/&quot; | grep -oE 'pagecounts-[0-9]+-[0-9]+\\.gz'; done; done | sort | uniq | tee links.txt $ cat links.txt | while read link; do wget http://dumps.wikimedia.org/other/pagecounts-raw/$(echo $link | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})[0-9]{2}-[0-9]+\\.gz/\\1/')/$(echo $link | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})[0-9]{2}-[0-9]+\\.gz/\\1-\\2/')/$link; done $ ls -1 /opt/wikistat/ | grep gz | while read i; do echo $i; gzip -cd /opt/wikistat/$i | ./wikistat-loader --time=&quot;$(echo -n $i | sed -r 's/pagecounts-([0-9]{4})([0-9]{2})([0-9]{2})-([0-9]{2})([0-9]{2})([0-9]{2})\\.gz/\\1-\\2-\\3 \\4-00-00/')&quot; | clickhouse-client --query=&quot;INSERT INTO wikistat FORMAT TabSeparated&quot;; done Original article","keywords":""},{"title":"AMPLab Big Data Benchmark","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/amplab-benchmark","content":"AMPLab Big Data Benchmark See https://amplab.cs.berkeley.edu/benchmark/ Sign up for a free account at https://aws.amazon.com. It requires a credit card, email, and phone number. Get a new access key at https://console.aws.amazon.com/iam/home?nc2=h_m_sc#security_credential Run the following in the console: $ sudo apt-get install s3cmd $ mkdir tiny; cd tiny; $ s3cmd sync s3://big-data-benchmark/pavlo/text-deflate/tiny/ . $ cd .. $ mkdir 1node; cd 1node; $ s3cmd sync s3://big-data-benchmark/pavlo/text-deflate/1node/ . $ cd .. $ mkdir 5nodes; cd 5nodes; $ s3cmd sync s3://big-data-benchmark/pavlo/text-deflate/5nodes/ . $ cd .. Run the following ClickHouse queries: CREATE TABLE rankings_tiny ( pageURL String, pageRank UInt32, avgDuration UInt32 ) ENGINE = Log; CREATE TABLE uservisits_tiny ( sourceIP String, destinationURL String, visitDate Date, adRevenue Float32, UserAgent String, cCode FixedString(3), lCode FixedString(6), searchWord String, duration UInt32 ) ENGINE = MergeTree(visitDate, visitDate, 8192); CREATE TABLE rankings_1node ( pageURL String, pageRank UInt32, avgDuration UInt32 ) ENGINE = Log; CREATE TABLE uservisits_1node ( sourceIP String, destinationURL String, visitDate Date, adRevenue Float32, UserAgent String, cCode FixedString(3), lCode FixedString(6), searchWord String, duration UInt32 ) ENGINE = MergeTree(visitDate, visitDate, 8192); CREATE TABLE rankings_5nodes_on_single ( pageURL String, pageRank UInt32, avgDuration UInt32 ) ENGINE = Log; CREATE TABLE uservisits_5nodes_on_single ( sourceIP String, destinationURL String, visitDate Date, adRevenue Float32, UserAgent String, cCode FixedString(3), lCode FixedString(6), searchWord String, duration UInt32 ) ENGINE = MergeTree(visitDate, visitDate, 8192); Go back to the console: $ for i in tiny/rankings/*.deflate; do echo $i; zlib-flate -uncompress &lt; $i | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO rankings_tiny FORMAT CSV&quot;; done $ for i in tiny/uservisits/*.deflate; do echo $i; zlib-flate -uncompress &lt; $i | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO uservisits_tiny FORMAT CSV&quot;; done $ for i in 1node/rankings/*.deflate; do echo $i; zlib-flate -uncompress &lt; $i | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO rankings_1node FORMAT CSV&quot;; done $ for i in 1node/uservisits/*.deflate; do echo $i; zlib-flate -uncompress &lt; $i | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO uservisits_1node FORMAT CSV&quot;; done $ for i in 5nodes/rankings/*.deflate; do echo $i; zlib-flate -uncompress &lt; $i | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO rankings_5nodes_on_single FORMAT CSV&quot;; done $ for i in 5nodes/uservisits/*.deflate; do echo $i; zlib-flate -uncompress &lt; $i | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO uservisits_5nodes_on_single FORMAT CSV&quot;; done Queries for obtaining data samples: SELECT pageURL, pageRank FROM rankings_1node WHERE pageRank &gt; 1000 SELECT substring(sourceIP, 1, 8), sum(adRevenue) FROM uservisits_1node GROUP BY substring(sourceIP, 1, 8) SELECT sourceIP, sum(adRevenue) AS totalRevenue, avg(pageRank) AS pageRank FROM rankings_1node ALL INNER JOIN ( SELECT sourceIP, destinationURL AS pageURL, adRevenue FROM uservisits_1node WHERE (visitDate &gt; '1980-01-01') AND (visitDate &lt; '1980-04-01') ) USING pageURL GROUP BY sourceIP ORDER BY totalRevenue DESC LIMIT 1 Original article","keywords":""},{"title":"Brown University Benchmark","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/brown-benchmark","content":"Brown University Benchmark MgBench is a new analytical benchmark for machine-generated log data, Andrew Crotty. Download the data: wget https://datasets.clickhouse.com/mgbench{1..3}.csv.xz Unpack the data: xz -v -d mgbench{1..3}.csv.xz Create tables: CREATE DATABASE mgbench; CREATE TABLE mgbench.logs1 ( log_time DateTime, machine_name LowCardinality(String), machine_group LowCardinality(String), cpu_idle Nullable(Float32), cpu_nice Nullable(Float32), cpu_system Nullable(Float32), cpu_user Nullable(Float32), cpu_wio Nullable(Float32), disk_free Nullable(Float32), disk_total Nullable(Float32), part_max_used Nullable(Float32), load_fifteen Nullable(Float32), load_five Nullable(Float32), load_one Nullable(Float32), mem_buffers Nullable(Float32), mem_cached Nullable(Float32), mem_free Nullable(Float32), mem_shared Nullable(Float32), swap_free Nullable(Float32), bytes_in Nullable(Float32), bytes_out Nullable(Float32) ) ENGINE = MergeTree() ORDER BY (machine_group, machine_name, log_time); CREATE TABLE mgbench.logs2 ( log_time DateTime, client_ip IPv4, request String, status_code UInt16, object_size UInt64 ) ENGINE = MergeTree() ORDER BY log_time; CREATE TABLE mgbench.logs3 ( log_time DateTime64, device_id FixedString(15), device_name LowCardinality(String), device_type LowCardinality(String), device_floor UInt8, event_type LowCardinality(String), event_unit FixedString(1), event_value Nullable(Float32) ) ENGINE = MergeTree() ORDER BY (event_type, log_time); Insert data: clickhouse-client --query &quot;INSERT INTO mgbench.logs1 FORMAT CSVWithNames&quot; &lt; mgbench1.csv clickhouse-client --query &quot;INSERT INTO mgbench.logs2 FORMAT CSVWithNames&quot; &lt; mgbench2.csv clickhouse-client --query &quot;INSERT INTO mgbench.logs3 FORMAT CSVWithNames&quot; &lt; mgbench3.csv Run benchmark queries: -- Q1.1: What is the CPU/network utilization for each web server since midnight? SELECT machine_name, MIN(cpu) AS cpu_min, MAX(cpu) AS cpu_max, AVG(cpu) AS cpu_avg, MIN(net_in) AS net_in_min, MAX(net_in) AS net_in_max, AVG(net_in) AS net_in_avg, MIN(net_out) AS net_out_min, MAX(net_out) AS net_out_max, AVG(net_out) AS net_out_avg FROM ( SELECT machine_name, COALESCE(cpu_user, 0.0) AS cpu, COALESCE(bytes_in, 0.0) AS net_in, COALESCE(bytes_out, 0.0) AS net_out FROM logs1 WHERE machine_name IN ('anansi','aragog','urd') AND log_time &gt;= TIMESTAMP '2017-01-11 00:00:00' ) AS r GROUP BY machine_name; -- Q1.2: Which computer lab machines have been offline in the past day? SELECT machine_name, log_time FROM logs1 WHERE (machine_name LIKE 'cslab%' OR machine_name LIKE 'mslab%') AND load_one IS NULL AND log_time &gt;= TIMESTAMP '2017-01-10 00:00:00' ORDER BY machine_name, log_time; -- Q1.3: What are the hourly average metrics during the past 10 days for a specific workstation? SELECT dt, hr, AVG(load_fifteen) AS load_fifteen_avg, AVG(load_five) AS load_five_avg, AVG(load_one) AS load_one_avg, AVG(mem_free) AS mem_free_avg, AVG(swap_free) AS swap_free_avg FROM ( SELECT CAST(log_time AS DATE) AS dt, EXTRACT(HOUR FROM log_time) AS hr, load_fifteen, load_five, load_one, mem_free, swap_free FROM logs1 WHERE machine_name = 'babbage' AND load_fifteen IS NOT NULL AND load_five IS NOT NULL AND load_one IS NOT NULL AND mem_free IS NOT NULL AND swap_free IS NOT NULL AND log_time &gt;= TIMESTAMP '2017-01-01 00:00:00' ) AS r GROUP BY dt, hr ORDER BY dt, hr; -- Q1.4: Over 1 month, how often was each server blocked on disk I/O? SELECT machine_name, COUNT(*) AS spikes FROM logs1 WHERE machine_group = 'Servers' AND cpu_wio &gt; 0.99 AND log_time &gt;= TIMESTAMP '2016-12-01 00:00:00' AND log_time &lt; TIMESTAMP '2017-01-01 00:00:00' GROUP BY machine_name ORDER BY spikes DESC LIMIT 10; -- Q1.5: Which externally reachable VMs have run low on memory? SELECT machine_name, dt, MIN(mem_free) AS mem_free_min FROM ( SELECT machine_name, CAST(log_time AS DATE) AS dt, mem_free FROM logs1 WHERE machine_group = 'DMZ' AND mem_free IS NOT NULL ) AS r GROUP BY machine_name, dt HAVING MIN(mem_free) &lt; 10000 ORDER BY machine_name, dt; -- Q1.6: What is the total hourly network traffic across all file servers? SELECT dt, hr, SUM(net_in) AS net_in_sum, SUM(net_out) AS net_out_sum, SUM(net_in) + SUM(net_out) AS both_sum FROM ( SELECT CAST(log_time AS DATE) AS dt, EXTRACT(HOUR FROM log_time) AS hr, COALESCE(bytes_in, 0.0) / 1000000000.0 AS net_in, COALESCE(bytes_out, 0.0) / 1000000000.0 AS net_out FROM logs1 WHERE machine_name IN ('allsorts','andes','bigred','blackjack','bonbon', 'cadbury','chiclets','cotton','crows','dove','fireball','hearts','huey', 'lindt','milkduds','milkyway','mnm','necco','nerds','orbit','peeps', 'poprocks','razzles','runts','smarties','smuggler','spree','stride', 'tootsie','trident','wrigley','york') ) AS r GROUP BY dt, hr ORDER BY both_sum DESC LIMIT 10; -- Q2.1: Which requests have caused server errors within the past 2 weeks? SELECT * FROM logs2 WHERE status_code &gt;= 500 AND log_time &gt;= TIMESTAMP '2012-12-18 00:00:00' ORDER BY log_time; -- Q2.2: During a specific 2-week period, was the user password file leaked? SELECT * FROM logs2 WHERE status_code &gt;= 200 AND status_code &lt; 300 AND request LIKE '%/etc/passwd%' AND log_time &gt;= TIMESTAMP '2012-05-06 00:00:00' AND log_time &lt; TIMESTAMP '2012-05-20 00:00:00'; -- Q2.3: What was the average path depth for top-level requests in the past month? SELECT top_level, AVG(LENGTH(request) - LENGTH(REPLACE(request, '/', ''))) AS depth_avg FROM ( SELECT SUBSTRING(request FROM 1 FOR len) AS top_level, request FROM ( SELECT POSITION(SUBSTRING(request FROM 2), '/') AS len, request FROM logs2 WHERE status_code &gt;= 200 AND status_code &lt; 300 AND log_time &gt;= TIMESTAMP '2012-12-01 00:00:00' ) AS r WHERE len &gt; 0 ) AS s WHERE top_level IN ('/about','/courses','/degrees','/events', '/grad','/industry','/news','/people', '/publications','/research','/teaching','/ugrad') GROUP BY top_level ORDER BY top_level; -- Q2.4: During the last 3 months, which clients have made an excessive number of requests? SELECT client_ip, COUNT(*) AS num_requests FROM logs2 WHERE log_time &gt;= TIMESTAMP '2012-10-01 00:00:00' GROUP BY client_ip HAVING COUNT(*) &gt;= 100000 ORDER BY num_requests DESC; -- Q2.5: What are the daily unique visitors? SELECT dt, COUNT(DISTINCT client_ip) FROM ( SELECT CAST(log_time AS DATE) AS dt, client_ip FROM logs2 ) AS r GROUP BY dt ORDER BY dt; -- Q2.6: What are the average and maximum data transfer rates (Gbps)? SELECT AVG(transfer) / 125000000.0 AS transfer_avg, MAX(transfer) / 125000000.0 AS transfer_max FROM ( SELECT log_time, SUM(object_size) AS transfer FROM logs2 GROUP BY log_time ) AS r; -- Q3.1: Did the indoor temperature reach freezing over the weekend? SELECT * FROM logs3 WHERE event_type = 'temperature' AND event_value &lt;= 32.0 AND log_time &gt;= '2019-11-29 17:00:00.000'; -- Q3.4: Over the past 6 months, how frequently were each door opened? SELECT device_name, device_floor, COUNT(*) AS ct FROM logs3 WHERE event_type = 'door_open' AND log_time &gt;= '2019-06-01 00:00:00.000' GROUP BY device_name, device_floor ORDER BY ct DESC; -- Q3.5: Where in the building do large temperature variations occur in winter and summer? WITH temperature AS ( SELECT dt, device_name, device_type, device_floor FROM ( SELECT dt, hr, device_name, device_type, device_floor, AVG(event_value) AS temperature_hourly_avg FROM ( SELECT CAST(log_time AS DATE) AS dt, EXTRACT(HOUR FROM log_time) AS hr, device_name, device_type, device_floor, event_value FROM logs3 WHERE event_type = 'temperature' ) AS r GROUP BY dt, hr, device_name, device_type, device_floor ) AS s GROUP BY dt, device_name, device_type, device_floor HAVING MAX(temperature_hourly_avg) - MIN(temperature_hourly_avg) &gt;= 25.0 ) SELECT DISTINCT device_name, device_type, device_floor, 'WINTER' FROM temperature WHERE dt &gt;= DATE '2018-12-01' AND dt &lt; DATE '2019-03-01' UNION SELECT DISTINCT device_name, device_type, device_floor, 'SUMMER' FROM temperature WHERE dt &gt;= DATE '2019-06-01' AND dt &lt; DATE '2019-09-01'; -- Q3.6: For each device category, what are the monthly power consumption metrics? SELECT yr, mo, SUM(coffee_hourly_avg) AS coffee_monthly_sum, AVG(coffee_hourly_avg) AS coffee_monthly_avg, SUM(printer_hourly_avg) AS printer_monthly_sum, AVG(printer_hourly_avg) AS printer_monthly_avg, SUM(projector_hourly_avg) AS projector_monthly_sum, AVG(projector_hourly_avg) AS projector_monthly_avg, SUM(vending_hourly_avg) AS vending_monthly_sum, AVG(vending_hourly_avg) AS vending_monthly_avg FROM ( SELECT dt, yr, mo, hr, AVG(coffee) AS coffee_hourly_avg, AVG(printer) AS printer_hourly_avg, AVG(projector) AS projector_hourly_avg, AVG(vending) AS vending_hourly_avg FROM ( SELECT CAST(log_time AS DATE) AS dt, EXTRACT(YEAR FROM log_time) AS yr, EXTRACT(MONTH FROM log_time) AS mo, EXTRACT(HOUR FROM log_time) AS hr, CASE WHEN device_name LIKE 'coffee%' THEN event_value END AS coffee, CASE WHEN device_name LIKE 'printer%' THEN event_value END AS printer, CASE WHEN device_name LIKE 'projector%' THEN event_value END AS projector, CASE WHEN device_name LIKE 'vending%' THEN event_value END AS vending FROM logs3 WHERE device_type = 'meter' ) AS r GROUP BY dt, yr, mo, hr ) AS s GROUP BY yr, mo ORDER BY yr, mo; The data is also available for interactive queries in the Playground, example. Original article","keywords":""},{"title":"Terabyte of Click Logs from Criteo","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/criteo","content":"Terabyte of Click Logs from Criteo Download the data from http://labs.criteo.com/downloads/download-terabyte-click-logs/ Create a table to import the log to: CREATE TABLE criteo_log (date Date, clicked UInt8, int1 Int32, int2 Int32, int3 Int32, int4 Int32, int5 Int32, int6 Int32, int7 Int32, int8 Int32, int9 Int32, int10 Int32, int11 Int32, int12 Int32, int13 Int32, cat1 String, cat2 String, cat3 String, cat4 String, cat5 String, cat6 String, cat7 String, cat8 String, cat9 String, cat10 String, cat11 String, cat12 String, cat13 String, cat14 String, cat15 String, cat16 String, cat17 String, cat18 String, cat19 String, cat20 String, cat21 String, cat22 String, cat23 String, cat24 String, cat25 String, cat26 String) ENGINE = Log Download the data: $ for i in {00..23}; do echo $i; zcat datasets/criteo/day_${i#0}.gz | sed -r 's/^/2000-01-'${i/00/24}'\\t/' | clickhouse-client --host=example-perftest01j --query=&quot;INSERT INTO criteo_log FORMAT TabSeparated&quot;; done Create a table for the converted data: CREATE TABLE criteo ( date Date, clicked UInt8, int1 Int32, int2 Int32, int3 Int32, int4 Int32, int5 Int32, int6 Int32, int7 Int32, int8 Int32, int9 Int32, int10 Int32, int11 Int32, int12 Int32, int13 Int32, icat1 UInt32, icat2 UInt32, icat3 UInt32, icat4 UInt32, icat5 UInt32, icat6 UInt32, icat7 UInt32, icat8 UInt32, icat9 UInt32, icat10 UInt32, icat11 UInt32, icat12 UInt32, icat13 UInt32, icat14 UInt32, icat15 UInt32, icat16 UInt32, icat17 UInt32, icat18 UInt32, icat19 UInt32, icat20 UInt32, icat21 UInt32, icat22 UInt32, icat23 UInt32, icat24 UInt32, icat25 UInt32, icat26 UInt32 ) ENGINE = MergeTree(date, intHash32(icat1), (date, intHash32(icat1)), 8192) Transform data from the raw log and put it in the second table: INSERT INTO criteo SELECT date, clicked, int1, int2, int3, int4, int5, int6, int7, int8, int9, int10, int11, int12, int13, reinterpretAsUInt32(unhex(cat1)) AS icat1, reinterpretAsUInt32(unhex(cat2)) AS icat2, reinterpretAsUInt32(unhex(cat3)) AS icat3, reinterpretAsUInt32(unhex(cat4)) AS icat4, reinterpretAsUInt32(unhex(cat5)) AS icat5, reinterpretAsUInt32(unhex(cat6)) AS icat6, reinterpretAsUInt32(unhex(cat7)) AS icat7, reinterpretAsUInt32(unhex(cat8)) AS icat8, reinterpretAsUInt32(unhex(cat9)) AS icat9, reinterpretAsUInt32(unhex(cat10)) AS icat10, reinterpretAsUInt32(unhex(cat11)) AS icat11, reinterpretAsUInt32(unhex(cat12)) AS icat12, reinterpretAsUInt32(unhex(cat13)) AS icat13, reinterpretAsUInt32(unhex(cat14)) AS icat14, reinterpretAsUInt32(unhex(cat15)) AS icat15, reinterpretAsUInt32(unhex(cat16)) AS icat16, reinterpretAsUInt32(unhex(cat17)) AS icat17, reinterpretAsUInt32(unhex(cat18)) AS icat18, reinterpretAsUInt32(unhex(cat19)) AS icat19, reinterpretAsUInt32(unhex(cat20)) AS icat20, reinterpretAsUInt32(unhex(cat21)) AS icat21, reinterpretAsUInt32(unhex(cat22)) AS icat22, reinterpretAsUInt32(unhex(cat23)) AS icat23, reinterpretAsUInt32(unhex(cat24)) AS icat24, reinterpretAsUInt32(unhex(cat25)) AS icat25, reinterpretAsUInt32(unhex(cat26)) AS icat26 FROM criteo_log; DROP TABLE criteo_log; Original article","keywords":""},{"title":"Recipes Dataset","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/recipes","content":"","keywords":""},{"title":"Download and Unpack the Dataset​","type":1,"pageTitle":"Recipes Dataset","url":"en/getting-started/example-datasets/recipes#download-and-unpack-the-dataset","content":"Go to the download page https://recipenlg.cs.put.poznan.pl/dataset.Accept Terms and Conditions and download zip file.Unpack the zip file with unzip. You will get the full_dataset.csv file. "},{"title":"Create a Table​","type":1,"pageTitle":"Recipes Dataset","url":"en/getting-started/example-datasets/recipes#create-a-table","content":"Run clickhouse-client and execute the following CREATE query: CREATE TABLE recipes ( title String, ingredients Array(String), directions Array(String), link String, source LowCardinality(String), NER Array(String) ) ENGINE = MergeTree ORDER BY title;  "},{"title":"Insert the Data​","type":1,"pageTitle":"Recipes Dataset","url":"en/getting-started/example-datasets/recipes#insert-the-data","content":"Run the following command: clickhouse-client --query &quot; INSERT INTO recipes SELECT title, JSONExtract(ingredients, 'Array(String)'), JSONExtract(directions, 'Array(String)'), link, source, JSONExtract(NER, 'Array(String)') FROM input('num UInt32, title String, ingredients String, directions String, link String, source LowCardinality(String), NER String') FORMAT CSVWithNames &quot; --input_format_with_names_use_header 0 --format_csv_allow_single_quote 0 --input_format_allow_errors_num 10 &lt; full_dataset.csv  This is a showcase how to parse custom CSV, as it requires multiple tunes. Explanation: The dataset is in CSV format, but it requires some preprocessing on insertion; we use table function input to perform preprocessing;The structure of CSV file is specified in the argument of the table function input;The field num (row number) is unneeded - we parse it from file and ignore;We use FORMAT CSVWithNames but the header in CSV will be ignored (by command line parameter --input_format_with_names_use_header 0), because the header does not contain the name for the first field;File is using only double quotes to enclose CSV strings; some strings are not enclosed in double quotes, and single quote must not be parsed as the string enclosing - that's why we also add the --format_csv_allow_single_quote 0 parameter;Some strings from CSV cannot parse, because they contain \\M/ sequence at the beginning of the value; the only value starting with backslash in CSV can be \\N that is parsed as SQL NULL. We add --input_format_allow_errors_num 10 parameter and up to ten malformed records can be skipped;There are arrays for ingredients, directions and NER fields; these arrays are represented in unusual form: they are serialized into string as JSON and then placed in CSV - we parse them as String and then use JSONExtract function to transform it to Array. "},{"title":"Validate the Inserted Data​","type":1,"pageTitle":"Recipes Dataset","url":"en/getting-started/example-datasets/recipes#validate-the-inserted-data","content":"By checking the row count: Query: SELECT count() FROM recipes;  Result: ┌─count()─┐ │ 2231141 │ └─────────┘  "},{"title":"Example Queries​","type":1,"pageTitle":"Recipes Dataset","url":"en/getting-started/example-datasets/recipes#example-queries","content":""},{"title":"Top Components by the Number of Recipes:​","type":1,"pageTitle":"Recipes Dataset","url":"en/getting-started/example-datasets/recipes#top-components-by-the-number-of-recipes","content":"In this example we learn how to use arrayJoin function to expand an array into a set of rows. Query: SELECT arrayJoin(NER) AS k, count() AS c FROM recipes GROUP BY k ORDER BY c DESC LIMIT 50  Result: ┌─k────────────────────┬──────c─┐ │ salt │ 890741 │ │ sugar │ 620027 │ │ butter │ 493823 │ │ flour │ 466110 │ │ eggs │ 401276 │ │ onion │ 372469 │ │ garlic │ 358364 │ │ milk │ 346769 │ │ water │ 326092 │ │ vanilla │ 270381 │ │ olive oil │ 197877 │ │ pepper │ 179305 │ │ brown sugar │ 174447 │ │ tomatoes │ 163933 │ │ egg │ 160507 │ │ baking powder │ 148277 │ │ lemon juice │ 146414 │ │ Salt │ 122557 │ │ cinnamon │ 117927 │ │ sour cream │ 116682 │ │ cream cheese │ 114423 │ │ margarine │ 112742 │ │ celery │ 112676 │ │ baking soda │ 110690 │ │ parsley │ 102151 │ │ chicken │ 101505 │ │ onions │ 98903 │ │ vegetable oil │ 91395 │ │ oil │ 85600 │ │ mayonnaise │ 84822 │ │ pecans │ 79741 │ │ nuts │ 78471 │ │ potatoes │ 75820 │ │ carrots │ 75458 │ │ pineapple │ 74345 │ │ soy sauce │ 70355 │ │ black pepper │ 69064 │ │ thyme │ 68429 │ │ mustard │ 65948 │ │ chicken broth │ 65112 │ │ bacon │ 64956 │ │ honey │ 64626 │ │ oregano │ 64077 │ │ ground beef │ 64068 │ │ unsalted butter │ 63848 │ │ mushrooms │ 61465 │ │ Worcestershire sauce │ 59328 │ │ cornstarch │ 58476 │ │ green pepper │ 58388 │ │ Cheddar cheese │ 58354 │ └──────────────────────┴────────┘ 50 rows in set. Elapsed: 0.112 sec. Processed 2.23 million rows, 361.57 MB (19.99 million rows/s., 3.24 GB/s.)  "},{"title":"The Most Complex Recipes with Strawberry​","type":1,"pageTitle":"Recipes Dataset","url":"en/getting-started/example-datasets/recipes#the-most-complex-recipes-with-strawberry","content":"SELECT title, length(NER), length(directions) FROM recipes WHERE has(NER, 'strawberry') ORDER BY length(directions) DESC LIMIT 10  Result: ┌─title────────────────────────────────────────────────────────────┬─length(NER)─┬─length(directions)─┐ │ Chocolate-Strawberry-Orange Wedding Cake │ 24 │ 126 │ │ Strawberry Cream Cheese Crumble Tart │ 19 │ 47 │ │ Charlotte-Style Ice Cream │ 11 │ 45 │ │ Sinfully Good a Million Layers Chocolate Layer Cake, With Strawb │ 31 │ 45 │ │ Sweetened Berries With Elderflower Sherbet │ 24 │ 44 │ │ Chocolate-Strawberry Mousse Cake │ 15 │ 42 │ │ Rhubarb Charlotte with Strawberries and Rum │ 20 │ 42 │ │ Chef Joey's Strawberry Vanilla Tart │ 7 │ 37 │ │ Old-Fashioned Ice Cream Sundae Cake │ 17 │ 37 │ │ Watermelon Cake │ 16 │ 36 │ └──────────────────────────────────────────────────────────────────┴─────────────┴────────────────────┘ 10 rows in set. Elapsed: 0.215 sec. Processed 2.23 million rows, 1.48 GB (10.35 million rows/s., 6.86 GB/s.)  In this example, we involve has function to filter by array elements and sort by the number of directions. There is a wedding cake that requires the whole 126 steps to produce! Show that directions: Query: SELECT arrayJoin(directions) FROM recipes WHERE title = 'Chocolate-Strawberry-Orange Wedding Cake'  Result: ┌─arrayJoin(directions)───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ Position 1 rack in center and 1 rack in bottom third of oven and preheat to 350F. │ │ Butter one 5-inch-diameter cake pan with 2-inch-high sides, one 8-inch-diameter cake pan with 2-inch-high sides and one 12-inch-diameter cake pan with 2-inch-high sides. │ │ Dust pans with flour; line bottoms with parchment. │ │ Combine 1/3 cup orange juice and 2 ounces unsweetened chocolate in heavy small saucepan. │ │ Stir mixture over medium-low heat until chocolate melts. │ │ Remove from heat. │ │ Gradually mix in 1 2/3 cups orange juice. │ │ Sift 3 cups flour, 2/3 cup cocoa, 2 teaspoons baking soda, 1 teaspoon salt and 1/2 teaspoon baking powder into medium bowl. │ │ using electric mixer, beat 1 cup (2 sticks) butter and 3 cups sugar in large bowl until blended (mixture will look grainy). │ │ Add 4 eggs, 1 at a time, beating to blend after each. │ │ Beat in 1 tablespoon orange peel and 1 tablespoon vanilla extract. │ │ Add dry ingredients alternately with orange juice mixture in 3 additions each, beating well after each addition. │ │ Mix in 1 cup chocolate chips. │ │ Transfer 1 cup plus 2 tablespoons batter to prepared 5-inch pan, 3 cups batter to prepared 8-inch pan and remaining batter (about 6 cups) to 12-inch pan. │ │ Place 5-inch and 8-inch pans on center rack of oven. │ │ Place 12-inch pan on lower rack of oven. │ │ Bake cakes until tester inserted into center comes out clean, about 35 minutes. │ │ Transfer cakes in pans to racks and cool completely. │ │ Mark 4-inch diameter circle on one 6-inch-diameter cardboard cake round. │ │ Cut out marked circle. │ │ Mark 7-inch-diameter circle on one 8-inch-diameter cardboard cake round. │ │ Cut out marked circle. │ │ Mark 11-inch-diameter circle on one 12-inch-diameter cardboard cake round. │ │ Cut out marked circle. │ │ Cut around sides of 5-inch-cake to loosen. │ │ Place 4-inch cardboard over pan. │ │ Hold cardboard and pan together; turn cake out onto cardboard. │ │ Peel off parchment.Wrap cakes on its cardboard in foil. │ │ Repeat turning out, peeling off parchment and wrapping cakes in foil, using 7-inch cardboard for 8-inch cake and 11-inch cardboard for 12-inch cake. │ │ Using remaining ingredients, make 1 more batch of cake batter and bake 3 more cake layers as described above. │ │ Cool cakes in pans. │ │ Cover cakes in pans tightly with foil. │ │ (Can be prepared ahead. │ │ Let stand at room temperature up to 1 day or double-wrap all cake layers and freeze up to 1 week. │ │ Bring cake layers to room temperature before using.) │ │ Place first 12-inch cake on its cardboard on work surface. │ │ Spread 2 3/4 cups ganache over top of cake and all the way to edge. │ │ Spread 2/3 cup jam over ganache, leaving 1/2-inch chocolate border at edge. │ │ Drop 1 3/4 cups white chocolate frosting by spoonfuls over jam. │ │ Gently spread frosting over jam, leaving 1/2-inch chocolate border at edge. │ │ Rub some cocoa powder over second 12-inch cardboard. │ │ Cut around sides of second 12-inch cake to loosen. │ │ Place cardboard, cocoa side down, over pan. │ │ Turn cake out onto cardboard. │ │ Peel off parchment. │ │ Carefully slide cake off cardboard and onto filling on first 12-inch cake. │ │ Refrigerate. │ │ Place first 8-inch cake on its cardboard on work surface. │ │ Spread 1 cup ganache over top all the way to edge. │ │ Spread 1/4 cup jam over, leaving 1/2-inch chocolate border at edge. │ │ Drop 1 cup white chocolate frosting by spoonfuls over jam. │ │ Gently spread frosting over jam, leaving 1/2-inch chocolate border at edge. │ │ Rub some cocoa over second 8-inch cardboard. │ │ Cut around sides of second 8-inch cake to loosen. │ │ Place cardboard, cocoa side down, over pan. │ │ Turn cake out onto cardboard. │ │ Peel off parchment. │ │ Slide cake off cardboard and onto filling on first 8-inch cake. │ │ Refrigerate. │ │ Place first 5-inch cake on its cardboard on work surface. │ │ Spread 1/2 cup ganache over top of cake and all the way to edge. │ │ Spread 2 tablespoons jam over, leaving 1/2-inch chocolate border at edge. │ │ Drop 1/3 cup white chocolate frosting by spoonfuls over jam. │ │ Gently spread frosting over jam, leaving 1/2-inch chocolate border at edge. │ │ Rub cocoa over second 6-inch cardboard. │ │ Cut around sides of second 5-inch cake to loosen. │ │ Place cardboard, cocoa side down, over pan. │ │ Turn cake out onto cardboard. │ │ Peel off parchment. │ │ Slide cake off cardboard and onto filling on first 5-inch cake. │ │ Chill all cakes 1 hour to set filling. │ │ Place 12-inch tiered cake on its cardboard on revolving cake stand. │ │ Spread 2 2/3 cups frosting over top and sides of cake as a first coat. │ │ Refrigerate cake. │ │ Place 8-inch tiered cake on its cardboard on cake stand. │ │ Spread 1 1/4 cups frosting over top and sides of cake as a first coat. │ │ Refrigerate cake. │ │ Place 5-inch tiered cake on its cardboard on cake stand. │ │ Spread 3/4 cup frosting over top and sides of cake as a first coat. │ │ Refrigerate all cakes until first coats of frosting set, about 1 hour. │ │ (Cakes can be made to this point up to 1 day ahead; cover and keep refrigerate.) │ │ Prepare second batch of frosting, using remaining frosting ingredients and following directions for first batch. │ │ Spoon 2 cups frosting into pastry bag fitted with small star tip. │ │ Place 12-inch cake on its cardboard on large flat platter. │ │ Place platter on cake stand. │ │ Using icing spatula, spread 2 1/2 cups frosting over top and sides of cake; smooth top. │ │ Using filled pastry bag, pipe decorative border around top edge of cake. │ │ Refrigerate cake on platter. │ │ Place 8-inch cake on its cardboard on cake stand. │ │ Using icing spatula, spread 1 1/2 cups frosting over top and sides of cake; smooth top. │ │ Using pastry bag, pipe decorative border around top edge of cake. │ │ Refrigerate cake on its cardboard. │ │ Place 5-inch cake on its cardboard on cake stand. │ │ Using icing spatula, spread 3/4 cup frosting over top and sides of cake; smooth top. │ │ Using pastry bag, pipe decorative border around top edge of cake, spooning more frosting into bag if necessary. │ │ Refrigerate cake on its cardboard. │ │ Keep all cakes refrigerated until frosting sets, about 2 hours. │ │ (Can be prepared 2 days ahead. │ │ Cover loosely; keep refrigerated.) │ │ Place 12-inch cake on platter on work surface. │ │ Press 1 wooden dowel straight down into and completely through center of cake. │ │ Mark dowel 1/4 inch above top of frosting. │ │ Remove dowel and cut with serrated knife at marked point. │ │ Cut 4 more dowels to same length. │ │ Press 1 cut dowel back into center of cake. │ │ Press remaining 4 cut dowels into cake, positioning 3 1/2 inches inward from cake edges and spacing evenly. │ │ Place 8-inch cake on its cardboard on work surface. │ │ Press 1 dowel straight down into and completely through center of cake. │ │ Mark dowel 1/4 inch above top of frosting. │ │ Remove dowel and cut with serrated knife at marked point. │ │ Cut 3 more dowels to same length. │ │ Press 1 cut dowel back into center of cake. │ │ Press remaining 3 cut dowels into cake, positioning 2 1/2 inches inward from edges and spacing evenly. │ │ Using large metal spatula as aid, place 8-inch cake on its cardboard atop dowels in 12-inch cake, centering carefully. │ │ Gently place 5-inch cake on its cardboard atop dowels in 8-inch cake, centering carefully. │ │ Using citrus stripper, cut long strips of orange peel from oranges. │ │ Cut strips into long segments. │ │ To make orange peel coils, wrap peel segment around handle of wooden spoon; gently slide peel off handle so that peel keeps coiled shape. │ │ Garnish cake with orange peel coils, ivy or mint sprigs, and some berries. │ │ (Assembled cake can be made up to 8 hours ahead. │ │ Let stand at cool room temperature.) │ │ Remove top and middle cake tiers. │ │ Remove dowels from cakes. │ │ Cut top and middle cakes into slices. │ │ To cut 12-inch cake: Starting 3 inches inward from edge and inserting knife straight down, cut through from top to bottom to make 6-inch-diameter circle in center of cake. │ │ Cut outer portion of cake into slices; cut inner portion into slices and serve with strawberries. │ └─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ 126 rows in set. Elapsed: 0.011 sec. Processed 8.19 thousand rows, 5.34 MB (737.75 thousand rows/s., 480.59 MB/s.)  "},{"title":"Online Playground​","type":1,"pageTitle":"Recipes Dataset","url":"en/getting-started/example-datasets/recipes#online-playground","content":"The dataset is also available in the Online Playground. Original article "},{"title":"Installation","type":0,"sectionRef":"#","url":"en/getting-started/install","content":"","keywords":"clickhouse install installation docs"},{"title":"System Requirements​","type":1,"pageTitle":"Installation","url":"en/getting-started/install#system-requirements","content":"ClickHouse can run on any Linux, FreeBSD, or Mac OS X with x86_64, AArch64, or PowerPC64LE CPU architecture. Official pre-built binaries are typically compiled for x86_64 and leverage SSE 4.2 instruction set, so unless otherwise stated usage of CPU that supports it becomes an additional system requirement. Here’s the command to check if current CPU has support for SSE 4.2: $ grep -q sse4_2 /proc/cpuinfo &amp;&amp; echo &quot;SSE 4.2 supported&quot; || echo &quot;SSE 4.2 not supported&quot;  To run ClickHouse on processors that do not support SSE 4.2 or have AArch64 or PowerPC64LE architecture, you should build ClickHouse from sources with proper configuration adjustments. "},{"title":"Available Installation Options​","type":1,"pageTitle":"Installation","url":"en/getting-started/install#available-installation-options","content":""},{"title":"From DEB Packages​","type":1,"pageTitle":"Installation","url":"en/getting-started/install#install-from-deb-packages","content":"It is recommended to use official pre-compiled deb packages for Debian or Ubuntu. Run these commands to install packages: sudo apt-get install -y apt-transport-https ca-certificates dirmngr sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 8919F6BD2B48D754 echo &quot;deb https://packages.clickhouse.com/deb stable main&quot; | sudo tee \\ /etc/apt/sources.list.d/clickhouse.list sudo apt-get update sudo apt-get install -y clickhouse-server clickhouse-client sudo service clickhouse-server start clickhouse-client # or &quot;clickhouse-client --password&quot; if you've set up a password.  Deprecated Method for installing deb-packages sudo apt-get install apt-transport-https ca-certificates dirmngr sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4 echo &quot;deb https://repo.clickhouse.com/deb/stable/ main/&quot; | sudo tee \\ /etc/apt/sources.list.d/clickhouse.list sudo apt-get update sudo apt-get install -y clickhouse-server clickhouse-client sudo service clickhouse-server start clickhouse-client # or &quot;clickhouse-client --password&quot; if you set up a password.  You can replace stable with lts or testing to use different release trains based on your needs. You can also download and install packages manually from here. Packages​ clickhouse-common-static — Installs ClickHouse compiled binary files.clickhouse-server — Creates a symbolic link for clickhouse-server and installs the default server configuration.clickhouse-client — Creates a symbolic link for clickhouse-client and other client-related tools. and installs client configuration files.clickhouse-common-static-dbg — Installs ClickHouse compiled binary files with debug info. info If you need to install specific version of ClickHouse you have to install all packages with the same version:sudo apt-get install clickhouse-server=21.8.5.7 clickhouse-client=21.8.5.7 clickhouse-common-static=21.8.5.7 "},{"title":"From RPM Packages​","type":1,"pageTitle":"Installation","url":"en/getting-started/install#from-rpm-packages","content":"It is recommended to use official pre-compiled rpm packages for CentOS, RedHat, and all other rpm-based Linux distributions. First, you need to add the official repository: sudo yum install -y yum-utils sudo yum-config-manager --add-repo https://packages.clickhouse.com/rpm/clickhouse.repo sudo yum install -y clickhouse-server clickhouse-client sudo /etc/init.d/clickhouse-server start clickhouse-client # or &quot;clickhouse-client --password&quot; if you set up a password.  Deprecated Method for installing rpm-packages sudo yum install yum-utils sudo rpm --import https://repo.clickhouse.com/CLICKHOUSE-KEY.GPG sudo yum-config-manager --add-repo https://repo.clickhouse.com/rpm/clickhouse.repo sudo yum install clickhouse-server clickhouse-client sudo /etc/init.d/clickhouse-server start clickhouse-client # or &quot;clickhouse-client --password&quot; if you set up a password.  If you want to use the most recent version, replace stable with testing (this is recommended for your testing environments). prestable is sometimes also available. Then run these commands to install packages: sudo yum install clickhouse-server clickhouse-client  You can also download and install packages manually from here. "},{"title":"From Tgz Archives​","type":1,"pageTitle":"Installation","url":"en/getting-started/install#from-tgz-archives","content":"It is recommended to use official pre-compiled tgz archives for all Linux distributions, where installation of deb or rpm packages is not possible. The required version can be downloaded with curl or wget from repository https://packages.clickhouse.com/tgz/. After that downloaded archives should be unpacked and installed with installation scripts. Example for the latest stable version: LATEST_VERSION=$(curl -s https://packages.clickhouse.com/tgz/stable/ | \\ grep -Eo '[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+' | sort -V -r | head -n 1) export LATEST_VERSION curl -O &quot;https://packages.clickhouse.com/tgz/stable/clickhouse-common-static-$LATEST_VERSION-amd64.tgz&quot; curl -O &quot;https://packages.clickhouse.com/tgz/stable/clickhouse-common-static-dbg-$LATEST_VERSION-amd64.tgz&quot; curl -O &quot;https://packages.clickhouse.com/tgz/stable/clickhouse-server-$LATEST_VERSION-amd64.tgz&quot; curl -O &quot;https://packages.clickhouse.com/tgz/stable/clickhouse-client-$LATEST_VERSION-amd64.tgz&quot; tar -xzvf &quot;clickhouse-common-static-$LATEST_VERSION-amd64.tgz&quot; sudo &quot;clickhouse-common-static-$LATEST_VERSION/install/doinst.sh&quot; tar -xzvf &quot;clickhouse-common-static-dbg-$LATEST_VERSION-amd64.tgz&quot; sudo &quot;clickhouse-common-static-dbg-$LATEST_VERSION/install/doinst.sh&quot; tar -xzvf &quot;clickhouse-server-$LATEST_VERSION-amd64.tgz&quot; sudo &quot;clickhouse-server-$LATEST_VERSION/install/doinst.sh&quot; sudo /etc/init.d/clickhouse-server start tar -xzvf &quot;clickhouse-client-$LATEST_VERSION-amd64.tgz&quot; sudo &quot;clickhouse-client-$LATEST_VERSION/install/doinst.sh&quot;  Deprecated Method for installing tgz archives export LATEST_VERSION=$(curl -s https://repo.clickhouse.com/tgz/stable/ | \\ grep -Eo '[0-9]+\\.[0-9]+\\.[0-9]+\\.[0-9]+' | sort -V -r | head -n 1) curl -O https://repo.clickhouse.com/tgz/stable/clickhouse-common-static-$LATEST_VERSION.tgz curl -O https://repo.clickhouse.com/tgz/stable/clickhouse-common-static-dbg-$LATEST_VERSION.tgz curl -O https://repo.clickhouse.com/tgz/stable/clickhouse-server-$LATEST_VERSION.tgz curl -O https://repo.clickhouse.com/tgz/stable/clickhouse-client-$LATEST_VERSION.tgz tar -xzvf clickhouse-common-static-$LATEST_VERSION.tgz sudo clickhouse-common-static-$LATEST_VERSION/install/doinst.sh tar -xzvf clickhouse-common-static-dbg-$LATEST_VERSION.tgz sudo clickhouse-common-static-dbg-$LATEST_VERSION/install/doinst.sh tar -xzvf clickhouse-server-$LATEST_VERSION.tgz sudo clickhouse-server-$LATEST_VERSION/install/doinst.sh sudo /etc/init.d/clickhouse-server start tar -xzvf clickhouse-client-$LATEST_VERSION.tgz sudo clickhouse-client-$LATEST_VERSION/install/doinst.sh  For production environments, it’s recommended to use the latest stable-version. You can find its number on GitHub page https://github.com/ClickHouse/ClickHouse/tags with postfix -stable. "},{"title":"From Docker Image​","type":1,"pageTitle":"Installation","url":"en/getting-started/install#from-docker-image","content":"To run ClickHouse inside Docker follow the guide on Docker Hub. Those images use official deb packages inside. "},{"title":"Single Binary​","type":1,"pageTitle":"Installation","url":"en/getting-started/install#from-single-binary","content":"You can install ClickHouse on Linux using a single portable binary from the latest commit of the master branch: [https://builds.clickhouse.com/master/amd64/clickhouse]. curl -O 'https://builds.clickhouse.com/master/amd64/clickhouse' &amp;&amp; chmod a+x clickhouse sudo ./clickhouse install  "},{"title":"From Precompiled Binaries for Non-Standard Environments​","type":1,"pageTitle":"Installation","url":"en/getting-started/install#from-binaries-non-linux","content":"For non-Linux operating systems and for AArch64 CPU arhitecture, ClickHouse builds are provided as a cross-compiled binary from the latest commit of the master branch (with a few hours delay). MacOS x86_64 curl -O 'https://builds.clickhouse.com/master/macos/clickhouse' &amp;&amp; chmod a+x ./clickhouse MacOS Aarch64 (Apple Silicon) curl -O 'https://builds.clickhouse.com/master/macos-aarch64/clickhouse' &amp;&amp; chmod a+x ./clickhouse FreeBSD x86_64 curl -O 'https://builds.clickhouse.com/master/freebsd/clickhouse' &amp;&amp; chmod a+x ./clickhouse Linux AArch64 curl -O 'https://builds.clickhouse.com/master/aarch64/clickhouse' &amp;&amp; chmod a+x ./clickhouse  Run sudo ./clickhouse install to install ClickHouse system-wide (also with needed configuration files, configuring users etc.). Then run clickhouse start commands to start the clickhouse-server and clickhouse-client to connect to it. Use the clickhouse client to connect to the server, or clickhouse local to process local data. "},{"title":"From Sources​","type":1,"pageTitle":"Installation","url":"en/getting-started/install#from-sources","content":"To manually compile ClickHouse, follow the instructions for Linux or Mac OS X. You can compile packages and install them or use programs without installing packages. Also by building manually you can disable SSE 4.2 requirement or build for AArch64 CPUs.  Client: programs/clickhouse-client Server: programs/clickhouse-server  You’ll need to create a data and metadata folders and chown them for the desired user. Their paths can be changed in server config (src/programs/server/config.xml), by default they are:  /var/lib/clickhouse/data/default/ /var/lib/clickhouse/metadata/default/  On Gentoo, you can just use emerge clickhouse to install ClickHouse from sources. "},{"title":"Launch​","type":1,"pageTitle":"Installation","url":"en/getting-started/install#launch","content":"To start the server as a daemon, run: $ sudo clickhouse start  There are also another ways to run ClickHouse: $ sudo service clickhouse-server start  If you do not have service command, run as $ sudo /etc/init.d/clickhouse-server start  If you have systemctl command, run as $ sudo systemctl start clickhouse-server.service  See the logs in the /var/log/clickhouse-server/ directory. If the server does not start, check the configurations in the file /etc/clickhouse-server/config.xml. You can also manually launch the server from the console: $ clickhouse-server --config-file=/etc/clickhouse-server/config.xml  In this case, the log will be printed to the console, which is convenient during development. If the configuration file is in the current directory, you do not need to specify the --config-file parameter. By default, it uses ./config.xml. ClickHouse supports access restriction settings. They are located in the users.xml file (next to config.xml). By default, access is allowed from anywhere for the default user, without a password. See user/default/networks. For more information, see the section “Configuration Files”. After launching server, you can use the command-line client to connect to it: $ clickhouse-client  By default, it connects to localhost:9000 on behalf of the user default without a password. It can also be used to connect to a remote server using --host argument. The terminal must use UTF-8 encoding. For more information, see the section “Command-line client”. Example: $ ./clickhouse-client ClickHouse client version 0.0.18749. Connecting to localhost:9000. Connected to ClickHouse server version 0.0.18749. :) SELECT 1 SELECT 1 ┌─1─┐ │ 1 │ └───┘ 1 rows in set. Elapsed: 0.003 sec. :)  Congratulations, the system works! To continue experimenting, you can download one of the test data sets or go through tutorial. Original article "},{"title":"Updating and Deleting ClickHouse Data","type":0,"sectionRef":"#","url":"en/guides/developer/mutations","content":"","keywords":"update delete mutation"},{"title":"Updating Existing Data​","type":1,"pageTitle":"Updating and Deleting ClickHouse Data","url":"en/guides/developer/mutations#updating-existing-data","content":"From ClickHouse client, enter your update ALTER TABLE command in this form: ALTER TABLE [&lt;database&gt;.]&lt;table&gt; UPDATE &lt;column&gt; = &lt;expression&gt; WHERE &lt;filter_expr&gt;  &lt;expression&gt; is the new value for the column where the &lt;filter_expr&gt; is satisfied. The &lt;expression&gt; must be the same datatype as the column or be convertable to the same datatype using the CAST operator. The &lt;filter_expr&gt; should return a UInt8 (zero or non-zero) value for each row of the data. Multiple UPDATE &lt;column&gt; statements can be combined in a single ALTER TABLE command separated by commas. Examples: A mutation like this allows updating replacing visitor_ids with new ones using a dictionary lookup: ALTER TABLE website.clicks UPDATE visitor_id = getDict('visitors', 'new_visitor_id', visitor_id) WHERE visit_date &lt; '2022-01-01' Modifying multiple values in one command can be more efficient than multiple commands: ALTER TABLE website.clicks UPDATE url = substring(url, position(url, '://') + 3), visitor_id = new_visit_id WHERE visit_date &lt; '2022-01-01' Mutations can be exectuted ON CLUSTER for sharded tables: ALTER TABLE clicks ON CLUSTER main_cluster UPDATE click_count = click_count / 2 WHERE visitor_id ILIKE '%robot%'  Note It is not possible to update columns that are part of the primary or sorting key. "},{"title":"Deleting Data​","type":1,"pageTitle":"Updating and Deleting ClickHouse Data","url":"en/guides/developer/mutations#deleting-data","content":"From ClickHouse client, enter your delete ALTER TABLE command in this form: ALTER TABLE [&lt;database&gt;.]&lt;table&gt; DELETE WHERE &lt;filter_expr&gt;  Again &lt;filter_expr&gt; should return a UInt8 value for each row of data. Examples Delete any records where a column is in an array of values: ALTER TABLE website.clicks DELETE WHERE visitor_id in (253, 1002, 4277) What does this query alter? ALTER TABLE clicks ON CLUSTER main_cluster WHERE visit_date &lt; '2022-01-02 15:00:00' AND page_id = '573'  Note To delete all of the data in a table, it is more efficient to use the command TRUNCATE TABLE [&lt;database].]&lt;table&gt; command. This command can also be executed ON CLUSTER. "},{"title":"Handling JSON","type":0,"sectionRef":"#","url":"en/guides/developer/working-with-json/json-intro","content":"Handling JSON JSON has established itself as one of the most popular language-independent data interchange formats. As a “semi-structured” data format, it balances user readability with greater space efficiency than alternatives such as XML. Although typically used as the data format for requests and responses in web APIs, it is increasingly used for logging and general-purpose dataset distribution. ClickHouse provides several approaches for handling JSON, each with its respective pros and cons and usage. More recent versions of ClickHouse have introduced new types which allow even greater flexibility and performance for JSON storage and querying. While these developments make older techniques less applicable, they can still be useful and are documented here for comprehension and those users on older versions. For example purposes, we utilize two datasets: a 1m row subset of the Github dataset and an example NGINX log in JSON format. The former includes nested columns, useful for example purposes. It is also deliberately sparse, which helps illustrate some challenges of JSON. The latter allows us to discuss standard techniques for JSON logs.","keywords":""},{"title":"Applying a Catboost Model in ClickHouse","type":0,"sectionRef":"#","url":"en/guides/developer/apply-catboost-model","content":"","keywords":""},{"title":"Prerequisites​","type":1,"pageTitle":"Applying a Catboost Model in ClickHouse","url":"en/guides/developer/apply-catboost-model#prerequisites","content":"If you do not have the Docker yet, install it. note Docker is a software platform that allows you to create containers that isolate a CatBoost and ClickHouse installation from the rest of the system. Before applying a CatBoost model: 1. Pull the Docker image from the registry: $ docker pull yandex/tutorial-catboost-clickhouse  This Docker image contains everything you need to run CatBoost and ClickHouse: code, runtime, libraries, environment variables, and configuration files. 2. Make sure the Docker image has been successfully pulled: $ docker image ls REPOSITORY TAG IMAGE ID CREATED SIZE yandex/tutorial-catboost-clickhouse latest 622e4d17945b 22 hours ago 1.37GB  3. Start a Docker container based on this image: $ docker run -it -p 8888:8888 yandex/tutorial-catboost-clickhouse  "},{"title":"1. Create a Table​","type":1,"pageTitle":"Applying a Catboost Model in ClickHouse","url":"en/guides/developer/apply-catboost-model#create-table","content":"To create a ClickHouse table for the training sample: 1. Start ClickHouse console client in the interactive mode: $ clickhouse client  note The ClickHouse server is already running inside the Docker container. 2. Create the table using the command: :) CREATE TABLE amazon_train ( date Date MATERIALIZED today(), ACTION UInt8, RESOURCE UInt32, MGR_ID UInt32, ROLE_ROLLUP_1 UInt32, ROLE_ROLLUP_2 UInt32, ROLE_DEPTNAME UInt32, ROLE_TITLE UInt32, ROLE_FAMILY_DESC UInt32, ROLE_FAMILY UInt32, ROLE_CODE UInt32 ) ENGINE = MergeTree ORDER BY date  3. Exit from ClickHouse console client: :) exit  "},{"title":"2. Insert the Data to the Table​","type":1,"pageTitle":"Applying a Catboost Model in ClickHouse","url":"en/guides/developer/apply-catboost-model#insert-data-to-table","content":"To insert the data: 1. Run the following command: $ clickhouse client --host 127.0.0.1 --query 'INSERT INTO amazon_train FORMAT CSVWithNames' &lt; ~/amazon/train.csv  2. Start ClickHouse console client in the interactive mode: $ clickhouse client  3. Make sure the data has been uploaded: :) SELECT count() FROM amazon_train SELECT count() FROM amazon_train +-count()-+ | 65538 | +-------+  "},{"title":"3. Integrate CatBoost into ClickHouse​","type":1,"pageTitle":"Applying a Catboost Model in ClickHouse","url":"en/guides/developer/apply-catboost-model#integrate-catboost-into-clickhouse","content":"note Optional step. The Docker image contains everything you need to run CatBoost and ClickHouse. To integrate CatBoost into ClickHouse: 1. Build the evaluation library. The fastest way to evaluate a CatBoost model is compile libcatboostmodel.&lt;so|dll|dylib&gt; library. For more information about how to build the library, see CatBoost documentation. 2. Create a new directory anywhere and with any name, for example, data and put the created library in it. The Docker image already contains the library data/libcatboostmodel.so. 3. Create a new directory for config model anywhere and with any name, for example, models. 4. Create a model configuration file with any name, for example, models/amazon_model.xml. 5. Describe the model configuration: &lt;models&gt; &lt;model&gt; &lt;!-- Model type. Now catboost only. --&gt; &lt;type&gt;catboost&lt;/type&gt; &lt;!-- Model name. --&gt; &lt;name&gt;amazon&lt;/name&gt; &lt;!-- Path to trained model. --&gt; &lt;path&gt;/home/catboost/tutorial/catboost_model.bin&lt;/path&gt; &lt;!-- Update interval. --&gt; &lt;lifetime&gt;0&lt;/lifetime&gt; &lt;/model&gt; &lt;/models&gt;  6. Add the path to CatBoost and the model configuration to the ClickHouse configuration: &lt;!-- File etc/clickhouse-server/config.d/models_config.xml. --&gt; &lt;catboost_dynamic_library_path&gt;/home/catboost/data/libcatboostmodel.so&lt;/catboost_dynamic_library_path&gt; &lt;models_config&gt;/home/catboost/models/*_model.xml&lt;/models_config&gt;  note You can change path to the CatBoost model configuration later without restarting server. "},{"title":"4. Run the Model Inference from SQL​","type":1,"pageTitle":"Applying a Catboost Model in ClickHouse","url":"en/guides/developer/apply-catboost-model#run-model-inference","content":"For test model run the ClickHouse client $ clickhouse client. Let’s make sure that the model is working: :) SELECT modelEvaluate('amazon', RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME, ROLE_TITLE, ROLE_FAMILY_DESC, ROLE_FAMILY, ROLE_CODE) &gt; 0 AS prediction, ACTION AS target FROM amazon_train LIMIT 10  note Function modelEvaluate returns tuple with per-class raw predictions for multiclass models. Let’s predict the probability: :) SELECT modelEvaluate('amazon', RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME, ROLE_TITLE, ROLE_FAMILY_DESC, ROLE_FAMILY, ROLE_CODE) AS prediction, 1. / (1 + exp(-prediction)) AS probability, ACTION AS target FROM amazon_train LIMIT 10  note More info about exp() function. Let’s calculate LogLoss on the sample: :) SELECT -avg(tg * log(prob) + (1 - tg) * log(1 - prob)) AS logloss FROM ( SELECT modelEvaluate('amazon', RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME, ROLE_TITLE, ROLE_FAMILY_DESC, ROLE_FAMILY, ROLE_CODE) AS prediction, 1. / (1. + exp(-prediction)) AS prob, ACTION AS tg FROM amazon_train )  note More info about avg() and log() functions. "},{"title":"Crowdsourced air traffic data from The OpenSky Network 2020","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/opensky","content":"","keywords":""},{"title":"Download the Dataset​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"en/getting-started/example-datasets/opensky#download-dataset","content":"Run the command: wget -O- https://zenodo.org/record/5092942 | grep -oP 'https://zenodo.org/record/5092942/files/flightlist_\\d+_\\d+\\.csv\\.gz' | xargs wget  Download will take about 2 minutes with good internet connection. There are 30 files with total size of 4.3 GB. "},{"title":"Create the Table​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"en/getting-started/example-datasets/opensky#create-table","content":"CREATE TABLE opensky ( callsign String, number String, icao24 String, registration String, typecode String, origin String, destination String, firstseen DateTime, lastseen DateTime, day DateTime, latitude_1 Float64, longitude_1 Float64, altitude_1 Float64, latitude_2 Float64, longitude_2 Float64, altitude_2 Float64 ) ENGINE = MergeTree ORDER BY (origin, destination, callsign);  "},{"title":"Import Data​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"en/getting-started/example-datasets/opensky#import-data","content":"Upload data into ClickHouse in parallel: ls -1 flightlist_*.csv.gz | xargs -P100 -I{} bash -c 'gzip -c -d &quot;{}&quot; | clickhouse-client --date_time_input_format best_effort --query &quot;INSERT INTO opensky FORMAT CSVWithNames&quot;'  Here we pass the list of files (ls -1 flightlist_*.csv.gz) to xargs for parallel processing.xargs -P100 specifies to use up to 100 parallel workers but as we only have 30 files, the number of workers will be only 30.For every file, xargs will run a script with bash -c. The script has substitution in form of {} and the xargs command will substitute the filename to it (we have asked it for xargs with -I{}).The script will decompress the file (gzip -c -d &quot;{}&quot;) to standard output (-c parameter) and the output is redirected to clickhouse-client.We also asked to parse DateTime fields with extended parser (--date_time_input_format best_effort) to recognize ISO-8601 format with timezone offsets. Finally, clickhouse-client will do insertion. It will read input data in CSVWithNames format. Parallel upload takes 24 seconds. If you don't like parallel upload, here is sequential variant: for file in flightlist_*.csv.gz; do gzip -c -d &quot;$file&quot; | clickhouse-client --date_time_input_format best_effort --query &quot;INSERT INTO opensky FORMAT CSVWithNames&quot;; done  "},{"title":"Validate the Data​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"en/getting-started/example-datasets/opensky#validate-data","content":"Query: SELECT count() FROM opensky;  Result: ┌──count()─┐ │ 66010819 │ └──────────┘  The size of dataset in ClickHouse is just 2.66 GiB, check it. Query: SELECT formatReadableSize(total_bytes) FROM system.tables WHERE name = 'opensky';  Result: ┌─formatReadableSize(total_bytes)─┐ │ 2.66 GiB │ └─────────────────────────────────┘  "},{"title":"Run Some Queries​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"en/getting-started/example-datasets/opensky#run-queries","content":"Total distance travelled is 68 billion kilometers. Query: SELECT formatReadableQuantity(sum(geoDistance(longitude_1, latitude_1, longitude_2, latitude_2)) / 1000) FROM opensky;  Result: ┌─formatReadableQuantity(divide(sum(geoDistance(longitude_1, latitude_1, longitude_2, latitude_2)), 1000))─┐ │ 68.72 billion │ └──────────────────────────────────────────────────────────────────────────────────────────────────────────┘  Average flight distance is around 1000 km. Query: SELECT avg(geoDistance(longitude_1, latitude_1, longitude_2, latitude_2)) FROM opensky;  Result: ┌─avg(geoDistance(longitude_1, latitude_1, longitude_2, latitude_2))─┐ │ 1041090.6465708319 │ └────────────────────────────────────────────────────────────────────┘  "},{"title":"Most busy origin airports and the average distance seen​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"en/getting-started/example-datasets/opensky#busy-airports-average-distance","content":"Query: SELECT origin, count(), round(avg(geoDistance(longitude_1, latitude_1, longitude_2, latitude_2))) AS distance, bar(distance, 0, 10000000, 100) AS bar FROM opensky WHERE origin != '' GROUP BY origin ORDER BY count() DESC LIMIT 100;  Result:  ┌─origin─┬─count()─┬─distance─┬─bar────────────────────────────────────┐ 1. │ KORD │ 745007 │ 1546108 │ ███████████████▍ │ 2. │ KDFW │ 696702 │ 1358721 │ █████████████▌ │ 3. │ KATL │ 667286 │ 1169661 │ ███████████▋ │ 4. │ KDEN │ 582709 │ 1287742 │ ████████████▊ │ 5. │ KLAX │ 581952 │ 2628393 │ ██████████████████████████▎ │ 6. │ KLAS │ 447789 │ 1336967 │ █████████████▎ │ 7. │ KPHX │ 428558 │ 1345635 │ █████████████▍ │ 8. │ KSEA │ 412592 │ 1757317 │ █████████████████▌ │ 9. │ KCLT │ 404612 │ 880355 │ ████████▋ │ 10. │ VIDP │ 363074 │ 1445052 │ ██████████████▍ │ 11. │ EDDF │ 362643 │ 2263960 │ ██████████████████████▋ │ 12. │ KSFO │ 361869 │ 2445732 │ ████████████████████████▍ │ 13. │ KJFK │ 349232 │ 2996550 │ █████████████████████████████▊ │ 14. │ KMSP │ 346010 │ 1287328 │ ████████████▋ │ 15. │ LFPG │ 344748 │ 2206203 │ ██████████████████████ │ 16. │ EGLL │ 341370 │ 3216593 │ ████████████████████████████████▏ │ 17. │ EHAM │ 340272 │ 2116425 │ █████████████████████▏ │ 18. │ KEWR │ 337696 │ 1826545 │ ██████████████████▎ │ 19. │ KPHL │ 320762 │ 1291761 │ ████████████▊ │ 20. │ OMDB │ 308855 │ 2855706 │ ████████████████████████████▌ │ 21. │ UUEE │ 307098 │ 1555122 │ ███████████████▌ │ 22. │ KBOS │ 304416 │ 1621675 │ ████████████████▏ │ 23. │ LEMD │ 291787 │ 1695097 │ ████████████████▊ │ 24. │ YSSY │ 272979 │ 1875298 │ ██████████████████▋ │ 25. │ KMIA │ 265121 │ 1923542 │ ███████████████████▏ │ 26. │ ZGSZ │ 263497 │ 745086 │ ███████▍ │ 27. │ EDDM │ 256691 │ 1361453 │ █████████████▌ │ 28. │ WMKK │ 254264 │ 1626688 │ ████████████████▎ │ 29. │ CYYZ │ 251192 │ 2175026 │ █████████████████████▋ │ 30. │ KLGA │ 248699 │ 1106935 │ ███████████ │ 31. │ VHHH │ 248473 │ 3457658 │ ██████████████████████████████████▌ │ 32. │ RJTT │ 243477 │ 1272744 │ ████████████▋ │ 33. │ KBWI │ 241440 │ 1187060 │ ███████████▋ │ 34. │ KIAD │ 239558 │ 1683485 │ ████████████████▋ │ 35. │ KIAH │ 234202 │ 1538335 │ ███████████████▍ │ 36. │ KFLL │ 223447 │ 1464410 │ ██████████████▋ │ 37. │ KDAL │ 212055 │ 1082339 │ ██████████▋ │ 38. │ KDCA │ 207883 │ 1013359 │ ██████████▏ │ 39. │ LIRF │ 207047 │ 1427965 │ ██████████████▎ │ 40. │ PANC │ 206007 │ 2525359 │ █████████████████████████▎ │ 41. │ LTFJ │ 205415 │ 860470 │ ████████▌ │ 42. │ KDTW │ 204020 │ 1106716 │ ███████████ │ 43. │ VABB │ 201679 │ 1300865 │ █████████████ │ 44. │ OTHH │ 200797 │ 3759544 │ █████████████████████████████████████▌ │ 45. │ KMDW │ 200796 │ 1232551 │ ████████████▎ │ 46. │ KSAN │ 198003 │ 1495195 │ ██████████████▊ │ 47. │ KPDX │ 197760 │ 1269230 │ ████████████▋ │ 48. │ SBGR │ 197624 │ 2041697 │ ████████████████████▍ │ 49. │ VOBL │ 189011 │ 1040180 │ ██████████▍ │ 50. │ LEBL │ 188956 │ 1283190 │ ████████████▋ │ 51. │ YBBN │ 188011 │ 1253405 │ ████████████▌ │ 52. │ LSZH │ 187934 │ 1572029 │ ███████████████▋ │ 53. │ YMML │ 187643 │ 1870076 │ ██████████████████▋ │ 54. │ RCTP │ 184466 │ 2773976 │ ███████████████████████████▋ │ 55. │ KSNA │ 180045 │ 778484 │ ███████▋ │ 56. │ EGKK │ 176420 │ 1694770 │ ████████████████▊ │ 57. │ LOWW │ 176191 │ 1274833 │ ████████████▋ │ 58. │ UUDD │ 176099 │ 1368226 │ █████████████▋ │ 59. │ RKSI │ 173466 │ 3079026 │ ██████████████████████████████▋ │ 60. │ EKCH │ 172128 │ 1229895 │ ████████████▎ │ 61. │ KOAK │ 171119 │ 1114447 │ ███████████▏ │ 62. │ RPLL │ 170122 │ 1440735 │ ██████████████▍ │ 63. │ KRDU │ 167001 │ 830521 │ ████████▎ │ 64. │ KAUS │ 164524 │ 1256198 │ ████████████▌ │ 65. │ KBNA │ 163242 │ 1022726 │ ██████████▏ │ 66. │ KSDF │ 162655 │ 1380867 │ █████████████▋ │ 67. │ ENGM │ 160732 │ 910108 │ █████████ │ 68. │ LIMC │ 160696 │ 1564620 │ ███████████████▋ │ 69. │ KSJC │ 159278 │ 1081125 │ ██████████▋ │ 70. │ KSTL │ 157984 │ 1026699 │ ██████████▎ │ 71. │ UUWW │ 156811 │ 1261155 │ ████████████▌ │ 72. │ KIND │ 153929 │ 987944 │ █████████▊ │ 73. │ ESSA │ 153390 │ 1203439 │ ████████████ │ 74. │ KMCO │ 153351 │ 1508657 │ ███████████████ │ 75. │ KDVT │ 152895 │ 74048 │ ▋ │ 76. │ VTBS │ 152645 │ 2255591 │ ██████████████████████▌ │ 77. │ CYVR │ 149574 │ 2027413 │ ████████████████████▎ │ 78. │ EIDW │ 148723 │ 1503985 │ ███████████████ │ 79. │ LFPO │ 143277 │ 1152964 │ ███████████▌ │ 80. │ EGSS │ 140830 │ 1348183 │ █████████████▍ │ 81. │ KAPA │ 140776 │ 420441 │ ████▏ │ 82. │ KHOU │ 138985 │ 1068806 │ ██████████▋ │ 83. │ KTPA │ 138033 │ 1338223 │ █████████████▍ │ 84. │ KFFZ │ 137333 │ 55397 │ ▌ │ 85. │ NZAA │ 136092 │ 1581264 │ ███████████████▋ │ 86. │ YPPH │ 133916 │ 1271550 │ ████████████▋ │ 87. │ RJBB │ 133522 │ 1805623 │ ██████████████████ │ 88. │ EDDL │ 133018 │ 1265919 │ ████████████▋ │ 89. │ ULLI │ 130501 │ 1197108 │ ███████████▊ │ 90. │ KIWA │ 127195 │ 250876 │ ██▌ │ 91. │ KTEB │ 126969 │ 1189414 │ ███████████▊ │ 92. │ VOMM │ 125616 │ 1127757 │ ███████████▎ │ 93. │ LSGG │ 123998 │ 1049101 │ ██████████▍ │ 94. │ LPPT │ 122733 │ 1779187 │ █████████████████▋ │ 95. │ WSSS │ 120493 │ 3264122 │ ████████████████████████████████▋ │ 96. │ EBBR │ 118539 │ 1579939 │ ███████████████▋ │ 97. │ VTBD │ 118107 │ 661627 │ ██████▌ │ 98. │ KVNY │ 116326 │ 692960 │ ██████▊ │ 99. │ EDDT │ 115122 │ 941740 │ █████████▍ │ 100. │ EFHK │ 114860 │ 1629143 │ ████████████████▎ │ └────────┴─────────┴──────────┴────────────────────────────────────────┘  "},{"title":"Number of flights from three major Moscow airports, weekly​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"en/getting-started/example-datasets/opensky#flights-from-moscow","content":"Query: SELECT toMonday(day) AS k, count() AS c, bar(c, 0, 10000, 100) AS bar FROM opensky WHERE origin IN ('UUEE', 'UUDD', 'UUWW') GROUP BY k ORDER BY k ASC;  Result:  ┌──────────k─┬────c─┬─bar──────────────────────────────────────────────────────────────────────────┐ 1. │ 2018-12-31 │ 5248 │ ████████████████████████████████████████████████████▍ │ 2. │ 2019-01-07 │ 6302 │ ███████████████████████████████████████████████████████████████ │ 3. │ 2019-01-14 │ 5701 │ █████████████████████████████████████████████████████████ │ 4. │ 2019-01-21 │ 5638 │ ████████████████████████████████████████████████████████▍ │ 5. │ 2019-01-28 │ 5731 │ █████████████████████████████████████████████████████████▎ │ 6. │ 2019-02-04 │ 5683 │ ████████████████████████████████████████████████████████▋ │ 7. │ 2019-02-11 │ 5759 │ █████████████████████████████████████████████████████████▌ │ 8. │ 2019-02-18 │ 5736 │ █████████████████████████████████████████████████████████▎ │ 9. │ 2019-02-25 │ 5873 │ ██████████████████████████████████████████████████████████▋ │ 10. │ 2019-03-04 │ 5965 │ ███████████████████████████████████████████████████████████▋ │ 11. │ 2019-03-11 │ 5900 │ ███████████████████████████████████████████████████████████ │ 12. │ 2019-03-18 │ 5823 │ ██████████████████████████████████████████████████████████▏ │ 13. │ 2019-03-25 │ 5899 │ ██████████████████████████████████████████████████████████▊ │ 14. │ 2019-04-01 │ 6043 │ ████████████████████████████████████████████████████████████▍ │ 15. │ 2019-04-08 │ 6098 │ ████████████████████████████████████████████████████████████▊ │ 16. │ 2019-04-15 │ 6196 │ █████████████████████████████████████████████████████████████▊ │ 17. │ 2019-04-22 │ 6486 │ ████████████████████████████████████████████████████████████████▋ │ 18. │ 2019-04-29 │ 6682 │ ██████████████████████████████████████████████████████████████████▋ │ 19. │ 2019-05-06 │ 6739 │ ███████████████████████████████████████████████████████████████████▍ │ 20. │ 2019-05-13 │ 6600 │ ██████████████████████████████████████████████████████████████████ │ 21. │ 2019-05-20 │ 6575 │ █████████████████████████████████████████████████████████████████▋ │ 22. │ 2019-05-27 │ 6786 │ ███████████████████████████████████████████████████████████████████▋ │ 23. │ 2019-06-03 │ 6872 │ ████████████████████████████████████████████████████████████████████▋ │ 24. │ 2019-06-10 │ 7045 │ ██████████████████████████████████████████████████████████████████████▍ │ 25. │ 2019-06-17 │ 7045 │ ██████████████████████████████████████████████████████████████████████▍ │ 26. │ 2019-06-24 │ 6852 │ ████████████████████████████████████████████████████████████████████▌ │ 27. │ 2019-07-01 │ 7248 │ ████████████████████████████████████████████████████████████████████████▍ │ 28. │ 2019-07-08 │ 7284 │ ████████████████████████████████████████████████████████████████████████▋ │ 29. │ 2019-07-15 │ 7142 │ ███████████████████████████████████████████████████████████████████████▍ │ 30. │ 2019-07-22 │ 7108 │ ███████████████████████████████████████████████████████████████████████ │ 31. │ 2019-07-29 │ 7251 │ ████████████████████████████████████████████████████████████████████████▌ │ 32. │ 2019-08-05 │ 7403 │ ██████████████████████████████████████████████████████████████████████████ │ 33. │ 2019-08-12 │ 7457 │ ██████████████████████████████████████████████████████████████████████████▌ │ 34. │ 2019-08-19 │ 7502 │ ███████████████████████████████████████████████████████████████████████████ │ 35. │ 2019-08-26 │ 7540 │ ███████████████████████████████████████████████████████████████████████████▍ │ 36. │ 2019-09-02 │ 7237 │ ████████████████████████████████████████████████████████████████████████▎ │ 37. │ 2019-09-09 │ 7328 │ █████████████████████████████████████████████████████████████████████████▎ │ 38. │ 2019-09-16 │ 5566 │ ███████████████████████████████████████████████████████▋ │ 39. │ 2019-09-23 │ 7049 │ ██████████████████████████████████████████████████████████████████████▍ │ 40. │ 2019-09-30 │ 6880 │ ████████████████████████████████████████████████████████████████████▋ │ 41. │ 2019-10-07 │ 6518 │ █████████████████████████████████████████████████████████████████▏ │ 42. │ 2019-10-14 │ 6688 │ ██████████████████████████████████████████████████████████████████▊ │ 43. │ 2019-10-21 │ 6667 │ ██████████████████████████████████████████████████████████████████▋ │ 44. │ 2019-10-28 │ 6303 │ ███████████████████████████████████████████████████████████████ │ 45. │ 2019-11-04 │ 6298 │ ██████████████████████████████████████████████████████████████▊ │ 46. │ 2019-11-11 │ 6137 │ █████████████████████████████████████████████████████████████▎ │ 47. │ 2019-11-18 │ 6051 │ ████████████████████████████████████████████████████████████▌ │ 48. │ 2019-11-25 │ 5820 │ ██████████████████████████████████████████████████████████▏ │ 49. │ 2019-12-02 │ 5942 │ ███████████████████████████████████████████████████████████▍ │ 50. │ 2019-12-09 │ 4891 │ ████████████████████████████████████████████████▊ │ 51. │ 2019-12-16 │ 5682 │ ████████████████████████████████████████████████████████▋ │ 52. │ 2019-12-23 │ 6111 │ █████████████████████████████████████████████████████████████ │ 53. │ 2019-12-30 │ 5870 │ ██████████████████████████████████████████████████████████▋ │ 54. │ 2020-01-06 │ 5953 │ ███████████████████████████████████████████████████████████▌ │ 55. │ 2020-01-13 │ 5698 │ ████████████████████████████████████████████████████████▊ │ 56. │ 2020-01-20 │ 5339 │ █████████████████████████████████████████████████████▍ │ 57. │ 2020-01-27 │ 5566 │ ███████████████████████████████████████████████████████▋ │ 58. │ 2020-02-03 │ 5801 │ ██████████████████████████████████████████████████████████ │ 59. │ 2020-02-10 │ 5692 │ ████████████████████████████████████████████████████████▊ │ 60. │ 2020-02-17 │ 5912 │ ███████████████████████████████████████████████████████████ │ 61. │ 2020-02-24 │ 6031 │ ████████████████████████████████████████████████████████████▎ │ 62. │ 2020-03-02 │ 6105 │ █████████████████████████████████████████████████████████████ │ 63. │ 2020-03-09 │ 5823 │ ██████████████████████████████████████████████████████████▏ │ 64. │ 2020-03-16 │ 4659 │ ██████████████████████████████████████████████▌ │ 65. │ 2020-03-23 │ 3720 │ █████████████████████████████████████▏ │ 66. │ 2020-03-30 │ 1720 │ █████████████████▏ │ 67. │ 2020-04-06 │ 849 │ ████████▍ │ 68. │ 2020-04-13 │ 710 │ ███████ │ 69. │ 2020-04-20 │ 725 │ ███████▏ │ 70. │ 2020-04-27 │ 920 │ █████████▏ │ 71. │ 2020-05-04 │ 859 │ ████████▌ │ 72. │ 2020-05-11 │ 1047 │ ██████████▍ │ 73. │ 2020-05-18 │ 1135 │ ███████████▎ │ 74. │ 2020-05-25 │ 1266 │ ████████████▋ │ 75. │ 2020-06-01 │ 1793 │ █████████████████▊ │ 76. │ 2020-06-08 │ 1979 │ ███████████████████▋ │ 77. │ 2020-06-15 │ 2297 │ ██████████████████████▊ │ 78. │ 2020-06-22 │ 2788 │ ███████████████████████████▊ │ 79. │ 2020-06-29 │ 3389 │ █████████████████████████████████▊ │ 80. │ 2020-07-06 │ 3545 │ ███████████████████████████████████▍ │ 81. │ 2020-07-13 │ 3569 │ ███████████████████████████████████▋ │ 82. │ 2020-07-20 │ 3784 │ █████████████████████████████████████▋ │ 83. │ 2020-07-27 │ 3960 │ ███████████████████████████████████████▌ │ 84. │ 2020-08-03 │ 4323 │ ███████████████████████████████████████████▏ │ 85. │ 2020-08-10 │ 4581 │ █████████████████████████████████████████████▋ │ 86. │ 2020-08-17 │ 4791 │ ███████████████████████████████████████████████▊ │ 87. │ 2020-08-24 │ 4928 │ █████████████████████████████████████████████████▎ │ 88. │ 2020-08-31 │ 4687 │ ██████████████████████████████████████████████▋ │ 89. │ 2020-09-07 │ 4643 │ ██████████████████████████████████████████████▍ │ 90. │ 2020-09-14 │ 4594 │ █████████████████████████████████████████████▊ │ 91. │ 2020-09-21 │ 4478 │ ████████████████████████████████████████████▋ │ 92. │ 2020-09-28 │ 4382 │ ███████████████████████████████████████████▋ │ 93. │ 2020-10-05 │ 4261 │ ██████████████████████████████████████████▌ │ 94. │ 2020-10-12 │ 4243 │ ██████████████████████████████████████████▍ │ 95. │ 2020-10-19 │ 3941 │ ███████████████████████████████████████▍ │ 96. │ 2020-10-26 │ 3616 │ ████████████████████████████████████▏ │ 97. │ 2020-11-02 │ 3586 │ ███████████████████████████████████▋ │ 98. │ 2020-11-09 │ 3403 │ ██████████████████████████████████ │ 99. │ 2020-11-16 │ 3336 │ █████████████████████████████████▎ │ 100. │ 2020-11-23 │ 3230 │ ████████████████████████████████▎ │ 101. │ 2020-11-30 │ 3183 │ ███████████████████████████████▋ │ 102. │ 2020-12-07 │ 3285 │ ████████████████████████████████▋ │ 103. │ 2020-12-14 │ 3367 │ █████████████████████████████████▋ │ 104. │ 2020-12-21 │ 3748 │ █████████████████████████████████████▍ │ 105. │ 2020-12-28 │ 3986 │ ███████████████████████████████████████▋ │ 106. │ 2021-01-04 │ 3906 │ ███████████████████████████████████████ │ 107. │ 2021-01-11 │ 3425 │ ██████████████████████████████████▎ │ 108. │ 2021-01-18 │ 3144 │ ███████████████████████████████▍ │ 109. │ 2021-01-25 │ 3115 │ ███████████████████████████████▏ │ 110. │ 2021-02-01 │ 3285 │ ████████████████████████████████▋ │ 111. │ 2021-02-08 │ 3321 │ █████████████████████████████████▏ │ 112. │ 2021-02-15 │ 3475 │ ██████████████████████████████████▋ │ 113. │ 2021-02-22 │ 3549 │ ███████████████████████████████████▍ │ 114. │ 2021-03-01 │ 3755 │ █████████████████████████████████████▌ │ 115. │ 2021-03-08 │ 3080 │ ██████████████████████████████▋ │ 116. │ 2021-03-15 │ 3789 │ █████████████████████████████████████▊ │ 117. │ 2021-03-22 │ 3804 │ ██████████████████████████████████████ │ 118. │ 2021-03-29 │ 4238 │ ██████████████████████████████████████████▍ │ 119. │ 2021-04-05 │ 4307 │ ███████████████████████████████████████████ │ 120. │ 2021-04-12 │ 4225 │ ██████████████████████████████████████████▎ │ 121. │ 2021-04-19 │ 4391 │ ███████████████████████████████████████████▊ │ 122. │ 2021-04-26 │ 4868 │ ████████████████████████████████████████████████▋ │ 123. │ 2021-05-03 │ 4977 │ █████████████████████████████████████████████████▋ │ 124. │ 2021-05-10 │ 5164 │ ███████████████████████████████████████████████████▋ │ 125. │ 2021-05-17 │ 4986 │ █████████████████████████████████████████████████▋ │ 126. │ 2021-05-24 │ 5024 │ ██████████████████████████████████████████████████▏ │ 127. │ 2021-05-31 │ 4824 │ ████████████████████████████████████████████████▏ │ 128. │ 2021-06-07 │ 5652 │ ████████████████████████████████████████████████████████▌ │ 129. │ 2021-06-14 │ 5613 │ ████████████████████████████████████████████████████████▏ │ 130. │ 2021-06-21 │ 6061 │ ████████████████████████████████████████████████████████████▌ │ 131. │ 2021-06-28 │ 2554 │ █████████████████████████▌ │ └────────────┴──────┴──────────────────────────────────────────────────────────────────────────────┘  "},{"title":"Online Playground​","type":1,"pageTitle":"Crowdsourced air traffic data from The OpenSky Network 2020","url":"en/getting-started/example-datasets/opensky#playground","content":"You can test other queries to this data set using the interactive resource Online Playground. For example, like this. However, please note that you cannot create temporary tables here. "},{"title":"Structured Approach","type":0,"sectionRef":"#","url":"en/guides/developer/working-with-json/json-structured","content":"Structured Approach First, we confirm we can read the JSON dataset and highlight the challenges of handling semi-structured data using more traditional types used in other databases. We don’t rely on Schema inference to map the JSON fields to columns in the example below - instead, we specify a format of JSONEachRow and map the fields explicitly to columns in the s3 functions. SELECT type, `actor.display_login`, `repo.name`, created_at FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/github/github-2022-flat.ndjson.gz', 'JSONEachRow', 'type String, `actor.avatar_url` String, `actor.display_login` String, ' || '`actor.id` Float64, `actor.login` String, `actor.url` String, `repo.id` Float64, ' || '`repo.name` String, `repo.url` String, created_at String, `payload.pull_request.updated_at` String, ' || '`payload.action` String, `payload.ref` String, `payload.ref_type` String, ' || '`payload.pull_request.user.login` String, `payload.pull_request.number` Float64, ' || '`payload.pull_request.title` String, `payload.pull_request.state` String, ' || '`payload.pull_request.author_association` String, `payload.pull_request.head.ref` String, ' || '`payload.pull_request.head.sha` String, `payload.pull_request.base.ref` String, ' || '`payload.pull_request.base.sha` String, `payload.size` Float64, `payload.distinct_size` Float64') LIMIT 10; type\tactor.display_login\trepo.name\tcreated_atPushEvent\tLakshmipatil2021\trevacprogramming/pps-test1-Lakshmipatil2021\t2022-01-04T07:00:00Z MemberEvent\tKStevenT\tKStevenT/HTML_ExternalWorkshop\t2022-01-04T07:00:00Z PushEvent\tSoumojit28\tSoumojit28/Oxytocin\t2022-01-04T07:00:00Z PushEvent\tgithub-actions\tdiogoaraujo017/diogoaraujo017\t2022-01-04T07:00:00Z PushEvent\tAman-Sonwani\tAman-Sonwani/crwn-clothing\t2022-01-04T07:00:00Z PushEvent\thuangshanyoumumingwutong\thuangshanyoumumingwutong/picgo\t2022-01-04T07:00:00Z PullRequestEvent\trfprod\trfprod/nx-ng-starter\t2022-01-04T07:00:00Z PushEvent\tHelikopter-Bojowy\tHelikopter-Bojowy/Exp-na-helikopterze\t2022-01-04T07:00:00Z IssueCommentEvent\tPRMerger-test-1\tMicrosoftDocs/CSIDev-Public\t2022-01-04T07:00:00Z PushEvent\tgithub-actions\tpioug/yield-data\t2022-01-04T07:00:00Z Note this dataset is a subset of the example used later, with no nested objects within the JSON itself - the fields have been flattened using a period separator. Although nested objects can be handled through an explicit mapping, it requires either the use of the new JSON object field or (for older ClickHouse versions) Tuples, Map and Nested structures (see Other Approaches) further complicate usage. This approach requires mapping all fields and has obvious limitations when the JSON is potentially dynamic or unknown. We could use an INSERT INTO SELECT statement to persist the results into a local Merge Tree table. Defining such a table would require the user to know all fields and express the verbose definition below. CREATE table github_flat ( type String, `actor.avatar_url` String, `actor.display_login` String, `actor.id` Float64, `actor.login` String, `actor.url` String, `repo.id` Float64, `repo.name` String, `repo.url` String, created_at String, `payload.pull_request.updated_at` String, `payload.action` String, `payload.ref` String, `payload.ref_type` String, `payload.pull_request.user.login` String, `payload.pull_request.number` Float64, `payload.pull_request.title` String, `payload.pull_request.state` String, `payload.pull_request.author_association` String, `payload.pull_request.head.ref` String, `payload.pull_request.head.sha` String, `payload.pull_request.base.ref` String, `payload.pull_request.base.sha` String, `payload.size` Float64, `payload.distinct_size` Float64 ) ENGINE = MergeTree ORDER BY (type, `repo.name`, created_at); INSERT INTO github_flat SELECT * FROM s3 ('https://datasets-documentation.s3.eu-west-3.amazonaws.com/github/github-2022-flat.ndjson.gz', 'JSONEachRow'); SELECT count() from github_flat; count()1000000 Furthermore, if new properties are added to the JSON, the table would need to be updated, i.e., via ALTER TABLE. Naturally, this leads us to use ClickHouse’s semi-structured features.","keywords":""},{"title":"UK Property Price Paid","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/uk-price-paid","content":"","keywords":""},{"title":"Download the Dataset​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#download-dataset","content":"Run the command: wget http://prod.publicdata.landregistry.gov.uk.s3-website-eu-west-1.amazonaws.com/pp-complete.csv  Download will take about 2 minutes with good internet connection. "},{"title":"Create the Table​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#create-table","content":"CREATE TABLE uk_price_paid ( price UInt32, date Date, postcode1 LowCardinality(String), postcode2 LowCardinality(String), type Enum8('terraced' = 1, 'semi-detached' = 2, 'detached' = 3, 'flat' = 4, 'other' = 0), is_new UInt8, duration Enum8('freehold' = 1, 'leasehold' = 2, 'unknown' = 0), addr1 String, addr2 String, street LowCardinality(String), locality LowCardinality(String), town LowCardinality(String), district LowCardinality(String), county LowCardinality(String), category UInt8 ) ENGINE = MergeTree ORDER BY (postcode1, postcode2, addr1, addr2);  "},{"title":"Preprocess and Import Data​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#preprocess-import-data","content":"We will use clickhouse-local tool for data preprocessing and clickhouse-client to upload it. In this example, we define the structure of source data from the CSV file and specify a query to preprocess the data with clickhouse-local. The preprocessing is: splitting the postcode to two different columns postcode1 and postcode2 that is better for storage and queries;coverting the time field to date as it only contains 00:00 time;ignoring the UUid field because we don't need it for analysis;transforming type and duration to more readable Enum fields with function transform;transforming is_new and category fields from single-character string (Y/N and A/B) to UInt8 field with 0 and 1. Preprocessed data is piped directly to clickhouse-client to be inserted into ClickHouse table in streaming fashion. clickhouse-local --input-format CSV --structure ' uuid String, price UInt32, time DateTime, postcode String, a String, b String, c String, addr1 String, addr2 String, street String, locality String, town String, district String, county String, d String, e String ' --query &quot; WITH splitByChar(' ', postcode) AS p SELECT price, toDate(time) AS date, p[1] AS postcode1, p[2] AS postcode2, transform(a, ['T', 'S', 'D', 'F', 'O'], ['terraced', 'semi-detached', 'detached', 'flat', 'other']) AS type, b = 'Y' AS is_new, transform(c, ['F', 'L', 'U'], ['freehold', 'leasehold', 'unknown']) AS duration, addr1, addr2, street, locality, town, district, county, d = 'B' AS category FROM table&quot; --date_time_input_format best_effort &lt; pp-complete.csv | clickhouse-client --query &quot;INSERT INTO uk_price_paid FORMAT TSV&quot;  It will take about 40 seconds. "},{"title":"Validate the Data​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#validate-data","content":"Query: SELECT count() FROM uk_price_paid;  Result: ┌──count()─┐ │ 26321785 │ └──────────┘  The size of dataset in ClickHouse is just 278 MiB, check it. Query: SELECT formatReadableSize(total_bytes) FROM system.tables WHERE name = 'uk_price_paid';  Result: ┌─formatReadableSize(total_bytes)─┐ │ 278.80 MiB │ └─────────────────────────────────┘  "},{"title":"Run Some Queries​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#run-queries","content":""},{"title":"Query 1. Average Price Per Year​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#average-price","content":"Query: SELECT toYear(date) AS year, round(avg(price)) AS price, bar(price, 0, 1000000, 80) FROM uk_price_paid GROUP BY year ORDER BY year;  Result: ┌─year─┬──price─┬─bar(round(avg(price)), 0, 1000000, 80)─┐ │ 1995 │ 67932 │ █████▍ │ │ 1996 │ 71505 │ █████▋ │ │ 1997 │ 78532 │ ██████▎ │ │ 1998 │ 85436 │ ██████▋ │ │ 1999 │ 96037 │ ███████▋ │ │ 2000 │ 107479 │ ████████▌ │ │ 2001 │ 118885 │ █████████▌ │ │ 2002 │ 137941 │ ███████████ │ │ 2003 │ 155889 │ ████████████▍ │ │ 2004 │ 178885 │ ██████████████▎ │ │ 2005 │ 189351 │ ███████████████▏ │ │ 2006 │ 203528 │ ████████████████▎ │ │ 2007 │ 219378 │ █████████████████▌ │ │ 2008 │ 217056 │ █████████████████▎ │ │ 2009 │ 213419 │ █████████████████ │ │ 2010 │ 236109 │ ██████████████████▊ │ │ 2011 │ 232805 │ ██████████████████▌ │ │ 2012 │ 238367 │ ███████████████████ │ │ 2013 │ 256931 │ ████████████████████▌ │ │ 2014 │ 279915 │ ██████████████████████▍ │ │ 2015 │ 297266 │ ███████████████████████▋ │ │ 2016 │ 313201 │ █████████████████████████ │ │ 2017 │ 346097 │ ███████████████████████████▋ │ │ 2018 │ 350116 │ ████████████████████████████ │ │ 2019 │ 351013 │ ████████████████████████████ │ │ 2020 │ 369420 │ █████████████████████████████▌ │ │ 2021 │ 386903 │ ██████████████████████████████▊ │ └──────┴────────┴────────────────────────────────────────┘  "},{"title":"Query 2. Average Price per Year in London​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#average-price-london","content":"Query: SELECT toYear(date) AS year, round(avg(price)) AS price, bar(price, 0, 2000000, 100) FROM uk_price_paid WHERE town = 'LONDON' GROUP BY year ORDER BY year;  Result: ┌─year─┬───price─┬─bar(round(avg(price)), 0, 2000000, 100)───────────────┐ │ 1995 │ 109116 │ █████▍ │ │ 1996 │ 118667 │ █████▊ │ │ 1997 │ 136518 │ ██████▋ │ │ 1998 │ 152983 │ ███████▋ │ │ 1999 │ 180637 │ █████████ │ │ 2000 │ 215838 │ ██████████▋ │ │ 2001 │ 232994 │ ███████████▋ │ │ 2002 │ 263670 │ █████████████▏ │ │ 2003 │ 278394 │ █████████████▊ │ │ 2004 │ 304666 │ ███████████████▏ │ │ 2005 │ 322875 │ ████████████████▏ │ │ 2006 │ 356191 │ █████████████████▋ │ │ 2007 │ 404054 │ ████████████████████▏ │ │ 2008 │ 420741 │ █████████████████████ │ │ 2009 │ 427753 │ █████████████████████▍ │ │ 2010 │ 480306 │ ████████████████████████ │ │ 2011 │ 496274 │ ████████████████████████▋ │ │ 2012 │ 519442 │ █████████████████████████▊ │ │ 2013 │ 616212 │ ██████████████████████████████▋ │ │ 2014 │ 724154 │ ████████████████████████████████████▏ │ │ 2015 │ 792129 │ ███████████████████████████████████████▌ │ │ 2016 │ 843655 │ ██████████████████████████████████████████▏ │ │ 2017 │ 982642 │ █████████████████████████████████████████████████▏ │ │ 2018 │ 1016835 │ ██████████████████████████████████████████████████▋ │ │ 2019 │ 1042849 │ ████████████████████████████████████████████████████▏ │ │ 2020 │ 1011889 │ ██████████████████████████████████████████████████▌ │ │ 2021 │ 960343 │ ████████████████████████████████████████████████ │ └──────┴─────────┴───────────────────────────────────────────────────────┘  Something happened in 2013. I don't have a clue. Maybe you have a clue what happened in 2020? "},{"title":"Query 3. The Most Expensive Neighborhoods​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#most-expensive-neighborhoods","content":"Query: SELECT town, district, count() AS c, round(avg(price)) AS price, bar(price, 0, 5000000, 100) FROM uk_price_paid WHERE date &gt;= '2020-01-01' GROUP BY town, district HAVING c &gt;= 100 ORDER BY price DESC LIMIT 100;  Result:  ┌─town─────────────────┬─district───────────────┬────c─┬───price─┬─bar(round(avg(price)), 0, 5000000, 100)────────────────────────────┐ │ LONDON │ CITY OF WESTMINSTER │ 3606 │ 3280239 │ █████████████████████████████████████████████████████████████████▌ │ │ LONDON │ CITY OF LONDON │ 274 │ 3160502 │ ███████████████████████████████████████████████████████████████▏ │ │ LONDON │ KENSINGTON AND CHELSEA │ 2550 │ 2308478 │ ██████████████████████████████████████████████▏ │ │ LEATHERHEAD │ ELMBRIDGE │ 114 │ 1897407 │ █████████████████████████████████████▊ │ │ LONDON │ CAMDEN │ 3033 │ 1805404 │ ████████████████████████████████████ │ │ VIRGINIA WATER │ RUNNYMEDE │ 156 │ 1753247 │ ███████████████████████████████████ │ │ WINDLESHAM │ SURREY HEATH │ 108 │ 1677613 │ █████████████████████████████████▌ │ │ THORNTON HEATH │ CROYDON │ 546 │ 1671721 │ █████████████████████████████████▍ │ │ BARNET │ ENFIELD │ 124 │ 1505840 │ ██████████████████████████████ │ │ COBHAM │ ELMBRIDGE │ 387 │ 1237250 │ ████████████████████████▋ │ │ LONDON │ ISLINGTON │ 2668 │ 1236980 │ ████████████████████████▋ │ │ OXFORD │ SOUTH OXFORDSHIRE │ 321 │ 1220907 │ ████████████████████████▍ │ │ LONDON │ RICHMOND UPON THAMES │ 704 │ 1215551 │ ████████████████████████▎ │ │ LONDON │ HOUNSLOW │ 671 │ 1207493 │ ████████████████████████▏ │ │ ASCOT │ WINDSOR AND MAIDENHEAD │ 407 │ 1183299 │ ███████████████████████▋ │ │ BEACONSFIELD │ BUCKINGHAMSHIRE │ 330 │ 1175615 │ ███████████████████████▌ │ │ RICHMOND │ RICHMOND UPON THAMES │ 874 │ 1110444 │ ██████████████████████▏ │ │ LONDON │ HAMMERSMITH AND FULHAM │ 3086 │ 1053983 │ █████████████████████ │ │ SURBITON │ ELMBRIDGE │ 100 │ 1011800 │ ████████████████████▏ │ │ RADLETT │ HERTSMERE │ 283 │ 1011712 │ ████████████████████▏ │ │ SALCOMBE │ SOUTH HAMS │ 127 │ 1011624 │ ████████████████████▏ │ │ WEYBRIDGE │ ELMBRIDGE │ 655 │ 1007265 │ ████████████████████▏ │ │ ESHER │ ELMBRIDGE │ 485 │ 986581 │ ███████████████████▋ │ │ LEATHERHEAD │ GUILDFORD │ 202 │ 977320 │ ███████████████████▌ │ │ BURFORD │ WEST OXFORDSHIRE │ 111 │ 966893 │ ███████████████████▎ │ │ BROCKENHURST │ NEW FOREST │ 129 │ 956675 │ ███████████████████▏ │ │ HINDHEAD │ WAVERLEY │ 137 │ 953753 │ ███████████████████ │ │ GERRARDS CROSS │ BUCKINGHAMSHIRE │ 419 │ 951121 │ ███████████████████ │ │ EAST MOLESEY │ ELMBRIDGE │ 192 │ 936769 │ ██████████████████▋ │ │ CHALFONT ST GILES │ BUCKINGHAMSHIRE │ 146 │ 925515 │ ██████████████████▌ │ │ LONDON │ TOWER HAMLETS │ 4388 │ 918304 │ ██████████████████▎ │ │ OLNEY │ MILTON KEYNES │ 235 │ 910646 │ ██████████████████▏ │ │ HENLEY-ON-THAMES │ SOUTH OXFORDSHIRE │ 540 │ 902418 │ ██████████████████ │ │ LONDON │ SOUTHWARK │ 3885 │ 892997 │ █████████████████▋ │ │ KINGSTON UPON THAMES │ KINGSTON UPON THAMES │ 960 │ 885969 │ █████████████████▋ │ │ LONDON │ EALING │ 2658 │ 871755 │ █████████████████▍ │ │ CRANBROOK │ TUNBRIDGE WELLS │ 431 │ 862348 │ █████████████████▏ │ │ LONDON │ MERTON │ 2099 │ 859118 │ █████████████████▏ │ │ BELVEDERE │ BEXLEY │ 346 │ 842423 │ ████████████████▋ │ │ GUILDFORD │ WAVERLEY │ 143 │ 841277 │ ████████████████▋ │ │ HARPENDEN │ ST ALBANS │ 657 │ 841216 │ ████████████████▋ │ │ LONDON │ HACKNEY │ 3307 │ 837090 │ ████████████████▋ │ │ LONDON │ WANDSWORTH │ 6566 │ 832663 │ ████████████████▋ │ │ MAIDENHEAD │ BUCKINGHAMSHIRE │ 123 │ 824299 │ ████████████████▍ │ │ KINGS LANGLEY │ DACORUM │ 145 │ 821331 │ ████████████████▍ │ │ BERKHAMSTED │ DACORUM │ 543 │ 818415 │ ████████████████▎ │ │ GREAT MISSENDEN │ BUCKINGHAMSHIRE │ 226 │ 802807 │ ████████████████ │ │ BILLINGSHURST │ CHICHESTER │ 144 │ 797829 │ ███████████████▊ │ │ WOKING │ GUILDFORD │ 176 │ 793494 │ ███████████████▋ │ │ STOCKBRIDGE │ TEST VALLEY │ 178 │ 793269 │ ███████████████▋ │ │ EPSOM │ REIGATE AND BANSTEAD │ 172 │ 791862 │ ███████████████▋ │ │ TONBRIDGE │ TUNBRIDGE WELLS │ 360 │ 787876 │ ███████████████▋ │ │ TEDDINGTON │ RICHMOND UPON THAMES │ 595 │ 786492 │ ███████████████▋ │ │ TWICKENHAM │ RICHMOND UPON THAMES │ 1155 │ 786193 │ ███████████████▋ │ │ LYNDHURST │ NEW FOREST │ 102 │ 785593 │ ███████████████▋ │ │ LONDON │ LAMBETH │ 5228 │ 774574 │ ███████████████▍ │ │ LONDON │ BARNET │ 3955 │ 773259 │ ███████████████▍ │ │ OXFORD │ VALE OF WHITE HORSE │ 353 │ 772088 │ ███████████████▍ │ │ TONBRIDGE │ MAIDSTONE │ 305 │ 770740 │ ███████████████▍ │ │ LUTTERWORTH │ HARBOROUGH │ 538 │ 768634 │ ███████████████▎ │ │ WOODSTOCK │ WEST OXFORDSHIRE │ 140 │ 766037 │ ███████████████▎ │ │ MIDHURST │ CHICHESTER │ 257 │ 764815 │ ███████████████▎ │ │ MARLOW │ BUCKINGHAMSHIRE │ 327 │ 761876 │ ███████████████▏ │ │ LONDON │ NEWHAM │ 3237 │ 761784 │ ███████████████▏ │ │ ALDERLEY EDGE │ CHESHIRE EAST │ 178 │ 757318 │ ███████████████▏ │ │ LUTON │ CENTRAL BEDFORDSHIRE │ 212 │ 754283 │ ███████████████ │ │ PETWORTH │ CHICHESTER │ 154 │ 754220 │ ███████████████ │ │ ALRESFORD │ WINCHESTER │ 219 │ 752718 │ ███████████████ │ │ POTTERS BAR │ WELWYN HATFIELD │ 174 │ 748465 │ ██████████████▊ │ │ HASLEMERE │ CHICHESTER │ 128 │ 746907 │ ██████████████▊ │ │ TADWORTH │ REIGATE AND BANSTEAD │ 502 │ 743252 │ ██████████████▋ │ │ THAMES DITTON │ ELMBRIDGE │ 244 │ 741913 │ ██████████████▋ │ │ REIGATE │ REIGATE AND BANSTEAD │ 581 │ 738198 │ ██████████████▋ │ │ BOURNE END │ BUCKINGHAMSHIRE │ 138 │ 735190 │ ██████████████▋ │ │ SEVENOAKS │ SEVENOAKS │ 1156 │ 730018 │ ██████████████▌ │ │ OXTED │ TANDRIDGE │ 336 │ 729123 │ ██████████████▌ │ │ INGATESTONE │ BRENTWOOD │ 166 │ 728103 │ ██████████████▌ │ │ LONDON │ BRENT │ 2079 │ 720605 │ ██████████████▍ │ │ LONDON │ HARINGEY │ 3216 │ 717780 │ ██████████████▎ │ │ PURLEY │ CROYDON │ 575 │ 716108 │ ██████████████▎ │ │ WELWYN │ WELWYN HATFIELD │ 222 │ 710603 │ ██████████████▏ │ │ RICKMANSWORTH │ THREE RIVERS │ 798 │ 704571 │ ██████████████ │ │ BANSTEAD │ REIGATE AND BANSTEAD │ 401 │ 701293 │ ██████████████ │ │ CHIGWELL │ EPPING FOREST │ 261 │ 701203 │ ██████████████ │ │ PINNER │ HARROW │ 528 │ 698885 │ █████████████▊ │ │ HASLEMERE │ WAVERLEY │ 280 │ 696659 │ █████████████▊ │ │ SLOUGH │ BUCKINGHAMSHIRE │ 396 │ 694917 │ █████████████▊ │ │ WALTON-ON-THAMES │ ELMBRIDGE │ 946 │ 692395 │ █████████████▋ │ │ READING │ SOUTH OXFORDSHIRE │ 318 │ 691988 │ █████████████▋ │ │ NORTHWOOD │ HILLINGDON │ 271 │ 690643 │ █████████████▋ │ │ FELTHAM │ HOUNSLOW │ 763 │ 688595 │ █████████████▋ │ │ ASHTEAD │ MOLE VALLEY │ 303 │ 687923 │ █████████████▋ │ │ BARNET │ BARNET │ 975 │ 686980 │ █████████████▋ │ │ WOKING │ SURREY HEATH │ 283 │ 686669 │ █████████████▋ │ │ MALMESBURY │ WILTSHIRE │ 323 │ 683324 │ █████████████▋ │ │ AMERSHAM │ BUCKINGHAMSHIRE │ 496 │ 680962 │ █████████████▌ │ │ CHISLEHURST │ BROMLEY │ 430 │ 680209 │ █████████████▌ │ │ HYTHE │ FOLKESTONE AND HYTHE │ 490 │ 676908 │ █████████████▌ │ │ MAYFIELD │ WEALDEN │ 101 │ 676210 │ █████████████▌ │ │ ASCOT │ BRACKNELL FOREST │ 168 │ 676004 │ █████████████▌ │ └──────────────────────┴────────────────────────┴──────┴─────────┴────────────────────────────────────────────────────────────────────┘  "},{"title":"Let's Speed Up Queries Using Projections​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#speedup-with-projections","content":"Projections allow to improve queries speed by storing pre-aggregated data. "},{"title":"Build a Projection​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#build-projection","content":"Create an aggregate projection by dimensions toYear(date), district, town: ALTER TABLE uk_price_paid ADD PROJECTION projection_by_year_district_town ( SELECT toYear(date), district, town, avg(price), sum(price), count() GROUP BY toYear(date), district, town );  Populate the projection for existing data (without it projection will be created for only newly inserted data): ALTER TABLE uk_price_paid MATERIALIZE PROJECTION projection_by_year_district_town SETTINGS mutations_sync = 1;  "},{"title":"Test Performance​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#test-performance","content":"Let's run the same 3 queries. Enable projections for selects: SET allow_experimental_projection_optimization = 1;  "},{"title":"Query 1. Average Price Per Year​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#average-price-projections","content":"Query: SELECT toYear(date) AS year, round(avg(price)) AS price, bar(price, 0, 1000000, 80) FROM uk_price_paid GROUP BY year ORDER BY year ASC;  Result: ┌─year─┬──price─┬─bar(round(avg(price)), 0, 1000000, 80)─┐ │ 1995 │ 67932 │ █████▍ │ │ 1996 │ 71505 │ █████▋ │ │ 1997 │ 78532 │ ██████▎ │ │ 1998 │ 85436 │ ██████▋ │ │ 1999 │ 96037 │ ███████▋ │ │ 2000 │ 107479 │ ████████▌ │ │ 2001 │ 118885 │ █████████▌ │ │ 2002 │ 137941 │ ███████████ │ │ 2003 │ 155889 │ ████████████▍ │ │ 2004 │ 178885 │ ██████████████▎ │ │ 2005 │ 189351 │ ███████████████▏ │ │ 2006 │ 203528 │ ████████████████▎ │ │ 2007 │ 219378 │ █████████████████▌ │ │ 2008 │ 217056 │ █████████████████▎ │ │ 2009 │ 213419 │ █████████████████ │ │ 2010 │ 236109 │ ██████████████████▊ │ │ 2011 │ 232805 │ ██████████████████▌ │ │ 2012 │ 238367 │ ███████████████████ │ │ 2013 │ 256931 │ ████████████████████▌ │ │ 2014 │ 279915 │ ██████████████████████▍ │ │ 2015 │ 297266 │ ███████████████████████▋ │ │ 2016 │ 313201 │ █████████████████████████ │ │ 2017 │ 346097 │ ███████████████████████████▋ │ │ 2018 │ 350116 │ ████████████████████████████ │ │ 2019 │ 351013 │ ████████████████████████████ │ │ 2020 │ 369420 │ █████████████████████████████▌ │ │ 2021 │ 386903 │ ██████████████████████████████▊ │ └──────┴────────┴────────────────────────────────────────┘  "},{"title":"Query 2. Average Price Per Year in London​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#average-price-london-projections","content":"Query: SELECT toYear(date) AS year, round(avg(price)) AS price, bar(price, 0, 2000000, 100) FROM uk_price_paid WHERE town = 'LONDON' GROUP BY year ORDER BY year ASC;  Result: ┌─year─┬───price─┬─bar(round(avg(price)), 0, 2000000, 100)───────────────┐ │ 1995 │ 109116 │ █████▍ │ │ 1996 │ 118667 │ █████▊ │ │ 1997 │ 136518 │ ██████▋ │ │ 1998 │ 152983 │ ███████▋ │ │ 1999 │ 180637 │ █████████ │ │ 2000 │ 215838 │ ██████████▋ │ │ 2001 │ 232994 │ ███████████▋ │ │ 2002 │ 263670 │ █████████████▏ │ │ 2003 │ 278394 │ █████████████▊ │ │ 2004 │ 304666 │ ███████████████▏ │ │ 2005 │ 322875 │ ████████████████▏ │ │ 2006 │ 356191 │ █████████████████▋ │ │ 2007 │ 404054 │ ████████████████████▏ │ │ 2008 │ 420741 │ █████████████████████ │ │ 2009 │ 427753 │ █████████████████████▍ │ │ 2010 │ 480306 │ ████████████████████████ │ │ 2011 │ 496274 │ ████████████████████████▋ │ │ 2012 │ 519442 │ █████████████████████████▊ │ │ 2013 │ 616212 │ ██████████████████████████████▋ │ │ 2014 │ 724154 │ ████████████████████████████████████▏ │ │ 2015 │ 792129 │ ███████████████████████████████████████▌ │ │ 2016 │ 843655 │ ██████████████████████████████████████████▏ │ │ 2017 │ 982642 │ █████████████████████████████████████████████████▏ │ │ 2018 │ 1016835 │ ██████████████████████████████████████████████████▋ │ │ 2019 │ 1042849 │ ████████████████████████████████████████████████████▏ │ │ 2020 │ 1011889 │ ██████████████████████████████████████████████████▌ │ │ 2021 │ 960343 │ ████████████████████████████████████████████████ │ └──────┴─────────┴───────────────────────────────────────────────────────┘  "},{"title":"Query 3. The Most Expensive Neighborhoods​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#most-expensive-neighborhoods-projections","content":"The condition (date &gt;= '2020-01-01') needs to be modified to match projection dimension (toYear(date) &gt;= 2020). Query: SELECT town, district, count() AS c, round(avg(price)) AS price, bar(price, 0, 5000000, 100) FROM uk_price_paid WHERE toYear(date) &gt;= 2020 GROUP BY town, district HAVING c &gt;= 100 ORDER BY price DESC LIMIT 100;  Result: ┌─town─────────────────┬─district───────────────┬────c─┬───price─┬─bar(round(avg(price)), 0, 5000000, 100)────────────────────────────┐ │ LONDON │ CITY OF WESTMINSTER │ 3606 │ 3280239 │ █████████████████████████████████████████████████████████████████▌ │ │ LONDON │ CITY OF LONDON │ 274 │ 3160502 │ ███████████████████████████████████████████████████████████████▏ │ │ LONDON │ KENSINGTON AND CHELSEA │ 2550 │ 2308478 │ ██████████████████████████████████████████████▏ │ │ LEATHERHEAD │ ELMBRIDGE │ 114 │ 1897407 │ █████████████████████████████████████▊ │ │ LONDON │ CAMDEN │ 3033 │ 1805404 │ ████████████████████████████████████ │ │ VIRGINIA WATER │ RUNNYMEDE │ 156 │ 1753247 │ ███████████████████████████████████ │ │ WINDLESHAM │ SURREY HEATH │ 108 │ 1677613 │ █████████████████████████████████▌ │ │ THORNTON HEATH │ CROYDON │ 546 │ 1671721 │ █████████████████████████████████▍ │ │ BARNET │ ENFIELD │ 124 │ 1505840 │ ██████████████████████████████ │ │ COBHAM │ ELMBRIDGE │ 387 │ 1237250 │ ████████████████████████▋ │ │ LONDON │ ISLINGTON │ 2668 │ 1236980 │ ████████████████████████▋ │ │ OXFORD │ SOUTH OXFORDSHIRE │ 321 │ 1220907 │ ████████████████████████▍ │ │ LONDON │ RICHMOND UPON THAMES │ 704 │ 1215551 │ ████████████████████████▎ │ │ LONDON │ HOUNSLOW │ 671 │ 1207493 │ ████████████████████████▏ │ │ ASCOT │ WINDSOR AND MAIDENHEAD │ 407 │ 1183299 │ ███████████████████████▋ │ │ BEACONSFIELD │ BUCKINGHAMSHIRE │ 330 │ 1175615 │ ███████████████████████▌ │ │ RICHMOND │ RICHMOND UPON THAMES │ 874 │ 1110444 │ ██████████████████████▏ │ │ LONDON │ HAMMERSMITH AND FULHAM │ 3086 │ 1053983 │ █████████████████████ │ │ SURBITON │ ELMBRIDGE │ 100 │ 1011800 │ ████████████████████▏ │ │ RADLETT │ HERTSMERE │ 283 │ 1011712 │ ████████████████████▏ │ │ SALCOMBE │ SOUTH HAMS │ 127 │ 1011624 │ ████████████████████▏ │ │ WEYBRIDGE │ ELMBRIDGE │ 655 │ 1007265 │ ████████████████████▏ │ │ ESHER │ ELMBRIDGE │ 485 │ 986581 │ ███████████████████▋ │ │ LEATHERHEAD │ GUILDFORD │ 202 │ 977320 │ ███████████████████▌ │ │ BURFORD │ WEST OXFORDSHIRE │ 111 │ 966893 │ ███████████████████▎ │ │ BROCKENHURST │ NEW FOREST │ 129 │ 956675 │ ███████████████████▏ │ │ HINDHEAD │ WAVERLEY │ 137 │ 953753 │ ███████████████████ │ │ GERRARDS CROSS │ BUCKINGHAMSHIRE │ 419 │ 951121 │ ███████████████████ │ │ EAST MOLESEY │ ELMBRIDGE │ 192 │ 936769 │ ██████████████████▋ │ │ CHALFONT ST GILES │ BUCKINGHAMSHIRE │ 146 │ 925515 │ ██████████████████▌ │ │ LONDON │ TOWER HAMLETS │ 4388 │ 918304 │ ██████████████████▎ │ │ OLNEY │ MILTON KEYNES │ 235 │ 910646 │ ██████████████████▏ │ │ HENLEY-ON-THAMES │ SOUTH OXFORDSHIRE │ 540 │ 902418 │ ██████████████████ │ │ LONDON │ SOUTHWARK │ 3885 │ 892997 │ █████████████████▋ │ │ KINGSTON UPON THAMES │ KINGSTON UPON THAMES │ 960 │ 885969 │ █████████████████▋ │ │ LONDON │ EALING │ 2658 │ 871755 │ █████████████████▍ │ │ CRANBROOK │ TUNBRIDGE WELLS │ 431 │ 862348 │ █████████████████▏ │ │ LONDON │ MERTON │ 2099 │ 859118 │ █████████████████▏ │ │ BELVEDERE │ BEXLEY │ 346 │ 842423 │ ████████████████▋ │ │ GUILDFORD │ WAVERLEY │ 143 │ 841277 │ ████████████████▋ │ │ HARPENDEN │ ST ALBANS │ 657 │ 841216 │ ████████████████▋ │ │ LONDON │ HACKNEY │ 3307 │ 837090 │ ████████████████▋ │ │ LONDON │ WANDSWORTH │ 6566 │ 832663 │ ████████████████▋ │ │ MAIDENHEAD │ BUCKINGHAMSHIRE │ 123 │ 824299 │ ████████████████▍ │ │ KINGS LANGLEY │ DACORUM │ 145 │ 821331 │ ████████████████▍ │ │ BERKHAMSTED │ DACORUM │ 543 │ 818415 │ ████████████████▎ │ │ GREAT MISSENDEN │ BUCKINGHAMSHIRE │ 226 │ 802807 │ ████████████████ │ │ BILLINGSHURST │ CHICHESTER │ 144 │ 797829 │ ███████████████▊ │ │ WOKING │ GUILDFORD │ 176 │ 793494 │ ███████████████▋ │ │ STOCKBRIDGE │ TEST VALLEY │ 178 │ 793269 │ ███████████████▋ │ │ EPSOM │ REIGATE AND BANSTEAD │ 172 │ 791862 │ ███████████████▋ │ │ TONBRIDGE │ TUNBRIDGE WELLS │ 360 │ 787876 │ ███████████████▋ │ │ TEDDINGTON │ RICHMOND UPON THAMES │ 595 │ 786492 │ ███████████████▋ │ │ TWICKENHAM │ RICHMOND UPON THAMES │ 1155 │ 786193 │ ███████████████▋ │ │ LYNDHURST │ NEW FOREST │ 102 │ 785593 │ ███████████████▋ │ │ LONDON │ LAMBETH │ 5228 │ 774574 │ ███████████████▍ │ │ LONDON │ BARNET │ 3955 │ 773259 │ ███████████████▍ │ │ OXFORD │ VALE OF WHITE HORSE │ 353 │ 772088 │ ███████████████▍ │ │ TONBRIDGE │ MAIDSTONE │ 305 │ 770740 │ ███████████████▍ │ │ LUTTERWORTH │ HARBOROUGH │ 538 │ 768634 │ ███████████████▎ │ │ WOODSTOCK │ WEST OXFORDSHIRE │ 140 │ 766037 │ ███████████████▎ │ │ MIDHURST │ CHICHESTER │ 257 │ 764815 │ ███████████████▎ │ │ MARLOW │ BUCKINGHAMSHIRE │ 327 │ 761876 │ ███████████████▏ │ │ LONDON │ NEWHAM │ 3237 │ 761784 │ ███████████████▏ │ │ ALDERLEY EDGE │ CHESHIRE EAST │ 178 │ 757318 │ ███████████████▏ │ │ LUTON │ CENTRAL BEDFORDSHIRE │ 212 │ 754283 │ ███████████████ │ │ PETWORTH │ CHICHESTER │ 154 │ 754220 │ ███████████████ │ │ ALRESFORD │ WINCHESTER │ 219 │ 752718 │ ███████████████ │ │ POTTERS BAR │ WELWYN HATFIELD │ 174 │ 748465 │ ██████████████▊ │ │ HASLEMERE │ CHICHESTER │ 128 │ 746907 │ ██████████████▊ │ │ TADWORTH │ REIGATE AND BANSTEAD │ 502 │ 743252 │ ██████████████▋ │ │ THAMES DITTON │ ELMBRIDGE │ 244 │ 741913 │ ██████████████▋ │ │ REIGATE │ REIGATE AND BANSTEAD │ 581 │ 738198 │ ██████████████▋ │ │ BOURNE END │ BUCKINGHAMSHIRE │ 138 │ 735190 │ ██████████████▋ │ │ SEVENOAKS │ SEVENOAKS │ 1156 │ 730018 │ ██████████████▌ │ │ OXTED │ TANDRIDGE │ 336 │ 729123 │ ██████████████▌ │ │ INGATESTONE │ BRENTWOOD │ 166 │ 728103 │ ██████████████▌ │ │ LONDON │ BRENT │ 2079 │ 720605 │ ██████████████▍ │ │ LONDON │ HARINGEY │ 3216 │ 717780 │ ██████████████▎ │ │ PURLEY │ CROYDON │ 575 │ 716108 │ ██████████████▎ │ │ WELWYN │ WELWYN HATFIELD │ 222 │ 710603 │ ██████████████▏ │ │ RICKMANSWORTH │ THREE RIVERS │ 798 │ 704571 │ ██████████████ │ │ BANSTEAD │ REIGATE AND BANSTEAD │ 401 │ 701293 │ ██████████████ │ │ CHIGWELL │ EPPING FOREST │ 261 │ 701203 │ ██████████████ │ │ PINNER │ HARROW │ 528 │ 698885 │ █████████████▊ │ │ HASLEMERE │ WAVERLEY │ 280 │ 696659 │ █████████████▊ │ │ SLOUGH │ BUCKINGHAMSHIRE │ 396 │ 694917 │ █████████████▊ │ │ WALTON-ON-THAMES │ ELMBRIDGE │ 946 │ 692395 │ █████████████▋ │ │ READING │ SOUTH OXFORDSHIRE │ 318 │ 691988 │ █████████████▋ │ │ NORTHWOOD │ HILLINGDON │ 271 │ 690643 │ █████████████▋ │ │ FELTHAM │ HOUNSLOW │ 763 │ 688595 │ █████████████▋ │ │ ASHTEAD │ MOLE VALLEY │ 303 │ 687923 │ █████████████▋ │ │ BARNET │ BARNET │ 975 │ 686980 │ █████████████▋ │ │ WOKING │ SURREY HEATH │ 283 │ 686669 │ █████████████▋ │ │ MALMESBURY │ WILTSHIRE │ 323 │ 683324 │ █████████████▋ │ │ AMERSHAM │ BUCKINGHAMSHIRE │ 496 │ 680962 │ █████████████▌ │ │ CHISLEHURST │ BROMLEY │ 430 │ 680209 │ █████████████▌ │ │ HYTHE │ FOLKESTONE AND HYTHE │ 490 │ 676908 │ █████████████▌ │ │ MAYFIELD │ WEALDEN │ 101 │ 676210 │ █████████████▌ │ │ ASCOT │ BRACKNELL FOREST │ 168 │ 676004 │ █████████████▌ │ └──────────────────────┴────────────────────────┴──────┴─────────┴────────────────────────────────────────────────────────────────────┘  "},{"title":"Summary​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#summary","content":"All 3 queries work much faster and read fewer rows. Query 1 no projection: 27 rows in set. Elapsed: 0.158 sec. Processed 26.32 million rows, 157.93 MB (166.57 million rows/s., 999.39 MB/s.) projection: 27 rows in set. Elapsed: 0.007 sec. Processed 105.96 thousand rows, 3.33 MB (14.58 million rows/s., 458.13 MB/s.) Query 2 no projection: 27 rows in set. Elapsed: 0.163 sec. Processed 26.32 million rows, 80.01 MB (161.75 million rows/s., 491.64 MB/s.) projection: 27 rows in set. Elapsed: 0.008 sec. Processed 105.96 thousand rows, 3.67 MB (13.29 million rows/s., 459.89 MB/s.) Query 3 no projection: 100 rows in set. Elapsed: 0.069 sec. Processed 26.32 million rows, 62.47 MB (382.13 million rows/s., 906.93 MB/s.) projection: 100 rows in set. Elapsed: 0.029 sec. Processed 8.08 thousand rows, 511.08 KB (276.06 thousand rows/s., 17.47 MB/s.)  "},{"title":"Test It in Playground​","type":1,"pageTitle":"UK Property Price Paid","url":"en/getting-started/example-datasets/uk-price-paid#playground","content":"The dataset is also available in the Online Playground. "},{"title":"MergeTree","type":0,"sectionRef":"#","url":"en/engines/table-engines/mergetree-family/mergetree","content":"","keywords":""},{"title":"Creating a Table​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-creating-a-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [TTL expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2] [TTL expr2], ... INDEX index_name1 expr1 TYPE type1(...) GRANULARITY value1, INDEX index_name2 expr2 TYPE type2(...) GRANULARITY value2, ... PROJECTION projection_name_1 (SELECT &lt;COLUMN LIST EXPR&gt; [GROUP BY] [ORDER BY]), PROJECTION projection_name_2 (SELECT &lt;COLUMN LIST EXPR&gt; [GROUP BY] [ORDER BY]) ) ENGINE = MergeTree() ORDER BY expr [PARTITION BY expr] [PRIMARY KEY expr] [SAMPLE BY expr] [TTL expr [DELETE|TO DISK 'xxx'|TO VOLUME 'xxx' [, ...] ] [WHERE conditions] [GROUP BY key_expr [SET v1 = aggr_func(v1) [, v2 = aggr_func(v2) ...]] ] ] [SETTINGS name=value, ...]  For a description of parameters, see the CREATE query description. "},{"title":"Query Clauses​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#mergetree-query-clauses","content":"ENGINE — Name and parameters of the engine. ENGINE = MergeTree(). The MergeTree engine does not have parameters. ORDER BY — The sorting key. A tuple of column names or arbitrary expressions. Example: ORDER BY (CounterID, EventDate). ClickHouse uses the sorting key as a primary key if the primary key is not defined obviously by the PRIMARY KEY clause. Use the ORDER BY tuple() syntax, if you do not need sorting. See Selecting the Primary Key. PARTITION BY — The partitioning key. Optional. In most cases you don't need partition key, and in most other cases you don't need partition key more granular than by months. Partitioning does not speed up queries (in contrast to the ORDER BY expression). You should never use too granular partitioning. Don't partition your data by client identifiers or names (instead make client identifier or name the first column in the ORDER BY expression). For partitioning by month, use the toYYYYMM(date_column) expression, where date_column is a column with a date of the type Date. The partition names here have the &quot;YYYYMM&quot; format. PRIMARY KEY — The primary key if it differs from the sorting key. Optional. By default the primary key is the same as the sorting key (which is specified by the ORDER BY clause). Thus in most cases it is unnecessary to specify a separate PRIMARY KEY clause. SAMPLE BY — An expression for sampling. Optional. If a sampling expression is used, the primary key must contain it. The result of a sampling expression must be an unsigned integer. Example: SAMPLE BY intHash32(UserID) ORDER BY (CounterID, EventDate, intHash32(UserID)). TTL — A list of rules specifying storage duration of rows and defining logic of automatic parts movement between disks and volumes. Optional. Expression must have one Date or DateTime column as a result. Example:TTL date + INTERVAL 1 DAY Type of the rule DELETE|TO DISK 'xxx'|TO VOLUME 'xxx'|GROUP BY specifies an action to be done with the part if the expression is satisfied (reaches current time): removal of expired rows, moving a part (if expression is satisfied for all rows in a part) to specified disk (TO DISK 'xxx') or to volume (TO VOLUME 'xxx'), or aggregating values in expired rows. Default type of the rule is removal (DELETE). List of multiple rules can be specified, but there should be no more than one DELETE rule. For more details, see TTL for columns and tables SETTINGS — Additional parameters that control the behavior of the MergeTree (optional): index_granularity — Maximum number of data rows between the marks of an index. Default value: 8192. See Data Storage.index_granularity_bytes — Maximum size of data granules in bytes. Default value: 10Mb. To restrict the granule size only by number of rows, set to 0 (not recommended). See Data Storage.min_index_granularity_bytes — Min allowed size of data granules in bytes. Default value: 1024b. To provide a safeguard against accidentally creating tables with very low index_granularity_bytes. See Data Storage.enable_mixed_granularity_parts — Enables or disables transitioning to control the granule size with the index_granularity_bytes setting. Before version 19.11, there was only the index_granularity setting for restricting granule size. The index_granularity_bytes setting improves ClickHouse performance when selecting data from tables with big rows (tens and hundreds of megabytes). If you have tables with big rows, you can enable this setting for the tables to improve the efficiency of SELECT queries.use_minimalistic_part_header_in_zookeeper — Storage method of the data parts headers in ZooKeeper. If use_minimalistic_part_header_in_zookeeper=1, then ZooKeeper stores less data. For more information, see the setting description in “Server configuration parameters”.min_merge_bytes_to_use_direct_io — The minimum data volume for merge operation that is required for using direct I/O access to the storage disk. When merging data parts, ClickHouse calculates the total storage volume of all the data to be merged. If the volume exceeds min_merge_bytes_to_use_direct_io bytes, ClickHouse reads and writes the data to the storage disk using the direct I/O interface (O_DIRECT option). If min_merge_bytes_to_use_direct_io = 0, then direct I/O is disabled. Default value: 10 * 1024 * 1024 * 1024 bytes.merge_with_ttl_timeout — Minimum delay in seconds before repeating a merge with delete TTL. Default value: 14400 seconds (4 hours).merge_with_recompression_ttl_timeout — Minimum delay in seconds before repeating a merge with recompression TTL. Default value: 14400 seconds (4 hours).try_fetch_recompressed_part_timeout — Timeout (in seconds) before starting merge with recompression. During this time ClickHouse tries to fetch recompressed part from replica which assigned this merge with recompression. Default value: 7200 seconds (2 hours).write_final_mark — Enables or disables writing the final index mark at the end of data part (after the last byte). Default value: 1. Don’t turn it off.merge_max_block_size — Maximum number of rows in block for merge operations. Default value: 8192.storage_policy — Storage policy. See Using Multiple Block Devices for Data Storage.min_bytes_for_wide_part, min_rows_for_wide_part — Minimum number of bytes/rows in a data part that can be stored in Wide format. You can set one, both or none of these settings. See Data Storage.max_parts_in_total — Maximum number of parts in all partitions.max_compress_block_size — Maximum size of blocks of uncompressed data before compressing for writing to a table. You can also specify this setting in the global settings (see max_compress_block_size setting). The value specified when table is created overrides the global value for this setting.min_compress_block_size — Minimum size of blocks of uncompressed data required for compression when writing the next mark. You can also specify this setting in the global settings (see min_compress_block_size setting). The value specified when table is created overrides the global value for this setting.max_partitions_to_read — Limits the maximum number of partitions that can be accessed in one query. You can also specify setting max_partitions_to_read in the global setting. Example of Sections Setting ENGINE MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID) SETTINGS index_granularity=8192  In the example, we set partitioning by month. We also set an expression for sampling as a hash by the user ID. This allows you to pseudorandomize the data in the table for each CounterID and EventDate. If you define a SAMPLE clause when selecting the data, ClickHouse will return an evenly pseudorandom data sample for a subset of users. The index_granularity setting can be omitted because 8192 is the default value. Deprecated Method for Creating a Table warning Do not use this method in new projects. If possible, switch old projects to the method described above. CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... ) ENGINE [=] MergeTree(date-column [, sampling_expression], (primary, key), index_granularity) MergeTree() Parameters date-column — The name of a column of the Date type. ClickHouse automatically creates partitions by month based on this column. The partition names are in the &quot;YYYYMM&quot; format.sampling_expression — An expression for sampling.(primary, key) — Primary key. Type: Tuple()index_granularity — The granularity of an index. The number of data rows between the “marks” of an index. The value 8192 is appropriate for most tasks. Example MergeTree(EventDate, intHash32(UserID), (CounterID, EventDate, intHash32(UserID)), 8192) The MergeTree engine is configured in the same way as in the example above for the main engine configuration method. "},{"title":"Data Storage​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#mergetree-data-storage","content":"A table consists of data parts sorted by primary key. When data is inserted in a table, separate data parts are created and each of them is lexicographically sorted by primary key. For example, if the primary key is (CounterID, Date), the data in the part is sorted by CounterID, and within each CounterID, it is ordered by Date. Data belonging to different partitions are separated into different parts. In the background, ClickHouse merges data parts for more efficient storage. Parts belonging to different partitions are not merged. The merge mechanism does not guarantee that all rows with the same primary key will be in the same data part. Data parts can be stored in Wide or Compact format. In Wide format each column is stored in a separate file in a filesystem, in Compact format all columns are stored in one file. Compact format can be used to increase performance of small and frequent inserts. Data storing format is controlled by the min_bytes_for_wide_part and min_rows_for_wide_part settings of the table engine. If the number of bytes or rows in a data part is less then the corresponding setting's value, the part is stored in Compact format. Otherwise it is stored in Wide format. If none of these settings is set, data parts are stored in Wide format. Each data part is logically divided into granules. A granule is the smallest indivisible data set that ClickHouse reads when selecting data. ClickHouse does not split rows or values, so each granule always contains an integer number of rows. The first row of a granule is marked with the value of the primary key for the row. For each data part, ClickHouse creates an index file that stores the marks. For each column, whether it’s in the primary key or not, ClickHouse also stores the same marks. These marks let you find data directly in column files. The granule size is restricted by the index_granularity and index_granularity_bytes settings of the table engine. The number of rows in a granule lays in the [1, index_granularity] range, depending on the size of the rows. The size of a granule can exceed index_granularity_bytes if the size of a single row is greater than the value of the setting. In this case, the size of the granule equals the size of the row. "},{"title":"Primary Keys and Indexes in Queries​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#primary-keys-and-indexes-in-queries","content":"Take the (CounterID, Date) primary key as an example. In this case, the sorting and index can be illustrated as follows:  Whole data: [---------------------------------------------] CounterID: [aaaaaaaaaaaaaaaaaabbbbcdeeeeeeeeeeeeefgggggggghhhhhhhhhiiiiiiiiikllllllll] Date: [1111111222222233331233211111222222333211111112122222223111112223311122333] Marks: | | | | | | | | | | | a,1 a,2 a,3 b,3 e,2 e,3 g,1 h,2 i,1 i,3 l,3 Marks numbers: 0 1 2 3 4 5 6 7 8 9 10  If the data query specifies: CounterID in ('a', 'h'), the server reads the data in the ranges of marks [0, 3) and [6, 8).CounterID IN ('a', 'h') AND Date = 3, the server reads the data in the ranges of marks [1, 3) and [7, 8).Date = 3, the server reads the data in the range of marks [1, 10]. The examples above show that it is always more effective to use an index than a full scan. A sparse index allows extra data to be read. When reading a single range of the primary key, up to index_granularity * 2 extra rows in each data block can be read. Sparse indexes allow you to work with a very large number of table rows, because in most cases, such indexes fit in the computer’s RAM. ClickHouse does not require a unique primary key. You can insert multiple rows with the same primary key. You can use Nullable-typed expressions in the PRIMARY KEY and ORDER BY clauses but it is strongly discouraged. To allow this feature, turn on the allow_nullable_key setting. The NULLS_LAST principle applies for NULL values in the ORDER BY clause. "},{"title":"Selecting the Primary Key​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#selecting-the-primary-key","content":"The number of columns in the primary key is not explicitly limited. Depending on the data structure, you can include more or fewer columns in the primary key. This may: Improve the performance of an index. If the primary key is (a, b), then adding another column c will improve the performance if the following conditions are met: There are queries with a condition on column c.Long data ranges (several times longer than the index_granularity) with identical values for (a, b) are common. In other words, when adding another column allows you to skip quite long data ranges. Improve data compression. ClickHouse sorts data by primary key, so the higher the consistency, the better the compression. Provide additional logic when merging data parts in the CollapsingMergeTree and SummingMergeTree engines. In this case it makes sense to specify the sorting key that is different from the primary key. A long primary key will negatively affect the insert performance and memory consumption, but extra columns in the primary key do not affect ClickHouse performance during SELECT queries. You can create a table without a primary key using the ORDER BY tuple() syntax. In this case, ClickHouse stores data in the order of inserting. If you want to save data order when inserting data by INSERT ... SELECT queries, set max_insert_threads = 1. To select data in the initial order, use single-threaded SELECT queries. "},{"title":"Choosing a Primary Key that Differs from the Sorting Key​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#choosing-a-primary-key-that-differs-from-the-sorting-key","content":"It is possible to specify a primary key (an expression with values that are written in the index file for each mark) that is different from the sorting key (an expression for sorting the rows in data parts). In this case the primary key expression tuple must be a prefix of the sorting key expression tuple. This feature is helpful when using the SummingMergeTree andAggregatingMergeTree table engines. In a common case when using these engines, the table has two types of columns: dimensions and measures. Typical queries aggregate values of measure columns with arbitrary GROUP BY and filtering by dimensions. Because SummingMergeTree and AggregatingMergeTree aggregate rows with the same value of the sorting key, it is natural to add all dimensions to it. As a result, the key expression consists of a long list of columns and this list must be frequently updated with newly added dimensions. In this case it makes sense to leave only a few columns in the primary key that will provide efficient range scans and add the remaining dimension columns to the sorting key tuple. ALTER of the sorting key is a lightweight operation because when a new column is simultaneously added to the table and to the sorting key, existing data parts do not need to be changed. Since the old sorting key is a prefix of the new sorting key and there is no data in the newly added column, the data is sorted by both the old and new sorting keys at the moment of table modification. "},{"title":"Use of Indexes and Partitions in Queries​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#use-of-indexes-and-partitions-in-queries","content":"For SELECT queries, ClickHouse analyzes whether an index can be used. An index can be used if the WHERE/PREWHERE clause has an expression (as one of the conjunction elements, or entirely) that represents an equality or inequality comparison operation, or if it has IN or LIKE with a fixed prefix on columns or expressions that are in the primary key or partitioning key, or on certain partially repetitive functions of these columns, or logical relationships of these expressions. Thus, it is possible to quickly run queries on one or many ranges of the primary key. In this example, queries will be fast when run for a specific tracking tag, for a specific tag and date range, for a specific tag and date, for multiple tags with a date range, and so on. Let’s look at the engine configured as follows:  ENGINE MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate) SETTINGS index_granularity=8192  In this case, in queries: SELECT count() FROM table WHERE EventDate = toDate(now()) AND CounterID = 34 SELECT count() FROM table WHERE EventDate = toDate(now()) AND (CounterID = 34 OR CounterID = 42) SELECT count() FROM table WHERE ((EventDate &gt;= toDate('2014-01-01') AND EventDate &lt;= toDate('2014-01-31')) OR EventDate = toDate('2014-05-01')) AND CounterID IN (101500, 731962, 160656) AND (CounterID = 101500 OR EventDate != toDate('2014-05-01'))  ClickHouse will use the primary key index to trim improper data and the monthly partitioning key to trim partitions that are in improper date ranges. The queries above show that the index is used even for complex expressions. Reading from the table is organized so that using the index can’t be slower than a full scan. In the example below, the index can’t be used. SELECT count() FROM table WHERE CounterID = 34 OR URL LIKE '%upyachka%'  To check whether ClickHouse can use the index when running a query, use the settings force_index_by_date and force_primary_key. The key for partitioning by month allows reading only those data blocks which contain dates from the proper range. In this case, the data block may contain data for many dates (up to an entire month). Within a block, data is sorted by primary key, which might not contain the date as the first column. Because of this, using a query with only a date condition that does not specify the primary key prefix will cause more data to be read than for a single date. "},{"title":"Use of Index for Partially-monotonic Primary Keys​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#use-of-index-for-partially-monotonic-primary-keys","content":"Consider, for example, the days of the month. They form a monotonic sequence for one month, but not monotonic for more extended periods. This is a partially-monotonic sequence. If a user creates the table with partially-monotonic primary key, ClickHouse creates a sparse index as usual. When a user selects data from this kind of table, ClickHouse analyzes the query conditions. If the user wants to get data between two marks of the index and both these marks fall within one month, ClickHouse can use the index in this particular case because it can calculate the distance between the parameters of a query and index marks. ClickHouse cannot use an index if the values of the primary key in the query parameter range do not represent a monotonic sequence. In this case, ClickHouse uses the full scan method. ClickHouse uses this logic not only for days of the month sequences, but for any primary key that represents a partially-monotonic sequence. "},{"title":"Data Skipping Indexes​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-data_skipping-indexes","content":"The index declaration is in the columns section of the CREATE query. INDEX index_name expr TYPE type(...) GRANULARITY granularity_value  For tables from the *MergeTree family, data skipping indices can be specified. These indices aggregate some information about the specified expression on blocks, which consist of granularity_value granules (the size of the granule is specified using the index_granularity setting in the table engine). Then these aggregates are used in SELECT queries for reducing the amount of data to read from the disk by skipping big blocks of data where the where query cannot be satisfied. Example CREATE TABLE table_name ( u64 UInt64, i32 Int32, s String, ... INDEX a (u64 * i32, s) TYPE minmax GRANULARITY 3, INDEX b (u64 * length(s)) TYPE set(1000) GRANULARITY 4 ) ENGINE = MergeTree() ...  Indices from the example can be used by ClickHouse to reduce the amount of data to read from disk in the following queries: SELECT count() FROM table WHERE s &lt; 'z' SELECT count() FROM table WHERE u64 * i32 == 10 AND u64 * length(s) &gt;= 1234  Available Types of Indices​ minmax Stores extremes of the specified expression (if the expression is tuple, then it stores extremes for each element of tuple), uses stored info for skipping blocks of data like the primary key. set(max_rows) Stores unique values of the specified expression (no more than max_rows rows, max_rows=0 means “no limits”). Uses the values to check if the WHERE expression is not satisfiable on a block of data. ngrambf_v1(n, size_of_bloom_filter_in_bytes, number_of_hash_functions, random_seed) Stores a Bloom filter that contains all ngrams from a block of data. Works only with datatypes: String, FixedString and Map. Can be used for optimization of EQUALS, LIKE and IN expressions. n — ngram size,size_of_bloom_filter_in_bytes — Bloom filter size in bytes (you can use large values here, for example, 256 or 512, because it can be compressed well).number_of_hash_functions — The number of hash functions used in the Bloom filter.random_seed — The seed for Bloom filter hash functions. tokenbf_v1(size_of_bloom_filter_in_bytes, number_of_hash_functions, random_seed) The same as ngrambf_v1, but stores tokens instead of ngrams. Tokens are sequences separated by non-alphanumeric characters. bloom_filter([false_positive]) — Stores a Bloom filter for the specified columns. The optional false_positive parameter is the probability of receiving a false positive response from the filter. Possible values: (0, 1). Default value: 0.025. Supported data types: Int*, UInt*, Float*, Enum, Date, DateTime, String, FixedString, Array, LowCardinality, Nullable, UUID, Map. For Map data type client can specify if index should be created for keys or values using mapKeys or mapValues function. The following functions can use the filter: equals, notEquals, in, notIn, has, hasAny, hasAll. Example of index creation for Map data type INDEX map_key_index mapKeys(map_column) TYPE bloom_filter GRANULARITY 1 INDEX map_key_index mapValues(map_column) TYPE bloom_filter GRANULARITY 1  INDEX sample_index (u64 * length(s)) TYPE minmax GRANULARITY 4 INDEX sample_index2 (u64 * length(str), i32 + f64 * 100, date, str) TYPE set(100) GRANULARITY 4 INDEX sample_index3 (lower(str), str) TYPE ngrambf_v1(3, 256, 2, 0) GRANULARITY 4  Functions Support​ Conditions in the WHERE clause contains calls of the functions that operate with columns. If the column is a part of an index, ClickHouse tries to use this index when performing the functions. ClickHouse supports different subsets of functions for using indexes. The set index can be used with all functions. Function subsets for other indexes are shown in the table below. Function (operator) / Index\tprimary key\tminmax\tngrambf_v1\ttokenbf_v1\tbloom_filterequals (=, ==)\t✔\t✔\t✔\t✔\t✔ notEquals(!=, &lt;&gt;)\t✔\t✔\t✔\t✔\t✔ like\t✔\t✔\t✔\t✔\t✗ notLike\t✔\t✔\t✔\t✔\t✗ startsWith\t✔\t✔\t✔\t✔\t✗ endsWith\t✗\t✗\t✔\t✔\t✗ multiSearchAny\t✗\t✗\t✔\t✗\t✗ in\t✔\t✔\t✔\t✔\t✔ notIn\t✔\t✔\t✔\t✔\t✔ less (&lt;)\t✔\t✔\t✗\t✗\t✗ greater (&gt;)\t✔\t✔\t✗\t✗\t✗ lessOrEquals (&lt;=)\t✔\t✔\t✗\t✗\t✗ greaterOrEquals (&gt;=)\t✔\t✔\t✗\t✗\t✗ empty\t✔\t✔\t✗\t✗\t✗ notEmpty\t✔\t✔\t✗\t✗\t✗ hasToken\t✗\t✗\t✗\t✔\t✗ Functions with a constant argument that is less than ngram size can’t be used by ngrambf_v1 for query optimization. note Bloom filters can have false positive matches, so the ngrambf_v1, tokenbf_v1, and bloom_filter indexes can not be used for optimizing queries where the result of a function is expected to be false. For example: Can be optimized: s LIKE '%test%'NOT s NOT LIKE '%test%'s = 1NOT s != 1startsWith(s, 'test') Can not be optimized: NOT s LIKE '%test%'s NOT LIKE '%test%'NOT s = 1s != 1NOT startsWith(s, 'test') "},{"title":"Projections​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#projections","content":"Projections are like materialized views but defined in part-level. It provides consistency guarantees along with automatic usage in queries. Projections are an experimental feature. To enable them you must set the allow_experimental_projection_optimization to 1. See also the force_optimize_projection setting. Projections are not supported in the SELECT statements with the FINAL modifier. "},{"title":"Projection Query​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#projection-query","content":"A projection query is what defines a projection. It implicitly selects data from the parent table.Syntax SELECT &lt;column list expr&gt; [GROUP BY] &lt;group keys expr&gt; [ORDER BY] &lt;expr&gt;  Projections can be modified or dropped with the ALTER statement. "},{"title":"Projection Storage​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#projection-storage","content":"Projections are stored inside the part directory. It's similar to an index but contains a subdirectory that stores an anonymous MergeTree table's part. The table is induced by the definition query of the projection. If there is a GROUP BY clause, the underlying storage engine becomes AggregatingMergeTree, and all aggregate functions are converted to AggregateFunction. If there is an ORDER BY clause, the MergeTree table uses it as its primary key expression. During the merge process the projection part is merged via its storage's merge routine. The checksum of the parent table's part is combined with the projection's part. Other maintenance jobs are similar to skip indices. "},{"title":"Query Analysis​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#projection-query-analysis","content":"Check if the projection can be used to answer the given query, that is, it generates the same answer as querying the base table.Select the best feasible match, which contains the least granules to read.The query pipeline which uses projections will be different from the one that uses the original parts. If the projection is absent in some parts, we can add the pipeline to &quot;project&quot; it on the fly. "},{"title":"Concurrent Data Access​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#concurrent-data-access","content":"For concurrent table access, we use multi-versioning. In other words, when a table is simultaneously read and updated, data is read from a set of parts that is current at the time of the query. There are no lengthy locks. Inserts do not get in the way of read operations. Reading from a table is automatically parallelized. "},{"title":"TTL for Columns and Tables​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-ttl","content":"Determines the lifetime of values. The TTL clause can be set for the whole table and for each individual column. Table-level TTL can also specify the logic of automatic moving data between disks and volumes, or recompressing parts where all the data has been expired. Expressions must evaluate to Date or DateTime data type. Syntax Setting time-to-live for a column: TTL time_column TTL time_column + interval  To define interval, use time interval operators, for example: TTL date_time + INTERVAL 1 MONTH TTL date_time + INTERVAL 15 HOUR  "},{"title":"Column TTL​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#mergetree-column-ttl","content":"When the values in the column expire, ClickHouse replaces them with the default values for the column data type. If all the column values in the data part expire, ClickHouse deletes this column from the data part in a filesystem. The TTL clause can’t be used for key columns. Examples Creating a table with TTL: CREATE TABLE example_table ( d DateTime, a Int TTL d + INTERVAL 1 MONTH, b Int TTL d + INTERVAL 1 MONTH, c String ) ENGINE = MergeTree PARTITION BY toYYYYMM(d) ORDER BY d;  Adding TTL to a column of an existing table ALTER TABLE example_table MODIFY COLUMN c String TTL d + INTERVAL 1 DAY;  Altering TTL of the column ALTER TABLE example_table MODIFY COLUMN c String TTL d + INTERVAL 1 MONTH;  "},{"title":"Table TTL​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#mergetree-table-ttl","content":"Table can have an expression for removal of expired rows, and multiple expressions for automatic move of parts between disks or volumes. When rows in the table expire, ClickHouse deletes all corresponding rows. For parts moving or recompressing, all rows of a part must satisfy the TTL expression criteria. TTL expr [DELETE|RECOMPRESS codec_name1|TO DISK 'xxx'|TO VOLUME 'xxx'][, DELETE|RECOMPRESS codec_name2|TO DISK 'aaa'|TO VOLUME 'bbb'] ... [WHERE conditions] [GROUP BY key_expr [SET v1 = aggr_func(v1) [, v2 = aggr_func(v2) ...]] ]  Type of TTL rule may follow each TTL expression. It affects an action which is to be done once the expression is satisfied (reaches current time): DELETE - delete expired rows (default action);RECOMPRESS codec_name - recompress data part with the codec_name;TO DISK 'aaa' - move part to the disk aaa;TO VOLUME 'bbb' - move part to the disk bbb;GROUP BY - aggregate expired rows. With WHERE clause you may specify which of the expired rows to delete or aggregate (it cannot be applied to moves or recompression). GROUP BY expression must be a prefix of the table primary key. If a column is not part of the GROUP BY expression and is not set explicitly in the SET clause, in result row it contains an occasional value from the grouped rows (as if aggregate function any is applied to it). Examples Creating a table with TTL: CREATE TABLE example_table ( d DateTime, a Int ) ENGINE = MergeTree PARTITION BY toYYYYMM(d) ORDER BY d TTL d + INTERVAL 1 MONTH [DELETE], d + INTERVAL 1 WEEK TO VOLUME 'aaa', d + INTERVAL 2 WEEK TO DISK 'bbb';  Altering TTL of the table: ALTER TABLE example_table MODIFY TTL d + INTERVAL 1 DAY;  Creating a table, where the rows are expired after one month. The expired rows where dates are Mondays are deleted: CREATE TABLE table_with_where ( d DateTime, a Int ) ENGINE = MergeTree PARTITION BY toYYYYMM(d) ORDER BY d TTL d + INTERVAL 1 MONTH DELETE WHERE toDayOfWeek(d) = 1;  Creating a table, where expired rows are recompressed: CREATE TABLE table_for_recompression ( d DateTime, key UInt64, value String ) ENGINE MergeTree() ORDER BY tuple() PARTITION BY key TTL d + INTERVAL 1 MONTH RECOMPRESS CODEC(ZSTD(17)), d + INTERVAL 1 YEAR RECOMPRESS CODEC(LZ4HC(10)) SETTINGS min_rows_for_wide_part = 0, min_bytes_for_wide_part = 0;  Creating a table, where expired rows are aggregated. In result rows x contains the maximum value accross the grouped rows, y — the minimum value, and d — any occasional value from grouped rows. CREATE TABLE table_for_aggregation ( d DateTime, k1 Int, k2 Int, x Int, y Int ) ENGINE = MergeTree ORDER BY (k1, k2) TTL d + INTERVAL 1 MONTH GROUP BY k1, k2 SET x = max(x), y = min(y);  "},{"title":"Removing Expired Data​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#mergetree-removing-expired-data","content":"Data with an expired TTL is removed when ClickHouse merges data parts. When ClickHouse detects that data is expired, it performs an off-schedule merge. To control the frequency of such merges, you can set merge_with_ttl_timeout. If the value is too low, it will perform many off-schedule merges that may consume a lot of resources. If you perform the SELECT query between merges, you may get expired data. To avoid it, use the OPTIMIZE query before SELECT. See Also ttl_only_drop_parts setting "},{"title":"Using Multiple Block Devices for Data Storage​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-multiple-volumes","content":""},{"title":"Introduction​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#introduction","content":"MergeTree family table engines can store data on multiple block devices. For example, it can be useful when the data of a certain table are implicitly split into “hot” and “cold”. The most recent data is regularly requested but requires only a small amount of space. On the contrary, the fat-tailed historical data is requested rarely. If several disks are available, the “hot” data may be located on fast disks (for example, NVMe SSDs or in memory), while the “cold” data - on relatively slow ones (for example, HDD). Data part is the minimum movable unit for MergeTree-engine tables. The data belonging to one part are stored on one disk. Data parts can be moved between disks in the background (according to user settings) as well as by means of the ALTER queries. "},{"title":"Terms​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#terms","content":"Disk — Block device mounted to the filesystem.Default disk — Disk that stores the path specified in the path server setting.Volume — Ordered set of equal disks (similar to JBOD).Storage policy — Set of volumes and the rules for moving data between them. The names given to the described entities can be found in the system tables, system.storage_policies and system.disks. To apply one of the configured storage policies for a table, use the storage_policy setting of MergeTree-engine family tables. "},{"title":"Configuration​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-multiple-volumes_configure","content":"Disks, volumes and storage policies should be declared inside the &lt;storage_configuration&gt; tag either in the main file config.xml or in a distinct file in the config.d directory. Configuration structure: &lt;storage_configuration&gt; &lt;disks&gt; &lt;disk_name_1&gt; &lt;!-- disk name --&gt; &lt;path&gt;/mnt/fast_ssd/clickhouse/&lt;/path&gt; &lt;/disk_name_1&gt; &lt;disk_name_2&gt; &lt;path&gt;/mnt/hdd1/clickhouse/&lt;/path&gt; &lt;keep_free_space_bytes&gt;10485760&lt;/keep_free_space_bytes&gt; &lt;/disk_name_2&gt; &lt;disk_name_3&gt; &lt;path&gt;/mnt/hdd2/clickhouse/&lt;/path&gt; &lt;keep_free_space_bytes&gt;10485760&lt;/keep_free_space_bytes&gt; &lt;/disk_name_3&gt; ... &lt;/disks&gt; ... &lt;/storage_configuration&gt;  Tags: &lt;disk_name_N&gt; — Disk name. Names must be different for all disks.path — path under which a server will store data (data and shadow folders), should be terminated with ‘/’.keep_free_space_bytes — the amount of free disk space to be reserved. The order of the disk definition is not important. Storage policies configuration markup: &lt;storage_configuration&gt; ... &lt;policies&gt; &lt;policy_name_1&gt; &lt;volumes&gt; &lt;volume_name_1&gt; &lt;disk&gt;disk_name_from_disks_configuration&lt;/disk&gt; &lt;max_data_part_size_bytes&gt;1073741824&lt;/max_data_part_size_bytes&gt; &lt;/volume_name_1&gt; &lt;volume_name_2&gt; &lt;!-- configuration --&gt; &lt;/volume_name_2&gt; &lt;!-- more volumes --&gt; &lt;/volumes&gt; &lt;move_factor&gt;0.2&lt;/move_factor&gt; &lt;/policy_name_1&gt; &lt;policy_name_2&gt; &lt;!-- configuration --&gt; &lt;/policy_name_2&gt; &lt;!-- more policies --&gt; &lt;/policies&gt; ... &lt;/storage_configuration&gt;  Tags: policy_name_N — Policy name. Policy names must be unique.volume_name_N — Volume name. Volume names must be unique.disk — a disk within a volume.max_data_part_size_bytes — the maximum size of a part that can be stored on any of the volume’s disks. If the a size of a merged part estimated to be bigger than max_data_part_size_bytes then this part will be written to a next volume. Basically this feature allows to keep new/small parts on a hot (SSD) volume and move them to a cold (HDD) volume when they reach large size. Do not use this setting if your policy has only one volume.move_factor — when the amount of available space gets lower than this factor, data automatically starts to move on the next volume if any (by default, 0.1). ClickHouse sorts existing parts by size from largest to smallest (in descending order) and selects parts with the total size that is sufficient to meet the move_factor condition. If the total size of all parts is insufficient, all parts will be moved. prefer_not_to_merge — Disables merging of data parts on this volume. When this setting is enabled, merging data on this volume is not allowed. This allows controlling how ClickHouse works with slow disks. Cofiguration examples: &lt;storage_configuration&gt; ... &lt;policies&gt; &lt;hdd_in_order&gt; &lt;!-- policy name --&gt; &lt;volumes&gt; &lt;single&gt; &lt;!-- volume name --&gt; &lt;disk&gt;disk1&lt;/disk&gt; &lt;disk&gt;disk2&lt;/disk&gt; &lt;/single&gt; &lt;/volumes&gt; &lt;/hdd_in_order&gt; &lt;moving_from_ssd_to_hdd&gt; &lt;volumes&gt; &lt;hot&gt; &lt;disk&gt;fast_ssd&lt;/disk&gt; &lt;max_data_part_size_bytes&gt;1073741824&lt;/max_data_part_size_bytes&gt; &lt;/hot&gt; &lt;cold&gt; &lt;disk&gt;disk1&lt;/disk&gt; &lt;/cold&gt; &lt;/volumes&gt; &lt;move_factor&gt;0.2&lt;/move_factor&gt; &lt;/moving_from_ssd_to_hdd&gt; &lt;small_jbod_with_external_no_merges&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;jbod1&lt;/disk&gt; &lt;/main&gt; &lt;external&gt; &lt;disk&gt;external&lt;/disk&gt; &lt;prefer_not_to_merge&gt;true&lt;/prefer_not_to_merge&gt; &lt;/external&gt; &lt;/volumes&gt; &lt;/small_jbod_with_external_no_merges&gt; &lt;/policies&gt; ... &lt;/storage_configuration&gt;  In given example, the hdd_in_order policy implements the round-robin approach. Thus this policy defines only one volume (single), the data parts are stored on all its disks in circular order. Such policy can be quite useful if there are several similar disks are mounted to the system, but RAID is not configured. Keep in mind that each individual disk drive is not reliable and you might want to compensate it with replication factor of 3 or more. If there are different kinds of disks available in the system, moving_from_ssd_to_hdd policy can be used instead. The volume hot consists of an SSD disk (fast_ssd), and the maximum size of a part that can be stored on this volume is 1GB. All the parts with the size larger than 1GB will be stored directly on the cold volume, which contains an HDD disk disk1. Also, once the disk fast_ssd gets filled by more than 80%, data will be transferred to the disk1 by a background process. The order of volume enumeration within a storage policy is important. Once a volume is overfilled, data are moved to the next one. The order of disk enumeration is important as well because data are stored on them in turns. When creating a table, one can apply one of the configured storage policies to it: CREATE TABLE table_with_non_default_policy ( EventDate Date, OrderID UInt64, BannerID UInt64, SearchPhrase String ) ENGINE = MergeTree ORDER BY (OrderID, BannerID) PARTITION BY toYYYYMM(EventDate) SETTINGS storage_policy = 'moving_from_ssd_to_hdd'  The default storage policy implies using only one volume, which consists of only one disk given in &lt;path&gt;. You could change storage policy after table creation with [ALTER TABLE ... MODIFY SETTING] query, new policy should include all old disks and volumes with same names. The number of threads performing background moves of data parts can be changed by background_move_pool_size setting. "},{"title":"Details​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#details","content":"In the case of MergeTree tables, data is getting to disk in different ways: As a result of an insert (INSERT query).During background merges and mutations.When downloading from another replica.As a result of partition freezing ALTER TABLE … FREEZE PARTITION. In all these cases except for mutations and partition freezing, a part is stored on a volume and a disk according to the given storage policy: The first volume (in the order of definition) that has enough disk space for storing a part (unreserved_space &gt; current_part_size) and allows for storing parts of a given size (max_data_part_size_bytes &gt; current_part_size) is chosen.Within this volume, that disk is chosen that follows the one, which was used for storing the previous chunk of data, and that has free space more than the part size (unreserved_space - keep_free_space_bytes &gt; current_part_size). Under the hood, mutations and partition freezing make use of hard links. Hard links between different disks are not supported, therefore in such cases the resulting parts are stored on the same disks as the initial ones. In the background, parts are moved between volumes on the basis of the amount of free space (move_factor parameter) according to the order the volumes are declared in the configuration file. Data is never transferred from the last one and into the first one. One may use system tables system.part_log (field type = MOVE_PART) and system.parts (fields path and disk) to monitor background moves. Also, the detailed information can be found in server logs. User can force moving a part or a partition from one volume to another using the query ALTER TABLE … MOVE PART|PARTITION … TO VOLUME|DISK …, all the restrictions for background operations are taken into account. The query initiates a move on its own and does not wait for background operations to be completed. User will get an error message if not enough free space is available or if any of the required conditions are not met. Moving data does not interfere with data replication. Therefore, different storage policies can be specified for the same table on different replicas. After the completion of background merges and mutations, old parts are removed only after a certain amount of time (old_parts_lifetime). During this time, they are not moved to other volumes or disks. Therefore, until the parts are finally removed, they are still taken into account for evaluation of the occupied disk space. User can assign new big parts to different disks of a JBOD volume in a balanced way using the min_bytes_to_rebalance_partition_over_jbod setting. "},{"title":"Using S3 for Data Storage​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-s3","content":"MergeTree family table engines can store data to S3 using a disk with type s3. This feature is under development and not ready for production. There are known drawbacks such as very low performance. Configuration markup: &lt;storage_configuration&gt; ... &lt;disks&gt; &lt;s3&gt; &lt;type&gt;s3&lt;/type&gt; &lt;endpoint&gt;https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/root-path/&lt;/endpoint&gt; &lt;access_key_id&gt;your_access_key_id&lt;/access_key_id&gt; &lt;secret_access_key&gt;your_secret_access_key&lt;/secret_access_key&gt; &lt;region&gt;&lt;/region&gt; &lt;server_side_encryption_customer_key_base64&gt;your_base64_encoded_customer_key&lt;/server_side_encryption_customer_key_base64&gt; &lt;proxy&gt; &lt;uri&gt;http://proxy1&lt;/uri&gt; &lt;uri&gt;http://proxy2&lt;/uri&gt; &lt;/proxy&gt; &lt;connect_timeout_ms&gt;10000&lt;/connect_timeout_ms&gt; &lt;request_timeout_ms&gt;5000&lt;/request_timeout_ms&gt; &lt;retry_attempts&gt;10&lt;/retry_attempts&gt; &lt;single_read_retries&gt;4&lt;/single_read_retries&gt; &lt;min_bytes_for_seek&gt;1000&lt;/min_bytes_for_seek&gt; &lt;metadata_path&gt;/var/lib/clickhouse/disks/s3/&lt;/metadata_path&gt; &lt;cache_enabled&gt;true&lt;/cache_enabled&gt; &lt;cache_path&gt;/var/lib/clickhouse/disks/s3/cache/&lt;/cache_path&gt; &lt;skip_access_check&gt;false&lt;/skip_access_check&gt; &lt;/s3&gt; &lt;/disks&gt; ... &lt;/storage_configuration&gt;  Required parameters: endpoint — S3 endpoint URL in path or virtual hosted styles. Endpoint URL should contain a bucket and root path to store data.access_key_id — S3 access key id.secret_access_key — S3 secret access key. Optional parameters: region — S3 region name.use_environment_credentials — Reads AWS credentials from the Environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN if they exist. Default value is false.use_insecure_imds_request — If set to true, S3 client will use insecure IMDS request while obtaining credentials from Amazon EC2 metadata. Default value is false.proxy — Proxy configuration for S3 endpoint. Each uri element inside proxy block should contain a proxy URL.connect_timeout_ms — Socket connect timeout in milliseconds. Default value is 10 seconds.request_timeout_ms — Request timeout in milliseconds. Default value is 5 seconds.retry_attempts — Number of retry attempts in case of failed request. Default value is 10.single_read_retries — Number of retry attempts in case of connection drop during read. Default value is 4.min_bytes_for_seek — Minimal number of bytes to use seek operation instead of sequential read. Default value is 1 Mb.metadata_path — Path on local FS to store metadata files for S3. Default value is /var/lib/clickhouse/disks/&lt;disk_name&gt;/.cache_enabled — Allows to cache mark and index files on local FS. Default value is true.cache_path — Path on local FS where to store cached mark and index files. Default value is /var/lib/clickhouse/disks/&lt;disk_name&gt;/cache/.skip_access_check — If true, disk access checks will not be performed on disk start-up. Default value is false.server_side_encryption_customer_key_base64 — If specified, required headers for accessing S3 objects with SSE-C encryption will be set. S3 disk can be configured as main or cold storage: &lt;storage_configuration&gt; ... &lt;disks&gt; &lt;s3&gt; &lt;type&gt;s3&lt;/type&gt; &lt;endpoint&gt;https://clickhouse-public-datasets.s3.amazonaws.com/my-bucket/root-path/&lt;/endpoint&gt; &lt;access_key_id&gt;your_access_key_id&lt;/access_key_id&gt; &lt;secret_access_key&gt;your_secret_access_key&lt;/secret_access_key&gt; &lt;/s3&gt; &lt;/disks&gt; &lt;policies&gt; &lt;s3_main&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;s3&lt;/disk&gt; &lt;/main&gt; &lt;/volumes&gt; &lt;/s3_main&gt; &lt;s3_cold&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;default&lt;/disk&gt; &lt;/main&gt; &lt;external&gt; &lt;disk&gt;s3&lt;/disk&gt; &lt;/external&gt; &lt;/volumes&gt; &lt;move_factor&gt;0.2&lt;/move_factor&gt; &lt;/s3_cold&gt; &lt;/policies&gt; ... &lt;/storage_configuration&gt;  In case of cold option a data can be moved to S3 if local disk free size will be smaller than move_factor * disk_size or by TTL move rule. "},{"title":"Using Azure Blob Storage for Data Storage​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#table_engine-mergetree-azure-blob-storage","content":"MergeTree family table engines can store data to Azure Blob Storage using a disk with type azure_blob_storage. As of February 2022, this feature is still a fresh addition, so expect that some Azure Blob Storage functionalities might be unimplemented. Configuration markup: &lt;storage_configuration&gt; ... &lt;disks&gt; &lt;blob_storage_disk&gt; &lt;type&gt;azure_blob_storage&lt;/type&gt; &lt;storage_account_url&gt;http://account.blob.core.windows.net&lt;/storage_account_url&gt; &lt;container_name&gt;container&lt;/container_name&gt; &lt;account_name&gt;account&lt;/account_name&gt; &lt;account_key&gt;pass123&lt;/account_key&gt; &lt;metadata_path&gt;/var/lib/clickhouse/disks/blob_storage_disk/&lt;/metadata_path&gt; &lt;cache_enabled&gt;true&lt;/cache_enabled&gt; &lt;cache_path&gt;/var/lib/clickhouse/disks/blob_storage_disk/cache/&lt;/cache_path&gt; &lt;skip_access_check&gt;false&lt;/skip_access_check&gt; &lt;/blob_storage_disk&gt; &lt;/disks&gt; ... &lt;/storage_configuration&gt;  Connection parameters: storage_account_url - Required, Azure Blob Storage account URL, like http://account.blob.core.windows.net or http://azurite1:10000/devstoreaccount1.container_name - Target container name, defaults to default-container.container_already_exists - If set to false, a new container container_name is created in the storage account, if set to true, disk connects to the container directly, and if left unset, disk connects to the account, checks if the container container_name exists, and creates it if it doesn't exist yet. Authentication parameters (the disk will try all available methods and Managed Identity Credential): connection_string - For authentication using a connection string.account_name and account_key - For authentication using Shared Key. Limit parameters (mainly for internal usage): max_single_part_upload_size - Limits the size of a single block upload to Blob Storage.min_bytes_for_seek - Limits the size of a seekable region.max_single_read_retries - Limits the number of attempts to read a chunk of data from Blob Storage.max_single_download_retries - Limits the number of attempts to download a readable buffer from Blob Storage.thread_pool_size - Limits the number of threads with which IDiskRemote is instantiated. Other parameters: metadata_path - Path on local FS to store metadata files for Blob Storage. Default value is /var/lib/clickhouse/disks/&lt;disk_name&gt;/.cache_enabled - Allows to cache mark and index files on local FS. Default value is true.cache_path - Path on local FS where to store cached mark and index files. Default value is /var/lib/clickhouse/disks/&lt;disk_name&gt;/cache/.skip_access_check - If true, disk access checks will not be performed on disk start-up. Default value is false. Examples of working configurations can be found in integration tests directory (see e.g. test_merge_tree_azure_blob_storage or test_azure_blob_storage_zero_copy_replication). "},{"title":"Virtual Columns​","type":1,"pageTitle":"MergeTree","url":"en/engines/table-engines/mergetree-family/mergetree#virtual-columns","content":"_part — Name of a part._part_index — Sequential index of the part in the query result._partition_id — Name of a partition._part_uuid — Unique part identifier (if enabled MergeTree setting assign_part_uuids)._partition_value — Values (a tuple) of a partition by expression._sample_factor — Sample factor (from the query). "},{"title":"Full-text search with ClickHouse and Quickwit","type":0,"sectionRef":"#","url":"en/guides/developer/full-text-search","content":"","keywords":""},{"title":"Installing ClickHouse​","type":1,"pageTitle":"Full-text search with ClickHouse and Quickwit","url":"en/guides/developer/full-text-search#installing-clickhouse","content":"The first step is to install Quickwit and ClickHouse if you don’t have them installed already, follow the instruction below to install ClickHouse: sudo apt-get install apt-transport-https ca-certificates dirmngr sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv E0C56BD4 echo &quot;deb https://repo.clickhouse.com/deb/stable/ main/&quot; | sudo tee \\ /etc/apt/sources.list.d/clickhouse.list sudo apt-get update sudo apt-get install -y clickhouse-server clickhouse-client sudo service clickhouse-server start  "},{"title":"Installing Quickwit​","type":1,"pageTitle":"Full-text search with ClickHouse and Quickwit","url":"en/guides/developer/full-text-search#installing-quickwit","content":"Quickwit is relying on two external libraries to work correctly. You will need to install them before installing Quickwit # Quickwit depends on the following external libraries to work correctly sudo apt-get -y update sudo apt-get -y install libpq-dev libssl-dev  Once these two libraries are installed you can go ahead and install Quickwit: curl -L https://install.quickwit.io | sh # Quickwit detects the config from CLI args or the QW_CONFIG env variable. # Let's set QW_CONFIG to the default config. cd quickwit-v*/ export QW_CONFIG=./config/quickwit.yaml  You can test that Quickwit has been properly installed by running the following command: ./quickwit --version  Now that both ClickHouse and Quickwit are installed and run all we have to do is add some data to both of them. "},{"title":"Indexing Data in QuickWit​","type":1,"pageTitle":"Full-text search with ClickHouse and Quickwit","url":"en/guides/developer/full-text-search#indexing-data-in-quickwit","content":"The first thing we need to do is provide a data schema for the data we are going to use. We are going to use a subset of the data provided by GitHub. You can find the original data here, the dataset we are going to use has slightly been modified in order to be more practical to use. curl -o gh-archive-index-config.yaml https://datasets-documentation.s3.eu-west-3.amazonaws.com/full-text-search/gh-archive-index-config.yaml ./quickwit index create --index-config gh-archive-index-config.yaml  Now that the data schema is defined, let’s download and index some data in Quickwit: wget https://datasets-documentation.s3.eu-west-3.amazonaws.com/full-text-search/gh-archive-2021-12-text-only.json.gz gunzip -c gh-archive-2021-12-text-only.json.gz | ./quickwit index ingest --index gh-archive  You can search through your data within Quickwit: ./quickwit index search --index gh-archive --query &quot;clickhouse&quot;  But we want to use it in conjunction with ClickHouse, so in order to do so, we will need to create a searcher. ./quickwit service run searcher  This command will start an HTTP server with a REST API. We are now ready to fetch some ids with the search stream endpoint. Let's start by streaming them on a simple query and with a CSV output format. curl &quot;http://0.0.0.0:7280/api/v1/gh-archive/search/stream?query=clickhouse&amp;outputFormat=csv&amp;fastField=id&quot;  In the remaining of this guide we will be using the ClickHouse binary output format to speed up queries using ClickHouse. "},{"title":"Adding Data to ClickHouse​","type":1,"pageTitle":"Full-text search with ClickHouse and Quickwit","url":"en/guides/developer/full-text-search#adding-data-to-clickhouse","content":"First thing first, we need to connect to the ClickHouse database. Let’s use theclickhouse-client to do it. clickhouse-client –password &lt;PASSWORD&gt;  The first thing we need to do is to create a database: CREATE DATABASE &quot;github&quot;; USE github;  Now we need to create the table that’s going to store our data: CREATE TABLE github.github_events ( `id` UInt64, `event_type` Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), `actor_login` LowCardinality(String), `repo_name` LowCardinality(String), `created_at` Int64, `action` Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20), `comment_id` UInt64, `body` String, `ref` LowCardinality(String), `number` UInt32, `title` String, `labels` Array(LowCardinality(String)), `additions` UInt32, `deletions` UInt32, `commit_id` String ) ENGINE = MergeTree ORDER BY (event_type, repo_name, created_at)  We are going to add some data to ClickHouse. It’s the same dataset as the one we have indexed in Quickwit but this time it does not include text fields since they are indexed already. wget https://datasets-documentation.s3.eu-west-3.amazonaws.com/full-text-search/gh-archive-2021-12.json.gz gunzip -c gh-archive-2021-12.json.gz | clickhouse-client --query=&quot;INSERT INTO github.github_events FORMAT JSONEachRow&quot; --password &lt;PASSWORD&gt;  Now that the data is in ClickHouse we can query them using the clikchouse-client. SELECT repo_name, count() AS stars FROM github.github_events WHERE event_type = 'WatchEvent' GROUP BY repo_name ORDER BY stars DESC LIMIT 5  "},{"title":"Full-text search within ClickHouse​","type":1,"pageTitle":"Full-text search with ClickHouse and Quickwit","url":"en/guides/developer/full-text-search#full-text-search-within-clickhouse","content":"So we have data both in ClickHouse and in QuickWit all there is to do now is to query them! The url function allows you to fetch ids using the Quickwit search stream: SELECT count(*) FROM url('http://127.0.0.1:7280/api/v1/gh-archive/search/stream?query=clickhouse&amp;fastField=id&amp;outputFormat=clickHouseRowBinary', RowBinary, 'id UInt64')  In this query above we are counting the number of ID returned by the ClickHouse query executed in QuickWit. As you can see below, it’s returning the following: ┌─count()─┐ │ 2012 │ └─────────┘ 1 rows in set. Elapsed: 0.010 sec. Processed 2.01 thousand rows, 16.10 KB (210.35 thousand rows/s., 1.68 MB/s.)  We can search multiple tokens by separating them with a + : SELECT count(*) FROM url('http://127.0.0.1:7280/api/v1/gh-archive/search/stream?query=clickhouse+cloki&amp;fastField=id&amp;outputFormat=clickHouseRowBinary', RowBinary, 'id UInt64')  In the query above we are searching for documents containing the wordsClickHouse AND cloki. Now we can tweak the query around to search forClickHouse OR cloki: SELECT count(*) FROM url('http://127.0.0.1:7280/api/v1/gh-archive/search/stream?query=clickhouse+OR+cloki&amp;fastField=id&amp;outputFormat=clickHouseRowBinary', RowBinary, 'id UInt64')  So the full text search is working, now let’s combine it with some GROUP BYthat would be done on the ClickHouse side. Here we want to know how many rows match the words: ClickHouse, cloki or quickwit and in which GitHub repository there located in. SELECT count(*) AS count, repo_name AS repo FROM github.github_events WHERE id IN ( SELECT id FROM url('http://127.0.0.1:7280/api/v1/gh-archive/search/stream?query=cloki+OR+clickhouse+OR+quickwit&amp;fastField=id&amp;outputFormat=clickHouseRowBinary', RowBinary, 'id UInt64') ) GROUP BY repo ORDER BY count DESC  And as you can see below, it is fast: ┌─count─┬─repo──────────────────────────────────────────────┐ │ 874 │ ClickHouse/ClickHouse │ │ 112 │ traceon/ClickHouse │ │ 112 │ quickwit-inc/quickwit │ │ 110 │ PostHog/posthog │ │ 73 │ PostHog/charts-clickhouse │ │ 64 │ datafuselabs/databend │ │ 54 │ airbytehq/airbyte │ │ 53 │ ClickHouse/clickhouse-jdbc │ │ 37 │ getsentry/snuba │ │ 37 │ PostHog/posthog.com │ … … │ 1 │ antrea-io/antrea │ │ 1 │ python/typeshed │ │ 1 │ Sunt-ing/database-system-readings │ │ 1 │ duckdb/duckdb │ │ 1 │ open-botech/ClickHouse │ └───────┴───────────────────────────────────────────────────┘ 195 rows in set. Elapsed: 0.518 sec. Processed 45.43 million rows, 396.87 MB (87.77 million rows/s., 766.79 MB/s.)  The query is really fast, returning the result in 0.5 second. "},{"title":"Conclusion​","type":1,"pageTitle":"Full-text search with ClickHouse and Quickwit","url":"en/guides/developer/full-text-search#conclusion","content":"Using Quickwit within ClickHouse gives a lot of flexibility in how you work with your data, especially when your data contains textual information and you need to be able to search through them very quickly. You can find more information on how to use Quickwit directly on their documentation. "},{"title":"Understanding ClickHouse Data Skipping Indexes","type":0,"sectionRef":"#","url":"en/guides/improving-query-performance/skipping-indexes","content":"","keywords":""},{"title":"Introduction to Skipping Indexes​","type":1,"pageTitle":"Understanding ClickHouse Data Skipping Indexes","url":"en/guides/improving-query-performance/skipping-indexes#introduction-to-skipping-indexes","content":"Many factors affect ClickHouse query performance. The critical element in most scenarios is whether ClickHouse can use the primary key when evaluating the query WHERE clause condition. Accordingly, selecting a primary key that applies to the most common query patterns is essential for effective table design. Neverthelss, no matter how carefully tuned the primary key, there will inevitably be query use cases that can not efficiently use it. Users commonly rely on ClickHouse for time series type data, but they often wish to analyze that same data according to other business dimensions, such as customer id, website URL, or product number. In that case, query performance can be considerably worse because a full scan of each column value may be required to apply the WHERE clause condition. While ClickHouse is still relatively fast in those circumstances, evaluating millions or billions of individual values will cause &quot;non-indexed&quot; queries to execute much more slowly than those based on the primary key. In a traditional relational database, one approach to this problem is to attach one or more &quot;secondary&quot; indexes to a table. This is a b-tree structure that permits the database to find all matching rows on disk in O(log(n)) time instead of O(n) time (a table scan), where n is the number of rows. However, this type of secondary index will not work for ClickHouse (or other column-oriented databases) because there are no individual rows on the disk to add to the index. Instead, ClickHouse provides a different type of index, which in specific circumstances can significantly improve query speed. These structures are labeled &quot;Skip&quot; indexes because they enable ClickHouse to skip reading significant chunks of data that are guaranteed to have no matching values. "},{"title":"Basic Operation​","type":1,"pageTitle":"Understanding ClickHouse Data Skipping Indexes","url":"en/guides/improving-query-performance/skipping-indexes#basic-operation","content":"Users can only employ Data Skipping Indexes on the MergeTree family of tables. Each data skipping has four primary arguments: Index name. The index name is used to create the index file in each partition. Also, it is required as a parameter when dropping or materializing the index.Index expression. The index expression is used to calculate the set of values stored in the index. It can be a combination of columns, simple operators, and/or a subset of functions determined by the index type.TYPE. The type of index controls the calculation that determines if it is possible to skip reading and evaluating each index block.GRANULARITY. Each indexed block consists of GRANULARITY granules. For example, if the granularity of the primary table index is 8192 rows, and the index granularity is 4, each indexed &quot;block&quot; will be 32768 rows. When a user creates a data skipping index, there will be two additional files in each data part directory for the table. skpidx{index_name}.idx, which contains the ordered expression values)skpidx{index_name}.mrk2, which contains the corresponding offsets into the associated data column files. If some portion of the WHERE clause filtering condition matches the skip index expression when executing a query and reading the relevant column files, ClickHouse will use the index file data to determine whether each relevant block of data must be processed or can be bypassed (assuming that the block has not already been excluded by applying the primary key). To use a very simplified example, consider the following table loaded with predictable data. CREATE TABLE skip_table ( my_key UInt64, my_value UInt64 ) ENGINE MergeTree primary key my_key SETTINGS index_granularity=8192; INSERT INTO skip_table SELECT number, intDiv(number,4096) FROM numbers(100000000);  When executing a simple query that does not use the primary key, all 100 million entries in the my_valuecolumn are scanned: SELECT * FROM skip_table WHERE my_value IN (125, 700) ┌─my_key─┬─my_value─┐ │ 512000 │ 125 │ │ 512001 │ 125 │ │ ... | ... | └────────┴──────────┘ 8192 rows in set. Elapsed: 0.079 sec. Processed 100.00 million rows, 800.10 MB (1.26 billion rows/s., 10.10 GB/s.  Now add a very basic skip index: ALTER TABLE skip_table ADD INDEX vix my_value TYPE set(100) GRANULARITY 2;  Normally skip indexes are only applied on newly inserted data, so just adding the index won't affect the above query. To index already existing data, use this statement: ALTER TABLE skip_table MATERIALIZE INDEX vix;  Rerun the query with the newly created index: SELECT * FROM skip_table WHERE my_value IN (125, 700) ┌─my_key─┬─my_value─┐ │ 512000 │ 125 │ │ 512001 │ 125 │ │ ... | ... | └────────┴──────────┘ 8192 rows in set. Elapsed: 0.051 sec. Processed 32.77 thousand rows, 360.45 KB (643.75 thousand rows/s., 7.08 MB/s.)  Instead of processing 100 million rows of 800 megabytes, ClickHouse has only read and analyzed 32768 rows of 360 kilobytes -- four granules of 8192 rows each. In a more visual form, this is how the 4096 rows with a my_value of 125 were read and selected, and how the following rows were skipped without reading from disk:  Users can access detailed information about skip index usage by enabling the trace when executing queries. From clickhouse-client, set the send_logs_level: SET send_logs_level='trace';  This will provide useful debugging information when trying to tune query SQL and table indexes. From the above above example, the debug log shows that the skip index dropped all but two granules: &lt;Debug&gt; default.skip_table (933d4b2c-8cea-4bf9-8c93-c56e900eefd1) (SelectExecutor): Index `vix` has dropped 6102/6104 granules.  "},{"title":"Skip Index Types​","type":1,"pageTitle":"Understanding ClickHouse Data Skipping Indexes","url":"en/guides/improving-query-performance/skipping-indexes#skip-index-types","content":"minmax​ This lightweight index type requires no parameters. It stores the minimum and maximum values of the index expression for each block (if the expression is a tuple, it separately stores the values for each member of the element of the tuple). This type is ideal for columns that tend to be loosely sorted by value. This index type is usually the least expensive to apply during query processing. This type of index only works correctly with a scalar or tuple expression -- the index will never be applied to expressions that return an array or map data type. set​ This lightweight index type accepts a single parameter of the max_size of the value set per block (0 permits an unlimited number of discrete values). This set contains all values in the block (or is empty if the number of values exceeds the max_size). This index type works well with columns with low cardinality within each set of granules (essentially, &quot;clumped together&quot;) but higher cardinality overall. The cost, performance, and effectiveness of this index is dependent on the cardinality within blocks. If each block contains a large number of unique values, either evaluating the query condition against a large index set will be very expensive, or the index will not be applied because the index is empty due to exceeding max_size. Bloom Filter Types​ A Bloom filter is a data structure that allows space-efficient testing of set membership at the cost of a slight chance of false positives. A false positive is not a significant concern in the case of skip indexes because the only disadvantage is reading a few unnecessary blocks. However, the potential for false positives does mean that the indexed expression should be expected to be true, otherwise valid data may be skipped. Because Bloom filters can more efficiently handle testing for a large number of discrete values, they can be appropriate for conditional expressions that produce more values to test. In particular, a Bloom filter index can be applied to arrays, where every value of the array is tested, and to maps, by converting either the keys or values to an array using the mapKeys or mapValues function. There are three Data Skipping Index types based on Bloom filters: The basic bloom_filter which takes a single optional parameter of the allowed &quot;false positive&quot; rate between 0 and 1 (if unspecified, .025 is used). The specialized tokenbf_v1. It takes three parameters, all related to tuning the bloom filter used: (1) the size of the filter in bytes (larger filters have fewer false positives, at some cost in storage), (2) number of hash functions applied (again, more hash filters reduce false positives), and (3) the seed for the bloom filter hash functions. See the calculator here for more detail on how these parameters affect bloom filter functionality. This index works only with String, FixedString, and Map datatypes. The input expression is split into character sequences separated by non-alphanumeric characters. For example, a column value of This is a candidate for a &quot;full text&quot; search will contain the tokens This is a candidate for full text search. It is intended for use in LIKE, EQUALS, IN, hasToken() and similar searches for words and other values within longer strings. For example, one possible use might be searching for a small number of class names or line numbers in a column of free form application log lines. The specialized ngrambf_v1. This index functions the same as the token index. It takes one additional parameter before the Bloom filter settings, the size of the ngrams to index. An ngram is a character string of length n of any characters, so the string A short string with an ngram size of 4 would be indexed as A sh`` sho, shor, hort, ort s, or st, r str, stri, trin, ring. This index can also be useful for text searches, particularly languages without word breaks, such as Chinese. "},{"title":"Skip Index Functions​","type":1,"pageTitle":"Understanding ClickHouse Data Skipping Indexes","url":"en/guides/improving-query-performance/skipping-indexes#skip-index-functions","content":"The core purpose of data-skipping indexes is to limit the amount of data analyzed by popular queries. Given the analytic nature of ClickHouse data, the pattern of those queries in most cases includes functional expressions. Accordingly, skip indexes must interact correctly with common functions to be efficient. This can happen either when: • data is inserted and the index is defined as a functional expression (with the result of the expression stored in the index files), or • the query is processed and the expression is applied to the stored index values to determine whether to exclude the block. Each type of skip index works on a subset of available ClickHouse functions appropriate to the index implementation listedhere. In general, set indexes and Bloom filter based indexes (another type of set index) are both unordered and therefore do not work with ranges. In contrast, minmax indexes work particularly well with ranges since determining whether ranges intersect is very fast. The efficacy of partial match functions LIKE, startsWith, endsWith, and hasToken depend on the index type used, the index expression, and the particular shape of the data. "},{"title":"Skip Index Settings​","type":1,"pageTitle":"Understanding ClickHouse Data Skipping Indexes","url":"en/guides/improving-query-performance/skipping-indexes#skip-index-settings","content":"There are two available settings that apply to skip indexes. use_skip_indexes (0 or 1, default 1). Not all queries can efficiently use skip indexes. If a particular filtering condition is likely to include most granules, applying the data skipping index incurs an unnecessary, and sometimes significant, cost. Set the value to 0 for queries that are unlikely to benefit from any skip indexes.force_data_skipping_indexes (comma separated list of index names). This setting can be used to prevent some kinds of inefficient queries. In circumstances where querying a table is too expensive unless a skip index is used, using this setting with one or more index names will return an exception for any query that does not use the listed index. This would prevent poorly written queries from consuming server resources. "},{"title":"Skip Best Practices​","type":1,"pageTitle":"Understanding ClickHouse Data Skipping Indexes","url":"en/guides/improving-query-performance/skipping-indexes#skip-best-practices","content":"Skip indexes are not intuitive, especially for users accustomed to secondary row-based indexes from the RDMS realm or inverted indexes from document stores. To get any benefit, applying a ClickHouse data skipping index must avoid enough granule reads to offset the cost of calculating the index. Critically, if a value occurs even once in an indexed block, it means the entire block must be read into memory and evaluated, and the index cost has been needlessly incurred. Consider the following data distribution:  Assume the primary/order by key is timestamp, and there is an index on visitor_id. Consider the following query: SELECT timestamp, url FROM table WHERE visitor_id = 1001 A traditional secondary index would be very advantageous with this kind of data distribution. Instead of reading all 32678 rows to find the 5 rows with the requested visitor_id, the secondary index would include just five row locations, and only those five rows would be read from disk. The exact opposite is true for a ClickHouse data skipping index. All 32678 values in the visitor_id column will be tested regardless of the type of skip index. Accordingly, the natural impulse to try to speed up ClickHouse queries by simply adding an index to key columns is often incorrect. This advanced functionality should only be used after investigating other alternatives, such as modifying the primary key (see How to Pick a Primary Key), using projections, or using materialized views. Even when a data skipping index is appropriate, careful tuning both the index and the table will often be necessary. In most cases a useful skip index requires a strong correlation between the primary key and the targeted, non-primary column/expression. If there is no correlation (as in the above diagram), the chances of the filtering condition being met by at least one of the rows in the block of several thousand values is high and few blocks will be skipped. In constrast, if a range of values for the primary key (like time of day) is strongly associated with the values in the potential index column (such as television viewer ages), then a minmax type of index is likely to be beneficial. Note that it may be possible to increase this correlation when inserting data, either by including additional columns in the sorting/ORDER BY key, or batching inserts in a way that values associated with the primary key are grouped on insert. For example, all of the events for a particular site_id could be grouped and inserted together by the ingest process, even if the primary key is a timestamp containing events from a large number of sites. This will result in many granules that contains only a few site ids, so many blocks could be skipped when searching by a specific site_id value. Another good candidate for a skip index is for high cardinality expressions where any one value is relatively sparse in the data. One example might be an observability platform that tracks error codes in API requests. Certain error codes, while rare in the data, might be particularly important for searches. A set skip index on the error_code column would allow bypassing the vast majority of blocks that don't contain errors and therefore significantly improve error focused queries. Finally, the key best practice is to test, test, test. Again, unlike b-tree secondary indexes or inverted indexes for searching documents, data skipping index behavior is not easily predictable. Adding them to a table incurs a meangingful cost both on data ingest and on queries that for any number of reasons don't benefit from the index. They should always be tested on real world type of data, and testing should include variations of the type, granularity size and other parameters. Testing will often reveal patterns and pitfalls that aren't obvious from thought experiments alone. "},{"title":"Semi-Structured Approach","type":0,"sectionRef":"#","url":"en/guides/developer/working-with-json/json-semi-structured","content":"","keywords":""},{"title":"Overview​","type":1,"pageTitle":"Semi-Structured Approach","url":"en/guides/developer/working-with-json/json-semi-structured#overview","content":"To address the challenges of semi-structured data ClickHouse provides a JSON Object type. This feature is only available in versions later than 22.3.1. It represents the future preferred mechanism for handling arbitrary JSON. The alternative approaches described later, which partially rely on imposing a strict schema, still have validity as extracting JSON fields into dedicated columns allows these to be optimized with codecs or utilized primary/sort keys. The JSON Object type is advantageous when dealing with complex nested structures, which are subject to change. The type automatically infers the columns from the structure during insertion and merges these into the existing table schema. By storing JSON keys and their values as columns and dynamic subcolumns, ClickHouse can exploit the same optimizations used for structured data and thus provide comparable performance. The user is also provided with an intuitive path syntax for column selection. Furthermore, a table can contain a JSON object column with a flexible schema and more strict conventional columns with predefined types. It is important to note that the JSON type primarily syntactically enhances JSON handling at insertion and query time, i.e., it still exploits the native existing ClickHouse types for the columns, with JSON objects represented using the Tuple type. As a result, previously, manual schema handling is handled automatically with querying significantly simpler. "},{"title":"Relying on Schema Inference​","type":1,"pageTitle":"Semi-Structured Approach","url":"en/guides/developer/working-with-json/json-semi-structured#relying-on-schema-inference","content":"Note that recent versions of ClickHouse (22.4.1+) will infer the schema for JSONEachRow. This inference will also work for JSON objects with nested structures. These will be inferred as JSON object fields. For example, executing a DESCRIBE shows the detected schema for the file, including the actor fields: DESCRIBE s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/github/github-2022.ndjson.gz', 'JSONEachRow') SETTINGS input_format_max_rows_to_read_for_schema_inference=100;  name\ttypetype\tNullable(String) actor\tObject('json') repo\tObject('json') created_at\tNullable(String) payload\tObject('json') Note the setting input_format_max_rows_to_read_for_schema_inference. This determines the number of rows used to infer a schema. In this case, the schema can be inferred within the default of 100 rows. If the first 100 rows contained columns with null values, this would need to be set higher. This schema inference simplifies SELECT statements. Try executing the following to see how the actor and repo columns are returned as JSON. SELECT type, actor, repo FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/github/github-2022.ndjson.gz', 'JSONEachRow') LIMIT 2;  type\tactor\trepoPushEvent\t{&quot;avatar_url&quot;:&quot;https:\\/\\/avatars.githubusercontent.com\\/u\\/93110249?&quot;,&quot;display_login&quot;:&quot;Lakshmipatil2021&quot;,&quot;id&quot;:93110249,&quot;login&quot;:&quot;Lakshmipatil2021&quot;,&quot;url&quot;:&quot;https:\\/\\/api.github.com\\/users\\/Lakshmipatil2021&quot;}\t{&quot;id&quot;:429298592,&quot;name&quot;:&quot;revacprogramming\\/pps-test1-Lakshmipatil2021&quot;,&quot;url&quot;:&quot;https:\\/\\/api.github.com\\/repos\\/revacprogramming\\/pps-test1-Lakshmipatil2021&quot;} MemberEvent\t{&quot;avatar_url&quot;:&quot;https:\\/\\/avatars.githubusercontent.com\\/u\\/95751520?&quot;,&quot;display_login&quot;:&quot;KStevenT&quot;,&quot;id&quot;:95751520,&quot;login&quot;:&quot;KStevenT&quot;,&quot;url&quot;:&quot;https:\\/\\/api.github.com\\/users\\/KStevenT&quot;}\t{&quot;id&quot;:443103546,&quot;name&quot;:&quot;KStevenT\\/HTML_ExternalWorkshop&quot;,&quot;url&quot;:&quot;https:\\/\\/api.github.com\\/repos\\/KStevenT\\/HTML_ExternalWorkshop&quot;} Schema inference and the introduction of the JSON Object Type allow us to handle nested data elegantly and avoid verbose definitions. However, we need to treat the entire row as a JSON object for dynamic properties on the root. Version 22.4 of ClickHouse introduces the JSONAsObject format to assist with this. "},{"title":"JSON Object Type​","type":1,"pageTitle":"Semi-Structured Approach","url":"en/guides/developer/working-with-json/json-semi-structured#json-object-type","content":"Using the same dataset as above, we explicitly declare that each row is a single object via the JSONAsObject format. This single object is mapped to a field event of the type Object(JSON) - in this case, we use the shorthand JSON. Note if we don’t explicitly specify event as the field name in the s3 function, a field json will be used: SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/github/github-2022.ndjson.gz', 'JSONAsObject', 'event JSON') LIMIT 1;  event{&quot;type&quot;:&quot;PushEvent&quot;,&quot;actor.avatar_url&quot;:&quot;https:\\/\\/avatars.githubusercontent.com\\/u\\/93110249?&quot;,&quot;actor.display_login&quot;:&quot;Lakshmipatil2021&quot;,&quot;actor.id&quot;:93110249,&quot;actor.login&quot;:&quot;Lakshmipatil2021&quot;,&quot;actor.url&quot;:&quot;https:\\/\\/api.github.com\\/users\\/Lakshmipatil2021&quot;,&quot;repo.id&quot;:429298592,&quot;repo.name&quot;:&quot;revacprogramming\\/pps-test1-Lakshmipatil2021&quot;,&quot;repo.url&quot;:&quot;https:\\/\\/api.github.com\\/repos\\/revacprogramming\\/pps-test1-Lakshmipatil2021&quot;,&quot;created_at&quot;:&quot;2022-01-04T07:00:00Z&quot;,&quot;payload.pull_request.updated_at&quot;:&quot;&quot;,&quot;payload.pull_request.user.login&quot;:&quot;&quot;,&quot;payload.pull_request.number&quot;:0,&quot;payload.pull_request.title&quot;:&quot;&quot;,&quot;payload.pull_request.state&quot;:&quot;&quot;,&quot;payload.pull_request.author_association&quot;:&quot;&quot;,&quot;payload.pull_request.head.ref&quot;:&quot;&quot;,&quot;payload.pull_request.head.sha&quot;:&quot;&quot;,&quot;payload.pull_request.base.ref&quot;:&quot;&quot;,&quot;payload.pull_request.base.sha&quot;:&quot;&quot;,&quot;payload.action&quot;:&quot;&quot;,&quot;payload.ref&quot;:&quot;refs\\/heads\\/main&quot;,&quot;payload.ref_type&quot;:&quot;&quot;,&quot;payload.size&quot;:1,&quot;payload.distinct_size&quot;:1} To query this data effectively, we currently need to store it into a MergeTree. This is subject to change in later versions. We, therefore, create a table and insert the rows using an INSERT INTO SELECT. First, create the table before inserting the rows. This can take a few minutes depending on the hardware and network latency to the s3 source bucket: Note the use of allow_experimental_object_type as the JSON object type is still an experimental feature. DROP TABLE IF EXISTS github_json; SET allow_experimental_object_type=1; CREATE table github_json(event JSON) ENGINE = MergeTree ORDER BY tuple() INSERT INTO github_json SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/github/github-2022.ndjson.gz', JSONAsObject, 'event JSON');  Confirm the table schema and row count as 1m. SELECT count() FROM github_json; DESCRIBE github_json; Object('json')  While the above confirms each row is treated as a JSON object, it provides no information on how the fields in the JSON are mapped columns. To obtain this, we can utilize the setting describe_extend_object_types. DESCRIBE github_json SETTINGS describe_extend_object_types=1; Tuple(actor Tuple(avatar_url String, display_login String, id Int32, login String, url String), created_at String, payload Tuple(action String, distinct_size Int32, pull_request Tuple(author_association String, base Tuple(ref String, sha String), head Tuple(ref String, sha String), number Int32, state String, title String, updated_at String, user Tuple(login String)), ref String, ref_type String, size Int16), repo Tuple(id Int32, name String, url String), type String)  The most interesting component of this mapping is the handling of the nested JSON. Note how the JSON structure below is mapped to repo Tuple(id Int32, name String, url String):  &quot;repo&quot;: { &quot;id&quot;: 429298592, &quot;name&quot;: &quot;revacprogramming/pps-test1-Lakshmipatil2021&quot;, &quot;url&quot;: &quot;https://api.github.com/repos/revacprogramming/pps-test1-Lakshmipatil2021&quot; }  This structure could be mapped manually but would require the user to structure data appropriate for insertion and adapt queries to utilize - see Other Approaches, significantly complicating usage. At this point, we are ready to exploit these dynamically created columns with queries. "},{"title":"Selecting Dynamic Subcolumns​","type":1,"pageTitle":"Semi-Structured Approach","url":"en/guides/developer/working-with-json/json-semi-structured#selecting-dynamic-subcolumns","content":"Querying the above table highlights some of the historical challenges of using Tuples for nested JSON data. SELECT event.type, event.repo, event.actor FROM github_json LIMIT 1;  event.type\tevent.repo\tevent.actorPushEvent\t(429298592,'revacprogramming/pps-test1-Lakshmipatil2021','https://api.github.com/repos/revacprogramming/pps-test1-Lakshmipatil2021'\\)\t('https://avatars.githubusercontent.com/u/93110249?','Lakshmipatil2021','',93110249,'Lakshmipatil2021','https://api.github.com/users/Lakshmipatil2021'\\) To return the original structure we need both JSONEachRow format and the parameter output_format_json_named_tuples_as_objects: SELECT event.type, event.repo, event.actor FROM github_json LIMIT 1 FORMAT JSONEachRow SETTINGS output_format_json_named_tuples_as_objects=1;  {&quot;event.type&quot;:&quot;PushEvent&quot;,&quot;event.repo&quot;:{&quot;id&quot;:429298592, &quot;name&quot;:&quot;revacprogramming\\/pps-test1-Lakshmipatil2021&quot;, &quot;url&quot;:&quot;https:\\/\\/api.github.com\\/repos\\/revacprogramming\\/pps-test1-Lakshmipatil2021&quot;}, &quot;event.actor&quot;:{&quot;avatar_url&quot;:&quot;https:\\/\\/avatars.githubusercontent.com\\/u\\/93110249?&quot;, &quot;display_login&quot;:&quot;Lakshmipatil2021&quot;,&quot;gravatar_id&quot;:&quot;&quot;,&quot;id&quot;:93110249, &quot;login&quot;:&quot;Lakshmipatil2021&quot;,&quot;url&quot;:&quot;https:\\/\\/api.github.com\\/users\\/Lakshmipatil2021&quot;}}  While the above-simplified example illustrates the mechanics of using JSON Object types, users need to query these JSON-based columns using the same filters and aggregation capabilities as any other type. We can translate some of the examples provided here to JSON queries to illustrate equivalence. Note this is a 1m row sample of data only, so results are meaningless. Counting the top repositories by stars becomes a simple query. Note the use of a period as a path delimiter in nested objects: SELECT event.repo.name, count() AS stars FROM github_json WHERE event.type = 'WatchEvent' GROUP BY event.repo.name ORDER BY stars DESC LIMIT 5;  event.repo.name\tstarsdwmkerr/hacker-laws\t283 tkellogg/dura\t200 aplus-framework/app\t157 seemoo-lab/opendrop\t111 heroku-python/flask-sockets\t92 More complex queries showing the list of top repositories over time are also possible. We adapt the query as it covers a short period (3 days). Also, note the need to parse the event.created_at field with the function parseDateTimeBestEffort as this has been inferred as a string. SELECT repo AS name, groupArrayInsertAt(toUInt32(c), toUInt64(dateDiff('hour', toDate('2022-01-01'), hour))) AS data FROM ( SELECT lower(event.repo.name) AS repo, toStartOfHour(parseDateTimeBestEffort(event.created_at)) AS hour, count() AS c FROM github_json WHERE (event.type = 'WatchEvent') AND (toYear(parseDateTimeBestEffort(event.created_at)) &gt;= 2022) AND (repo IN ( SELECT lower(event.repo.name) AS repo FROM github_json WHERE (event.type = 'WatchEvent') AND (toYear(parseDateTimeBestEffort(event.created_at)) &gt;= 2022) GROUP BY event.repo.name ORDER BY count() DESC LIMIT 10 )) GROUP BY repo, hour ) GROUP BY repo ORDER BY repo ASC;  "},{"title":"Adding Primary Keys​","type":1,"pageTitle":"Semi-Structured Approach","url":"en/guides/developer/working-with-json/json-semi-structured#adding-primary-keys","content":"The above example is not realistic in that it has no primary or sort key i.e., it uses tuple(). This negates the benefit of the index features in ClickHouse. To add a primary key, and still exploit the JSON object capabilities, we recommended using a dedicated subkey for the JSON. This requires inserting the data using the JSONAsRow format instead of JSONAsObject. For example, consider the JSON below and the corresponding table definition and insert statement. SET allow_experimental_object_type=1; DROP TABLE IF EXISTS github_json; CREATE table github_json ( event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), repo_name LowCardinality(String), event JSON ) ENGINE = MergeTree ORDER BY (event_type, repo_name);  Inserting data requires us to use the JSONEachRow format. Note how the event sub field now holds our dynamic JSON, whilst the root keys are explicitly defined. INSERT INTO github_json FORMAT JSONEachRow {&quot;event&quot;:{&quot;type&quot;:&quot;PushEvent&quot;,&quot;actor&quot;:{&quot;avatar_url&quot;:&quot;https://avatars.githubusercontent.com/u/41898282?&quot;, &quot;display_login&quot;:&quot;github-actions&quot;,&quot;gravatar_id&quot;:&quot;&quot;,&quot;id&quot;:41898282,&quot;login&quot;:&quot;github-actions[bot]&quot;, &quot;url&quot;:&quot;https://api.github.com/users/github-actions[bot]&quot;},&quot;repo&quot;:{&quot;id&quot;:410071248, &quot;name&quot;:&quot;pioug/yield-data&quot;,&quot;url&quot;:&quot;https://api.github.com/repos/pioug/yield-data&quot;}}, &quot;event_type&quot;:&quot;PushEvent&quot;,&quot;repo_name&quot;:&quot;pioug/yield-data&quot;}  This requires a restructuring of our JSON, which is inconvenient at best. Ideally, we need a more flexible approach that allows us to modify the fields we wish to extract as root keys over time without needing to change our data pipelines. Inserting our row as a String inside an EPHEMERAL column message_raw, we can extract specific fields of interest using DEFAULT expressions for the root fields. The String EPHEMERAL column is also mapped to a JSON object column message that provides the usual flexibility. This EPHEMERAL column will not be persisted and will be discarded at INSERT time. Our primary key fields are as a result duplicated i.e. they occur at the root of the document, as well as in the message JSON. DROP TABLE IF EXISTS github_json; SET allow_experimental_object_type = 1; CREATE table github_json ( event_type LowCardinality(String) DEFAULT JSONExtractString(message_raw, 'type'), repo_name LowCardinality(String) DEFAULT JSONExtractString(message_raw, 'repo.name'), message JSON DEFAULT message_raw, message_raw String EPHEMERAL ) ENGINE = MergeTree ORDER BY (event_type, repo_name);  Insertion thus requires a modified structure - note how the JSON is parsed as a string inside message_raw. INSERT INTO github_json (message_raw) FORMAT JSONEachRow {&quot;message_raw&quot;: &quot;{\\&quot;type\\&quot;:\\&quot;PushEvent\\&quot;, \\&quot;created_at\\&quot;: \\&quot;2022-01-04 07:00:00\\&quot;, \\&quot;actor\\&quot;:{\\&quot;avatar_url\\&quot;:\\&quot;https://avatars.githubusercontent.com/u/41898282?\\&quot;, \\&quot;display_login\\&quot;:\\&quot;github-actions\\&quot;,\\&quot;gravatar_id\\&quot;:\\&quot;\\&quot;,\\&quot;id\\&quot;:41898282,\\&quot;login\\&quot;:\\&quot;github-actions[bot]\\&quot;, \\&quot;url\\&quot;:\\&quot;https://api.github.com/users/github-actions[bot]\\&quot;},\\&quot;repo\\&quot;:{\\&quot;id\\&quot;:410071248,\\&quot;name\\&quot;:\\&quot;pioug/yield-data\\&quot;, \\&quot;url\\&quot;:\\&quot;https://api.github.com/repos/pioug/yield-data\\&quot;}}&quot;}  To add fields to the root, we in turn just need to ALTER the table definition adding fields as required. For details on how to retrospectively add columns, see the technique used in Other Approaches. "},{"title":"Limitations and Best Practices​","type":1,"pageTitle":"Semi-Structured Approach","url":"en/guides/developer/working-with-json/json-semi-structured#limitations-and-best-practices","content":"Dynamic columns in JSON objects are as fast predefined types. The flexible schema is an extremely powerful feature at every little syntax overhead and a natural fit for handling data such as logs - where keys are frequently added through dynamic properties such as container labels in Kubernetes. Parsing of JSON, and inference of the schema does incur a cost at insertion time. Because of this, we recommend keeping column counts below 10k. Should you need to exceed this, consult ClickHouse support. There are also limitations as to how dynamic columns can be used. As noted earlier, they cannot be used as primary or sort keys. Furthermore, they cannot be configured to use specific codecs. For optimal performance, we recommend the JSON object type be used for a specific subkey of the JSON and the root keys be declared explicitly. This allows them to be configured with specific codecs or used for sort/primary keys. As shown in Adding Primary Keys, this requires the use of the JSONAsRow format vs. inserting the entire row as JSON with the JSONAsObject format. "},{"title":"Handling Data Changes​","type":1,"pageTitle":"Semi-Structured Approach","url":"en/guides/developer/working-with-json/json-semi-structured#handling-data-changes","content":""},{"title":"Adding Columns​","type":1,"pageTitle":"Semi-Structured Approach","url":"en/guides/developer/working-with-json/json-semi-structured#adding-columns","content":"Handling semi-structured data requires ClickHouse to adapt new columns as they are added or their type changes. We explore some of these behaviors below. Consider the simple example below: { &quot;type&quot;: &quot;PushEvent&quot;, &quot;actor&quot;: { &quot;id&quot;: 93110249 }, &quot;repo&quot;: { &quot;id&quot;: 429298592, &quot;name&quot;: &quot;revacprogramming/pps-test1-Lakshmipatil2021&quot;, &quot;url&quot;: &quot;https://api.github.com/repos/revacprogramming/pps-test1-Lakshmipatil2021&quot; } }  Creating a table to accept this data and performing the insert is trivial. SET allow_experimental_object_type=1; CREATE table github_tmp (event JSON) ENGINE = MergeTree ORDER BY tuple(); INSERT INTO github_tmp FORMAT JSONAsObject {&quot;type&quot;:&quot;PushEvent&quot;,&quot;actor&quot;:{&quot;id&quot;:93110249},&quot;repo&quot;:{&quot;id&quot;:429298592, &quot;name&quot;:&quot;revacprogramming/pps-test1-Lakshmipatil2021&quot;, &quot;url&quot;:&quot;https://api.github.com/repos/revacprogramming/pps-test1-Lakshmipatil2021&quot;}}  Inspecting the types we can see the columns created: SET describe_extend_object_types=1; DESCRIBE github_tmp; Tuple(actor Tuple(id Int32), repo Tuple(id Int32, name String, url String), type String)  Suppose now we insert the following object. This adds additional fields to the actor object: { &quot;type&quot;: &quot;PushEvent&quot;, &quot;actor&quot;: { &quot;avatar_url&quot;: &quot;https://avatars.githubusercontent.com/u/81258380?&quot;, &quot;display_login&quot;: &quot;Helikopter-Bojowy&quot;, &quot;gravatar_id&quot;: &quot;&quot;, &quot;id&quot;: 81258380, &quot;login&quot;: &quot;Helikopter-Bojowy&quot;, &quot;url&quot;: &quot;https://api.github.com/users/Helikopter-Bojowy&quot; }, &quot;repo&quot;: { &quot;id&quot;: 352069365, &quot;name&quot;: &quot;Helikopter-Bojowy/Exp-na-helikopterze&quot;, &quot;url&quot;: &quot;https://api.github.com/repos/Helikopter-Bojowy/Exp-na-helikopterze&quot; } }  INSERT INTO github_tmp FORMAT JSONAsObject {&quot;type&quot;:&quot;PushEvent&quot;,&quot;actor&quot;:{&quot;avatar_url&quot;:&quot;https://avatars.githubusercontent.com/u/81258380?&quot;, &quot;display_login&quot;:&quot;Helikopter-Bojowy&quot;,&quot;gravatar_id&quot;:&quot;&quot;,&quot;id&quot;:81258380,&quot;login&quot;:&quot;Helikopter-Bojowy&quot;, &quot;url&quot;:&quot;https://api.github.com/users/Helikopter-Bojowy&quot;},&quot;repo&quot;:{&quot;id&quot;:352069365, &quot;name&quot;:&quot;Helikopter-Bojowy/Exp-na-helikopterze&quot;, &quot;url&quot;:&quot;https://api.github.com/repos/Helikopter-Bojowy/Exp-na-helikopterze&quot;}}  If we inspect the schema, we can see the columns have automatically been inferred and added: SET describe_extend_object_types=1; DESCRIBE github_tmp; Tuple(actor Tuple(avatar_url String, display_login String, gravatar_id String, id Int32, login String, url String), repo Tuple(id Int32, name String, url String), type String)  "},{"title":"Changing Columns​","type":1,"pageTitle":"Semi-Structured Approach","url":"en/guides/developer/working-with-json/json-semi-structured#changing-columns","content":"Despite best efforts, JSON is often inconsistent in types. Whilst some data stores, such as Kafka, can enforce a schema on JSON this is often not enforced. As a result, ClickHouse can receive the same field in multiple types. This often requires unifying types. Consider the following example: { &quot;type&quot;: &quot;PushEvent&quot;, &quot;actor&quot;: { &quot;id&quot;: 10 } }  Here actor.id is an integer. If inserted to a table, it will be mapped to an Int8 as shown below: SET allow_experimental_object_type=1; CREATE table github_types ( event JSON ) ENGINE = MergeTree ORDER BY tuple(); INSERT INTO github_types FORMAT JSONAsObject {&quot;type&quot;:&quot;PushEvent&quot;,&quot;actor&quot;:{&quot;id&quot;:10}} SET describe_extend_object_types=1; DESCRIBE github_types; Tuple(actor Tuple(id Int8), type String)  Now Github has alot more users than can be represented by an Int8. A typical user id is much larger. Consider the more realistic example below: INSERT INTO github_types FORMAT JSONAsObject {&quot;type&quot;:&quot;PushEvent&quot;,&quot;actor&quot;:{&quot;id&quot;:93110249}}  As shown the id field is now represented as an Int32. SET describe_extend_object_types=1; DESCRIBE github_types; Tuple(actor Tuple(id Int32), type String)  Suppose that Github decides that ids can be alphanumeric, or more realistic a value is inserted as a string e.g. { &quot;type&quot;: &quot;PushEvent&quot;, &quot;actor&quot;: { &quot;id&quot;: &quot;81258380&quot; } }  INSERT INTO github_types FORMAT JSONAsObject {&quot;type&quot;:&quot;PushEvent&quot;,&quot;actor&quot;:{&quot;id&quot;:&quot;81258380&quot;}} SET describe_extend_object_types=1; DESCRIBE github_types; Tuple(actor Tuple(id String), type String)  As shown, ClickHouse is now forced to represent the actor.id column as a string. This sort of coercion is supported for most types that have variable representation e.g. Int, Float. If necessary, ClickHouse will unify to the higher bit type that allows all current values to be represented. If necessary, converting to a String represents the least precise definition. Warning: This changing in types can break queries if you rely on type specific functions e.g. sum for numerics. We recommend you ensure your data is consistent where possible and rely on this feature as a backup vs best practice. Note that not all types can be unified. Attempting the following, after inserting any of the previous data will result in an error: INSERT INTO github_types FORMAT JSONAsObject {&quot;type&quot;:&quot;PushEvent&quot;,&quot;actor&quot;:{&quot;id&quot;:[&quot;92258380&quot;]}}  The inverse of this would also fail i.e. if for the first row id was an Array(String) and subsequent rows were only a String. Likewise objects (represented as Tuples) cannot be unified with scalar types such as String. The contents of these can, however, be coerced. For example, consider the following where actor.id is first an Array(Int8) and then an Array(String). DROP TABLE github_types; SET allow_experimental_object_type=1; CREATE table github_types ( event JSON ) ENGINE = MergeTree ORDER BY tuple(); INSERT INTO github_types FORMAT JSONAsObject {&quot;type&quot;:&quot;PushEvent&quot;,&quot;actor&quot;:{&quot;id&quot;:[10]}} SET describe_extend_object_types=1; DESCRIBE github_types; Tuple(actor Tuple(id Array(Int8)), type String) INSERT INTO github_types FORMAT JSONAsObject {&quot;type&quot;:&quot;PushEvent&quot;,&quot;actor&quot;:{&quot;id&quot;:[&quot;92258380&quot;]}} SET describe_extend_object_types=1; DESCRIBE github_types; Tuple(actor Tuple(id Array(String)), type String)  "},{"title":"Handling JSON Formats​","type":1,"pageTitle":"Semi-Structured Approach","url":"en/guides/developer/working-with-json/json-semi-structured#handling-json-formats","content":"ClickHouse can handle JSON in a number of formats, other than JSONEachRow and JSONAsObject. These are useful on both input and output and are described here. "},{"title":"New York Public Library \"What's on the Menu?\" Dataset","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/menus","content":"","keywords":""},{"title":"Download the Dataset​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"en/getting-started/example-datasets/menus#download-dataset","content":"Run the command: wget https://s3.amazonaws.com/menusdata.nypl.org/gzips/2021_08_01_07_01_17_data.tgz  Replace the link to the up to date link from http://menus.nypl.org/data if needed. Download size is about 35 MB. "},{"title":"Unpack the Dataset​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"en/getting-started/example-datasets/menus#unpack-dataset","content":"tar xvf 2021_08_01_07_01_17_data.tgz  Uncompressed size is about 150 MB. The data is normalized consisted of four tables: Menu — Information about menus: the name of the restaurant, the date when menu was seen, etc.Dish — Information about dishes: the name of the dish along with some characteristic.MenuPage — Information about the pages in the menus, because every page belongs to some menu.MenuItem — An item of the menu. A dish along with its price on some menu page: links to dish and menu page. "},{"title":"Create the Tables​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"en/getting-started/example-datasets/menus#create-tables","content":"We use Decimal data type to store prices. CREATE TABLE dish ( id UInt32, name String, description String, menus_appeared UInt32, times_appeared Int32, first_appeared UInt16, last_appeared UInt16, lowest_price Decimal64(3), highest_price Decimal64(3) ) ENGINE = MergeTree ORDER BY id; CREATE TABLE menu ( id UInt32, name String, sponsor String, event String, venue String, place String, physical_description String, occasion String, notes String, call_number String, keywords String, language String, date String, location String, location_type String, currency String, currency_symbol String, status String, page_count UInt16, dish_count UInt16 ) ENGINE = MergeTree ORDER BY id; CREATE TABLE menu_page ( id UInt32, menu_id UInt32, page_number UInt16, image_id String, full_height UInt16, full_width UInt16, uuid UUID ) ENGINE = MergeTree ORDER BY id; CREATE TABLE menu_item ( id UInt32, menu_page_id UInt32, price Decimal64(3), high_price Decimal64(3), dish_id UInt32, created_at DateTime, updated_at DateTime, xpos Float64, ypos Float64 ) ENGINE = MergeTree ORDER BY id;  "},{"title":"Import the Data​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"en/getting-started/example-datasets/menus#import-data","content":"Upload data into ClickHouse, run: clickhouse-client --format_csv_allow_single_quotes 0 --input_format_null_as_default 0 --query &quot;INSERT INTO dish FORMAT CSVWithNames&quot; &lt; Dish.csv clickhouse-client --format_csv_allow_single_quotes 0 --input_format_null_as_default 0 --query &quot;INSERT INTO menu FORMAT CSVWithNames&quot; &lt; Menu.csv clickhouse-client --format_csv_allow_single_quotes 0 --input_format_null_as_default 0 --query &quot;INSERT INTO menu_page FORMAT CSVWithNames&quot; &lt; MenuPage.csv clickhouse-client --format_csv_allow_single_quotes 0 --input_format_null_as_default 0 --date_time_input_format best_effort --query &quot;INSERT INTO menu_item FORMAT CSVWithNames&quot; &lt; MenuItem.csv  We use CSVWithNames format as the data is represented by CSV with header. We disable format_csv_allow_single_quotes as only double quotes are used for data fields and single quotes can be inside the values and should not confuse the CSV parser. We disable input_format_null_as_default as our data does not have NULL. Otherwise ClickHouse will try to parse \\N sequences and can be confused with \\ in data. The setting date_time_input_format best_effort allows to parse DateTime fields in wide variety of formats. For example, ISO-8601 without seconds like '2000-01-01 01:02' will be recognized. Without this setting only fixed DateTime format is allowed. "},{"title":"Denormalize the Data​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"en/getting-started/example-datasets/menus#denormalize-data","content":"Data is presented in multiple tables in normalized form. It means you have to perform JOIN if you want to query, e.g. dish names from menu items. For typical analytical tasks it is way more efficient to deal with pre-JOINed data to avoid doing JOIN every time. It is called &quot;denormalized&quot; data. We will create a table menu_item_denorm where will contain all the data JOINed together: CREATE TABLE menu_item_denorm ENGINE = MergeTree ORDER BY (dish_name, created_at) AS SELECT price, high_price, created_at, updated_at, xpos, ypos, dish.id AS dish_id, dish.name AS dish_name, dish.description AS dish_description, dish.menus_appeared AS dish_menus_appeared, dish.times_appeared AS dish_times_appeared, dish.first_appeared AS dish_first_appeared, dish.last_appeared AS dish_last_appeared, dish.lowest_price AS dish_lowest_price, dish.highest_price AS dish_highest_price, menu.id AS menu_id, menu.name AS menu_name, menu.sponsor AS menu_sponsor, menu.event AS menu_event, menu.venue AS menu_venue, menu.place AS menu_place, menu.physical_description AS menu_physical_description, menu.occasion AS menu_occasion, menu.notes AS menu_notes, menu.call_number AS menu_call_number, menu.keywords AS menu_keywords, menu.language AS menu_language, menu.date AS menu_date, menu.location AS menu_location, menu.location_type AS menu_location_type, menu.currency AS menu_currency, menu.currency_symbol AS menu_currency_symbol, menu.status AS menu_status, menu.page_count AS menu_page_count, menu.dish_count AS menu_dish_count FROM menu_item JOIN dish ON menu_item.dish_id = dish.id JOIN menu_page ON menu_item.menu_page_id = menu_page.id JOIN menu ON menu_page.menu_id = menu.id;  "},{"title":"Validate the Data​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"en/getting-started/example-datasets/menus#validate-data","content":"Query: SELECT count() FROM menu_item_denorm;  Result: ┌─count()─┐ │ 1329175 │ └─────────┘  "},{"title":"Run Some Queries​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"en/getting-started/example-datasets/menus#run-queries","content":""},{"title":"Averaged historical prices of dishes​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"en/getting-started/example-datasets/menus#query-averaged-historical-prices","content":"Query: SELECT round(toUInt32OrZero(extract(menu_date, '^\\\\d{4}')), -1) AS d, count(), round(avg(price), 2), bar(avg(price), 0, 100, 100) FROM menu_item_denorm WHERE (menu_currency = 'Dollars') AND (d &gt; 0) AND (d &lt; 2022) GROUP BY d ORDER BY d ASC;  Result: ┌────d─┬─count()─┬─round(avg(price), 2)─┬─bar(avg(price), 0, 100, 100)─┐ │ 1850 │ 618 │ 1.5 │ █▍ │ │ 1860 │ 1634 │ 1.29 │ █▎ │ │ 1870 │ 2215 │ 1.36 │ █▎ │ │ 1880 │ 3909 │ 1.01 │ █ │ │ 1890 │ 8837 │ 1.4 │ █▍ │ │ 1900 │ 176292 │ 0.68 │ ▋ │ │ 1910 │ 212196 │ 0.88 │ ▊ │ │ 1920 │ 179590 │ 0.74 │ ▋ │ │ 1930 │ 73707 │ 0.6 │ ▌ │ │ 1940 │ 58795 │ 0.57 │ ▌ │ │ 1950 │ 41407 │ 0.95 │ ▊ │ │ 1960 │ 51179 │ 1.32 │ █▎ │ │ 1970 │ 12914 │ 1.86 │ █▋ │ │ 1980 │ 7268 │ 4.35 │ ████▎ │ │ 1990 │ 11055 │ 6.03 │ ██████ │ │ 2000 │ 2467 │ 11.85 │ ███████████▋ │ │ 2010 │ 597 │ 25.66 │ █████████████████████████▋ │ └──────┴─────────┴──────────────────────┴──────────────────────────────┘  Take it with a grain of salt. "},{"title":"Burger Prices​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"en/getting-started/example-datasets/menus#query-burger-prices","content":"Query: SELECT round(toUInt32OrZero(extract(menu_date, '^\\\\d{4}')), -1) AS d, count(), round(avg(price), 2), bar(avg(price), 0, 50, 100) FROM menu_item_denorm WHERE (menu_currency = 'Dollars') AND (d &gt; 0) AND (d &lt; 2022) AND (dish_name ILIKE '%burger%') GROUP BY d ORDER BY d ASC;  Result: ┌────d─┬─count()─┬─round(avg(price), 2)─┬─bar(avg(price), 0, 50, 100)───────────┐ │ 1880 │ 2 │ 0.42 │ ▋ │ │ 1890 │ 7 │ 0.85 │ █▋ │ │ 1900 │ 399 │ 0.49 │ ▊ │ │ 1910 │ 589 │ 0.68 │ █▎ │ │ 1920 │ 280 │ 0.56 │ █ │ │ 1930 │ 74 │ 0.42 │ ▋ │ │ 1940 │ 119 │ 0.59 │ █▏ │ │ 1950 │ 134 │ 1.09 │ ██▏ │ │ 1960 │ 272 │ 0.92 │ █▋ │ │ 1970 │ 108 │ 1.18 │ ██▎ │ │ 1980 │ 88 │ 2.82 │ █████▋ │ │ 1990 │ 184 │ 3.68 │ ███████▎ │ │ 2000 │ 21 │ 7.14 │ ██████████████▎ │ │ 2010 │ 6 │ 18.42 │ ████████████████████████████████████▋ │ └──────┴─────────┴──────────────────────┴───────────────────────────────────────┘  "},{"title":"Vodka​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"en/getting-started/example-datasets/menus#query-vodka","content":"Query: SELECT round(toUInt32OrZero(extract(menu_date, '^\\\\d{4}')), -1) AS d, count(), round(avg(price), 2), bar(avg(price), 0, 50, 100) FROM menu_item_denorm WHERE (menu_currency IN ('Dollars', '')) AND (d &gt; 0) AND (d &lt; 2022) AND (dish_name ILIKE '%vodka%') GROUP BY d ORDER BY d ASC;  Result: ┌────d─┬─count()─┬─round(avg(price), 2)─┬─bar(avg(price), 0, 50, 100)─┐ │ 1910 │ 2 │ 0 │ │ │ 1920 │ 1 │ 0.3 │ ▌ │ │ 1940 │ 21 │ 0.42 │ ▋ │ │ 1950 │ 14 │ 0.59 │ █▏ │ │ 1960 │ 113 │ 2.17 │ ████▎ │ │ 1970 │ 37 │ 0.68 │ █▎ │ │ 1980 │ 19 │ 2.55 │ █████ │ │ 1990 │ 86 │ 3.6 │ ███████▏ │ │ 2000 │ 2 │ 3.98 │ ███████▊ │ └──────┴─────────┴──────────────────────┴─────────────────────────────┘  To get vodka we have to write ILIKE '%vodka%' and this definitely makes a statement. "},{"title":"Caviar​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"en/getting-started/example-datasets/menus#query-caviar","content":"Let's print caviar prices. Also let's print a name of any dish with caviar. Query: SELECT round(toUInt32OrZero(extract(menu_date, '^\\\\d{4}')), -1) AS d, count(), round(avg(price), 2), bar(avg(price), 0, 50, 100), any(dish_name) FROM menu_item_denorm WHERE (menu_currency IN ('Dollars', '')) AND (d &gt; 0) AND (d &lt; 2022) AND (dish_name ILIKE '%caviar%') GROUP BY d ORDER BY d ASC;  Result: ┌────d─┬─count()─┬─round(avg(price), 2)─┬─bar(avg(price), 0, 50, 100)──────┬─any(dish_name)──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ 1090 │ 1 │ 0 │ │ Caviar │ │ 1880 │ 3 │ 0 │ │ Caviar │ │ 1890 │ 39 │ 0.59 │ █▏ │ Butter and caviar │ │ 1900 │ 1014 │ 0.34 │ ▋ │ Anchovy Caviar on Toast │ │ 1910 │ 1588 │ 1.35 │ ██▋ │ 1/1 Brötchen Caviar │ │ 1920 │ 927 │ 1.37 │ ██▋ │ ASTRAKAN CAVIAR │ │ 1930 │ 289 │ 1.91 │ ███▋ │ Astrachan caviar │ │ 1940 │ 201 │ 0.83 │ █▋ │ (SPECIAL) Domestic Caviar Sandwich │ │ 1950 │ 81 │ 2.27 │ ████▌ │ Beluga Caviar │ │ 1960 │ 126 │ 2.21 │ ████▍ │ Beluga Caviar │ │ 1970 │ 105 │ 0.95 │ █▊ │ BELUGA MALOSSOL CAVIAR AMERICAN DRESSING │ │ 1980 │ 12 │ 7.22 │ ██████████████▍ │ Authentic Iranian Beluga Caviar the world's finest black caviar presented in ice garni and a sampling of chilled 100° Russian vodka │ │ 1990 │ 74 │ 14.42 │ ████████████████████████████▋ │ Avocado Salad, Fresh cut avocado with caviare │ │ 2000 │ 3 │ 7.82 │ ███████████████▋ │ Aufgeschlagenes Kartoffelsueppchen mit Forellencaviar │ │ 2010 │ 6 │ 15.58 │ ███████████████████████████████▏ │ &quot;OYSTERS AND PEARLS&quot; &quot;Sabayon&quot; of Pearl Tapioca with Island Creek Oysters and Russian Sevruga Caviar │ └──────┴─────────┴──────────────────────┴──────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  At least they have caviar with vodka. Very nice. "},{"title":"Online Playground​","type":1,"pageTitle":"New York Public Library \"What's on the Menu?\" Dataset","url":"en/getting-started/example-datasets/menus#playground","content":"The data is uploaded to ClickHouse Playground, example. "},{"title":"Configuring ClickHouse Keeper","type":0,"sectionRef":"#","url":"en/guides/sre/clickhouse-keeper","content":"","keywords":""},{"title":"1. Configure Nodes with Keeper settings​","type":1,"pageTitle":"Configuring ClickHouse Keeper","url":"en/guides/sre/clickhouse-keeper#1-configure-nodes-with-keeper-settings","content":"Install 3 ClickHouse instances on 3 hosts (chnode1, chnode2, chnode3). (View the Quick Start for details on installing ClickHouse.) On each node, add the following entry to allow external communication through the network interface. &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; Add the following ClickHouse Keeper configuration to all three servers updating the &lt;server_id&gt; setting for each server; for chnode1 would be 1, chnode2 would be 2, etc. &lt;keeper_server&gt; &lt;tcp_port&gt;9181&lt;/tcp_port&gt; &lt;server_id&gt;1&lt;/server_id&gt; &lt;log_storage_path&gt;/var/lib/clickhouse/coordination/log&lt;/log_storage_path&gt; &lt;snapshot_storage_path&gt;/var/lib/clickhouse/coordination/snapshots&lt;/snapshot_storage_path&gt; &lt;coordination_settings&gt; &lt;operation_timeout_ms&gt;10000&lt;/operation_timeout_ms&gt; &lt;session_timeout_ms&gt;30000&lt;/session_timeout_ms&gt; &lt;raft_logs_level&gt;warning&lt;/raft_logs_level&gt; &lt;/coordination_settings&gt; &lt;raft_configuration&gt; &lt;server&gt; &lt;id&gt;1&lt;/id&gt; &lt;hostname&gt;chnode1.domain.com&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;2&lt;/id&gt; &lt;hostname&gt;chnode2.domain.com&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;3&lt;/id&gt; &lt;hostname&gt;chnode3.domain.com&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;/server&gt; &lt;/raft_configuration&gt; &lt;/keeper_server&gt; These are the basic settings used above: Parameter\tDescription\tExampletcp_port\tport to be used by clients of keeper\t9181 default equivalent of 2181 as in zookeeper server_id\tidentifier for each CLickHouse Keeper server used in raft configuration\t1 coordination_settings\tsection to parameters such as timeouts\ttimeouts: 10000, log level: trace server\tdefinition of server participating\tlist of each server definition raft_configuration\tsettings for each server in the keeper cluster\tserver and settings for each id\tnumeric id of the server for keeper services\t1 hostname\thostname, IP or FQDN of each server in the keeper cluster\tchnode1.domain.com port\tport to listen on for interserver keeper connections\t9444 note View the ClickHouse Keeper docs page for details on all the available parameters. Enable the Zookeeper component. It will use the ClickHouse Keeper engine: &lt;zookeeper&gt; &lt;node&gt; &lt;host&gt;chnode1.domain.com&lt;/host&gt; &lt;port&gt;9181&lt;/port&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;chnode2.domain.com&lt;/host&gt; &lt;port&gt;9181&lt;/port&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;chnode3.domain.com&lt;/host&gt; &lt;port&gt;9181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper&gt; These are the basic settings used above: Parameter\tDescription\tExamplenode\tlist of nodes for ClickHouse Keeper connections\tsettings entry for each server host\thostname, IP or FQDN of each ClickHouse keepr node\tchnode1.domain.com port\tClickHouse Keeper client port\t9181 Restart ClickHouse and verify that each Keeper instance is running. Execute the following command on each server. The ruok command returns imok if Keeper is running and healthy: # echo ruok | nc localhost 9181; echo imok The system database has a table named zookeeper that contains the details of your ClickHouse Keeper instances. Let's view the table: SELECT * FROM system.zookeeper WHERE path IN ('/', '/clickhouse') The table looks like: ┌─name───────┬─value─┬─czxid─┬─mzxid─┬───────────────ctime─┬───────────────mtime─┬─version─┬─cversion─┬─aversion─┬─ephemeralOwner─┬─dataLength─┬─numChildren─┬─pzxid─┬─path────────┐ │ clickhouse │ │ 124 │ 124 │ 2022-03-07 00:49:34 │ 2022-03-07 00:49:34 │ 0 │ 2 │ 0 │ 0 │ 0 │ 2 │ 5693 │ / │ │ task_queue │ │ 125 │ 125 │ 2022-03-07 00:49:34 │ 2022-03-07 00:49:34 │ 0 │ 1 │ 0 │ 0 │ 0 │ 1 │ 126 │ /clickhouse │ │ tables │ │ 5693 │ 5693 │ 2022-03-07 00:49:34 │ 2022-03-07 00:49:34 │ 0 │ 3 │ 0 │ 0 │ 0 │ 3 │ 6461 │ /clickhouse │ └────────────┴───────┴───────┴───────┴─────────────────────┴─────────────────────┴─────────┴──────────┴──────────┴────────────────┴────────────┴─────────────┴───────┴─────────────┘  "},{"title":"2. Configure a cluster in ClickHouse​","type":1,"pageTitle":"Configuring ClickHouse Keeper","url":"en/guides/sre/clickhouse-keeper#2--configure-a-cluster-in-clickhouse","content":"Let's configure a simple cluster with 2 shards and only one replica on 2 of the nodes. The third node will be used to achieve a quorum for the requirement in ClickHouse Keeper. Update the configuration on chnode1 and chnode2. The following cluster defines 1 shard on each node for a total of 2 shards with no replication. In this example, some of the data will be on node and some will be on the other node: &lt;cluster_2S_1R&gt; &lt;shard&gt; &lt;replica&gt; &lt;host&gt;chnode1.domain.com&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;ClickHouse123!&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;shard&gt; &lt;replica&gt; &lt;host&gt;chnode2.domain.com&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;ClickHouse123!&lt;/password&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/cluster_2S_1R&gt; Parameter\tDescription\tExampleshard\tlist of replicas on the cluster definition\tlist of replicas for each shard replica\tlist of settings for each replica\tsettings entries for each replica host\thostname, IP or FQDN of server that will host a replica shard\tchnode1.domain.com port\tport used to communicate using the native tcp protocol\t9000 user\tusername that will be used to authenticate to the cluster instances\tdefault password\tpassword for the user define to allow connections to cluster instances\tClickHouse123! Restart ClickHouse and verify the cluster was created: SHOW clusters; You should see your cluster: ┌─cluster───────┐ │ cluster_2S_1R │ └───────────────┘  "},{"title":"3. Create and test distributed table​","type":1,"pageTitle":"Configuring ClickHouse Keeper","url":"en/guides/sre/clickhouse-keeper#3-create-and-test-distributed-table","content":"Create a new database on the new cluster using ClickHouse client on chnode1. The ON CLUSTER clause automatically creates the database on both nodes. CREATE DATABASE db1 ON CLUSTER 'cluster_2S_1R'; Create a new table on the db1 database. Once again, ON CLUSTER creates the table on both nodes. CREATE TABLE db1.table1 on cluster 'cluster_2S_1R' ( `id` UInt64, `column1` String ) ENGINE = MergeTree ORDER BY column1 On the chnode1 node, add a couple of rows: INSERT INTO db1.table1 (id, column1) VALUES (1, 'abc'), (2, 'def') Add a couple of rows on the chnode2 node: INSERT INTO db1.table1 (id, column1) VALUES (3, 'ghi'), (4, 'jkl') Notice that running a SELECT statement on each node only shows the data on that node. For example, on chnode1: SELECT * FROM db1.table1 Query id: 7ef1edbc-df25-462b-a9d4-3fe6f9cb0b6d ┌─id─┬─column1─┐ │ 1 │ abc │ │ 2 │ def │ └────┴─────────┘ 2 rows in set. Elapsed: 0.006 sec. On chnode2: SELECT * FROM db1.table1 Query id: c43763cc-c69c-4bcc-afbe-50e764adfcbf ┌─id─┬─column1─┐ │ 3 │ ghi │ │ 4 │ jkl │ └────┴─────────┘ You can create a Distributed table to represent the data on the two shards. Tables with the Distributed table engine do not store any data of their own, but allow distributed query processing on multiple servers. Reads hit all the shards, and writes can be distributed across the shards. Run the following query on chnode1: CREATE TABLE db1.dist_table ( id UInt64, column1 String ) ENGINE = Distributed(cluster_2S_1R,db1,table1) Notice querying dist_table returns all four rows of data from the two shards: SELECT * FROM db1.dist_table Query id: 495bffa0-f849-4a0c-aeea-d7115a54747a ┌─id─┬─column1─┐ │ 1 │ abc │ │ 2 │ def │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 3 │ ghi │ │ 4 │ jkl │ └────┴─────────┘ 4 rows in set. Elapsed: 0.018 sec.  "},{"title":"Summary​","type":1,"pageTitle":"Configuring ClickHouse Keeper","url":"en/guides/sre/clickhouse-keeper#summary","content":"This guide demostrated how to setup a cluster using ClickHouse Keeper. With ClickHouse Keeper, you can configure clusters and define distributed tables that can be replicated across shards. "},{"title":"OnTime","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/ontime","content":"","keywords":""},{"title":"Import from Raw Data​","type":1,"pageTitle":"OnTime","url":"en/getting-started/example-datasets/ontime#import-from-raw-data","content":"Downloading data: wget --no-check-certificate --continue https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{1987..2021}_{1..12}.zip  Creating a table: CREATE TABLE `ontime` ( `Year` UInt16, `Quarter` UInt8, `Month` UInt8, `DayofMonth` UInt8, `DayOfWeek` UInt8, `FlightDate` Date, `Reporting_Airline` String, `DOT_ID_Reporting_Airline` Int32, `IATA_CODE_Reporting_Airline` String, `Tail_Number` String, `Flight_Number_Reporting_Airline` String, `OriginAirportID` Int32, `OriginAirportSeqID` Int32, `OriginCityMarketID` Int32, `Origin` FixedString(5), `OriginCityName` String, `OriginState` FixedString(2), `OriginStateFips` String, `OriginStateName` String, `OriginWac` Int32, `DestAirportID` Int32, `DestAirportSeqID` Int32, `DestCityMarketID` Int32, `Dest` FixedString(5), `DestCityName` String, `DestState` FixedString(2), `DestStateFips` String, `DestStateName` String, `DestWac` Int32, `CRSDepTime` Int32, `DepTime` Int32, `DepDelay` Int32, `DepDelayMinutes` Int32, `DepDel15` Int32, `DepartureDelayGroups` String, `DepTimeBlk` String, `TaxiOut` Int32, `WheelsOff` Int32, `WheelsOn` Int32, `TaxiIn` Int32, `CRSArrTime` Int32, `ArrTime` Int32, `ArrDelay` Int32, `ArrDelayMinutes` Int32, `ArrDel15` Int32, `ArrivalDelayGroups` Int32, `ArrTimeBlk` String, `Cancelled` UInt8, `CancellationCode` FixedString(1), `Diverted` UInt8, `CRSElapsedTime` Int32, `ActualElapsedTime` Int32, `AirTime` Nullable(Int32), `Flights` Int32, `Distance` Int32, `DistanceGroup` UInt8, `CarrierDelay` Int32, `WeatherDelay` Int32, `NASDelay` Int32, `SecurityDelay` Int32, `LateAircraftDelay` Int32, `FirstDepTime` String, `TotalAddGTime` String, `LongestAddGTime` String, `DivAirportLandings` String, `DivReachedDest` String, `DivActualElapsedTime` String, `DivArrDelay` String, `DivDistance` String, `Div1Airport` String, `Div1AirportID` Int32, `Div1AirportSeqID` Int32, `Div1WheelsOn` String, `Div1TotalGTime` String, `Div1LongestGTime` String, `Div1WheelsOff` String, `Div1TailNum` String, `Div2Airport` String, `Div2AirportID` Int32, `Div2AirportSeqID` Int32, `Div2WheelsOn` String, `Div2TotalGTime` String, `Div2LongestGTime` String, `Div2WheelsOff` String, `Div2TailNum` String, `Div3Airport` String, `Div3AirportID` Int32, `Div3AirportSeqID` Int32, `Div3WheelsOn` String, `Div3TotalGTime` String, `Div3LongestGTime` String, `Div3WheelsOff` String, `Div3TailNum` String, `Div4Airport` String, `Div4AirportID` Int32, `Div4AirportSeqID` Int32, `Div4WheelsOn` String, `Div4TotalGTime` String, `Div4LongestGTime` String, `Div4WheelsOff` String, `Div4TailNum` String, `Div5Airport` String, `Div5AirportID` Int32, `Div5AirportSeqID` Int32, `Div5WheelsOn` String, `Div5TotalGTime` String, `Div5LongestGTime` String, `Div5WheelsOff` String, `Div5TailNum` String ) ENGINE = MergeTree PARTITION BY Year ORDER BY (IATA_CODE_Reporting_Airline, FlightDate) SETTINGS index_granularity = 8192;  Loading data with multiple threads: ls -1 *.zip | xargs -I{} -P $(nproc) bash -c &quot;echo {}; unzip -cq {} '*.csv' | sed 's/\\.00//g' | clickhouse-client --input_format_with_names_use_header=0 --query='INSERT INTO ontime FORMAT CSVWithNames'&quot;  (if you will have memory shortage or other issues on your server, remove the -P $(nproc) part) "},{"title":"Download of Prepared Partitions​","type":1,"pageTitle":"OnTime","url":"en/getting-started/example-datasets/ontime#download-of-prepared-partitions","content":"$ curl -O https://datasets.clickhouse.com/ontime/partitions/ontime.tar $ tar xvf ontime.tar -C /var/lib/clickhouse # path to ClickHouse data directory $ # check permissions of unpacked data, fix if required $ sudo service clickhouse-server restart $ clickhouse-client --query &quot;select count(*) from datasets.ontime&quot;  note If you will run the queries described below, you have to use the full table name, datasets.ontime. !!! info &quot;Info&quot; If you are using the prepared partitions or the Online Playground replace any occurrence of IATA_CODE_Reporting_Airline or IATA_CODE_Reporting_Airline AS Carrier in the following queries with Carrier (see describe ontime). "},{"title":"Queries​","type":1,"pageTitle":"OnTime","url":"en/getting-started/example-datasets/ontime#queries","content":"Q0. SELECT avg(c1) FROM ( SELECT Year, Month, count(*) AS c1 FROM ontime GROUP BY Year, Month );  Q1. The number of flights per day from the year 2000 to 2008 SELECT DayOfWeek, count(*) AS c FROM ontime WHERE Year&gt;=2000 AND Year&lt;=2008 GROUP BY DayOfWeek ORDER BY c DESC;  Q2. The number of flights delayed by more than 10 minutes, grouped by the day of the week, for 2000-2008 SELECT DayOfWeek, count(*) AS c FROM ontime WHERE DepDelay&gt;10 AND Year&gt;=2000 AND Year&lt;=2008 GROUP BY DayOfWeek ORDER BY c DESC;  Q3. The number of delays by the airport for 2000-2008 SELECT Origin, count(*) AS c FROM ontime WHERE DepDelay&gt;10 AND Year&gt;=2000 AND Year&lt;=2008 GROUP BY Origin ORDER BY c DESC LIMIT 10;  Q4. The number of delays by carrier for 2007 SELECT IATA_CODE_Reporting_Airline AS Carrier, count(*) FROM ontime WHERE DepDelay&gt;10 AND Year=2007 GROUP BY Carrier ORDER BY count(*) DESC;  Q5. The percentage of delays by carrier for 2007 SELECT Carrier, c, c2, c*100/c2 as c3 FROM ( SELECT IATA_CODE_Reporting_Airline AS Carrier, count(*) AS c FROM ontime WHERE DepDelay&gt;10 AND Year=2007 GROUP BY Carrier ) q JOIN ( SELECT IATA_CODE_Reporting_Airline AS Carrier, count(*) AS c2 FROM ontime WHERE Year=2007 GROUP BY Carrier ) qq USING Carrier ORDER BY c3 DESC;  Better version of the same query: SELECT IATA_CODE_Reporting_Airline AS Carrier, avg(DepDelay&gt;10)*100 AS c3 FROM ontime WHERE Year=2007 GROUP BY Carrier ORDER BY c3 DESC  Q6. The previous request for a broader range of years, 2000-2008 SELECT Carrier, c, c2, c*100/c2 as c3 FROM ( SELECT IATA_CODE_Reporting_Airline AS Carrier, count(*) AS c FROM ontime WHERE DepDelay&gt;10 AND Year&gt;=2000 AND Year&lt;=2008 GROUP BY Carrier ) q JOIN ( SELECT IATA_CODE_Reporting_Airline AS Carrier, count(*) AS c2 FROM ontime WHERE Year&gt;=2000 AND Year&lt;=2008 GROUP BY Carrier ) qq USING Carrier ORDER BY c3 DESC;  Better version of the same query: SELECT IATA_CODE_Reporting_Airline AS Carrier, avg(DepDelay&gt;10)*100 AS c3 FROM ontime WHERE Year&gt;=2000 AND Year&lt;=2008 GROUP BY Carrier ORDER BY c3 DESC;  Q7. Percentage of flights delayed for more than 10 minutes, by year SELECT Year, c1/c2 FROM ( select Year, count(*)*100 as c1 from ontime WHERE DepDelay&gt;10 GROUP BY Year ) q JOIN ( select Year, count(*) as c2 from ontime GROUP BY Year ) qq USING (Year) ORDER BY Year;  Better version of the same query: SELECT Year, avg(DepDelay&gt;10)*100 FROM ontime GROUP BY Year ORDER BY Year;  Q8. The most popular destinations by the number of directly connected cities for various year ranges SELECT DestCityName, uniqExact(OriginCityName) AS u FROM ontime WHERE Year &gt;= 2000 and Year &lt;= 2010 GROUP BY DestCityName ORDER BY u DESC LIMIT 10;  Q9. SELECT Year, count(*) AS c1 FROM ontime GROUP BY Year;  Q10. SELECT min(Year), max(Year), IATA_CODE_Reporting_Airline AS Carrier, count(*) AS cnt, sum(ArrDelayMinutes&gt;30) AS flights_delayed, round(sum(ArrDelayMinutes&gt;30)/count(*),2) AS rate FROM ontime WHERE DayOfWeek NOT IN (6,7) AND OriginState NOT IN ('AK', 'HI', 'PR', 'VI') AND DestState NOT IN ('AK', 'HI', 'PR', 'VI') AND FlightDate &lt; '2010-01-01' GROUP by Carrier HAVING cnt&gt;100000 and max(Year)&gt;1990 ORDER by rate DESC LIMIT 1000;  Bonus: SELECT avg(cnt) FROM ( SELECT Year,Month,count(*) AS cnt FROM ontime WHERE DepDel15=1 GROUP BY Year,Month ); SELECT avg(c1) FROM ( SELECT Year,Month,count(*) AS c1 FROM ontime GROUP BY Year,Month ); SELECT DestCityName, uniqExact(OriginCityName) AS u FROM ontime GROUP BY DestCityName ORDER BY u DESC LIMIT 10; SELECT OriginCityName, DestCityName, count() AS c FROM ontime GROUP BY OriginCityName, DestCityName ORDER BY c DESC LIMIT 10; SELECT OriginCityName, count() AS c FROM ontime GROUP BY OriginCityName ORDER BY c DESC LIMIT 10;  You can also play with the data in Playground, example. This performance test was created by Vadim Tkachenko. See: https://www.percona.com/blog/2009/10/02/analyzing-air-traffic-performance-with-infobright-and-monetdb/https://www.percona.com/blog/2009/10/26/air-traffic-queries-in-luciddb/https://www.percona.com/blog/2009/11/02/air-traffic-queries-in-infinidb-early-alpha/https://www.percona.com/blog/2014/04/21/using-apache-hadoop-and-impala-together-with-mysql-for-data-analysis/https://www.percona.com/blog/2016/01/07/apache-spark-with-air-ontime-performance-data/http://nickmakos.blogspot.ru/2012/08/analyzing-air-traffic-performance-with.html Original article "},{"title":"Configuring ClickHouse to Use LDAP for Authentication and Role Mapping","type":0,"sectionRef":"#","url":"en/guides/sre/configuring-ldap","content":"","keywords":""},{"title":"1. Configure LDAP connection settings in ClickHouse​","type":1,"pageTitle":"Configuring ClickHouse to Use LDAP for Authentication and Role Mapping","url":"en/guides/sre/configuring-ldap#1-configure-ldap-connection-settings-in-clickhouse","content":"Test your connection to this public LDAP server: $ ldapsearch -x -b dc=example,dc=com -H ldap://ldap.forumsys.com The reply will be something like this: # extended LDIF # # LDAPv3 # base &lt;dc=example,dc=com&gt; with scope subtree # filter: (objectclass=*) # requesting: ALL # # example.com dn: dc=example,dc=com objectClass: top objectClass: dcObject objectClass: organization o: example.com dc: example ... Edit the config.xml file and add the following to configure LDAP: &lt;ldap_servers&gt; &lt;test_ldap_server&gt; &lt;host&gt;ldap.forumsys.com&lt;/host&gt; &lt;port&gt;389&lt;/port&gt; &lt;bind_dn&gt;uid={user_name},dc=example,dc=com&lt;/bind_dn&gt; &lt;enable_tls&gt;no&lt;/enable_tls&gt; &lt;tls_require_cert&gt;never&lt;/tls_require_cert&gt; &lt;/test_ldap_server&gt; &lt;/ldap_servers&gt; note The &lt;test_ldap_server&gt; tags is an arbitrary label to identify a particular LDAP server. These are the basic settings used above: Parameter\tDescription\tExamplehost\thostname or IP of LDAP server\tldap.forumsys.com port\tdirectory port for LDAP server\t389 bind_dn\ttemplate path to users\tuid={user_name},dc=example,dc=com enable_tls\twhether to use secure ldap\tno tls_require_cert\twhether to require certificate for connection\tnever note In this example, since the public server uses 389 and does not use a secure port, we disable TLS for demonstration purposes. note View the LDAP doc page for more details on the LDAP settings. Add the &lt;ldap&gt; section to &lt;user_directories&gt; section to configure the user role mapping. This section defines when a user is authenticated and what role the user will receive. In this basic example, any user authenticating to LDAP will receive the scientists_role which will be defined at a later step in ClickHouse. The section should look similar to this: &lt;user_directories&gt; &lt;users_xml&gt; &lt;path&gt;users.xml&lt;/path&gt; &lt;/users_xml&gt; &lt;local_directory&gt; &lt;path&gt;/var/lib/clickhouse/access/&lt;/path&gt; &lt;/local_directory&gt; &lt;ldap&gt; &lt;server&gt;test_ldap_server&lt;/server&gt; &lt;roles&gt; &lt;scientists_role /&gt; &lt;/roles&gt; &lt;role_mapping&gt; &lt;base_dn&gt;dc=example,dc=com&lt;/base_dn&gt; &lt;search_filter&gt;(&amp;amp;(objectClass=groupOfUniqueNames)(uniqueMember={bind_dn}))&lt;/search_filter&gt; &lt;attribute&gt;cn&lt;/attribute&gt; &lt;/role_mapping&gt; &lt;/ldap&gt; &lt;/user_directories&gt; These are the basic settings used above: Parameter\tDescription\tExampleserver\tlabel defined in the prior ldap_servers section\ttest_ldap_server roles\tname of the roles defined in ClickHouse the users will be mapped to\tscientists_role base_dn\tbase path to start search for groups with user\tdc=example,dc=com search_filter\tldap search filter to identify groups to select for mapping users\t(&amp;(objectClass=groupOfUniqueNames)(uniqueMember={bind_dn})) attribute\twhich attribute name should value be returned from\tcn Restart your ClickHouse server to apply the settings. "},{"title":"2. Configure ClickHouse database roles and permissions​","type":1,"pageTitle":"Configuring ClickHouse to Use LDAP for Authentication and Role Mapping","url":"en/guides/sre/configuring-ldap#2-configure-clickhouse-database-roles-and-permissions","content":"note The procedures in this section assumes that SQL Access Control and Account Management in ClickHouse has been enabled. To enable, view the SQL Users and Roles guide. Create a role in clickhouse with the same name used in the role mapping section of the config.xml file CREATE ROLE scientists_role; Grant needed privileges to the role. The following statement grants admin privileges to any user able to authenticate through LDAP: GRANT ALL ON *.* TO scientists_role;  "},{"title":"3. Test the LDAP configuration​","type":1,"pageTitle":"Configuring ClickHouse to Use LDAP for Authentication and Role Mapping","url":"en/guides/sre/configuring-ldap#3-test-the-ldap-configuration","content":"Login using the ClickHouse client $ clickhouse-client --user einstein --password password ClickHouse client version 22.2.2.1. Connecting to localhost:9000 as user einstein. Connected to ClickHouse server version 22.2.2 revision 54455. chnode1 :) note Use the ldapsearch command in step 1 to view all of the users available in the directory and for all of the users the password is password Test that the user was mapped correctly to the scientists_role role and has admin permissions SHOW DATABASES Query id: 93b785ff-1482-4eda-95b0-b2d68b2c5e0f ┌─name───────────────┐ │ INFORMATION_SCHEMA │ │ db1_mysql │ │ db2 │ │ db3 │ │ db4_mysql │ │ db5_merge │ │ default │ │ information_schema │ │ system │ └────────────────────┘ 9 rows in set. Elapsed: 0.004 sec.  "},{"title":"Summary​","type":1,"pageTitle":"Configuring ClickHouse to Use LDAP for Authentication and Role Mapping","url":"en/guides/sre/configuring-ldap#summary","content":"This article demostrated the basics of configuring ClickHouse to authenticate to an LDAP server and also to map to a role. There are also options for configuring individual users in ClickHouse but having those users be authenticated by LDAP without configuring automated role mapping. The LDAP module can also be used to connect to Active Directory. "},{"title":"Configuring SSL-TLS","type":0,"sectionRef":"#","url":"en/guides/sre/configuring-ssl","content":"","keywords":""},{"title":"1. Create a ClickHouse Deployment​","type":1,"pageTitle":"Configuring SSL-TLS","url":"en/guides/sre/configuring-ssl#1-create-a-clickhouse-deployment","content":"This guide was written using Ubuntu 20.04 and ClickHouse installed on the following hosts using the DEB package (using apt). The domain is marsnet.local: Host\tIP Addresschnode1\t192.168.1.221 chnode2\t192.168.1.222 chnode3\t192.168.1.223 note View the Quick Start for more details on how to install ClickHouse. "},{"title":"2. Create SSL certicates​","type":1,"pageTitle":"Configuring SSL-TLS","url":"en/guides/sre/configuring-ssl#2-create-ssl-certicates","content":"note Using self-signed certificates are for demonstration purposes only and should not used in production. Certificate requests should be created to be signed by the organization and validated using the CA chain that will be configured in the settings. However, these steps can be used to configure and test settings, then can be replaced by the actual certificates that will be used. Generate a key that will be used for the new CA: openssl genrsa -out marsnet_ca.key 2048 Generate a new self-signed CA certificate. The following will create a new certificate that will be used to sign other certificates using the CA key: openssl req -x509 -subj &quot;/CN=marsnet.local CA&quot; -nodes -key marsnet_ca.key -days 1095 -out marsnet_ca.crt note Backup the key and CA certificate in a secure location not in the cluster. After generating the node certificates, the key should be deleted from the cluster nodes. Verify the contents of the new CA certificate: openssl x509 -in marsnet_ca.crt -text Create a certificate request (CSR) and generate a key for each node: openssl req -newkey rsa:2048 -nodes -subj &quot;/CN=chnode1&quot; -addext &quot;subjectAltName = DNS:chnode1.marsnet.local,IP:192.168.1.221&quot; -keyout chnode1.key -out chnode1.csr openssl req -newkey rsa:2048 -nodes -subj &quot;/CN=chnode2&quot; -addext &quot;subjectAltName = DNS:chnode2.marsnet.local,IP:192.168.1.222&quot; -keyout chnode2.key -out chnode2.csr openssl req -newkey rsa:2048 -nodes -subj &quot;/CN=chnode3&quot; -addext &quot;subjectAltName = DNS:chnode3.marsnet.local,IP:192.168.1.223&quot; -keyout chnode3.key -out chnode3.csr Using the CSR and CA, create new certificate and key pairs: openssl x509 -req -in chnode1.csr -out chnode1.crt -CAcreateserial -CA marsnet_ca.crt -CAkey marsnet_ca.key -days 365 openssl x509 -req -in chnode2.csr -out chnode2.crt -CAcreateserial -CA marsnet_ca.crt -CAkey marsnet_ca.key -days 365 openssl x509 -req -in chnode3.csr -out chnode3.crt -CAcreateserial -CA marsnet_ca.crt -CAkey marsnet_ca.key -days 365 Verify the certs for subject and issuer: openssl x509 -in chnode1.crt -text -noout Check that the new certificates verify against the CA cert: openssl verify -CAfile marsnet_ca.crt chnode1.crt chnode1.crt: OK  "},{"title":"3. Create and Configure a directory to store certificates and keys.​","type":1,"pageTitle":"Configuring SSL-TLS","url":"en/guides/sre/configuring-ssl#3-create-and-configure-a-directory-to-store-certificates-and-keys","content":"note This must be done on each node. Use appropriate certificates and keys on each host. Create a folder in a directory accessible by ClickHouse in each node. We recommend the default configuration directory (e.g. /etc/clickhouse-server): mkdir /etc/clickhouse-server/certs Copy the CA certificate, node certifiate and key corresponding to each node to the new certs directory. Update owner and permissions to allow ClickHouse to read the certificates: chown clickhouse:clickhouse -R /etc/clickhouse-server/certs chmod 600 /etc/clickhouse-server/certs/* chmod 755 /etc/clickhouse-server/certs ll /etc/clickhouse-server/certs total 20 drw-r--r-- 2 clickhouse clickhouse 4096 Apr 12 20:23 ./ drwx------ 5 clickhouse clickhouse 4096 Apr 12 20:23 ../ -rw------- 1 clickhouse clickhouse 997 Apr 12 20:22 chnode1.crt -rw------- 1 clickhouse clickhouse 1708 Apr 12 20:22 chnode1.key -rw------- 1 clickhouse clickhouse 1131 Apr 12 20:23 marsnet_ca.crt  "},{"title":"4. Configure the environment with basic clusters using ClickHouse Keeper​","type":1,"pageTitle":"Configuring SSL-TLS","url":"en/guides/sre/configuring-ssl#4-configure-the-environment-with-basic-clusters-using-clickhouse-keeper","content":"For this deployment environment, the following ClickHouse Keeper settings are used in each node. Each server will have its own &lt;server_id&gt;. (For example, &lt;server_id&gt;1&lt;/server_id&gt; for node chnode1, and so on.) note Recommended port is 9281 for ClickHouse Keeper. However, the port is configurable and can be set if this port is in use already by another application in the environment. For a full explanation of all options, visit https://clickhouse.com/docs/en/operations/clickhouse-keeper/ Add the following inside the &lt;clickhouse&gt; tag in ClickHouse server config.xml note For production environments, it is recommended to use a separate .xml config file in the config.d directory. For more information, visit https://clickhouse.com/docs/en/operations/configuration-files/ &lt;keeper_server&gt; &lt;tcp_port_secure&gt;9281&lt;/tcp_port_secure&gt; &lt;server_id&gt;1&lt;/server_id&gt; &lt;log_storage_path&gt;/var/lib/clickhouse/coordination/log&lt;/log_storage_path&gt; &lt;snapshot_storage_path&gt;/var/lib/clickhouse/coordination/snapshots&lt;/snapshot_storage_path&gt; &lt;coordination_settings&gt; &lt;operation_timeout_ms&gt;10000&lt;/operation_timeout_ms&gt; &lt;session_timeout_ms&gt;30000&lt;/session_timeout_ms&gt; &lt;raft_logs_level&gt;trace&lt;/raft_logs_level&gt; &lt;/coordination_settings&gt; &lt;raft_configuration&gt; &lt;server&gt; &lt;id&gt;1&lt;/id&gt; &lt;hostname&gt;chnode1.marsnet.local&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;2&lt;/id&gt; &lt;hostname&gt;chnode2.marsnet.local&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;3&lt;/id&gt; &lt;hostname&gt;chnode3.marsnet.local&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;/server&gt; &lt;/raft_configuration&gt; &lt;/keeper_server&gt; Uncomment and update the keeper settings on all nodes and set the &lt;secure&gt; flag to 1: &lt;zookeeper&gt; &lt;node&gt; &lt;host&gt;chnode1.marsnet.local&lt;/host&gt; &lt;port&gt;9281&lt;/port&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;chnode2.marsnet.local&lt;/host&gt; &lt;port&gt;9281&lt;/port&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;chnode3.marsnet.local&lt;/host&gt; &lt;port&gt;9281&lt;/port&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;/node&gt; &lt;/zookeeper&gt; Update and add the following cluster settings to chnode1 and chnode2. chnode3 will be used for the ClickHouse Keeper quorum. note For this configuration, only one example cluster is configured. The test sample clusters must be either removed, commented out or if an existing cluster exists that is being tested, then the port must be updated and the &lt;secure&gt; option must be added. The &lt;user and &lt;password&gt; must be set if the default user was initially configured to have a password in the installation or in the users.xml file. The following creates a cluster with one shard replica on two servers (one on each node). &lt;remote_servers&gt; &lt;cluster_1S_2R&gt; &lt;shard&gt; &lt;replica&gt; &lt;host&gt;chnode1.marsnet.local&lt;/host&gt; &lt;port&gt;9440&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;ClickHouse123!&lt;/password&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;chnode2.marsnet.local&lt;/host&gt; &lt;port&gt;9440&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;ClickHouse123!&lt;/password&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;/replica&gt; &lt;/shard&gt; &lt;/cluster_1S_2R&gt; &lt;/remote_servers&gt; Define macros values to be able to create a ReplicatedMergeTree table for testing. On chnode1: &lt;macros&gt; &lt;shard&gt;1&lt;/shard&gt; &lt;replica&gt;replica_1&lt;/replica&gt; &lt;/macros&gt; On chnode2: &lt;macros&gt; &lt;shard&gt;1&lt;/shard&gt; &lt;replica&gt;replica_2&lt;/replica&gt; &lt;/macros&gt;  "},{"title":"5. Configure SSL-TLS interfaces on ClickHouse nodes​","type":1,"pageTitle":"Configuring SSL-TLS","url":"en/guides/sre/configuring-ssl#5-configure-ssl-tls-interfaces-on-clickhouse-nodes","content":"The settings below are configured in the ClickHouse server config.xml Set the display name for the deployment (optional): &lt;display_name&gt;clickhouse&lt;/display_name&gt; Set ClickHouse to listen on external ports: &lt;listen_host&gt;0.0.0.0&lt;/listen_host&gt; Configure the https port and disable the http port on each node: &lt;https_port&gt;8443&lt;/https_port&gt; &lt;!--&lt;http_port&gt;8123&lt;/http_port&gt;--&gt; Configure the ClickHouse Native secure TCP port and disable the default non-secure port on each node: &lt;tcp_port_secure&gt;9440&lt;/tcp_port_secure&gt; &lt;!--&lt;tcp_port&gt;9000&lt;/tcp_port&gt;--&gt; Configure the interserver https port and disable the default non-secure port on each node: &lt;interserver_https_port&gt;9010&lt;/interserver_https_port&gt; &lt;!--&lt;interserver_http_port&gt;9009&lt;/interserver_http_port&gt;--&gt; Configure OpenSSL with certificates and paths note Each filename and path must be updated to match the node that it is being configured on. For example, update the &lt;certificateFile&gt; entry to be chnode2.crt when configuring in chnode2 host. &lt;openSSL&gt; &lt;server&gt; &lt;certificateFile&gt;/etc/clickhouse-server/certs/chnode1.crt&lt;/certificateFile&gt; &lt;privateKeyFile&gt;/etc/clickhouse-server/certs/chnode1.key&lt;/privateKeyFile&gt; &lt;verificationMode&gt;relaxed&lt;/verificationMode&gt; &lt;caConfig&gt;/etc/clickhouse-server/certs/marsnet_ca.crt&lt;/caConfig&gt; &lt;cacheSessions&gt;true&lt;/cacheSessions&gt; &lt;disableProtocols&gt;sslv2,sslv3&lt;/disableProtocols&gt; &lt;preferServerCiphers&gt;true&lt;/preferServerCiphers&gt; &lt;/server&gt; &lt;client&gt; &lt;loadDefaultCAFile&gt;false&lt;/loadDefaultCAFile&gt; &lt;caConfig&gt;/etc/clickhouse-server/certs/marsnet_ca.crt&lt;/caConfig&gt; &lt;cacheSessions&gt;true&lt;/cacheSessions&gt; &lt;disableProtocols&gt;sslv2,sslv3&lt;/disableProtocols&gt; &lt;preferServerCiphers&gt;true&lt;/preferServerCiphers&gt; &lt;verificationMode&gt;relaxed&lt;/verificationMode&gt; &lt;invalidCertificateHandler&gt; &lt;name&gt;RejectCertificateHandler&lt;/name&gt; &lt;/invalidCertificateHandler&gt; &lt;/client&gt; &lt;/openSSL&gt; For more information, visit https://clickhouse.com/docs/en/operations/server-configuration-parameters/settings/#server_configuration_parameters-openssl Configure gRPC for SSL on every node: &lt;grpc&gt; &lt;enable_ssl&gt;1&lt;/enable_ssl&gt; &lt;ssl_cert_file&gt;/etc/clickhouse-server/certs/chnode1.crt&lt;/ssl_cert_file&gt; &lt;ssl_key_file&gt;/etc/clickhouse-server/certs/chnode1.key&lt;/ssl_key_file&gt; &lt;ssl_require_client_auth&gt;true&lt;/ssl_require_client_auth&gt; &lt;ssl_ca_cert_file&gt;/etc/clickhouse-server/certs/marsnet_ca.crt&lt;/ssl_ca_cert_file&gt; &lt;transport_compression_type&gt;none&lt;/transport_compression_type&gt; &lt;transport_compression_level&gt;0&lt;/transport_compression_level&gt; &lt;max_send_message_size&gt;-1&lt;/max_send_message_size&gt; &lt;max_receive_message_size&gt;-1&lt;/max_receive_message_size&gt; &lt;verbose_logs&gt;false&lt;/verbose_logs&gt; &lt;/grpc&gt; For more information, visit https://clickhouse.com/docs/en/interfaces/grpc/ Configure ClickHouse client on at least one of the nodes to use SSL for connections in its own config.xml file (by default in /etc/clickhouse-client/): &lt;openSSL&gt; &lt;client&gt; &lt;loadDefaultCAFile&gt;false&lt;/loadDefaultCAFile&gt; &lt;caConfig&gt;/etc/clickhouse-server/certs/marsnet_ca.crt&lt;/caConfig&gt; &lt;cacheSessions&gt;true&lt;/cacheSessions&gt; &lt;disableProtocols&gt;sslv2,sslv3&lt;/disableProtocols&gt; &lt;preferServerCiphers&gt;true&lt;/preferServerCiphers&gt; &lt;invalidCertificateHandler&gt; &lt;name&gt;RejectCertificateHandler&lt;/name&gt; &lt;/invalidCertificateHandler&gt; &lt;/client&gt; &lt;/openSSL&gt; Disable default emulation ports for MySQL and PostgreSQL: &lt;!--mysql_port&gt;9004&lt;/mysql_port--&gt; &lt;!--postgresql_port&gt;9005&lt;/postgresql_port--&gt;  "},{"title":"6. Testing​","type":1,"pageTitle":"Configuring SSL-TLS","url":"en/guides/sre/configuring-ssl#6-testing","content":"Start all nodes, one at a time: service clickhouse-server start Verify secure ports are up and listening, should look similar to this example on each node: root@chnode1:/etc/clickhouse-server# netstat -ano | grep tcp tcp 0 0 0.0.0.0:9010 0.0.0.0:* LISTEN off (0.00/0/0) tcp 0 0 127.0.0.53:53 0.0.0.0:* LISTEN off (0.00/0/0) tcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN off (0.00/0/0) tcp 0 0 0.0.0.0:8443 0.0.0.0:* LISTEN off (0.00/0/0) tcp 0 0 0.0.0.0:9440 0.0.0.0:* LISTEN off (0.00/0/0) tcp 0 0 0.0.0.0:9281 0.0.0.0:* LISTEN off (0.00/0/0) tcp 0 0 192.168.1.221:33046 192.168.1.222:9444 ESTABLISHED off (0.00/0/0) tcp 0 0 192.168.1.221:42730 192.168.1.223:9444 ESTABLISHED off (0.00/0/0) tcp 0 0 192.168.1.221:51952 192.168.1.222:9281 ESTABLISHED off (0.00/0/0) tcp 0 0 192.168.1.221:22 192.168.1.210:49801 ESTABLISHED keepalive (6618.05/0/0) tcp 0 64 192.168.1.221:22 192.168.1.210:59195 ESTABLISHED on (0.24/0/0) tcp6 0 0 :::22 :::* LISTEN off (0.00/0/0) tcp6 0 0 :::9444 :::* LISTEN off (0.00/0/0) tcp6 0 0 192.168.1.221:9444 192.168.1.222:59046 ESTABLISHED off (0.00/0/0) tcp6 0 0 192.168.1.221:9444 192.168.1.223:41976 ESTABLISHED off (0.00/0/0) ClickHouse Port\tDescription8443\thttps interface 9010\tinterserver https port 9281\tClickHouse Keeper secure port 9440\tsecure Native TCP protocol 9444\tClickHouse Keeper Raft port Start the ClickHouse client using --secure flag and SSL port: root@chnode1:/etc/clickhouse-server# clickhouse-client --user default --password ClickHouse123! --port 9440 --secure --host chnode1.marsnet.local ClickHouse client version 22.3.3.44 (official build). Connecting to chnode1.marsnet.local:9440 as user default. Connected to ClickHouse server version 22.3.3 revision 54455. clickhouse :) Log into the Play UI using the https interface at https://chnode1.marsnet.local:8443/play. note the browser will show an untrusted certificate since it is being reached from a workstation and the certificates are not in the root CA stores on the client machine. When using certificates issued from a public authority or enterprise CA, it should show trusted. Create a replicated table: clickhouse :) CREATE TABLE repl_table ON CLUSTER cluster_1S_2R ( id UInt64, column1 Date, column2 String ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/default/repl_table', '{replica}' ) ORDER BY (id); ┌─host──────────────────┬─port─┬─status─┬─error─┬─num_hosts_remaining─┬─num_hosts_active─┐ │ chnode2.marsnet.local │ 9440 │ 0 │ │ 1 │ 0 │ │ chnode1.marsnet.local │ 9440 │ 0 │ │ 0 │ 0 │ └───────────────────────┴──────┴────────┴───────┴─────────────────────┴──────────────────┘ Add a couple rows on chnode1: INSERT INTO repl_table (id, column1, column2) VALUES (1,'2022-04-01','abc'), (2,'2022-04-02','def'); Verify the replication by viewing the rows on chnode2: SELECT * FROM repl_table ┌─id─┬────column1─┬─column2─┐ │ 1 │ 2022-04-01 │ abc │ │ 2 │ 2022-04-02 │ def │ └────┴────────────┴─────────┘  "},{"title":"Summary​","type":1,"pageTitle":"Configuring SSL-TLS","url":"en/guides/sre/configuring-ssl#summary","content":"This article focused on getting a ClickHouse environment configured with SSL/TLS. The settings will differ for different requirements in production environments; for example, certificate verification levels, protocols, ciphers, etc. But you should now have a good understanding of the steps involved in configuring and implementing secure connections. "},{"title":"Star Schema Benchmark","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/star-schema","content":"Star Schema Benchmark Compiling dbgen: $ git clone git@github.com:vadimtk/ssb-dbgen.git $ cd ssb-dbgen $ make Generating data: warning With -s 100 dbgen generates 600 million rows (67 GB), while while -s 1000 it generates 6 billion rows (which takes a lot of time) $ ./dbgen -s 1000 -T c $ ./dbgen -s 1000 -T l $ ./dbgen -s 1000 -T p $ ./dbgen -s 1000 -T s $ ./dbgen -s 1000 -T d Creating tables in ClickHouse: CREATE TABLE customer ( C_CUSTKEY UInt32, C_NAME String, C_ADDRESS String, C_CITY LowCardinality(String), C_NATION LowCardinality(String), C_REGION LowCardinality(String), C_PHONE String, C_MKTSEGMENT LowCardinality(String) ) ENGINE = MergeTree ORDER BY (C_CUSTKEY); CREATE TABLE lineorder ( LO_ORDERKEY UInt32, LO_LINENUMBER UInt8, LO_CUSTKEY UInt32, LO_PARTKEY UInt32, LO_SUPPKEY UInt32, LO_ORDERDATE Date, LO_ORDERPRIORITY LowCardinality(String), LO_SHIPPRIORITY UInt8, LO_QUANTITY UInt8, LO_EXTENDEDPRICE UInt32, LO_ORDTOTALPRICE UInt32, LO_DISCOUNT UInt8, LO_REVENUE UInt32, LO_SUPPLYCOST UInt32, LO_TAX UInt8, LO_COMMITDATE Date, LO_SHIPMODE LowCardinality(String) ) ENGINE = MergeTree PARTITION BY toYear(LO_ORDERDATE) ORDER BY (LO_ORDERDATE, LO_ORDERKEY); CREATE TABLE part ( P_PARTKEY UInt32, P_NAME String, P_MFGR LowCardinality(String), P_CATEGORY LowCardinality(String), P_BRAND LowCardinality(String), P_COLOR LowCardinality(String), P_TYPE LowCardinality(String), P_SIZE UInt8, P_CONTAINER LowCardinality(String) ) ENGINE = MergeTree ORDER BY P_PARTKEY; CREATE TABLE supplier ( S_SUPPKEY UInt32, S_NAME String, S_ADDRESS String, S_CITY LowCardinality(String), S_NATION LowCardinality(String), S_REGION LowCardinality(String), S_PHONE String ) ENGINE = MergeTree ORDER BY S_SUPPKEY; Inserting data: $ clickhouse-client --query &quot;INSERT INTO customer FORMAT CSV&quot; &lt; customer.tbl $ clickhouse-client --query &quot;INSERT INTO part FORMAT CSV&quot; &lt; part.tbl $ clickhouse-client --query &quot;INSERT INTO supplier FORMAT CSV&quot; &lt; supplier.tbl $ clickhouse-client --query &quot;INSERT INTO lineorder FORMAT CSV&quot; &lt; lineorder.tbl Converting “star schema” to denormalized “flat schema”: SET max_memory_usage = 20000000000; CREATE TABLE lineorder_flat ENGINE = MergeTree PARTITION BY toYear(LO_ORDERDATE) ORDER BY (LO_ORDERDATE, LO_ORDERKEY) AS SELECT l.LO_ORDERKEY AS LO_ORDERKEY, l.LO_LINENUMBER AS LO_LINENUMBER, l.LO_CUSTKEY AS LO_CUSTKEY, l.LO_PARTKEY AS LO_PARTKEY, l.LO_SUPPKEY AS LO_SUPPKEY, l.LO_ORDERDATE AS LO_ORDERDATE, l.LO_ORDERPRIORITY AS LO_ORDERPRIORITY, l.LO_SHIPPRIORITY AS LO_SHIPPRIORITY, l.LO_QUANTITY AS LO_QUANTITY, l.LO_EXTENDEDPRICE AS LO_EXTENDEDPRICE, l.LO_ORDTOTALPRICE AS LO_ORDTOTALPRICE, l.LO_DISCOUNT AS LO_DISCOUNT, l.LO_REVENUE AS LO_REVENUE, l.LO_SUPPLYCOST AS LO_SUPPLYCOST, l.LO_TAX AS LO_TAX, l.LO_COMMITDATE AS LO_COMMITDATE, l.LO_SHIPMODE AS LO_SHIPMODE, c.C_NAME AS C_NAME, c.C_ADDRESS AS C_ADDRESS, c.C_CITY AS C_CITY, c.C_NATION AS C_NATION, c.C_REGION AS C_REGION, c.C_PHONE AS C_PHONE, c.C_MKTSEGMENT AS C_MKTSEGMENT, s.S_NAME AS S_NAME, s.S_ADDRESS AS S_ADDRESS, s.S_CITY AS S_CITY, s.S_NATION AS S_NATION, s.S_REGION AS S_REGION, s.S_PHONE AS S_PHONE, p.P_NAME AS P_NAME, p.P_MFGR AS P_MFGR, p.P_CATEGORY AS P_CATEGORY, p.P_BRAND AS P_BRAND, p.P_COLOR AS P_COLOR, p.P_TYPE AS P_TYPE, p.P_SIZE AS P_SIZE, p.P_CONTAINER AS P_CONTAINER FROM lineorder AS l INNER JOIN customer AS c ON c.C_CUSTKEY = l.LO_CUSTKEY INNER JOIN supplier AS s ON s.S_SUPPKEY = l.LO_SUPPKEY INNER JOIN part AS p ON p.P_PARTKEY = l.LO_PARTKEY; Running the queries: Q1.1 SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue FROM lineorder_flat WHERE toYear(LO_ORDERDATE) = 1993 AND LO_DISCOUNT BETWEEN 1 AND 3 AND LO_QUANTITY &lt; 25; Q1.2 SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue FROM lineorder_flat WHERE toYYYYMM(LO_ORDERDATE) = 199401 AND LO_DISCOUNT BETWEEN 4 AND 6 AND LO_QUANTITY BETWEEN 26 AND 35; Q1.3 SELECT sum(LO_EXTENDEDPRICE * LO_DISCOUNT) AS revenue FROM lineorder_flat WHERE toISOWeek(LO_ORDERDATE) = 6 AND toYear(LO_ORDERDATE) = 1994 AND LO_DISCOUNT BETWEEN 5 AND 7 AND LO_QUANTITY BETWEEN 26 AND 35; Q2.1 SELECT sum(LO_REVENUE), toYear(LO_ORDERDATE) AS year, P_BRAND FROM lineorder_flat WHERE P_CATEGORY = 'MFGR#12' AND S_REGION = 'AMERICA' GROUP BY year, P_BRAND ORDER BY year, P_BRAND; Q2.2 SELECT sum(LO_REVENUE), toYear(LO_ORDERDATE) AS year, P_BRAND FROM lineorder_flat WHERE P_BRAND &gt;= 'MFGR#2221' AND P_BRAND &lt;= 'MFGR#2228' AND S_REGION = 'ASIA' GROUP BY year, P_BRAND ORDER BY year, P_BRAND; Q2.3 SELECT sum(LO_REVENUE), toYear(LO_ORDERDATE) AS year, P_BRAND FROM lineorder_flat WHERE P_BRAND = 'MFGR#2239' AND S_REGION = 'EUROPE' GROUP BY year, P_BRAND ORDER BY year, P_BRAND; Q3.1 SELECT C_NATION, S_NATION, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenue FROM lineorder_flat WHERE C_REGION = 'ASIA' AND S_REGION = 'ASIA' AND year &gt;= 1992 AND year &lt;= 1997 GROUP BY C_NATION, S_NATION, year ORDER BY year ASC, revenue DESC; Q3.2 SELECT C_CITY, S_CITY, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenue FROM lineorder_flat WHERE C_NATION = 'UNITED STATES' AND S_NATION = 'UNITED STATES' AND year &gt;= 1992 AND year &lt;= 1997 GROUP BY C_CITY, S_CITY, year ORDER BY year ASC, revenue DESC; Q3.3 SELECT C_CITY, S_CITY, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenue FROM lineorder_flat WHERE (C_CITY = 'UNITED KI1' OR C_CITY = 'UNITED KI5') AND (S_CITY = 'UNITED KI1' OR S_CITY = 'UNITED KI5') AND year &gt;= 1992 AND year &lt;= 1997 GROUP BY C_CITY, S_CITY, year ORDER BY year ASC, revenue DESC; Q3.4 SELECT C_CITY, S_CITY, toYear(LO_ORDERDATE) AS year, sum(LO_REVENUE) AS revenue FROM lineorder_flat WHERE (C_CITY = 'UNITED KI1' OR C_CITY = 'UNITED KI5') AND (S_CITY = 'UNITED KI1' OR S_CITY = 'UNITED KI5') AND toYYYYMM(LO_ORDERDATE) = 199712 GROUP BY C_CITY, S_CITY, year ORDER BY year ASC, revenue DESC; Q4.1 SELECT toYear(LO_ORDERDATE) AS year, C_NATION, sum(LO_REVENUE - LO_SUPPLYCOST) AS profit FROM lineorder_flat WHERE C_REGION = 'AMERICA' AND S_REGION = 'AMERICA' AND (P_MFGR = 'MFGR#1' OR P_MFGR = 'MFGR#2') GROUP BY year, C_NATION ORDER BY year ASC, C_NATION ASC; Q4.2 SELECT toYear(LO_ORDERDATE) AS year, S_NATION, P_CATEGORY, sum(LO_REVENUE - LO_SUPPLYCOST) AS profit FROM lineorder_flat WHERE C_REGION = 'AMERICA' AND S_REGION = 'AMERICA' AND (year = 1997 OR year = 1998) AND (P_MFGR = 'MFGR#1' OR P_MFGR = 'MFGR#2') GROUP BY year, S_NATION, P_CATEGORY ORDER BY year ASC, S_NATION ASC, P_CATEGORY ASC; Q4.3 SELECT toYear(LO_ORDERDATE) AS year, S_CITY, P_BRAND, sum(LO_REVENUE - LO_SUPPLYCOST) AS profit FROM lineorder_flat WHERE S_NATION = 'UNITED STATES' AND (year = 1997 OR year = 1998) AND P_CATEGORY = 'MFGR#14' GROUP BY year, S_CITY, P_BRAND ORDER BY year ASC, S_CITY ASC, P_BRAND ASC; Original article","keywords":""},{"title":"Rebalancing Data","type":0,"sectionRef":"#","url":"en/guides/sre/scaling-clusters","content":"Rebalancing Data ClickHouse does not support automatic shard rebalancing. However, there are ways to rebalance shards in order of preference: Adjust the shard for the distributed table, allowing writes to be biased to the new shard. This potentially will cause load imbalances and hot spots on the cluster but can be viable in most scenarios where write throughput is not extremely high. It does not require the user to change their write target i.e. It can remain as the distributed table. This does not assist with rebalancing existing data. As an alternative to (1), modify the existing cluster and write exclusively to the new shard until the cluster is balanced - manually weighting writes. This has the same limitations as (1). If you need to rebalance existing data and you have partitioned your data, consider detaching partitions and manually relocating them to another node before reattaching to the new shard. This is more manual than subsequent techniques but may be faster and less resource-intensive. This is a manual operation and thus needs to consider the rebalancing of the data. Create a new cluster with the new topology and copy the data using ClickHouse Copier. Alternatively, create a new database within the existing cluster and migrate the data using ClickHouse Copier. This can be potentially computationally expensive and may impact your production environment. Building a new cluster on separate hardware, and applying this technique, is an option to mitigate this at the expense of cost. Export the data from the source cluster to the new cluster via an INSERT FROM SELECT. This will not be performant on very large datasets and will potentially incur significant IO on the source cluster and use considerable network resources. This represents a last resort. There is an internal effort to reconsider how rebalancing could be implemented. There is some relevant discussion here.","keywords":""},{"title":"Defining SQL Users and Roles","type":0,"sectionRef":"#","url":"en/guides/sre/users-and-roles","content":"","keywords":""},{"title":"1. Enabling SQL user mode and defining users​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"en/guides/sre/users-and-roles#1-enabling-sql-user-mode-and-defining-users","content":"Enable SQL user mode in the users.xml file under the &lt;default&gt; user: &lt;access_management&gt;1&lt;/access_management&gt; note The default user is the only user that gets created with a fresh install and is also the account used for internode communications, by default. In production, it is recommended to disable this user once the inter-node commnication has been configured with a SQL admin user and inter-node communications have been set with &lt;secret&gt;, cluster credentials and/or internode http and transport protocol credentials since the default account is used for internode communication. Restart the nodes to apply the changes. Start the ClickHouse client: clickhouse-client --user default --password &lt;password&gt; Create a SQL administrator account: CREATE USER clickhouse_admin IDENTIFIED WITH plaintext_password BY 'password'; note In this example, a plain text password is used. However, there are several options available for other user directories such as LDAP and Active Directory. Please refer to user guides and documentation for configuring other options. Grant the new user full administrative rights GRANT ALL ON *.* TO clickhouse_admin WITH GRANT OPTION; Create regular user to restrict columns CREATE USER column_user IDENTIFIED WITH plaintext_password BY 'password'; Create a regular user to restrict by row values CREATE USER row_user IDENTIFIED WITH plaintext_password BY 'password';  "},{"title":"2. Creating a sample database, table and rows​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"en/guides/sre/users-and-roles#2-creating-a-sample-database-table-and-rows","content":"Create a test database CREATE DATABASE db1; Create a table CREATE TABLE db1.table1 ( id UInt64, column1 String, column2 String ) ENGINE MergeTree ORDER BY id; Populate the table with sample rows INSERT INTO db1.table1 (id, column1, column2) VALUES (1, 'A', 'abc'), (2, 'A', 'def'), (3, 'B', 'abc'), (4, 'B', 'def'); Verify the table: SELECT * FROM db1.table1 Query id: 475015cc-6f51-4b20-bda2-3c9c41404e49 ┌─id─┬─column1─┬─column2─┐ │ 1 │ A │ abc │ │ 2 │ A │ def │ │ 3 │ B │ abc │ │ 4 │ B │ def │ └────┴─────────┴─────────┘  "},{"title":"3. Creating roles​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"en/guides/sre/users-and-roles#3-creating-roles","content":"With this set of examples, roles for different privileges such as columns and rows will be created, privileges will be granted to the roles and users will be assigned to each role. Roles are used to define groups of users for certain privileges instead of managing each user seperately. Create a role to restrict users of this role to only see column1 in database db1 and table1: CREATE ROLE column1_users; Set privileges to allow view on column1 GRANT SELECT(id, column1) ON db1.table1 TO column1_users; Add the column_user user to the column1_users role GRANT column1_users TO column_user; Create a role to restrict users of this role to only see selected rows, in this case only rows containing A in column1 CREATE ROLE A_rows_users; Add the row_user to the A_rows_users role GRANT A_rows_users TO row_user; Create a policy to allow view on only where column1 has the values of A CREATE ROW POLICY A_row_filter ON db1.table1 FOR SELECT USING column1 = 'A' TO A_rows_users; Set privileges to the database and table GRANT SELECT(id, column1, column2) ON db1.table1 TO A_rows_users; grant explicit permissions for other roles to still have access to all rows CREATE ROW POLICY allow_other_users_filter ON db1.table1 FOR SELECT USING 1 TO clickhouse_admin, column1_users; note When attaching a policy to a table, the system will apply that policy and only those users and roles defined will be able to do operations on the table, all others will be denied any operations. In order to not have the restrictive row policy applied to other users, another policy must be defined to allow other users and roles to have regular or other types of access. "},{"title":"4. Testing role privileges with column restricted user​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"en/guides/sre/users-and-roles#4-testing-role-privileges-with-column-restricted-user","content":"Log into the clickhouse client using the clickhouse_admin user clickhouse-client --user clickhouse_admin --password password Verify access to database, table and all rows with the admin user. SELECT * FROM db1.table1 Query id: f5e906ea-10c6-45b0-b649-36334902d31d ┌─id─┬─column1─┬─column2─┐ │ 1 │ A │ abc │ │ 2 │ A │ def │ │ 3 │ B │ abc │ │ 4 │ B │ def │ └────┴─────────┴─────────┘ Log into the ClickHouse client using the column_user user clickhouse-client --user column_user --password password Test SELECT using all columns SELECT * FROM db1.table1 Query id: 5576f4eb-7450-435c-a2d6-d6b49b7c4a23 0 rows in set. Elapsed: 0.006 sec. Received exception from server (version 22.3.2): Code: 497. DB::Exception: Received from localhost:9000. DB::Exception: column_user: Not enough privileges. To execute this query it's necessary to have grant SELECT(id, column1, column2) ON db1.table1. (ACCESS_DENIED) note Access is denied since all columns were specified and the user only has access to id and column1 Verify SELECT query with only columns specified and allowed: SELECT id, column1 FROM db1.table1 Query id: cef9a083-d5ce-42ff-9678-f08dc60d4bb9 ┌─id─┬─column1─┐ │ 1 │ A │ │ 2 │ A │ │ 3 │ B │ │ 4 │ B │ └────┴─────────┘  "},{"title":"5. Testing role privileges with row restricted user​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"en/guides/sre/users-and-roles#5-testing-role-privileges-with-row-restricted-user","content":"Log into the ClickHouse client using row_user clickhouse-client --user row_user --password password View rows available SELECT * FROM db1.table1 Query id: a79a113c-1eca-4c3f-be6e-d034f9a220fb ┌─id─┬─column1─┬─column2─┐ │ 1 │ A │ abc │ │ 2 │ A │ def │ └────┴─────────┴─────────┘  "},{"title":"4. Modifying Users and Roles​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"en/guides/sre/users-and-roles#4-modifying-users-and-roles","content":"Users can be assigned multiple roles for a combination of privileges needed. When using multiple roles, the system will combine the roles to determine privileges, the net effect will be that the role permissions will be cumulative. For example, if one role1 allows for only select on column1 and role2 allows for select on column1 and column2 then the user will have access to both columns. Using the admin account, create new user to restrict by both row and column with default roles CREATE USER row_and_column_user IDENTIFIED WITH plaintext_password BY 'password' DEFAULT ROLE A_rows_users; Remove prior privileges for A_rows_users role REVOKE SELECT(id, column1, column2) ON db1.table1 FROM A_rows_users; Allow A_row_users role to only select from column1 GRANT SELECT(id, column1) ON db1.table1 TO A_rows_users; Log into the ClickHouse client using row_and_column_user clickhouse-client --user row_and_column_user --password password; Test with all columns: SELECT * FROM db1.table1 Query id: 8cdf0ff5-e711-4cbe-bd28-3c02e52e8bc4  0 rows in set. Elapsed: 0.005 sec. Received exception from server (version 22.3.2): Code: 497. DB::Exception: Received from localhost:9000. DB::Exception: row_and_column_user: Not enough privileges. To execute this query it's necessary to have grant SELECT(id, column1, column2) ON db1.table1. (ACCESS_DENIED) ```  Test with limited allowed columns: SELECT id, column1 FROM db1.table1 Query id: 5e30b490-507a-49e9-9778-8159799a6ed0 ┌─id─┬─column1─┐ │ 1 │ A │ │ 2 │ A │ └────┴─────────┘ Examples on how to delete privileges, policies, unassign users from roles, delete users and roles: Remove privilege from a role REVOKE SELECT(column1, id) ON db1.table1 FROM A_rows_users; Delete a policy DROP ROW POLICY A_row_filter ON db1.table1; Unassign a user from a role REVOKE A_rows_users FROM row_user; Delete a role DROP ROLE A_rows_users; Delete a user DROP USER row_user;  "},{"title":"5. Troubleshooting​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"en/guides/sre/users-and-roles#5-troubleshooting","content":"There are occasions when privileges intersect or combine to produce unexpected results, the following commands can be used to narrow the issue using an admin account Listing the grants and roles for a user SHOW GRANTS FOR row_and_column_user Query id: 6a73a3fe-2659-4aca-95c5-d012c138097b ┌─GRANTS FOR row_and_column_user───────────────────────────┐ │ GRANT A_rows_users, column1_users TO row_and_column_user │ └──────────────────────────────────────────────────────────┘ List roles in ClickHouse SHOW ROLES Query id: 1e21440a-18d9-4e75-8f0e-66ec9b36470a ┌─name────────────┐ │ A_rows_users │ │ column1_users │ └─────────────────┘ Display the policies SHOW ROW POLICIES Query id: f2c636e9-f955-4d79-8e80-af40ea227ebc ┌─name───────────────────────────────────┐ │ A_row_filter ON db1.table1 │ │ allow_other_users_filter ON db1.table1 │ └────────────────────────────────────────┘ View how a policy was defined and current privileges SHOW CREATE ROW POLICY A_row_filter ON db1.table1 Query id: 0d3b5846-95c7-4e62-9cdd-91d82b14b80b ┌─CREATE ROW POLICY A_row_filter ON db1.table1────────────────────────────────────────────────┐ │ CREATE ROW POLICY A_row_filter ON db1.table1 FOR SELECT USING column1 = 'A' TO A_rows_users │ └─────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"Summary​","type":1,"pageTitle":"Defining SQL Users and Roles","url":"en/guides/sre/users-and-roles#summary","content":"This article demostrated the basics of creating SQL users and roles and provided steps to set and modify privileges for users and roles. For more detailed information on each please refer to our user guides and reference documenation. "},{"title":"Connect Airbyte to ClickHouse","type":0,"sectionRef":"#","url":"en/integrations/airbyte-and-clickhouse","content":"","keywords":"clickhouse airbyte connect integrate etl data integration"},{"title":"1. Download and run Airbyte​","type":1,"pageTitle":"Connect Airbyte to ClickHouse","url":"en/integrations/airbyte-and-clickhouse#1-download-and-run-airbyte","content":"Airbyte runs on Docker and uses docker-compose. Make sure to download and install the latest versions of Docker. Deploy Airbyte by cloning the official Github repository and running docker-compose up in your favorite terminal: git clone https://github.com/airbytehq/airbyte.git cd airbyte docker-compose up Once you see the Airbyte banner in your terminal, you can connect to localhost:8000 note Alternatively, you can signup and use Airbyte Cloud "},{"title":"2. Add ClickHouse as a destination​","type":1,"pageTitle":"Connect Airbyte to ClickHouse","url":"en/integrations/airbyte-and-clickhouse#2-add-clickhouse-as-a-destination","content":"In this section, we will display how to add a ClickHouse instance as a destination. Start your ClickHouse server (Airbyte is compatible with ClickHouse version 21.8.10.19 or above): clickhouse-server start Within Airbyte, select the &quot;Destinations&quot; page and add a new destination: Pick a name for your destination and select ClickHouse from the &quot;Destination type&quot; drop-down list: Fill out the &quot;Set up the destination&quot; form by providing your ClickHouse hostname and ports, database name, username and password and select if it's a TLS connection (equivalent to the --secure flag in the clickhouse-client). Congratulations! you have now added ClickHouse as a destination in Airbyte. note In order to use ClickHouse as a destination, the user you'll use need to have the permissions to create databases, tables and insert rows. We recommend creating a dedicated user for Airbyte (eg. my_airbyte_user) with the following permissions: GRANT CREATE ON * TO my_airbyte_user;  "},{"title":"3. Add a dataset as a source​","type":1,"pageTitle":"Connect Airbyte to ClickHouse","url":"en/integrations/airbyte-and-clickhouse#3-add-a-dataset-as-a-source","content":"The example dataset we will use is the New York City Taxi Data (on Github). For this tutorial, we will use a subset of this dataset which corresponds to the month of July 2021. Within Airbyte, select the &quot;Sources&quot; page and add a new source of type file. Fill out the &quot;Set up the source&quot; form by naming the source and providing the URL of the NYC Taxi July 2021 file (see below). Make sure to pick csv as file format, HTTPS Public Web as Storage Provider and nyc_taxi_072021 as Dataset Name. https://s3.amazonaws.com/nyc-tlc/trip+data/yellow_tripdata_2021-07.csv Congratulations! You have now added a source file in Airbyte. "},{"title":"4. Create a connection and load the dataset into ClickHouse​","type":1,"pageTitle":"Connect Airbyte to ClickHouse","url":"en/integrations/airbyte-and-clickhouse#4-create-a-connection-and-load-the-dataset-into-clickhouse","content":"Within Airbyte, select the &quot;Connections&quot; page and add a new connection Select &quot;Use existing source&quot; and select the New York City Taxi Data, the select &quot;Use existing destination&quot; and select you ClickHouse instance. Fill out the &quot;Set up the connection&quot; form by choosing a Replication Frequency (we will use manual for this tutorial) and select nyc_taxi_072021 as the stream you want to sync. Make sure you pick Normalized Tabular Data as a Normalization. Now that the connection is created, click on &quot;Sync now&quot; to trigger the data loading (since we picked Manual as a Replication Frequency) Your data will start loading, you can expand the view to see Airbyte logs and progress. Once the operation finishes, you'll see a Completed successfully message in the logs: Connect to your ClickHouse instance using your preferred SQL Client and check the resulting table: SELECT * FROM nyc_taxi_072021 LIMIT 10 The response should look like: Query id: 1dbe609f-9136-49cf-a642-51a2305e1027 ┌─extra─┬─mta_tax─┬─VendorID─┬─RatecodeID─┬─tip_amount─┬─fare_amount─┬─DOLocationID─┬─PULocationID─┬─payment_type─┬─tolls_amount─┬─total_amount─┬─trip_distance─┬─passenger_count─┬─store_and_fwd_flag─┬─congestion_surcharge─┬─tpep_pickup_datetime─┬─improvement_surcharge─┬─tpep_dropoff_datetime─┬─_airbyte_ab_id───────────────────────┬─────_airbyte_emitted_at─┬─_airbyte_normalized_at─┬─_airbyte_nyc_taxi_072021_hashid──┐ │ 3.5 │ 0.5 │ 1 │ 1 │ 0 │ 11.5 │ 237 │ 162 │ 2 │ 0 │ 15.8 │ 2.3 │ 1 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-07 17:49:32 │ 0.3 │ 2021-07-07 18:04:30 │ 00000005-a90c-41b7-8883-1ab75c0ad9da │ 2022-03-16 13:02:50.000 │ 2022-03-16 13:09:48 │ DE8F3E68A49EC6CB00919501E6726335 │ │ 0 │ 0.5 │ 2 │ 1 │ 10 │ 23 │ 256 │ 233 │ 1 │ 0 │ 36.3 │ 5.4 │ 1 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-15 07:23:36 │ 0.3 │ 2021-07-15 07:50:28 │ 00001877-58ba-4614-90d4-4e5eba3cd593 │ 2022-03-16 13:04:46.000 │ 2022-03-16 13:09:48 │ 7915C6A4D33BCE7CF58D66CF1F2E1A61 │ │ 0.5 │ 0.5 │ 2 │ 1 │ 5 │ 30.5 │ 138 │ 137 │ 1 │ 6.55 │ 45.85 │ 10.93 │ 1 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-18 05:00:28 │ 0.3 │ 2021-07-18 05:18:54 │ 00001885-d93e-49d7-a92c-c09fd49e8b39 │ 2022-03-16 13:05:37.000 │ 2022-03-16 13:09:48 │ A7346163EA6D6F0CBBA562CE1C5F9401 │ │ 2.5 │ 0.5 │ 1 │ 1 │ 0 │ 5 │ 100 │ 186 │ 2 │ 0 │ 8.3 │ 1 │ 1 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-07 09:47:59 │ 0.3 │ 2021-07-07 09:52:13 │ 000029d1-2e26-4d83-9efe-51cb182282d9 │ 2022-03-16 13:02:42.000 │ 2022-03-16 13:09:48 │ C6389A8B2B6E24A74612F7FB53DAA9A0 │ │ 1 │ 0.5 │ 2 │ 1 │ 4 │ 19.5 │ 13 │ 161 │ 1 │ 0 │ 27.8 │ 5.06 │ 3 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-12 17:54:49 │ 0.3 │ 2021-07-12 18:17:43 │ 00003433-6886-4267-b8a9-da1b366537c4 │ 2022-03-16 13:04:06.000 │ 2022-03-16 13:09:48 │ 8E7C4E55F366901E4B6DFB02C3CAE838 │ │ 0 │ 0.5 │ 2 │ 1 │ 0 │ 7 │ 233 │ 140 │ 2 │ 0 │ 10.3 │ 1.3 │ 1 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-15 13:06:34 │ 0.3 │ 2021-07-15 13:13:24 │ 000049ae-b0c8-4e07-a3e6-ea19916fb6c3 │ 2022-03-16 13:04:51.000 │ 2022-03-16 13:09:48 │ 704F99F611D1A71713A4870406E28B54 │ │ 3.5 │ 0.5 │ 1 │ 1 │ 9.8 │ 35 │ 138 │ 230 │ 1 │ 0 │ 49.1 │ 9.9 │ 0 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-09 16:09:24 │ 0.3 │ 2021-07-09 16:45:15 │ 00004cc2-868e-4465-a24b-7efcb5da8cd4 │ 2022-03-16 13:03:20.000 │ 2022-03-16 13:09:48 │ 8AB6444AD089BA300B303447C4B70500 │ │ 2.5 │ 0.5 │ 1 │ 1 │ 3 │ 10 │ 232 │ 224 │ 1 │ 0 │ 16.3 │ 2.6 │ 0 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-06 15:21:57 │ 0.3 │ 2021-07-06 15:30:09 │ 00005277-bc5f-4d1e-b116-d3777fef87f7 │ 2022-03-16 13:02:33.000 │ 2022-03-16 13:09:48 │ AC5A4F12E7EC61116F146DE90375A74B │ │ 0.5 │ 0.5 │ 2 │ 1 │ 2.34 │ 6.5 │ 42 │ 41 │ 1 │ 0 │ 10.14 │ 1.02 │ 1 │ ᴺᵁᴸᴸ │ 0 │ 2021-07-16 20:27:38 │ 0.3 │ 2021-07-16 20:33:46 │ 0000571b-6698-43f4-878d-d0d3f91e40d1 │ 2022-03-16 13:05:16.000 │ 2022-03-16 13:09:48 │ A447703038C0257801F7DA3CBBCA47CB │ │ 0 │ 0.5 │ 2 │ 1 │ 0 │ 24 │ 232 │ 48 │ 2 │ 0 │ 27.3 │ 6.74 │ 1 │ ᴺᵁᴸᴸ │ 2.5 │ 2021-07-10 15:00:11 │ 0.3 │ 2021-07-10 15:27:38 │ 000060b7-76b5-4d73-ae7f-0c475f69078b │ 2022-03-16 13:03:35.000 │ 2022-03-16 13:09:48 │ 6A593070389760D2339DDBD76E913447 │ └───────┴─────────┴──────────┴────────────┴────────────┴─────────────┴──────────────┴──────────────┴──────────────┴──────────────┴──────────────┴───────────────┴─────────────────┴────────────────────┴──────────────────────┴──────────────────────┴───────────────────────┴───────────────────────┴──────────────────────────────────────┴─────────────────────────┴────────────────────────┴──────────────────────────────────┘ SELECT count(*) FROM nyc_taxi_072021 The response is: Query id: a9172d39-50f7-421e-8330-296de0baa67e ┌─count()─┐ │ 2821515 │ └─────────┘  Notice that Airbyte automatically inferred the data types and added 4 columns to the destination table. These columns are used by Airbyte to manage the replication logic and log the operations. More details are available in the Airbyte official documentation. `_airbyte_ab_id` String, `_airbyte_emitted_at` DateTime64(3, 'GMT'), `_airbyte_normalized_at` DateTime, `_airbyte_nyc_taxi_072021_hashid` String Now that the dataset is loaded on your ClickHouse instance, you can create an new table and use more suitable ClickHouse data types (more details). Congratulations - you have successfully loaded the NYC taxi data into ClickHouse using Airbyte! "},{"title":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","type":0,"sectionRef":"#","url":"en/guides/improving-query-performance/sparse-primary-indexes","content":"","keywords":""},{"title":"Data Set​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#data-set","content":"Throughout this article we will use a sample anonymized web traffic data set. We will use a subset of 8.87 million rows (events) from the sample data set.The uncompressed data size is 8.87 million events and about 700 MB. This compresses to 200 mb when stored in ClickHouse.In our subset, each row contains three columns that indicate an internet user (UserID column) who clicked on a URL (URL column) at a specific time (EventTime column). With these three columns we can already formulate some typical web analytics queries such as: &quot;What are the top 10 most clicked urls for a specific user?”&quot;What are the top 10 users that most frequently clicked a specific URL?&quot;“What are the most popular times (e.g. days of the week) at which a user clicks on a specific URL?” "},{"title":"Test Machine​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#test-machine","content":"All runtime numbers given in this document are based on running ClickHouse 22.2.1 locally on a MacBook Pro with the Apple M1 Pro chip and 16GB of RAM. "},{"title":"A full table scan​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#a-full-table-scan","content":"In order to see how a query is executed over our data set without a primary key, we create a table (with a MergeTree table engine) by executing the following SQL DDL statement: CREATE TABLE hits_NoPrimaryKey ( `UserID` UInt32, `URL` String, `EventTime` DateTime ) ENGINE = MergeTree PRIMARY KEY tuple();  Next insert a subset of the hits data set into the table with the following SQL insert statement. This uses the URL table function in combination with schema inference in order to load a subset of the full dataset hosted remotely at clickhouse.com: INSERT INTO hits_NoPrimaryKey SELECT intHash32(c11::UInt64) AS UserID, c15 AS URL, c5 AS EventTime FROM url('https://datasets.clickhouse.com/hits/tsv/hits_v1.tsv.xz') WHERE URL != '';  The response is: Ok. 0 rows in set. Elapsed: 145.993 sec. Processed 8.87 million rows, 18.40 GB (60.78 thousand rows/s., 126.06 MB/s.)  ClickHouse client’s result output shows us that the statement above inserted 8.87 million rows into the table. Lastly, in order to simplify the discussions later on in this article and to make the diagrams and results reproducible, we optimize the table using the FINAL keyword: OPTIMIZE TABLE hits_NoPrimaryKey FINAL;  note In general it is not required nor recommended to immediately optimize a table after loading data into it. Why this is necessary for this example will become apparent. Now we execute our first web analytics query. The following is calculating the top 10 most clicked urls for the internet user with the UserID 749927693: SELECT URL, count(URL) as Count FROM hits_NoPrimaryKey WHERE UserID = 749927693 GROUP BY URL ORDER BY Count DESC LIMIT 10;  The response is: ┌─URL────────────────────────────┬─Count─┐ │ http://auto.ru/chatay-barana.. │ 170 │ │ http://auto.ru/chatay-id=371...│ 52 │ │ http://public_search │ 45 │ │ http://kovrik-medvedevushku-...│ 36 │ │ http://forumal │ 33 │ │ http://korablitz.ru/L_1OFFER...│ 14 │ │ http://auto.ru/chatay-id=371...│ 14 │ │ http://auto.ru/chatay-john-D...│ 13 │ │ http://auto.ru/chatay-john-D...│ 10 │ │ http://wot/html?page/23600_m...│ 9 │ └────────────────────────────────┴───────┘ 10 rows in set. Elapsed: 0.022 sec. Processed 8.87 million rows, 70.45 MB (398.53 million rows/s., 3.17 GB/s.)  ClickHouse client’s result output indicates that ClickHouse executed a full table scan! Each single row of the 8.87 million rows of our table was streamed into ClickHouse. That doesn’t scale. To make this (way) more efficient and (much) faster, we need to use a table with a appropriate primary key. This will allow ClickHouse to automatically (based on the primary key’s column(s)) create a sparse primary index which can then be used to significantly speed up the execution of our example query.  "},{"title":"A table with a primary key​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#a-table-with-a-primary-key","content":"Create a table that has a compound primary key with key columns UserID and URL: CREATE TABLE hits_UserID_URL ( `UserID` UInt32, `URL` String, `EventTime` DateTime ) ENGINE = MergeTree PRIMARY KEY (UserID, URL) ORDER BY (UserID, URL, EventTime) SETTINGS index_granularity = 8192, index_granularity_bytes = 0;  DDL Statement Details In order to simplify the discussions later on in this article, as well as make the diagrams and results reproducible, the DDL statement specifies a compound sorting key for the table via an ORDER BY clause explicitly controls how many index entries the primary index will have through the settings: index_granularity: explicitly set to its default value of 8192. This means that for each group of 8192 rows, the primary index will have one index entry, e.g. if the table contains 16384 rows then the index will have two index entries. index_granularity_bytes: set to 0 in order to disable adaptive index granularity. Adaptive index granularity means that ClickHouse automatically creates one index entry for a group of n rows if either n is less than 8192 but the size of the combined row data for that n rows is larger than or equal 10 MB (the default value for index_granularity_bytes) orif the combined row data size for n rows is less than 10 MB but n is 8192. The primary key in the DDL statement above causes the creation of primary index based on the two specified key columns.  Next insert the data: INSERT INTO hits_UserID_URL SELECT intHash32(c11::UInt64) AS UserID, c15 AS URL, c5 AS EventTime FROM url('https://datasets.clickhouse.com/hits/tsv/hits_v1.tsv.xz') WHERE URL != '';  The response looks like: 0 rows in set. Elapsed: 149.432 sec. Processed 8.87 million rows, 18.40 GB (59.38 thousand rows/s., 123.16 MB/s.)   And optimize the table: OPTIMIZE TABLE hits_UserID_URL FINAL;   We can use the following query to obtain metadata about our table: SELECT part_type, path, formatReadableQuantity(rows) AS rows, formatReadableSize(data_uncompressed_bytes) AS data_uncompressed_bytes, formatReadableSize(data_compressed_bytes) AS data_compressed_bytes, formatReadableSize(primary_key_bytes_in_memory) AS primary_key_bytes_in_memory, marks, formatReadableSize(bytes_on_disk) AS bytes_on_disk FROM system.parts WHERE (table = 'hits_UserID_URL') AND (active = 1) FORMAT Vertical;  The response is: part_type: Wide path: ./store/d9f/d9f36a1a-d2e6-46d4-8fb5-ffe9ad0d5aed/all_1_9_2/ rows: 8.87 million data_uncompressed_bytes: 733.28 MiB data_compressed_bytes: 206.94 MiB primary_key_bytes_in_memory: 96.93 KiB marks: 1083 bytes_on_disk: 207.07 MiB 1 rows in set. Elapsed: 0.003 sec.  The output of the ClickHouse client shows: The table’s data is stored in wide format in a specific directory on disk meaning that there will be one data file (and one mark file) per table column inside that directory.The table has 8.87 million rows.The uncompressed data size of all rows together is 733.28 MB.The compressed on disk data size of all rows together is 206.94 MB.The table has a primary index with 1083 entries (called ‘marks’) and the size of the index is 96.93 KB.In total the table’s data and mark files and primary index file together take 207.07 MB on disk. "},{"title":"An index design for massive data scales​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#an-index-design-for-massive-data-scales","content":"In traditional relational database management systems the primary index would contain one entry per table row. For our data set this would result in the primary index - often a B(+)-Tree data structure - containing 8.87 million entries. Such an index allows the fast location of specific rows, resulting in high efficiency for lookup queries and point updates. Searching an entry in a B(+)-Tree data structure has average time complexity of O(log2 n). For a table of 8.87 million rows this means 23 steps are required to locate any index entry. This capability comes at a cost: additional disk and memory overheads and higher insertion costs when adding new rows to to the table and entries to the index (and also sometimes rebalancing of the B-Tree). Considering the challenges associated with B-Tee indexes, table engines in ClickHouse utilise a different approach. The ClickHouse MergeTree Engine Family has been designed and optimized to handle massive data volumes. These tables are designed to receive millions of row inserts per second and store very large (100s of Petabytes) volumes of data. Data is quickly written to a table part by part, with rules applied for merging the parts in the background. In ClickHouse each part has its own primary index. When parts are merged then also the merged part’s primary indexes are merged. At the very large scale that ClickHouse is designed for, it is paramount to be very disk and memory efficient. Therefore, instead of indexing every row, the primary index for a part has one index entry (known as a ‘mark’) per group of rows (called ‘granule’). This sparse indexing is possible because ClickHouse is storing the rows for a part on disk ordered by the primary key column(s). Instead of directly locating single rows (like a B-Tree based index), the sparse primary index allows it to quickly (via a binary search over index entries) identify groups of rows that could possibly match the query. The located groups of potentially matching rows (granules) are then in parallel streamed into the ClickHouse engine in order to find the matches. This index design allows for the primary index to be small (it can and must completely fit into the main memory), whilst still significantly speeding up query execution times: especially for range queries that are typical in data analytics use cases. The following illustrates in detail how ClickHouse is building and using its sparse primary index. Later on in the article we will discuss some best practices for choosing, removing, and ordering the table columns that are used to build the index (primary key columns).  "},{"title":"Data is stored on disk ordered by primary key column(s)​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#data-is-stored-on-disk-ordered-by-primary-key-columns","content":"Our table that we created above has a compound primary key (UserID, URL) anda compound sorting key (UserID, URL, EventTime). note If we would have specified only the sorting key, then the primary key would be implicitly defined to be equal to the sorting key. In order to be memory efficient we explicitly specified a primary key that only contains columns that our queries are filtering on. The primary index that is based on the primary key is completely loaded into the main memory. In order to have consistency in the article’s diagrams and in order to maximise compression ratio we defined a separate sorting key that includes all of our table's columns (depending on the compression algorithm used, it can be beneficial for the compression rate to have values sorted in a column). The primary key needs to be a prefix of the sorting key if both are specified. The inserted rows are stored on disk in lexicographical order (ascending) by the primary key columns (and the additional EventTime column from the sorting key). note ClickHouse allows inserting multiple rows with identical primary key column values. In this case (see row 1 and row 2 in the diagram below), the final order is determined by the specified sorting key and therefore the value of the EventTime column. ClickHouse is a column-oriented database management system. As show in the diagram below for the on disk representation, there is a single data file (*.bin) per table column where all the values for that column are stored in a compressed format, andthe 8.87 million rows are stored on disk in lexicographic ascending order by the primary key columns (and the additional sort key columns) i.e. in this case first by UserID,then by URL,and lastly by EventTime:  UserID.bin, URL.bin, and EventTime.bin are the data files on disk where the values of the UserID , URL , and EventTime columns are stored.   note As the primary key defines the lexicographical order of the rows on disk, a table can only have one primary key. We are numbering rows starting with 0 in order to be aligned with the ClickHouse internal row numbering scheme that is also used for logging messages.  "},{"title":"Data is organized into granules for parallel data processing​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#data-is-organized-into-granules-for-parallel-data-processing","content":"For data processing purposes, a table's column values are logically divided into granules. A granule is the smallest indivisible data set that is streamed into ClickHouse for data processing. This means that instead of reading individual rows, ClickHouse is always reading (in a streaming fashion and in parallel) a whole group (granule) of rows. note Column values are not physically stored inside granules: granules are just a logical organization of the column values for query processing. The following diagram shows how the (column values of) 8.87 million rows of our table are organized into 1083 granules, as a result of the table's DDL statement containing the setting index_granularity (set to its default value of 8192).  The first (based on physical order on disk) 8192 rows (their column values) logically belong to granule 0, then the next 8192 rows (their column values) belong to granule 1 and so on. note The last granule (granule 1082) &quot;contains&quot; less than 8192 rows. We marked some column values from our primary key columns (UserID, URL) in orange. These orange marked column values are the minimum value of each primary key column in each granule. The exception here is the last granule (granule 1082 in the diagram above) where we mark the maximum values. As we will see below, these orange marked column values will be the entries in the table's primary index. We are numbering granules starting with 0 in order to be aligned with the ClickHouse internal numbering scheme that is also used for logging messages.  "},{"title":"The primary index has one entry per granule​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#the-primary-index-has-one-entry-per-granule","content":"The primary index is created based on the granules shown in the diagram above. This index is an uncompressed flat array file (primary.idx), containing so-called numerical index marks starting at 0. The diagram below shows that the index stores the minimum primary key column values (the values marked in orange in the diagram above) for each granule. For example the first index entry (‘mark 0’ in the diagram below) is storing the minimum values for the primary key columns of granule 0 from the diagram above,the second index entry (‘mark 1’ in the diagram below) is storing the minimum values for the primary key columns of granule 1 from the diagram above, and so on.  In total the index has 1083 entries for our table with 8.87 million rows and 1083 granules:  note The last index entry (‘mark 1082’ in the diagram below) is storing the maximum values for the primary key columns of granule 1082 from the diagram above. Index entries (index marks) are not based on specific rows from our table but on granules. E.g. for index entry ‘mark 0’ in the diagram above there is no row in our table where UserID is 240.923 and URL is &quot;goal://metry=10000467796a411...&quot;, instead, there is a granule 0 for the table where within that granule the minimum UserID vale is 240.923 and the minimum URL value is &quot;goal://metry=10000467796a411...&quot; and these two values are from separate rows. The primary index file is completely loaded into the main memory. If the file is larger than the available free memory space then ClickHouse will raise an error. The primary key entries are called index marks because each index entry is marking the start of a specific data range. Specifically for the example table: UserID index marks: The stored UserID values in the primary index are sorted in ascending order. ‘mark 1’ in the diagram above thus indicates that the UserID values of all table rows in granule 1, and in all following granules, are guaranteed to be greater than or equal to 4.073.710. As we will see later, this global order enables ClickHouse to use a binary search algorithm over the index marks for the first key column when a query is filtering on the first column of the primary key. URL index marks: The quite similar cardinality of the primary key columns UserID and URL means that the index marks for all key columns after the first column in general only indicate a data range per granule. For example, ‘mark 0’ for the URL column in the diagram above is indicating that the URL values of all table rows in granule 0 are guaranteed to be larger or equal to goal://metry=10000467796a411.... However, this same guarantee cannot also be given for the URL values of all table rows in granule 1 because ‘mark 1‘ for the UserID column has a different UserID value than ‘mark 0‘. We will discuss the consequences of this on query execution performance in more detail later. "},{"title":"The primary index is used for selecting granules​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#the-primary-index-is-used-for-selecting-granules","content":"We can now execute our queries with support from the primary index.  The following calculates the top 10 most clicked urls for the UserID 749927693. SELECT URL, count(URL) AS Count FROM hits_UserID_URL WHERE UserID = 749927693 GROUP BY URL ORDER BY Count DESC LIMIT 10;  The response is:  ┌─URL────────────────────────────┬─Count─┐ │ http://auto.ru/chatay-barana.. │ 170 │ │ http://auto.ru/chatay-id=371...│ 52 │ │ http://public_search │ 45 │ │ http://kovrik-medvedevushku-...│ 36 │ │ http://forumal │ 33 │ │ http://korablitz.ru/L_1OFFER...│ 14 │ │ http://auto.ru/chatay-id=371...│ 14 │ │ http://auto.ru/chatay-john-D...│ 13 │ │ http://auto.ru/chatay-john-D...│ 10 │ │ http://wot/html?page/23600_m...│ 9 │ └────────────────────────────────┴───────┘ 10 rows in set. Elapsed: 0.005 sec. Processed 8.19 thousand rows, 740.18 KB (1.53 million rows/s., 138.59 MB/s.)  The output for the ClickHouse client is now showing that instead of doing a full table scan, only 8.19 thousand rows were streamed into ClickHouse. If trace logging is enabled then the ClickHouse server log file shows that ClickHouse was running a binary search over the 1083 UserID index marks, in order to identify granules that possibly can contain rows with a UserID column value of 749927693. This requires 19 steps with an average time complexity of O(log2 n): ...Executor): Key condition: (column 0 in [749927693, 749927693]) ...Executor): Running binary search on index range for part all_1_9_2 (1083 marks) ...Executor): Found (LEFT) boundary mark: 176 ...Executor): Found (RIGHT) boundary mark: 177 ...Executor): Found continuous range in 19 steps ...Executor): Selected 1/1 parts by partition key, 1 parts by primary key, 1/1083 marks by primary key, 1 marks to read from 1 ranges ...Reading ...approx. 8192 rows starting from 1441792  We can see in the trace log above, that one mark out of the 1083 existing marks satisfied the query. Trace Log Details Mark 176 was identified (the 'found left boundary mark' is inclusive, the 'found right boundary mark' is exclusive), and therefore all 8192 rows from granule 176 (which starts at row 1.441.792 - we will see that later on in this article) are then streamed into ClickHouse in order to find the actual rows with a UserID column value of 749927693. We can also reproduce this by using the EXPLAIN clause in our example query: EXPLAIN indexes = 1 SELECT URL, count(URL) AS Count FROM hits_UserID_URL WHERE UserID = 749927693 GROUP BY URL ORDER BY Count DESC LIMIT 10;  The response looks like: ┌─explain───────────────────────────────────────────────────────────────────────────────┐ │ Expression (Projection) │ │ Limit (preliminary LIMIT (without OFFSET)) │ │ Sorting (Sorting for ORDER BY) │ │ Expression (Before ORDER BY) │ │ Aggregating │ │ Expression (Before GROUP BY) │ │ Filter (WHERE) │ │ SettingQuotaAndLimits (Set limits and quota after reading from storage) │ │ ReadFromMergeTree │ │ Indexes: │ │ PrimaryKey │ │ Keys: │ │ UserID │ │ Condition: (UserID in [749927693, 749927693]) │ │ Parts: 1/1 │ │ Granules: 1/1083 │ └───────────────────────────────────────────────────────────────────────────────────────┘ 16 rows in set. Elapsed: 0.003 sec.  The client output is showing that one out of the 1083 granules was selected as possibly containing rows with a UserID column value of 749927693. Conclusion When a query is filtering on a column that is part of a compound key and is the first key column, then ClickHouse is running the binary search algorithm over the key column's index marks.  As discussed above, ClickHouse is using its sparse primary index for quickly (via binary search) selecting granules that could possibly contain rows that match a query. This is the first stage (granule selection) of ClickHouse query execution. In the second stage (data reading), ClickHouse is locating the selected granules in order to stream all their rows into the ClickHouse engine in order to find the rows that are actually matching the query. We discuss that second stage in more detail in the following section.  "},{"title":"Mark files are used for locating granules​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#mark-files-are-used-for-locating-granules","content":"The following diagram illustrates a part of the primary index file for our table.  As discussed above, via a binary search over the index’s 1083 UserID marks, mark 176 were identified. Its corresponding granule 176 can therefore possibly contain rows with a UserID column value of 749.927.693. Granule Selection Details The diagram above shows that mark 176 is the first index entry where both the minimum UserID value of the associated granule 176 is smaller than 749.927.693, and the minimum UserID value of granule 177 for the next mark (mark 177) is greater than this value. Therefore only the corresponding granule 176 for mark 176 can possibly contain rows with a UserID column value of 749.927.693. In order to confirm (or not) that some row(s) in granule 176 contain a UserID column value of 749.927.693, all 8192 rows belonging to this granule need to be streamed into ClickHouse. To achieve this, ClickHouse needs to know the physical location of granule 176. In ClickHouse the physical locations of all granules for our table are stored in mark files. Similar to data files, there is one mark file per table column. The following diagram shows the three mark files UserID.mrk, URL.mrk, and EventTime.mrk that store the physical locations of the granules for the table’s UserID, URL, and EventTime columns.  We have discussed how the primary index is a flat uncompressed array file (primary.idx), containing index marks that are numbered starting at 0. Similarily, a mark file is also a flat uncompressed array file (*.mrk) containing marks that are numbered starting at 0. Once ClickHouse has identified and selected the index mark for a granule that can possibly contain matching rows for a query, a positional array lookup can be performed in the mark files in order to obtain the physical locations of the granule. Each mark file entry for a specific column is storing two locations in the form of offsets: The first offset ('block_offset' in the diagram above) is locating the block in the compressed column data file that contains the compressed version of the selected granule. This compressed block potentially contains a few compressed granules. The located compressed file block is uncompressed into the main memory on read. The second offset ('granule_offset' in the diagram above) from the mark-file provides the location of the granule within the uncompressed block data. All the 8192 rows belonging to the located uncompressed granule are then streamed into ClickHouse for further processing. Why Mark Files Why does the primary index not directly contain the physical locations of the granules that are corresponding to index marks? Because at that very large scale that ClickHouse is designed for, it is important to be very disk and memory efficient. The primary index file needs to fit into the main memory. For our example query ClickHouse used the primary index and selected a single granule that can possibly contain rows matching our query. Only for that one granule does ClickHouse then need the physical locations in order to stream the corresponding rows for further processing. Furthermore, this offset information is only needed for the UserID and URL columns. Offset information is not needed for columns that are not used in the query e.g. the EventTime. For our sample query, Clickhouse needs only the two physical location offsets for granule 176 in the UserID data file (UserID.bin) and the two physical location offsets for granule 176 in the URL data file (URL.data). The indirection provided by mark files avoids storing, directly within the primary index, entries for the physical locations of all 1083 granules for all three columns: thus avoiding having unnecessary (potentially unused) data in main memory. The following diagram and the text below illustrates how for our example query ClickHouse locates granule 176 in the UserID.bin data file.  We discussed earlier in this article that ClickHouse selected the primary index mark 176 and therefore granule 176 as possibly containing matching rows for our query. ClickHouse now uses the selected mark number (176) from the index for a positional array lookup in the UserID.mrk mark file in order to get the two offsets for locating granule 176. As shown, the first offset is locating the compressed file block within the UserID.bin data file that in turn contains the compressed version of granule 176. Once the located file block is uncompressed into the main memory, the second offset from the mark file can be used to locate granule 176 within the uncompressed data. ClickHouse needs to locate (and stream all values from) granule 176 from both the UserID.bin data file and the URL.bin data file in order to execute our example query (top 10 most clicked urls for the internet user with the UserID 749.927.693). The diagram above shows how ClickHouse is locating the granule for the UserID.bin data file. In parallel, ClickHouse is doing the same for granule 176 for the URL.bin data file. The two respective granules are aligned and streamed into the ClickHouse engine for further processing i.e. aggregating and counting the URL values per group for all rows where the UserID is 749.927.693, before finally outputting the 10 largest URL groups in descending count order.  "},{"title":"Performance issues when filtering on secondary key columns​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#performance-issues-when-filtering-on-secondary-key-columns","content":"When a query is filtering on a column that is part of a compound key and is the first key column, then ClickHouse is running the binary search algorithm over the key column's index marks. But what happens when a query is filtering on a column that is part of a compound key, but is not the first key column? note We discuss a scenario when a query is explicitly not filtering on the first key colum, but on any key column after the first. When a query is filtering on both the first key column and on any key column(s) after the first then ClickHouse is running binary search over the first key column's index marks.    We use a query that calculates the top 10 users that have most frequently clicked on the URL &quot;http://public_search&quot;: SELECT UserID, count(UserID) AS Count FROM hits_UserID_URL WHERE URL = 'http://public_search' GROUP BY UserID ORDER BY Count DESC LIMIT 10;  The response is:  ┌─────UserID─┬─Count─┐ │ 2459550954 │ 3741 │ │ 1084649151 │ 2484 │ │ 723361875 │ 729 │ │ 3087145896 │ 695 │ │ 2754931092 │ 672 │ │ 1509037307 │ 582 │ │ 3085460200 │ 573 │ │ 2454360090 │ 556 │ │ 3884990840 │ 539 │ │ 765730816 │ 536 │ └────────────┴───────┘ 10 rows in set. Elapsed: 0.086 sec. Processed 8.81 million rows, 799.69 MB (102.11 million rows/s., 9.27 GB/s.)  The client output indicates that ClickHouse almost executed a full table scan despite the URL column being part of the compound primary key! ClickHouse reads 8.81 million rows from the 8.87 million rows of the table. If trace logging is enabled then the ClickHouse server log file shows that ClickHouse used a generic exclusion search over the 1083 URL index marks in order to identify those granules that possibly can contain rows with a URL column value of &quot;http://public_search&quot;: ...Executor): Key condition: (column 1 in ['http://public_search', 'http://public_search']) ...Executor): Used generic exclusion search over index for part all_1_9_2 with 1537 steps ...Executor): Selected 1/1 parts by partition key, 1 parts by primary key, 1076/1083 marks by primary key, 1076 marks to read from 5 ranges ...Executor): Reading approx. 8814592 rows with 10 streams  We can see in the sample trace log above, that 1076 (via the marks) out of 1083 granules were selected as possibly containing rows with a matching URL value. This results in 8.81 million rows being streamed into the ClickHouse engine (in parallel by using 10 streams), in order to identify the rows that are actually contain the URL value &quot;http://public_search&quot;. However, as we will see later only 39 granules out of that selected 1076 granules actually contain matching rows. Whilst the primary index based on the compound primary key (UserID, URL) was very useful for speeding up queries filtering for rows with a specific UserID value, the index is not providing significant help with speeding up the query that filters for rows with a specific URL value. The reason for this is that the URL column is not the first key column and therefore ClickHouse is using a generic exclusion search algorithm (instead of binary search) over the URL column's index marks, and the effectiveness of that algorithm is dependant on the cardinality difference between the URL column and it's predecessor key column UserID. In order to illustrate that, we give some details about how the generic exclusion search works. Generic exclusion search algorithm details The following is illustrating how the ClickHouse generic exclusion search algorithm works when granules are selected via any column after the first, when the predecessor key column has a low(er) or high(er) cardinality. As an example for both cases we will assume: a query that is searching for rows with URL value = &quot;W3&quot;.an abstract version of our hits table with simplified values for UserID and URL.the same compound primary key (UserID, URL) for the index. This means rows are first ordered by UserID values. Rows with the same UserID value are then ordered by URL.a granule size of two i.e. each granule contains two rows. We have marked the minimum key column values for each granule in orange in the diagrams below.. Predecessor key column has low(er) cardinality Suppose UserID had low cardinality. In this case it would be likely that the same UserID value is spread over multiple table rows and granules and therefore index marks. For index marks with the same UserID, the URL values for the index marks are sorted in ascending order (because the table rows are ordered first by UserID and then by URL). This allows efficient filtering as described below: There are three different scenarios for the granule selection process for our abstract sample data in the diagram above: Index mark 0 for which the (minimum) URL value is smaller than W3 and for which the URL value of the directly succeeding index mark is also smaller than W3 can be excluded because mark 0, 1, and 2 have the same UserID value. Note that this exclusion-precondition ensures that granule 0 and the next granule 1 are completely composed of U1 UserID values so that ClickHouse can assume that also the maximum URL value in granule 0 is smaller than W3 and exclude the granule. Index mark 1 for which the URL value is smaller (or equal) than W3 and for which the URL value of the directly succeeding index mark is greater (or equal) than W3 is selected because it means that granule 1 can possibly contain rows with URL W3). Index marks 2 and 3 for which the URL value is greater than W3 can be excluded, since index marks of a primary index store the minimum key column values for each granule and therefore granule 2 and 3 can't possibly contain URL value W3. Predecessor key column has high(er) cardinality When the UserID has high cardinality then it is unlikely that the same UserID value is spread over multiple table rows and granules. This means the URL values for the index marks are not monotonically increasing: As we can see in the diagram above, all shown marks whose URL values are smaller than W3 are getting selected for streaming its associated granule's rows into the ClickHouse engine. This is because whilst all index marks in the diagram fall into scenario 1 described above, they do not satisfy the mentioned exclusion-precondition that the two directly succeeding index marks both have the same UserID value as the current mark and thus can’t be excluded. For example, consider index mark 0 for which the URL value is smaller than W3 and for which the URL value of the directly succeeding index mark is also smaller than W3. This can not be excluded because the two directly succeeding index marks 1 and 2 do not have the same UserID value as the current mark 0. Note the requirement for the two succeeding index marks to have the same UserID value. This ensures that the granules for the current and the next mark are completely composed of U1 UserID values. If only the next mark had the same UserID, the URL value of the next mark could potentially stem from a table row with a different UserID - which is indeed the case when you look at the diagram above i.e. W2 stems from a row with U2 not U1. This ultimately prevents ClickHouse from making assumptions about the maximum URL value in granule 0. Instead it has to assume that granule 0 potentially contains rows with URL value W3 and is forced to select mark 0. The same scenario is true for mark 1, 2, and 3. Conclusion The generic exclusion search algorithm that ClickHouse is using instead of the binary search algorithm when a query is filtering on a column that is part of a compound key, but is not the first key column is most effective when the predecessor key column has low(er) cardinality. In our sample data set both key columns (UserID, URL) have similar high cardinality, and, as explained, the generic exclusion search algorithm is not very effective when the predecessor key column of the URL column has a high(er) or similar cardinality. note about data skipping index Because of the similarly high cardinality of UserID and URL, our query filtering on URL also wouldn't benefit much from creating a secondary data skipping index on the URL column of our table with compound primary key (UserID, URL). For example this two statements create and populate a minmax data skipping index on the URL column of our table: ALTER TABLE hits_UserID_URL ADD INDEX url_skipping_index URL TYPE minmax GRANULARITY 4; ALTER TABLE hits_UserID_URL MATERIALIZE INDEX url_skipping_index; ClickHouse now created an additional index that is storing - per group of 4 consecutive granules (note the GRANULARITY 4 clause in the ALTER TABLE statement above) - the minimum and maximum URL value: The first index entry (‘mark 0’ in the diagram above) is storing the minimum and maximum URL values for the rows belonging to the first 4 granules of our table. The second index entry (‘mark 1’) is storing the minimum and maximum URL values for the rows belonging to the next 4 granules of our table, and so on. (ClickHouse also created a special mark file for to the data skipping index for locating the groups of granules associated with the index marks.) Because of the similarly high cardinality of UserID and URL, this secondary data skipping index can't help with excluding granules from being selected when our query filtering on URL is executed. The specific URL value that the query is looking for (i.e. 'http://public_search') very likely is between the minimum and maximum value stored by the index for each group of granules resulting in ClickHouse being forced to select the group of granules (because they might contain row(s) matching the query). As a consequence, if we want to significantly speed up our sample query that filters for rows with a specific URL then we need to use a primary index optimized to that query. If in addition we want to keep the good performance of our sample query that filters for rows with a specific UserID then we need to use multiple primary indexes. The following is showing ways for achieving that.  "},{"title":"Performance tuning with multiple primary indexes​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#performance-tuning-with-multiple-primary-indexes","content":"If we want to significantly speed up both of our sample queries - the one that filters for rows with a specific UserID and the one that filters for rows with a specific URL - then we need to use multiple primary indexes by using one if these three options: Creating a second table with a different primary key.Creating a materialized view on our existing table.Adding a projection to our existing table. All three options will effectively duplicate our sample data into a additional table in order to reorganize the table primary index and row sort order. However, the three options differ in how transparent that additional table is to the user with respect to the routing of queries and insert statements. When creating a second table with a different primary key then queries must be explicitly send to the table version best suited for the query, and new data must be inserted explicitly into both tables in order to keep the tables in sync:  With a materialized view the additional table is hidden and data is automatically kept in sync between both tables:  And the projection is the most transparent option because next to automatically keeping the hidden additional table in sync with data changes, ClickHouse will automatically chose the most effective table version for queries:  In the following we discuss this three options for creating and using multiple primary indexes in more detail and with real examples.  "},{"title":"Multiple primary indexes via secondary tables​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#multiple-primary-indexes-via-secondary-tables","content":" We are creating a new additional table where we switch the order of the key columns (compared to our original table) in the primary key: CREATE TABLE hits_URL_UserID ( `UserID` UInt32, `URL` String, `EventTime` DateTime ) ENGINE = MergeTree PRIMARY KEY (URL, UserID) ORDER BY (URL, UserID, EventTime) SETTINGS index_granularity = 8192, index_granularity_bytes = 0;  Insert all 8.87 million rows from our original table into the additional table: INSERT INTO hits_URL_UserID SELECT * from hits_UserID_URL;  The response looks like: Ok. 0 rows in set. Elapsed: 2.898 sec. Processed 8.87 million rows, 838.84 MB (3.06 million rows/s., 289.46 MB/s.)  And finally optimize the table: OPTIMIZE TABLE hits_URL_UserID FINAL;  Because we switched the order of the columns in the primary key, the inserted rows are now stored on disk in a different lexicographical order (compared to our original table) and therefore also the 1083 granules of that table are containing different values than before:  This is the resulting primary key:  That can now be used to significantly speed up the execution of our example query filtering on the URL column in order to calculate the top 10 users that most frequently clicked on the URL &quot;http://public_search&quot;: SELECT UserID, count(UserID) AS Count FROM hits_URL_UserID WHERE URL = 'http://public_search' GROUP BY UserID ORDER BY Count DESC LIMIT 10;  The response is:  ┌─────UserID─┬─Count─┐ │ 2459550954 │ 3741 │ │ 1084649151 │ 2484 │ │ 723361875 │ 729 │ │ 3087145896 │ 695 │ │ 2754931092 │ 672 │ │ 1509037307 │ 582 │ │ 3085460200 │ 573 │ │ 2454360090 │ 556 │ │ 3884990840 │ 539 │ │ 765730816 │ 536 │ └────────────┴───────┘ 10 rows in set. Elapsed: 0.017 sec. Processed 319.49 thousand rows, 11.38 MB (18.41 million rows/s., 655.75 MB/s.)  Now, instead of almost doing a full table scan, ClickHouse executed that query much more effective. With the primary index from the original table where UserID was the first, and URL the second key column, ClickHouse used a generic exclusion search over the index marks for executing that query and that was not very effective because of the similarly high cardinality of UserID and URL. With URL as the first column in the primary index, ClickHouse is now running binary search over the index marks. The corresponding trace log in the ClickHouse server log file confirms that: ...Executor): Key condition: (column 0 in ['http://public_search', 'http://public_search']) ...Executor): Running binary search on index range for part all_1_9_2 (1083 marks) ...Executor): Found (LEFT) boundary mark: 644 ...Executor): Found (RIGHT) boundary mark: 683 ...Executor): Found continuous range in 19 steps ...Executor): Selected 1/1 parts by partition key, 1 parts by primary key, 39/1083 marks by primary key, 39 marks to read from 1 ranges ...Executor): Reading approx. 319488 rows with 2 streams  ClickHouse selected only 39 index marks, instead of 1076 when generic exclusion search was used. Note that the additional table is optimized for speeding up the execution of our example query filtering on URLs. Similar to the bad performance of that query with our original table, our example query filtering on UserIDs will not run very effectively with the new additional table, because UserID is now the second key column in the primary index of that table and therefore ClickHouse will use generic exclusion search for granule selection, which is not very effective for similarly high cardinality of UserID and URL. Open the details box for specifics. Query filtering on UserIDs now has bad performance SELECT URL, count(URL) AS Count FROM hits_URL_UserID WHERE UserID = 749927693 GROUP BY URL ORDER BY Count DESC LIMIT 10; The response is: ┌─URL────────────────────────────┬─Count─┐ │ http://auto.ru/chatay-barana.. │ 170 │ │ http://auto.ru/chatay-id=371...│ 52 │ │ http://public_search │ 45 │ │ http://kovrik-medvedevushku-...│ 36 │ │ http://forumal │ 33 │ │ http://korablitz.ru/L_1OFFER...│ 14 │ │ http://auto.ru/chatay-id=371...│ 14 │ │ http://auto.ru/chatay-john-D...│ 13 │ │ http://auto.ru/chatay-john-D...│ 10 │ │ http://wot/html?page/23600_m...│ 9 │ └────────────────────────────────┴───────┘ 10 rows in set. Elapsed: 0.024 sec. Processed 8.02 million rows, 73.04 MB (340.26 million rows/s., 3.10 GB/s.) Server Log: ...Executor): Key condition: (column 1 in [749927693, 749927693]) ...Executor): Used generic exclusion search over index for part all_1_9_2 with 1453 steps ...Executor): Selected 1/1 parts by partition key, 1 parts by primary key, 980/1083 marks by primary key, 980 marks to read from 23 ranges ...Executor): Reading approx. 8028160 rows with 10 streams  We now have two tables. Optimized for speeding up queries filtering on UserIDs, and speeding up queries filtering on URLs, respectively:  "},{"title":"Multiple primary indexes via materialized views​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#multiple-primary-indexes-via-materialized-views","content":"Create a materialized view on our existing table. CREATE MATERIALIZED VIEW mv_hits_URL_UserID ENGINE = MergeTree() PRIMARY KEY (URL, UserID) ORDER BY (URL, UserID, EventTime) POPULATE AS SELECT * FROM hits_UserID_URL;  The response looks like: Ok. 0 rows in set. Elapsed: 2.935 sec. Processed 8.87 million rows, 838.84 MB (3.02 million rows/s., 285.84 MB/s.)  note we switch the order of the key columns (compared to our original table ) in the view's primary keythe materialzed view is backed by a hidden table whose row order and primary index is based on the given primary key definitionwe use the POPULATE keyword in order to immediately populate the hidden table with all 8.87 million rows from the source table hits_UserID_URLif new rows are inserted into the source table hits_UserID_URL, then that rows are automatically also inserted into the hidden tableEffectively the implicitly created hidden table has the same row order and primary index as the secondary table that we created explicitly: ClickHouse is storing the column data files (.bin), the mark files (.mrk2) and the primary index (primary.idx) of the hidden table in a special folder withing the ClickHouse server's data directory: The hidden table (and it's primary index) backing the materialized view can now be used to significantly speed up the execution of our example query filtering on the URL column: SELECT UserID, count(UserID) AS Count FROM mv_hits_URL_UserID WHERE URL = 'http://public_search' GROUP BY UserID ORDER BY Count DESC LIMIT 10;  The response is: ┌─────UserID─┬─Count─┐ │ 2459550954 │ 3741 │ │ 1084649151 │ 2484 │ │ 723361875 │ 729 │ │ 3087145896 │ 695 │ │ 2754931092 │ 672 │ │ 1509037307 │ 582 │ │ 3085460200 │ 573 │ │ 2454360090 │ 556 │ │ 3884990840 │ 539 │ │ 765730816 │ 536 │ └────────────┴───────┘ 10 rows in set. Elapsed: 0.026 sec. Processed 335.87 thousand rows, 13.54 MB (12.91 million rows/s., 520.38 MB/s.)  Because effectively the hidden table (and it's primary index) backing the materialized view is identical to the secondary table that we created explicitly, the query is executed in the same effective way as with the explicitly created table. The corresponding trace log in the ClickHouse server log file confirms that ClickHouse is running binary search over the index marks: ...Executor): Key condition: (column 0 in ['http://public_search', 'http://public_search']) ...Executor): Running binary search on index range ... ... ...Executor): Selected 4/4 parts by partition key, 4 parts by primary key, 41/1083 marks by primary key, 41 marks to read from 4 ranges ...Executor): Reading approx. 335872 rows with 4 streams  "},{"title":"Multiple primary indexes via projections​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#multiple-primary-indexes-via-projections","content":"Projections are an experimental feature at the moment, therefore we need to tell ClickHouse that we know what we are doing first: SET allow_experimental_projection_optimization = 1;  Create a projection on our existing table: ALTER TABLE hits_UserID_URL ADD PROJECTION prj_url_userid ( SELECT * ORDER BY (URL, UserID) );  And materialize the projection: ALTER TABLE hits_UserID_URL MATERIALIZE PROJECTION prj_url_userid;  note the projection is creating a hidden table whose row order and primary index is based on the given ORDER BY clause of the projectionwe use the MATERIALIZE keyword in order to immediately populate the hidden table with all 8.87 million rows from the source table hits_UserID_URLif new rows are inserted into the source table hits_UserID_URL, then that rows are automatically also inserted into the hidden tablea query is always (syntactically) targeting the source table hits_UserID_URL, but if the row order and primary index of the hidden table allows a more effective query execution, then that hidden table will be used insteadEffectively the implicitly created hidden table has the same row order and primary index as the secondary table that we created explicitly: ClickHouse is storing the column data files (.bin), the mark files (.mrk2) and the primary index (primary.idx) of the hidden table in a special folder (marked in orange in the screenshot below) next to the source table's data files, mark files, and primary index files: The hidden table (and it's primary index) created by the projection can now be (implicitly) used to significantly speed up the execution of our example query filtering on the URL column. Note that the query is syntactically targeting the source table of the projection. SELECT UserID, count(UserID) AS Count FROM hits_UserID_URL WHERE URL = 'http://public_search' GROUP BY UserID ORDER BY Count DESC LIMIT 10;  The response is: ┌─────UserID─┬─Count─┐ │ 2459550954 │ 3741 │ │ 1084649151 │ 2484 │ │ 723361875 │ 729 │ │ 3087145896 │ 695 │ │ 2754931092 │ 672 │ │ 1509037307 │ 582 │ │ 3085460200 │ 573 │ │ 2454360090 │ 556 │ │ 3884990840 │ 539 │ │ 765730816 │ 536 │ └────────────┴───────┘ 10 rows in set. Elapsed: 0.029 sec. Processed 319.49 thousand rows, 1 1.38 MB (11.05 million rows/s., 393.58 MB/s.)  Because effectively the hidden table (and it's primary index) created by the projection is identical to the secondary table that we created explicitly, the query is executed in the same effective way as with the explicitly created table. The corresponding trace log in the ClickHouse server log file confirms that ClickHouse is running binary search over the index marks: ...Executor): Key condition: (column 0 in ['http://public_search', 'http://public_search']) ...Executor): Running binary search on index range for part prj_url_userid (1083 marks) ...Executor): ... ...Executor): Choose complete Normal projection prj_url_userid ...Executor): projection required columns: URL, UserID ...Executor): Selected 1/1 parts by partition key, 1 parts by primary key, 39/1083 marks by primary key, 39 marks to read from 1 ranges ...Executor): Reading approx. 319488 rows with 2 streams  "},{"title":"Removing inefficient key columns​","type":1,"pageTitle":"A Practical Introduction to Sparse Primary Indexes in ClickHouse","url":"en/guides/improving-query-performance/sparse-primary-indexes#removing-inefficient-key-columns","content":"The primary index of our table with compound primary key (UserID, URL) was very useful for speeding up a query filtering on UserID. But that index is not providing significant help with speeding up a query filtering on URL, despite the URL column being part of the compound primary key. And vice versa: The primary index of our table with compound primary key (URL, UserID) was speeding up a query filtering on URL, but didn't provide much support for a query filtering on UserID. Because of the similarly high cardinality of the primary key columns UserID and URL, a query that filters on the second key column doesn’t benefit much from the second key column being in the index. Therefore it makes sense to remove the second key column from the primary index (resulting in less memory consumption of the index) and to use multiple primary indexes instead. However if the key columns in a compound primary key have big differences in cardinality, then it is beneficial for queries to order the primary key columns by cardinality in ascending order. The higher the cardinality difference between the key columns is, the more the order of those columns in the key matters. We will demonstrate that in a future article. Stay tuned. "},{"title":"Connecting ClickHouse","type":0,"sectionRef":"#","url":"en/integrations/dbt/dbt-connecting","content":"Connecting ClickHouse Create a dbt project. In this case we name this after our imdb source. When prompted, select clickhouse as the database source. clickhouse-user@clickhouse:~$ dbt init imdb 16:52:40 Running with dbt=1.0.4 Which database would you like to use? [1] clickhouse (Don't see the one you want? https://docs.getdbt.com/docs/available-adapters) Enter a number: 1 16:53:21 No sample profile found for clickhouse. 16:53:21 Your new dbt project &quot;imdb&quot; was created! For more information on how to configure the profiles.yml file, please consult the dbt documentation here: https://docs.getdbt.com/docs/configure-your-profile cd into your project folder: cd imdb At this point, you will need the text editor of your choice. In the examples below, we use the popular VSCode. Opening the IMDB directory, you should see a collection of yml and sql files: Update your dbt_project.yml file to specify our first model - actor_summary and set profile to clickhouse_imdb. We next need to provide dbt with the connection details for our ClickHouse instance. Add the following to your ~/.dbt/profiles.yml. clickhouse_imdb: target: dev outputs: dev: type: clickhouse schema: imdb_dbt host: localhost port: 9000 user: default password: password Secure: False Note the need to modify the user and password. There are additional available settings documented here. From the IMDB directory, execute the dbt debug command to confirm whether dbt is able to connect to ClickHouse. clickhouse-user@clickhouse:~/imdb$ dbt debug 17:33:53 Running with dbt=1.0.4 dbt version: 1.0.4 python version: 3.10.1 python path: /home/dale/.pyenv/versions/3.10.1/bin/python3.10 os info: Linux-5.13.0-10039-tuxedo-x86_64-with-glibc2.31 Using profiles.yml file at /home/dale/.dbt/profiles.yml Using dbt_project.yml file at /opt/dbt/imdb/dbt_project.yml Configuration: profiles.yml file [OK found and valid] dbt_project.yml file [OK found and valid] Required dependencies: - git [OK found] Connection: host: localhost port: 9000 user: default schema: imdb_dbt secure: False verify: False Connection test: [OK connection ok] All checks passed! Confirm the response includes Connection test: [OK connection ok] indicating a successful connection.","keywords":""},{"title":"Other Approaches","type":0,"sectionRef":"#","url":"en/guides/developer/working-with-json/json-other-approaches","content":"","keywords":""},{"title":"Handle as Structured Data​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#handle-as-structured-data","content":"If your JSON has a fixed schema, mapping it to an explicit schema provides the most optimal performance. Specifically, users can control codecs, configure data skipping indexes and utilize columns in primary and sort keys. This approach represents the most optimal means of handling JSON. It is limited in a number of ways, however, specifically: JSON values need to be consistent and mappable to columns. If the data is inconsistent or dirty, insert logic will need to be modified.All columns and their types must be known upfront. Changes will need to be made to the table should JSON keys be added - prior knowledge of this is required. For the example above, most of the fields have obvious types. However, we have a few options for the object request field: nested, tuple, and map (assuming no support for JSON objects). "},{"title":"Using Nested​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#using-nested","content":"Below we provide an example of using nested. CREATE table http ( `@timestamp` Int32 EPHEMERAL 0, clientip IPv4, request Nested(method LowCardinality(String), path String, version LowCardinality(String)), status UInt16, size UInt32, timestamp DateTime DEFAULT toDateTime(`@timestamp`) ) ENGINE = MergeTree() ORDER BY (status, timestamp); SET input_format_import_nested_json = 1; INSERT INTO http (`@timestamp`, clientip, request.method, request.path, request.version, status, size) FORMAT JSONEachRow {&quot;@timestamp&quot;:897819077,&quot;clientip&quot;:&quot;45.212.12.0&quot;,&quot;request&quot;:{&quot;method&quot;:[&quot;GET&quot;], &quot;path&quot;:[&quot;/french/images/hm_nav_bar.gif&quot;],&quot;version&quot;:[&quot;HTTP/1.0&quot;]},&quot;status&quot;:200,&quot;size&quot;:3305}  A few important points to note here: We need to use the setting input_format_import_nested_json to insert the JSON as a nested structure. Without this, we are required to flatten the JSON i.e. INSERT INTO http_uint FORMAT JSONEachRow {&quot;@timestamp&quot;:897819077,&quot;clientip&quot;:&quot;45.212.12.0&quot;,&quot;request.method&quot;:[&quot;GET&quot;], &quot;request.path&quot;:[&quot;/french/images/hm_nav_bar.gif&quot;],&quot;request.version&quot;:[&quot;HTTP/1.0&quot;], &quot;status&quot;:200,&quot;size&quot;:3305} The nested fields method, path, and version need to be passed as JSON arrays The columns must be specified in INSERT - this is actually because of the EPHEMERAL column @timestamp, which requires a type conversion. Columns can be queried using a dot notation. SELECT clientip, status, size, `request.method` FROM http WHERE has(request.method, 'GET');  Notice how we are required to query request.method as an Array. It is easiest to think of a nested data structure as multiple column arrays of the same length. The fields method, path, and version are all separate Array(Type) columns in effect with one critical constraint: the length of the method, path, and version fields must be the same. If your nested structure fits this constraint, and you are comfortable ensuring the values are inserted as strings, nested provides a simple means of querying JSON. Note the use of Arrays for the sub-columns means the full breath Array functions can potentially be exploited, including the Array Join clause - useful if your columns have multiple values. Additionally, nested fields can be used in primary and sort keys. Given the constraints and input format for the JSON, we insert our sample dataset using the following query. Note the use of the map operators to access the request fields - this results from schema inference detecting a map for the request field in the s3 data. The following statement inserts 10m rows, so this may take a few minutes to execute. Apply a LIMIT if required. INSERT INTO http (`@timestamp`, clientip, request.method, request.path, request.version, status, size) SELECT `@timestamp`, clientip, [request['method']], [request['path']], [request['version']], status, size FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/http/documents-01.ndjson.gz', 'JSONEachRow');  Querying this data requires us to access the request fields as arrays. Below we summarize the errors and http methods over a fixed time period. SELECT status, request.method[1] as method, count() as c FROM http WHERE status &gt;= 400 AND timestamp BETWEEN '1998-01-01 00:00:00' AND '1998-06-01 00:00:00' GROUP by method, status ORDER BY c DESC LIMIT 5;  status\tmethod\tc404\tGET\t11267 404\tHEAD\t276 500\tGET\t160 500\tPOST\t115 400\tGET\t81 "},{"title":"Using Tuples​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#using-tuples","content":"The nested object request can also be represented as a Tuple. This provides comparable functionality to nested, addressing some of its constraints at the expense of other limitations. For example, by not using Arrays we do not have the same constraint that subfields of an object have to be the same length. This lets us represent more varied structures. However, unlike nested fields, the subfields of tuples cannot be used in primary and sort keys. First, create an example table for the http data: DROP TABLE IF EXISTS http; CREATE table http ( `@timestamp` Int32 EPHEMERAL 0, clientip IPv4, request Tuple(method LowCardinality(String), path String, version LowCardinality(String)), status UInt16, size UInt32, timestamp DateTime DEFAULT toDateTime(`@timestamp`) ) ENGINE = MergeTree() ORDER BY (status, timestamp);  Insertion of data requires changes to the nested field structure. Specifically, note how the “request” object below must be passed as an array of values. INSERT INTO http (`@timestamp`, clientip, request, status, size) FORMAT JSONEachRow {&quot;@timestamp&quot;:893964617,&quot;clientip&quot;:&quot;40.135.0.0&quot;,&quot;request&quot;:[&quot;GET&quot;, &quot;/images/hm_bg.jpg&quot;, &quot;HTTP/1.0&quot;], &quot;status&quot;:200,&quot;size&quot;:24736}  We have minimal data in our example above, but as shown below we can query the tuple fields by their period delimited names. We also aren’t required to use Array functions like nested. SELECT `request.method`, status, timestamp FROM http WHERE request.method = 'GET';  request.method\tstatus\ttimestampGET\t200\t1998-04-30 19:30:17 The principal disadvantage of tuples, other than the requirement to convert our objects into lists, is the sub fields cannot be used as primary or sort keys. The following will thus fail. DROP TABLE IF EXISTS http; CREATE table http ( `@timestamp` Int32 EPHEMERAL 0, clientip IPv4, request Tuple(method LowCardinality(String), path String, version LowCardinality(String)), status UInt16, size UInt32, timestamp DateTime DEFAULT toDateTime(`@timestamp`) ) ENGINE = MergeTree() ORDER BY (status, request.method, timestamp);  However, the entire tuple can be used for this purpose. The following is valid. DROP TABLE IF EXISTS http; CREATE table http ( `@timestamp` Int32 EPHEMERAL 0, clientip IPv4, request Tuple(method LowCardinality(String), path String, version LowCardinality(String)), status UInt16, size UInt32, timestamp DateTime DEFAULT toDateTime(`@timestamp`) ) ENGINE = MergeTree() ORDER BY (status, request, timestamp);  As noted in Semi-Structured Approach, the JSON object type available in 22.3 utilizes tuples for nested structures - abstracting the above complexity with a more intuitive query interface. To insert our sample data from s3 we can use the following query. Note the need to form a tuple at insert time for the request field i.e. (request['method'], request['path'], request['version']). INSERT INTO http(`@timestamp`, clientip, request, status, size) SELECT `@timestamp`, clientip, (request['method'], request['path'], request['version']), status, size FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/http/documents-01.ndjson.gz', 'JSONEachRow');  To reproduce our earlier query analyzing error rates by status code, we don’t require any special syntax:  SELECT status, request.method as method, count() as c FROM http WHERE status &gt;= 400 AND timestamp BETWEEN '1998-01-01 00:00:00' AND '1998-06-01 00:00:00' GROUP by method, status ORDER BY c DESC LIMIT 5;  "},{"title":"Using Maps​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#using-maps","content":"Maps represent a simple way to represent nested structures, with some noticeable limitations: The fields must be of all the same type.The values are also restricted to String, Integer, Array, LowCardinality, or FixedString types.Accessing subfields requires a special map syntax - since the fields don’t exist as columns i.e. the entire object is a column. Provided we assume the subfields of our request object are all Strings, we use a map to hold this structure. DROP TABLE IF EXISTS http; CREATE table http ( `@timestamp` Int32 EPHEMERAL 0, clientip IPv4, request Map(String, String), status UInt16, size UInt32, timestamp DateTime DEFAULT toDateTime(`@timestamp`) ) ENGINE = MergeTree() ORDER BY (status, request, timestamp);  Unlike Nested and Tuple, we aren’t required to make changes to our JSON structures at insertion. INSERT INTO http (`@timestamp`, clientip, request, status, size) FORMAT JSONEachRow {&quot;@timestamp&quot;:897819077,&quot;clientip&quot;:&quot;45.212.12.0&quot;,&quot;request&quot;:{&quot;method&quot;: &quot;GET&quot;,&quot;path&quot;:&quot;/french/images/hm_nav_bar.gif&quot;,&quot;version&quot;:&quot;HTTP/1.1&quot;},&quot;status&quot;:200,&quot;size&quot;:3305}  Querying these fields within the request object requires a map syntax e.g. SELECT * FROM http;  clientip\trequest\tstatus\tsize\ttimestamp45.212.12.0\t{'method':'GET','path':'/french/images/hm_nav_bar.gif','version':'HTTP/1.1'}\t200\t3305\t1998-06-14 10:11:17 SELECT timestamp, request['method'] as method, status FROM http WHERE request['method'] = 'GET';  timestamp\tmethod\tstatus1998-06-14 10:11:17\tGET\t200 A full set of map functions is available to query this time, described here. If your data is not of a consistent type, functions exist to perform the necessary coercion. The following example, exploits the fact that data objects can also be inserted into a map in the structure [(key, value), (key, value),...] e.g. [('method', 'GET'),('path', '/french/images/hm\\_nav\\_bar.gif'),('version', 'HTTP/1.1')] This function in turn allows us to insert our full s3 dataset with no need to reformat the data. INSERT INTO http (`@timestamp`, clientip, request, status, size) SELECT `@timestamp`, clientip, request, status, size FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/http/documents-01.ndjson.gz', 'JSONEachRow');  To reproduce our earlier query example which analyzes status codes by HTTP method, we require the use of the map syntax: SELECT status, request['method'] as method, count() as c FROM http WHERE status &gt;= 400 AND timestamp BETWEEN '1998-01-01 00:00:00' AND '1998-06-01 00:00:00' GROUP by method, status ORDER BY c DESC LIMIT 5;  "},{"title":"Nested vs Tuple vs Map​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#nested-vs-tuple-vs-map","content":"Each of the above strategies for handling nested JSON has its respective advantages and disadvantages. The following captures these differences. Type\tRequires custom INSERT format\tRequires custom notation to read fields\tConstraints on structure e.g. list lengths or types\tObject fields can be used for primary/sort keys\tCreates more columns on diskNested\tYes\tNo\tYes*\tYes\tYes Tuple\tYes\tNo\tNo\tNo\tNo Map\tNo\tYes\tYes**\tNo\tNo *Nested requires values (represented as arrays) to have the same length **Values must be the same type "},{"title":"Store as String​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#store-as-string","content":"Handling data using the structured approach described in Handle as Structured Data, is often not viable for those users with dynamic JSON which is either subject to change or for which the schema is not well understood. For absolute flexibility, users can simply store JSON as Strings before using functions to extract fields as required. This represents the extreme opposite to handling JSON as a structured object. This flexibility incurs costs with significant disadvantages - primarily an increase in query syntax complexity as well as degraded performance. Our table schema, in this case, is trivial: DROP TABLE IF EXISTS http; CREATE table http_json ( message String ) ENGINE = MergeTree ORDER BY tuple();  Insertion requires us to send each JSON row as a String. Here we use the format JSONAsString to ensure our object is interpreted. INSERT INTO http FORMAT JSONAsString {&quot;@timestamp&quot;:897819077,&quot;clientip&quot;:&quot;45.212.12.0&quot;,&quot;request&quot;:{&quot;method&quot;:&quot;GET&quot;, &quot;path&quot;:&quot;/french/images/hm_nav_bar.gif&quot;,&quot;version&quot;:&quot;HTTP/1.0&quot;},&quot;status&quot;:200,&quot;size&quot;:3305}  To illustrate queries we can insert our sample from s3: INSERT INTO http SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/http/documents-01.ndjson.gz', 'JSONAsString');  The below query counts the requests with a status code greater than 200, grouping by http method. SELECT JSONExtractString(JSONExtractString(message, 'request'), 'method') as method, JSONExtractInt(message, 'status') as status, count() as count FROM http WHERE status &gt;= 400 AND method == 'GET' GROUP BY method, status;  method\tstatus\tcountGET\t404\t11267 GET\t400\t81 GET\t500\t160 Despite using functions to parse the String, this query should still return for the 10m rows in a few seconds. Notice how the functions require both a reference to the String field message and a path in the JSON to extract. Nested paths require functions to be nested e.g. JSONExtractString(JSONExtractString(message, 'request'), 'method') extracts the field request.method. The extraction of nested paths can be simplified through the functions JSON_QUERY AND JSON_VALUE as shown below: SELECT JSONExtractInt(message, 'status') AS status, JSON_VALUE(message, '$.request.method') as method, count() as c FROM http WHERE status &gt;= 400 AND toDateTime(JSONExtractUInt(message, '@timestamp')) BETWEEN '1998-01-01 00:00:00' AND '1998-06-01 00:00:00' GROUP by method, status ORDER BY c DESC LIMIT 5;  status\tmethod\tc404\tGET\t11267 404\tHEAD\t276 500\tGET\t160 500\tPOST\t115 400\tGET\t81 Notice the use of an xpath expression here to filter the JSON by method i.e. JSON_VALUE(message, '$.request.method'). String functions are appreciably slower (&gt; 10x) than explicit type conversions with indices. The above queries always require a full table scan and processing of every row. While these queries will still be fast on a small dataset such as this, performance will degrade on larger datasets. The flexibility this approach provides comes at a clear performance and syntax cost. It can, however, be coupled with other approaches where users extract only the explicit fields they need for indices or frequent queries. For further details on this approach, see Hybrid approach. "},{"title":"Visit Functions​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#visit-functions","content":"The above examples use the JSON* family of functions. These utilize a full JSON parser based on simdjson, that is rigorous in its parsing and will distinguish between the same field nested at different levels. These functions are able to deal with JSON that is syntactically correct but not well-formatted, e.g. double spaces between keys. A faster and more strict set of functions are available. These visitParam* functions offer potentially superior performance, primarily by making strict assumptions as to the structure and format of the JSON. Specifically: Field names must be constants Consistent encoding of field names e.g. visitParamHas('{&quot;abc&quot;:&quot;def&quot;}', 'abc') = 1, but visitParamHas('{&quot;\\u0061\\u0062\\u0063&quot;:&quot;def&quot;}', 'abc') = 0 The field names are unique across all nested structures. No differentiation is made between nesting levels, and matching is indiscriminate. In the event of multiple matching fields, the first occurrence is used. No special characters outside of string literals. This includes spaces. The following is invalid and will not parse. {&quot;@timestamp&quot;: 893964617, &quot;clientip&quot;: &quot;40.135.0.0&quot;, &quot;request&quot;: {&quot;method&quot;: &quot;GET&quot;, &quot;path&quot;: &quot;/images/hm_bg.jpg&quot;, &quot;version&quot;: &quot;HTTP/1.0&quot;}, &quot;status&quot;: 200, &quot;size&quot;: 24736} whereas, will parse correctly {&quot;@timestamp&quot;:893964617,&quot;clientip&quot;:&quot;40.135.0.0&quot;,&quot;request&quot;:{&quot;method&quot;:&quot;GET&quot;, &quot;path&quot;:&quot;/images/hm_bg.jpg&quot;,&quot;version&quot;:&quot;HTTP/1.0&quot;},&quot;status&quot;:200,&quot;size&quot;:24736}  In some circumstances, where performance is critical and your JSON meets the above requirements, these may be appropriate. An example of the earlier query, re-written to use visitParam functions is shown below: SELECT visitParamExtractUInt(message, 'status') AS status, visitParamExtractString(message, 'method') as method, count() as c FROM http WHERE status &gt;= 400 AND toDateTime(visitParamExtractUInt(message, '@timestamp')) BETWEEN '1998-01-01 00:00:00' AND '1998-06-01 00:00:00' GROUP by method, status ORDER BY c DESC LIMIT 5;  status\tmethod\tc404\tGET\t11267 404\tHEAD\t276 500\tGET\t160 500\tPOST\t115 400\tGET\t81 Note that these functions are also aliased to simpleJSON* equivalents. The above query can be rewritten to: SELECT simpleJSONExtractUInt(message, 'status') AS status, simpleJSONExtractString(message, 'method') as method, count() as c FROM http WHERE status &gt;= 400 AND toDateTime(simpleJSONExtractUInt(message, '@timestamp')) BETWEEN '1998-01-01 00:00:00' AND '1998-06-01 00:00:00' GROUP by method, status;  "},{"title":"Using Pairwise Arrays​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#using-pairwise-arrays","content":"Pairwise arrays provide a balance between the flexibility of representing JSON as Strings and the performance of a more structured approach. The schema is flexible in that any new fields can be potentially added to the root. This, however, requires a significantly more complex query syntax and isn’t compatible with nested structures. As an example, consider the following table: CREATE TABLE http_with_arrays ( keys Array(String), values Array(String) ) ENGINE = MergeTree ORDER BY tuple();  To insert into this table, we need to structure the JSON as a list of keys and values. The following query illustrates the use of the JSONExtractKeysAndValues to achieve this: SELECT arrayMap(x -&gt; x.1, JSONExtractKeysAndValues(json, 'String')) as keys, arrayMap(x -&gt; x.2, JSONExtractKeysAndValues(json, 'String')) as values FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/http/documents-01.ndjson.gz', 'JSONAsString') LIMIT 1;  keys\tvalues['@timestamp','clientip','request','status','size']\t['893964617','40.135.0.0','{&quot;method&quot;:&quot;GET&quot;,&quot;path&quot;:&quot;/images/hm_bg.jpg&quot;,&quot;version&quot;:&quot;HTTP/1.0&quot;}','200','24736'] Note how the request column remains a nested structure represented as a string. We can insert any new keys to the root. We can also have arbitrary differences in the JSON itself. To insert into our local table, execute the following: INSERT INTO http_with_arrays SELECT arrayMap(x -&gt; x.1, JSONExtractKeysAndValues(message, 'String')) keys FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/http/documents-01.ndjson.gz', 'JSONEachRow');  Querying this structure requires using the indexOf function to identify the index of the required key (which should be consistent with the order of the values). This can in turn be used to access the values array column i.e. values[indexOf(keys, 'status')]. We still require a JSON parsing method for the request column - in this case, simpleJSONExtractString. SELECT toUInt16(values[indexOf(keys, 'status')]) as status, simpleJSONExtractString(values[indexOf(keys, 'request')], 'method') as method, count() as c FROM http_with_arrays WHERE status &gt;= 400 AND toDateTime(values[indexOf(keys, '@timestamp')]) BETWEEN '1998-01-01 00:00:00' AND '1998-06-01 00:00:00' GROUP by method, status ORDER BY c DESC LIMIT 5;  status\tmethod\tc404\tGET\t11267 404\tHEAD\t276 500\tGET\t160 500\tPOST\t115 400\tGET\t81 "},{"title":"Hybrid Approach with Materialized Columns​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#hybrid-approach-with-materialized-columns","content":"The approaches outlined above are not either OR. While parsing JSON fields to structured columns offers the best query performance, it also potentially incurs the highest insertion overhead if done in ClickHouse. Practically, it is also sometimes not possible due to dirty or variable data or even potentially an unknown schema. Conversely, keeping the JSON as Strings or using pairwise arrays, while flexible, significantly increases query complexity and makes accessing the data the function of someone with ClickHouse expertise. As a compromise, users can use a hybrid approach: representing the JSON as a String initially, extracting columns as required. While not essential, Materialized Columns can assist with this. For example, maybe we start with the following initial schema: DROP TABLE IF EXISTS http; CREATE table http ( message String, method String DEFAULT JSONExtractString(JSONExtractString(message, 'request'), 'method'), status UInt16 DEFAULT toUInt16(JSONExtractInt(message, 'status')), size UInt32 DEFAULT toUInt32(JSONExtractInt(message, 'size')), timestamp DateTime DEFAULT toDateTime(JSONExtractUInt(message, '@timestamp')) ) ENGINE = MergeTree() ORDER BY (status, timestamp);  Here we have simply moved our functions to extract data from the SELECT to DEFAULT values. This is somewhat of an artificial case as our JSON is simple and could, in reality, easily be mapped. Typically the columns extracted would be a small subset of a much larger schema. INSERT INTO http (message) SELECT json as message FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/http/documents-01.ndjson.gz', 'JSONAsString');  At this point we may decide we need to add the column client_ip after querying it frequently: ALTER TABLE http ADD COLUMN client_ip IPv4 DEFAULT toIPv4(JSONExtractString(message, 'clientip'));  The above change will only be incremental, i.e., the column will not exist for data inserted prior to the change. You can still query this column as it will be computed at SELECT time - although at an additional cost. Merges will also cause this column to be added to newly formed parts. To address this, we can use a mutation to update the existing data: ALTER TABLE http UPDATE client_ip = client_ip WHERE 1 = 1  The second call here returns immediately and executes asynchronously. Users can track the progress of the update, which requires rewriting the data on disk, using the system.mutations table. Further details here. Note that this is a potentially expensive operation and should be scheduled accordingly. It is, however, more optimal than an OPTIMIZE TABLE &lt;table_name&gt; FINAL since it only writes the changed column. "},{"title":"Default vs Materialized​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#default-vs-materialized","content":"The use of default columns represents one of the ways to achieve “Materialized columns”. There is also a MATERIALIZED column syntax. This differs from DEFAULT in a few ways: MATERIALIZED columns cannot be provided on INSERT i.e. they must always be computed from other columns. Conversely, DEFAULT columns can be optionally provided.SELECT * will skip MATERIALIZED columns i.e. they must be specifically requested. This allows a table dump to be reloaded back into a table of the same definition. "},{"title":"Assessing Storage Usage​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#assessing-storage-usage","content":"While extracting columns incurs a storage cost, typically, this can be minimized with a careful selection of codecs. Users will often wish to assess the cost of materializing a column prior. This cost only has a storage overhead if not queried - during the column-oriented nature of ClickHouse. We recommend testing the materialization on a subset of the data using a test table. The cost can, in turn, be computed using the following query, which can also provide an estimate of the compression achieved. SELECT table, name, type, compression_codec, formatReadableSize(data_compressed_bytes) as compressed_size, formatReadableSize(data_uncompressed_bytes) as uncompressed_size, data_compressed_bytes / data_uncompressed_bytes as compression_ratio FROM system.columns WHERE database = currentDatabase() ORDER BY table, name;  table\tname\ttype\tcompression_codec\tcompressed_size\tuncompressed_size\tcompression_ratiohttp\tclient_ip\tIPv4 23.51 MiB\t38.15 MiB\t0.61624925 http\tmessage\tString 203.00 MiB\t1.48 GiB\t0.1336674472634663 http\tmethod\tString 363.75 KiB\t38.18 MiB\t0.009304780749750751 http\tsize\tUInt32 24.19 MiB\t38.15 MiB\t0.6341134 http\tstatus\tUInt16 87.49 KiB\t19.07 MiB\t0.00447955 http\ttimestamp\tDateTime 4.98 MiB\t38.15 MiB\t0.1306381 "},{"title":"Using Materialized Views​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#using-materialized-views","content":"Using the hybrid approach described above requires significant processing at insertion time. This complicates data insertion logic and potentially introduces fragility in your data ingestion layer. To address this, we can exploit materialized views. The general concept here is to exploit a table with the null engine for receiving inserts. This table engine doesn’t store any data and acts as a “buffer” for the materialized view only. For each insert block, the materialized view will trigger, perform the processing the required and insert rows into a target table that we can in turn query. In cases where we need to update the schema, extracting a new field from the blob, we simply update our table schema and then modify the materialized view accordingly to extract the field. Our materialized view and null table engine effectively act as an ETL pipeline, as shown below:  First we create our null table engine for receiving inserts: CREATE TABLE http_etl ( message String ) ENGINE = Null;  Our target MergeTree table has a subset of the fields - ones we are maybe confident will occur in the JSON string. Note we retain a String field message for other data that can be used with JSON* functions if required. DROP TABLE IF EXISTS http; CREATE table http ( message String, method String, status UInt16, size UInt32, timestamp DateTime ) ENGINE = MergeTree() ORDER BY (status, timestamp);  Our materialized view in turn extracts the fields that have been declared in the http table schema. CREATE MATERIALIZED VIEW http_mv TO http AS SELECT message, JSONExtractString(JSONExtractString(message, 'request'), 'method') as method, toUInt16(JSONExtractInt(message, 'status')) as status, toUInt32(JSONExtractInt(message, 'size')) as size, toDateTime(JSONExtractUInt(message, '@timestamp')) as timestamp FROM http_etl;  Using the sample data from our s3 bucket, the insert is simplified to: INSERT INTO http_etl SELECT json as message FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/http/documents-01.ndjson.gz', 'JSONAsString');  Our analysis of error codes and http methods thus becomes trivial: SELECT status, method, count() as c FROM http WHERE status &gt;= 400 AND timestamp BETWEEN '1998-01-01 00:00:00' AND '1998-06-01 00:00:00' GROUP by method, status ORDER BY c DESC;  "},{"title":"Updating Materialized Views​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#updating-materialized-views","content":"Suppose we later wish to extract the field client_ip from our JSON blob. First we update our target table. ALTER TABLE http ADD COLUMN client_ip IPv4;  Using the setting allow_experimental_alter_materialized_view_structure we can modify our Materialized View: SET allow_experimental_alter_materialized_view_structure = 1; ALTER TABLE http_mv MODIFY QUERY SELECT message, JSONExtractString(JSONExtractString(message, 'request'), 'method') as method, toUInt16(JSONExtractInt(message, 'status')) as status, toUInt32(JSONExtractInt(message, 'size')) as size, toIPv4(JSONExtractString(message, 'clientip')) as client_ip, toDateTime(JSONExtractUInt(message, '@timestamp')) as timestamp FROM http_etl;  Note how this feature is experimental. You can alternatively drop the view using DROP VIEW and recreate it - however this does require pausing insertions. If an update of the target table is required, see the use of mutations in Hybrid Approach. "},{"title":"Using for Pairwise Arrays​","type":1,"pageTitle":"Other Approaches","url":"en/guides/developer/working-with-json/json-other-approaches#using-for-pairwise-arrays","content":"In the above example, we represented fields we wished to frequently query explicitly as columns. A materialized view could also be potentially used to extract pairwise arrays. This shifts potentially expensive logic from the SELECT statement. For example: CREATE TABLE http_with_arrays ( keys Array(String), values Array(String) ) ENGINE = MergeTree ORDER BY tuple(); CREATE MATERIALIZED VIEW http_mv TO http_with_arrays AS SELECT arrayMap(x -&gt; x.1, JSONExtractKeysAndValues(message, 'String')) as keys, arrayMap(x -&gt; x.2, JSONExtractKeysAndValues(message, 'String')) as values FROM http_etl; INSERT INTO http_etl SELECT json as message FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/http/documents-01.ndjson.gz', 'JSONAsString');  "},{"title":"Creating an Incremental Materialization","type":0,"sectionRef":"#","url":"en/integrations/dbt/dbt-incremental-model","content":"","keywords":""},{"title":"Internals​","type":1,"pageTitle":"Creating an Incremental Materialization","url":"en/integrations/dbt/dbt-incremental-model#internals","content":"We can identify the statements executed to achieve the above incremental update by querying ClickHouse’s query log. SELECT event_time, query FROM system.query_log WHERE type='QueryStart' AND query LIKE '%dbt%' AND event_time &gt; subtractMinutes(now(), 15) ORDER BY event_time LIMIT 100;  Adjust the above query to the period of execution. We leave result inspection to the user but highlight the general strategy used by the plugin to perform incremental updates: The plugin creates a temporary table actor_sumary__dbt_tmp using the Memory engine. Rows that have changed are streamed into this table. create temporary table actor_summary__dbt_tmp engine = Memory order by ((updated_at, id, name)) as (with actor_summary as (SELECT id, any(actor_name) as name, uniqExact(movie_id) as num_movies, avg(rank) as avg_rank, uniqExact(genre) as genres, uniqExact(director_name) as directors, max(created_at) as updated_at FROM ( SELECT imdb.actors.id as id, concat(imdb.actors.first_name, ' ', imdb.actors.last_name) as actor_name, imdb.movies.id as movie_id, imdb.movies.rank as rank, genre, concat(imdb.directors.first_name, ' ', imdb.directors.last_name) as director_name, created_at FROM imdb.actors JOIN imdb.roles ON imdb.roles.actor_id = imdb.actors.id LEFT OUTER JOIN imdb.movies ON imdb.movies.id = imdb.roles.movie_id LEFT OUTER JOIN imdb.genres ON imdb.genres.movie_id = imdb.movies.id LEFT OUTER JOIN imdb.movie_directors ON imdb.movie_directors.movie_id = imdb.movies.id LEFT OUTER JOIN imdb.directors ON imdb.directors.id = imdb.movie_directors.director_id ) GROUP BY id) select * from actor_summary where id &gt; (select max(id) from imdb_dbt.actor_summary) or updated_at &gt; (select max(updated_at) from imdb_dbt.actor_summary)); The previous materialized table is renamed actor_summary_old. A new table actor_summary is created. The rows from the old table are, in turn, streamed from the old to new, with a check to make sure row ids do not exist in the temporary table. This effectively handles updates: insert into imdb_dbt.actor_summary (&quot;id&quot;, &quot;name&quot;, &quot;num_movies&quot;, &quot;avg_rank&quot;, &quot;genres&quot;, &quot;directors&quot;, &quot;updated_at&quot;) select &quot;id&quot;, &quot;name&quot;, &quot;num_movies&quot;, &quot;avg_rank&quot;, &quot;genres&quot;, &quot;directors&quot;, &quot;updated_at&quot; from imdb_dbt.actor_summary__dbt_old where (id) not in (select (id) from actor_summary__dbt_tmp); Finally, results from the temporary table are streamed into the new actor_summary table: insert into imdb_dbt.actor_summary (&quot;id&quot;, &quot;name&quot;, &quot;num_movies&quot;, &quot;avg_rank&quot;, &quot;genres&quot;, &quot;directors&quot;, &quot;updated_at&quot;) select &quot;id&quot;, &quot;name&quot;, &quot;num_movies&quot;, &quot;avg_rank&quot;, &quot;genres&quot;, &quot;directors&quot;, &quot;updated_at&quot; from actor_summary__dbt_tmp;  This strategy may encounter challenges on very large models. For further details see Limitations. "},{"title":"ClickHouse and dbt","type":0,"sectionRef":"#","url":"en/integrations/dbt/dbt-intro","content":"ClickHouse and dbt dbt (data build tool) enables analytics engineers to transform data in their warehouses by simply writing select statements. dbt handles materializing these select statements into objects in the database in the form of tables and views - performing the T of Extract Load and Transform (ELT). Users can create a model defined by a SELECT statement. Within dbt, these models can be cross-referenced and layered to allow the construction of higher-level concepts. The boilerplate SQL required to connect models is automatically generated. Furthermore, dbt identifies dependencies between models and ensures they are created in the appropriate order using a directed acyclic graph (DAG). Dbt is compatible with ClickHouse through a community-maintained plugin. We describe the process for connecting ClickHouse with a simple example based on a publicly available IMDB dataset. We additionally highlight some of the limitations of the current connector. Concepts dbt introduces the concept of a model. This is defined as a SQL statement, potentially joining many tables. A model can be “materialized” in a number of ways. A materialization represents a build strategy for the model’s select query. The code behind a materialization is boilerplate SQL that wraps your SELECT query in a statement in order to create a new or update an existing relation. dbt provides 4 types of materialization: view (default): The model is built as a view in the database.table: The model is built as a table in the database.ephemeral: The model is not directly built in the database but is instead pulled into dependent models as common table expressions.incremental: The model is initially materialized as a table, and in subsequent runs, dbt inserts new rows and updates changed rows in the table. Additional syntax and clauses define how these models should be updated if their underlying data changes. dbt generally recommends starting with the view materialization until performance becomes a concern. The table materialization provides a query time performance improvement by capturing the results of the model’s query as a table at the expense of increased storage. The incremental approach builds on this further to allow subsequent updates to the underlying data to be captured in the target table. The current plugin for ClickHouse supports the view, table, and incremental materializations. Ephemeral is not supported. The plugin also supports dbt snapshots and seeds which we explore in this guide. For the following guides, we assume you have a ClickHouse instance available.","keywords":""},{"title":"Limitations","type":0,"sectionRef":"#","url":"en/integrations/dbt/dbt-limitations","content":"Limitations The current ClickHouse plugin for dbt has several limitations users should be aware of: As noted in Creating an Incremental Materialization, incremental changes are currently loaded into an in-memory table. ClickHouse does not provide distribution for this table, so changesets must not be larger than the memory of the orchestrator node receiving the query. Users should either avoid using the plugin on datasets where changes/additions are very high or schedule this operation to run at appropriate intervals to ensure all changes can be captured within memory limits. As noted here, we recommend that in-memory table engines should not exceed 100 million rows. Use this as an upper bound and schedule dbt run’s accordingly, i.e., ensure that the execution interval is frequent enough to not exceed this limit. The plugin currently materializes models as tables using an INSERT TO SELECT. This effectively means data duplication. Very large datasets (PB) can result in extremely long run times, making some models unviable. Aim to minimize the number of rows returned by any query, utilizing GROUP BY where possible. Prefer models which summarize data over those which simply perform a transform whilst maintaining row counts of the source. Ephemeral materializations are not supported. To use Distributed tables to represent a model, users must create the underlying replicated tables on each node manually. The Distributed table can, in turn, be created on top of these. The plugin does not manage cluster creation. Only the ClickHouse native protocol is supported. There is no support for HTTP. When dbt creates a relation (table/view) in a database, it usually creates it as: {{ database }}.{{ schema }}.{{ table/view id }}. ClickHouse has no notion of schemas. The plugin therefore uses {{schema}}.{{ table/view id }}, where schema is the ClickHouse database. Further Information The previous guides only touch the surface of dbt functionality. Users are recommended to read the excellent dbt documentation. Additional configuration for the plugin is described here.","keywords":""},{"title":"New York Taxi Data","type":0,"sectionRef":"#","url":"en/getting-started/example-datasets/nyc-taxi","content":"","keywords":""},{"title":"How to Import the Raw Data​","type":1,"pageTitle":"New York Taxi Data","url":"en/getting-started/example-datasets/nyc-taxi#how-to-import-the-raw-data","content":"See https://github.com/toddwschneider/nyc-taxi-data and http://tech.marksblogg.com/billion-nyc-taxi-rides-redshift.html for the description of a dataset and instructions for downloading. Downloading will result in about 227 GB of uncompressed data in CSV files. The download takes about an hour over a 1 Gbit connection (parallel downloading from s3.amazonaws.com recovers at least half of a 1 Gbit channel). Some of the files might not download fully. Check the file sizes and re-download any that seem doubtful. Some of the files might contain invalid rows. You can fix them as follows: sed -E '/(.*,){18,}/d' data/yellow_tripdata_2010-02.csv &gt; data/yellow_tripdata_2010-02.csv_ sed -E '/(.*,){18,}/d' data/yellow_tripdata_2010-03.csv &gt; data/yellow_tripdata_2010-03.csv_ mv data/yellow_tripdata_2010-02.csv_ data/yellow_tripdata_2010-02.csv mv data/yellow_tripdata_2010-03.csv_ data/yellow_tripdata_2010-03.csv  Then the data must be pre-processed in PostgreSQL. This will create selections of points in the polygons (to match points on the map with the boroughs of New York City) and combine all the data into a single denormalized flat table by using a JOIN. To do this, you will need to install PostgreSQL with PostGIS support. Be careful when running initialize_database.sh and manually re-check that all the tables were created correctly. It takes about 20-30 minutes to process each month’s worth of data in PostgreSQL, for a total of about 48 hours. You can check the number of downloaded rows as follows: $ time psql nyc-taxi-data -c &quot;SELECT count(*) FROM trips;&quot; ## Count 1298979494 (1 row) real 7m9.164s  (This is slightly more than 1.1 billion rows reported by Mark Litwintschik in a series of blog posts.) The data in PostgreSQL uses 370 GB of space. Exporting the data from PostgreSQL: COPY ( SELECT trips.id, trips.vendor_id, trips.pickup_datetime, trips.dropoff_datetime, trips.store_and_fwd_flag, trips.rate_code_id, trips.pickup_longitude, trips.pickup_latitude, trips.dropoff_longitude, trips.dropoff_latitude, trips.passenger_count, trips.trip_distance, trips.fare_amount, trips.extra, trips.mta_tax, trips.tip_amount, trips.tolls_amount, trips.ehail_fee, trips.improvement_surcharge, trips.total_amount, trips.payment_type, trips.trip_type, trips.pickup, trips.dropoff, cab_types.type cab_type, weather.precipitation_tenths_of_mm rain, weather.snow_depth_mm, weather.snowfall_mm, weather.max_temperature_tenths_degrees_celsius max_temp, weather.min_temperature_tenths_degrees_celsius min_temp, weather.average_wind_speed_tenths_of_meters_per_second wind, pick_up.gid pickup_nyct2010_gid, pick_up.ctlabel pickup_ctlabel, pick_up.borocode pickup_borocode, pick_up.boroname pickup_boroname, pick_up.ct2010 pickup_ct2010, pick_up.boroct2010 pickup_boroct2010, pick_up.cdeligibil pickup_cdeligibil, pick_up.ntacode pickup_ntacode, pick_up.ntaname pickup_ntaname, pick_up.puma pickup_puma, drop_off.gid dropoff_nyct2010_gid, drop_off.ctlabel dropoff_ctlabel, drop_off.borocode dropoff_borocode, drop_off.boroname dropoff_boroname, drop_off.ct2010 dropoff_ct2010, drop_off.boroct2010 dropoff_boroct2010, drop_off.cdeligibil dropoff_cdeligibil, drop_off.ntacode dropoff_ntacode, drop_off.ntaname dropoff_ntaname, drop_off.puma dropoff_puma FROM trips LEFT JOIN cab_types ON trips.cab_type_id = cab_types.id LEFT JOIN central_park_weather_observations_raw weather ON weather.date = trips.pickup_datetime::date LEFT JOIN nyct2010 pick_up ON pick_up.gid = trips.pickup_nyct2010_gid LEFT JOIN nyct2010 drop_off ON drop_off.gid = trips.dropoff_nyct2010_gid ) TO '/opt/milovidov/nyc-taxi-data/trips.tsv';  The data snapshot is created at a speed of about 50 MB per second. While creating the snapshot, PostgreSQL reads from the disk at a speed of about 28 MB per second. This takes about 5 hours. The resulting TSV file is 590612904969 bytes. Create a temporary table in ClickHouse: CREATE TABLE trips ( trip_id UInt32, vendor_id String, pickup_datetime DateTime, dropoff_datetime Nullable(DateTime), store_and_fwd_flag Nullable(FixedString(1)), rate_code_id Nullable(UInt8), pickup_longitude Nullable(Float64), pickup_latitude Nullable(Float64), dropoff_longitude Nullable(Float64), dropoff_latitude Nullable(Float64), passenger_count Nullable(UInt8), trip_distance Nullable(Float64), fare_amount Nullable(Float32), extra Nullable(Float32), mta_tax Nullable(Float32), tip_amount Nullable(Float32), tolls_amount Nullable(Float32), ehail_fee Nullable(Float32), improvement_surcharge Nullable(Float32), total_amount Nullable(Float32), payment_type Nullable(String), trip_type Nullable(UInt8), pickup Nullable(String), dropoff Nullable(String), cab_type Nullable(String), precipitation Nullable(UInt8), snow_depth Nullable(UInt8), snowfall Nullable(UInt8), max_temperature Nullable(UInt8), min_temperature Nullable(UInt8), average_wind_speed Nullable(UInt8), pickup_nyct2010_gid Nullable(UInt8), pickup_ctlabel Nullable(String), pickup_borocode Nullable(UInt8), pickup_boroname Nullable(String), pickup_ct2010 Nullable(String), pickup_boroct2010 Nullable(String), pickup_cdeligibil Nullable(FixedString(1)), pickup_ntacode Nullable(String), pickup_ntaname Nullable(String), pickup_puma Nullable(String), dropoff_nyct2010_gid Nullable(UInt8), dropoff_ctlabel Nullable(String), dropoff_borocode Nullable(UInt8), dropoff_boroname Nullable(String), dropoff_ct2010 Nullable(String), dropoff_boroct2010 Nullable(String), dropoff_cdeligibil Nullable(String), dropoff_ntacode Nullable(String), dropoff_ntaname Nullable(String), dropoff_puma Nullable(String) ) ENGINE = Log;  It is needed for converting fields to more correct data types and, if possible, to eliminate NULLs. $ time clickhouse-client --query=&quot;INSERT INTO trips FORMAT TabSeparated&quot; &lt; trips.tsv real 75m56.214s  Data is read at a speed of 112-140 Mb/second. Loading data into a Log type table in one stream took 76 minutes. The data in this table uses 142 GB. (Importing data directly from Postgres is also possible using COPY ... TO PROGRAM.) Unfortunately, all the fields associated with the weather (precipitation…average_wind_speed) were filled with NULL. Because of this, we will remove them from the final data set. To start, we’ll create a table on a single server. Later we will make the table distributed. Create and populate a summary table: CREATE TABLE trips_mergetree ENGINE = MergeTree(pickup_date, pickup_datetime, 8192) AS SELECT trip_id, CAST(vendor_id AS Enum8('1' = 1, '2' = 2, 'CMT' = 3, 'VTS' = 4, 'DDS' = 5, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14)) AS vendor_id, toDate(pickup_datetime) AS pickup_date, ifNull(pickup_datetime, toDateTime(0)) AS pickup_datetime, toDate(dropoff_datetime) AS dropoff_date, ifNull(dropoff_datetime, toDateTime(0)) AS dropoff_datetime, assumeNotNull(store_and_fwd_flag) IN ('Y', '1', '2') AS store_and_fwd_flag, assumeNotNull(rate_code_id) AS rate_code_id, assumeNotNull(pickup_longitude) AS pickup_longitude, assumeNotNull(pickup_latitude) AS pickup_latitude, assumeNotNull(dropoff_longitude) AS dropoff_longitude, assumeNotNull(dropoff_latitude) AS dropoff_latitude, assumeNotNull(passenger_count) AS passenger_count, assumeNotNull(trip_distance) AS trip_distance, assumeNotNull(fare_amount) AS fare_amount, assumeNotNull(extra) AS extra, assumeNotNull(mta_tax) AS mta_tax, assumeNotNull(tip_amount) AS tip_amount, assumeNotNull(tolls_amount) AS tolls_amount, assumeNotNull(ehail_fee) AS ehail_fee, assumeNotNull(improvement_surcharge) AS improvement_surcharge, assumeNotNull(total_amount) AS total_amount, CAST((assumeNotNull(payment_type) AS pt) IN ('CSH', 'CASH', 'Cash', 'CAS', 'Cas', '1') ? 'CSH' : (pt IN ('CRD', 'Credit', 'Cre', 'CRE', 'CREDIT', '2') ? 'CRE' : (pt IN ('NOC', 'No Charge', 'No', '3') ? 'NOC' : (pt IN ('DIS', 'Dispute', 'Dis', '4') ? 'DIS' : 'UNK'))) AS Enum8('CSH' = 1, 'CRE' = 2, 'UNK' = 0, 'NOC' = 3, 'DIS' = 4)) AS payment_type_, assumeNotNull(trip_type) AS trip_type, ifNull(toFixedString(unhex(pickup), 25), toFixedString('', 25)) AS pickup, ifNull(toFixedString(unhex(dropoff), 25), toFixedString('', 25)) AS dropoff, CAST(assumeNotNull(cab_type) AS Enum8('yellow' = 1, 'green' = 2, 'uber' = 3)) AS cab_type, assumeNotNull(pickup_nyct2010_gid) AS pickup_nyct2010_gid, toFloat32(ifNull(pickup_ctlabel, '0')) AS pickup_ctlabel, assumeNotNull(pickup_borocode) AS pickup_borocode, CAST(assumeNotNull(pickup_boroname) AS Enum8('Manhattan' = 1, 'Queens' = 4, 'Brooklyn' = 3, '' = 0, 'Bronx' = 2, 'Staten Island' = 5)) AS pickup_boroname, toFixedString(ifNull(pickup_ct2010, '000000'), 6) AS pickup_ct2010, toFixedString(ifNull(pickup_boroct2010, '0000000'), 7) AS pickup_boroct2010, CAST(assumeNotNull(ifNull(pickup_cdeligibil, ' ')) AS Enum8(' ' = 0, 'E' = 1, 'I' = 2)) AS pickup_cdeligibil, toFixedString(ifNull(pickup_ntacode, '0000'), 4) AS pickup_ntacode, CAST(assumeNotNull(pickup_ntaname) AS Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195)) AS pickup_ntaname, toUInt16(ifNull(pickup_puma, '0')) AS pickup_puma, assumeNotNull(dropoff_nyct2010_gid) AS dropoff_nyct2010_gid, toFloat32(ifNull(dropoff_ctlabel, '0')) AS dropoff_ctlabel, assumeNotNull(dropoff_borocode) AS dropoff_borocode, CAST(assumeNotNull(dropoff_boroname) AS Enum8('Manhattan' = 1, 'Queens' = 4, 'Brooklyn' = 3, '' = 0, 'Bronx' = 2, 'Staten Island' = 5)) AS dropoff_boroname, toFixedString(ifNull(dropoff_ct2010, '000000'), 6) AS dropoff_ct2010, toFixedString(ifNull(dropoff_boroct2010, '0000000'), 7) AS dropoff_boroct2010, CAST(assumeNotNull(ifNull(dropoff_cdeligibil, ' ')) AS Enum8(' ' = 0, 'E' = 1, 'I' = 2)) AS dropoff_cdeligibil, toFixedString(ifNull(dropoff_ntacode, '0000'), 4) AS dropoff_ntacode, CAST(assumeNotNull(dropoff_ntaname) AS Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195)) AS dropoff_ntaname, toUInt16(ifNull(dropoff_puma, '0')) AS dropoff_puma FROM trips  This takes 3030 seconds at a speed of about 428,000 rows per second. To load it faster, you can create the table with the Log engine instead of MergeTree. In this case, the download works faster than 200 seconds. The table uses 126 GB of disk space. SELECT formatReadableSize(sum(bytes)) FROM system.parts WHERE table = 'trips_mergetree' AND active  ┌─formatReadableSize(sum(bytes))─┐ │ 126.18 GiB │ └────────────────────────────────┘  Among other things, you can run the OPTIMIZE query on MergeTree. But it’s not required since everything will be fine without it. "},{"title":"Download of Prepared Partitions​","type":1,"pageTitle":"New York Taxi Data","url":"en/getting-started/example-datasets/nyc-taxi#download-of-prepared-partitions","content":"$ curl -O https://datasets.clickhouse.com/trips_mergetree/partitions/trips_mergetree.tar $ tar xvf trips_mergetree.tar -C /var/lib/clickhouse # path to ClickHouse data directory $ # check permissions of unpacked data, fix if required $ sudo service clickhouse-server restart $ clickhouse-client --query &quot;select count(*) from datasets.trips_mergetree&quot;  info If you will run the queries described below, you have to use the full table name, datasets.trips_mergetree. "},{"title":"Results on Single Server​","type":1,"pageTitle":"New York Taxi Data","url":"en/getting-started/example-datasets/nyc-taxi#results-on-single-server","content":"Q1: SELECT cab_type, count(*) FROM trips_mergetree GROUP BY cab_type  0.490 seconds. Q2: SELECT passenger_count, avg(total_amount) FROM trips_mergetree GROUP BY passenger_count  1.224 seconds. Q3: SELECT passenger_count, toYear(pickup_date) AS year, count(*) FROM trips_mergetree GROUP BY passenger_count, year  2.104 seconds. Q4: SELECT passenger_count, toYear(pickup_date) AS year, round(trip_distance) AS distance, count(*) FROM trips_mergetree GROUP BY passenger_count, year, distance ORDER BY year, count(*) DESC  3.593 seconds. The following server was used: Two Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz, 16 physical cores total, 128 GiB RAM, 8x6 TB HD on hardware RAID-5 Execution time is the best of three runs. But starting from the second run, queries read data from the file system cache. No further caching occurs: the data is read out and processed in each run. Creating a table on three servers: On each server: CREATE TABLE default.trips_mergetree_third ( trip_id UInt32, vendor_id Enum8('1' = 1, '2' = 2, 'CMT' = 3, 'VTS' = 4, 'DDS' = 5, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14), pickup_date Date, pickup_datetime DateTime, dropoff_date Date, dropoff_datetime DateTime, store_and_fwd_flag UInt8, rate_code_id UInt8, pickup_longitude Float64, pickup_latitude Float64, dropoff_longitude Float64, dropoff_latitude Float64, passenger_count UInt8, trip_distance Float64, fare_amount Float32, extra Float32, mta_tax Float32, tip_amount Float32, tolls_amount Float32, ehail_fee Float32, improvement_surcharge Float32, total_amount Float32, payment_type_ Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4), trip_type UInt8, pickup FixedString(25), dropoff FixedString(25), cab_type Enum8('yellow' = 1, 'green' = 2, 'uber' = 3), pickup_nyct2010_gid UInt8, pickup_ctlabel Float32, pickup_borocode UInt8, pickup_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5), pickup_ct2010 FixedString(6), pickup_boroct2010 FixedString(7), pickup_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2), pickup_ntacode FixedString(4), pickup_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195), pickup_puma UInt16, dropoff_nyct2010_gid UInt8, dropoff_ctlabel Float32, dropoff_borocode UInt8, dropoff_boroname Enum8('' = 0, 'Manhattan' = 1, 'Bronx' = 2, 'Brooklyn' = 3, 'Queens' = 4, 'Staten Island' = 5), dropoff_ct2010 FixedString(6), dropoff_boroct2010 FixedString(7), dropoff_cdeligibil Enum8(' ' = 0, 'E' = 1, 'I' = 2), dropoff_ntacode FixedString(4), dropoff_ntaname Enum16('' = 0, 'Airport' = 1, 'Allerton-Pelham Gardens' = 2, 'Annadale-Huguenot-Prince\\'s Bay-Eltingville' = 3, 'Arden Heights' = 4, 'Astoria' = 5, 'Auburndale' = 6, 'Baisley Park' = 7, 'Bath Beach' = 8, 'Battery Park City-Lower Manhattan' = 9, 'Bay Ridge' = 10, 'Bayside-Bayside Hills' = 11, 'Bedford' = 12, 'Bedford Park-Fordham North' = 13, 'Bellerose' = 14, 'Belmont' = 15, 'Bensonhurst East' = 16, 'Bensonhurst West' = 17, 'Borough Park' = 18, 'Breezy Point-Belle Harbor-Rockaway Park-Broad Channel' = 19, 'Briarwood-Jamaica Hills' = 20, 'Brighton Beach' = 21, 'Bronxdale' = 22, 'Brooklyn Heights-Cobble Hill' = 23, 'Brownsville' = 24, 'Bushwick North' = 25, 'Bushwick South' = 26, 'Cambria Heights' = 27, 'Canarsie' = 28, 'Carroll Gardens-Columbia Street-Red Hook' = 29, 'Central Harlem North-Polo Grounds' = 30, 'Central Harlem South' = 31, 'Charleston-Richmond Valley-Tottenville' = 32, 'Chinatown' = 33, 'Claremont-Bathgate' = 34, 'Clinton' = 35, 'Clinton Hill' = 36, 'Co-op City' = 37, 'College Point' = 38, 'Corona' = 39, 'Crotona Park East' = 40, 'Crown Heights North' = 41, 'Crown Heights South' = 42, 'Cypress Hills-City Line' = 43, 'DUMBO-Vinegar Hill-Downtown Brooklyn-Boerum Hill' = 44, 'Douglas Manor-Douglaston-Little Neck' = 45, 'Dyker Heights' = 46, 'East Concourse-Concourse Village' = 47, 'East Elmhurst' = 48, 'East Flatbush-Farragut' = 49, 'East Flushing' = 50, 'East Harlem North' = 51, 'East Harlem South' = 52, 'East New York' = 53, 'East New York (Pennsylvania Ave)' = 54, 'East Tremont' = 55, 'East Village' = 56, 'East Williamsburg' = 57, 'Eastchester-Edenwald-Baychester' = 58, 'Elmhurst' = 59, 'Elmhurst-Maspeth' = 60, 'Erasmus' = 61, 'Far Rockaway-Bayswater' = 62, 'Flatbush' = 63, 'Flatlands' = 64, 'Flushing' = 65, 'Fordham South' = 66, 'Forest Hills' = 67, 'Fort Greene' = 68, 'Fresh Meadows-Utopia' = 69, 'Ft. Totten-Bay Terrace-Clearview' = 70, 'Georgetown-Marine Park-Bergen Beach-Mill Basin' = 71, 'Glen Oaks-Floral Park-New Hyde Park' = 72, 'Glendale' = 73, 'Gramercy' = 74, 'Grasmere-Arrochar-Ft. Wadsworth' = 75, 'Gravesend' = 76, 'Great Kills' = 77, 'Greenpoint' = 78, 'Grymes Hill-Clifton-Fox Hills' = 79, 'Hamilton Heights' = 80, 'Hammels-Arverne-Edgemere' = 81, 'Highbridge' = 82, 'Hollis' = 83, 'Homecrest' = 84, 'Hudson Yards-Chelsea-Flatiron-Union Square' = 85, 'Hunters Point-Sunnyside-West Maspeth' = 86, 'Hunts Point' = 87, 'Jackson Heights' = 88, 'Jamaica' = 89, 'Jamaica Estates-Holliswood' = 90, 'Kensington-Ocean Parkway' = 91, 'Kew Gardens' = 92, 'Kew Gardens Hills' = 93, 'Kingsbridge Heights' = 94, 'Laurelton' = 95, 'Lenox Hill-Roosevelt Island' = 96, 'Lincoln Square' = 97, 'Lindenwood-Howard Beach' = 98, 'Longwood' = 99, 'Lower East Side' = 100, 'Madison' = 101, 'Manhattanville' = 102, 'Marble Hill-Inwood' = 103, 'Mariner\\'s Harbor-Arlington-Port Ivory-Graniteville' = 104, 'Maspeth' = 105, 'Melrose South-Mott Haven North' = 106, 'Middle Village' = 107, 'Midtown-Midtown South' = 108, 'Midwood' = 109, 'Morningside Heights' = 110, 'Morrisania-Melrose' = 111, 'Mott Haven-Port Morris' = 112, 'Mount Hope' = 113, 'Murray Hill' = 114, 'Murray Hill-Kips Bay' = 115, 'New Brighton-Silver Lake' = 116, 'New Dorp-Midland Beach' = 117, 'New Springville-Bloomfield-Travis' = 118, 'North Corona' = 119, 'North Riverdale-Fieldston-Riverdale' = 120, 'North Side-South Side' = 121, 'Norwood' = 122, 'Oakland Gardens' = 123, 'Oakwood-Oakwood Beach' = 124, 'Ocean Hill' = 125, 'Ocean Parkway South' = 126, 'Old Astoria' = 127, 'Old Town-Dongan Hills-South Beach' = 128, 'Ozone Park' = 129, 'Park Slope-Gowanus' = 130, 'Parkchester' = 131, 'Pelham Bay-Country Club-City Island' = 132, 'Pelham Parkway' = 133, 'Pomonok-Flushing Heights-Hillcrest' = 134, 'Port Richmond' = 135, 'Prospect Heights' = 136, 'Prospect Lefferts Gardens-Wingate' = 137, 'Queens Village' = 138, 'Queensboro Hill' = 139, 'Queensbridge-Ravenswood-Long Island City' = 140, 'Rego Park' = 141, 'Richmond Hill' = 142, 'Ridgewood' = 143, 'Rikers Island' = 144, 'Rosedale' = 145, 'Rossville-Woodrow' = 146, 'Rugby-Remsen Village' = 147, 'Schuylerville-Throgs Neck-Edgewater Park' = 148, 'Seagate-Coney Island' = 149, 'Sheepshead Bay-Gerritsen Beach-Manhattan Beach' = 150, 'SoHo-TriBeCa-Civic Center-Little Italy' = 151, 'Soundview-Bruckner' = 152, 'Soundview-Castle Hill-Clason Point-Harding Park' = 153, 'South Jamaica' = 154, 'South Ozone Park' = 155, 'Springfield Gardens North' = 156, 'Springfield Gardens South-Brookville' = 157, 'Spuyten Duyvil-Kingsbridge' = 158, 'St. Albans' = 159, 'Stapleton-Rosebank' = 160, 'Starrett City' = 161, 'Steinway' = 162, 'Stuyvesant Heights' = 163, 'Stuyvesant Town-Cooper Village' = 164, 'Sunset Park East' = 165, 'Sunset Park West' = 166, 'Todt Hill-Emerson Hill-Heartland Village-Lighthouse Hill' = 167, 'Turtle Bay-East Midtown' = 168, 'University Heights-Morris Heights' = 169, 'Upper East Side-Carnegie Hill' = 170, 'Upper West Side' = 171, 'Van Cortlandt Village' = 172, 'Van Nest-Morris Park-Westchester Square' = 173, 'Washington Heights North' = 174, 'Washington Heights South' = 175, 'West Brighton' = 176, 'West Concourse' = 177, 'West Farms-Bronx River' = 178, 'West New Brighton-New Brighton-St. George' = 179, 'West Village' = 180, 'Westchester-Unionport' = 181, 'Westerleigh' = 182, 'Whitestone' = 183, 'Williamsbridge-Olinville' = 184, 'Williamsburg' = 185, 'Windsor Terrace' = 186, 'Woodhaven' = 187, 'Woodlawn-Wakefield' = 188, 'Woodside' = 189, 'Yorkville' = 190, 'park-cemetery-etc-Bronx' = 191, 'park-cemetery-etc-Brooklyn' = 192, 'park-cemetery-etc-Manhattan' = 193, 'park-cemetery-etc-Queens' = 194, 'park-cemetery-etc-Staten Island' = 195), dropoff_puma UInt16) ENGINE = MergeTree(pickup_date, pickup_datetime, 8192)  On the source server: CREATE TABLE trips_mergetree_x3 AS trips_mergetree_third ENGINE = Distributed(perftest, default, trips_mergetree_third, rand())  The following query redistributes data: INSERT INTO trips_mergetree_x3 SELECT * FROM trips_mergetree  This takes 2454 seconds. On three servers: Q1: 0.212 seconds. Q2: 0.438 seconds. Q3: 0.733 seconds. Q4: 1.241 seconds. No surprises here, since the queries are scaled linearly. We also have the results from a cluster of 140 servers: Q1: 0.028 sec. Q2: 0.043 sec. Q3: 0.051 sec. Q4: 0.072 sec. In this case, the query processing time is determined above all by network latency. We ran queries using a client located in a different datacenter than where the cluster was located, which added about 20 ms of latency. "},{"title":"Summary​","type":1,"pageTitle":"New York Taxi Data","url":"en/getting-started/example-datasets/nyc-taxi#summary","content":"servers\tQ1\tQ2\tQ3\tQ41, E5-2650v2\t0.490\t1.224\t2.104\t3.593 3, E5-2650v2\t0.212\t0.438\t0.733\t1.241 1, AWS c5n.4xlarge\t0.249\t1.279\t1.738\t3.527 1, AWS c5n.9xlarge\t0.130\t0.584\t0.777\t1.811 3, AWS c5n.9xlarge\t0.057\t0.231\t0.285\t0.641 140, E5-2650v2\t0.028\t0.043\t0.051\t0.072 Original article "},{"title":"Using Seeds","type":0,"sectionRef":"#","url":"en/integrations/dbt/dbt-seeds","content":"Using Seeds dbt provides the ability to load data from CSV files. This capability is not suited to loading large exports of a database and is more designed for small files typically used for code tables and dictionaries, e.g. mapping country codes to country names. For a simple example, we generate and then upload a list of genre codes using the seed functionality. We generate a list of genre codes from our existing dataset. From the dbt directory, use the clickhouse-client to create a file seeds/genre_codes.csv: clickhouse-user@clickhouse:~/imdb$ clickhouse-client --password &lt;password&gt; --query &quot;SELECT genre, ucase(substring(genre, 1, 3)) as code FROM imdb.genres GROUP BY genre LIMIT 100 FORMAT CSVWithNames&quot; &gt; seeds/genre_codes.csv Execute the dbt seed command. This will create a new table genre_codes in our database imdb_dbt (as defined by our schema configuration) with the rows from our csv file. clickhouse-user@clickhouse:~/imdb$ dbt seed 17:03:23 Running with dbt=1.0.4 17:03:23 Found 1 model, 0 tests, 1 snapshot, 0 analyses, 181 macros, 0 operations, 1 seed file, 6 sources, 0 exposures, 0 metrics 17:03:23 17:03:24 Concurrency: 1 threads (target='dev') 17:03:24 17:03:24 1 of 1 START seed file imdb_dbt.genre_codes..................................... [RUN] 17:03:24 1 of 1 OK loaded seed file imdb_dbt.genre_codes................................. [INSERT 21 in 0.65s] 17:03:24 17:03:24 Finished running 1 seed in 1.62s. 17:03:24 17:03:24 Completed successfully 17:03:24 17:03:24 Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1 Confirm these have been loaded: SELECT * FROM imdb_dbt.genre_codes LIMIT 10; +-------+----+ |genre |code| +-------+----+ |Drama |DRA | |Romance|ROM | |Short |SHO | |Mystery|MYS | |Adult |ADU | |Family |FAM | |Action |ACT | |Sci-Fi |SCI | |Horror |HOR | |War |WAR | +-------+----+= ","keywords":""},{"title":"Installation","type":0,"sectionRef":"#","url":"en/integrations/dbt/dbt-setup","content":"","keywords":""},{"title":"dbt​","type":1,"pageTitle":"Installation","url":"en/integrations/dbt/dbt-setup#dbt","content":"We assume the use of the dbt CLI for the following examples. Users may also wish to consider dbt Cloud, which offers a web-based Integrated Development Environment (IDE) allowing users to edit and run projects. dbt offers a number of options for CLI installation. Follow the instructions described here. At this stage install dbt-core only. We recommend the use of pip. python install dbt-core  Important: The following is tested under python 3.9. "},{"title":"ClickHouse plugin​","type":1,"pageTitle":"Installation","url":"en/integrations/dbt/dbt-setup#clickhouse-plugin","content":"Install the dbt ClickHouse plugin: pip install dbt-clickhouse  Prepare ClickHouse dbt excels when modeling highly relational data. For the purposes of example, we provide a small IMDB dataset with the following relational schema. This dataset originates from the relational dataset repository. This is trivial relative to common schemas used with dbt but represents a manageable sample:  We use a subset of these tables as shown. Create the following tables: CREATE DATABASE imdb ENGINE=Atomic; CREATE TABLE imdb.actors ( id UInt32, first_name String, last_name String, gender FixedString(1) ) ENGINE = MergeTree ORDER BY (id, first_name, last_name, gender); CREATE TABLE imdb.directors ( id UInt32, first_name String, last_name String ) ENGINE = MergeTree ORDER BY (id, first_name, last_name); CREATE TABLE imdb.genres ( movie_id UInt32, genre String ) ENGINE = MergeTree ORDER BY (movie_id, genre); CREATE TABLE imdb.movie_directors ( director_id UInt32, movie_id UInt64 ) ENGINE = MergeTree ORDER BY (director_id, movie_id); CREATE TABLE imdb.movies ( id UInt32, name String, year UInt32, rank Float32 DEFAULT 0 ) ENGINE = MergeTree ORDER BY (id, name, year); CREATE TABLE imdb.roles ( created_at DateTime DEFAULT now(), actor_id UInt32, movie_id UInt32, role String ) ENGINE = MergeTree ORDER BY (actor_id, movie_id);  note The column created_at for the table roles, which defaults to a value of now(). We use this later to identify incremental updates to our models - see Incremental Models. We use the s3 function to read the source data from public endpoints to insert data. Run the following commands to populate the tables: INSERT INTO imdb.actors SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/imdb/imdb_ijs_actors.tsv.gz', 'TSVWithNames'); INSERT INTO imdb.directors SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/imdb/imdb_ijs_directors.tsv.gz', 'TSVWithNames'); INSERT INTO imdb.genres SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/imdb/imdb_ijs_movies_genres.tsv.gz', 'TSVWithNames'); INSERT INTO imdb.movie_directors SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/imdb/imdb_ijs_movies_directors.tsv.gz', 'TSVWithNames'); INSERT INTO imdb.movies SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/imdb/imdb_ijs_movies.tsv.gz', 'TSVWithNames'); INSERT INTO imdb.roles SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/imdb/imdb_ijs_roles.tsv.gz', 'TSVWithNames');  The execution of these may vary depending on your bandwidth, but each should only take a few seconds to complete. Execute the following query to compute a summary of each actor, ordered by the most movie appearances, and to confirm the data was loaded successfully: SELECT id, any(actor_name) as name, uniqExact(movie_id) as num_movies, avg(rank) as avg_rank, uniqExact(genre) as unique_genres, uniqExact(director_name) as uniq_directors, max(created_at) as updated_at FROM ( SELECT imdb.actors.id as id, concat(imdb.actors.first_name, ' ', imdb.actors.last_name) as actor_name, imdb.movies.id as movie_id, Imdb.movies.rank as rank, genre, concat(imdb.directors.first_name, ' ', imdb.directors.last_name) as director_name, created_at FROM imdb.actors JOIN imdb.roles ON imdb.roles.actor_id = imdb.actors.id LEFT OUTER JOIN imdb.movies ON imdb.movies.id = imdb.roles.movie_id LEFT OUTER JOIN imdb.genres ON imdb.genres.movie_id = imdb.movies.id LEFT OUTER JOIN imdb.movie_directors ON imdb.movie_directors.movie_id = imdb.movies.id LEFT OUTER JOIN imdb.directors ON imdb.directors.id = imdb.movie_directors.director_id ) GROUP BY id ORDER BY num_movies DESC LIMIT 5;  The response should look like: +------+------------+----------+------------------+-------------+--------------+-------------------+ |id |name |num_movies|avg_rank |unique_genres|uniq_directors|updated_at | +------+------------+----------+------------------+-------------+--------------+-------------------+ |45332 |Mel Blanc |832 |6.175853582979779 |18 |84 |2022-04-26 14:01:45| |621468|Bess Flowers|659 |5.57727638854796 |19 |293 |2022-04-26 14:01:46| |372839|Lee Phelps |527 |5.032976449684617 |18 |261 |2022-04-26 14:01:46| |283127|Tom London |525 |2.8721716524875673|17 |203 |2022-04-26 14:01:46| |356804|Bud Osborne |515 |2.0389507108727773|15 |149 |2022-04-26 14:01:46| +------+------------+----------+------------------+-------------+--------------+-------------------+  In the later guides, we will convert this query into a model - materializing it in ClickHouse as a dbt view and table. "},{"title":"dbt-table-model","type":0,"sectionRef":"#","url":"en/integrations/dbt/dbt-table-model","content":"","keywords":""},{"title":"Creating a Table Materialization​","type":1,"pageTitle":"dbt-table-model","url":"en/integrations/dbt/dbt-table-model#creating-a-table-materialization","content":"In the previous example, our model was materialized as a view. While this might offer sufficient performance for some queries, more complex SELECTs or frequently executed queries may be better materialized as a table. This materialization is useful for models that will be queried by BI tools to ensure users have a faster experience. This effectively causes the query results to be stored as a new table, with the associated storage overheads - effectively, an INSERT TO SELECT is executed. Note that this table will be reconstructed each time i.e., it is not incremental. Large result sets may therefore result in long execution times - see dbt Limitations. Modify the file actors_summary.sql such that the materialized parameter is set to table. Notice how ORDER BY is defined, and notice we use the MergeTree table engine: {{ config(order_by='(updated_at, id, name)', engine='MergeTree()', materialized='table') }} From the imdb directory execute the command dbt run. This execution may take a little longer to execute - around 10s on most machines. clickhouse-user@clickhouse:~/imdb$ dbt run 15:13:27 Running with dbt=1.0.4 15:13:27 Found 1 model, 0 tests, 1 snapshot, 0 analyses, 181 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics 15:13:27 15:13:28 Concurrency: 1 threads (target='dev') 15:13:28 15:13:28 1 of 1 START table model imdb_dbt.actor_summary................................. [RUN] 15:13:37 1 of 1 OK created table model imdb_dbt.actor_summary............................ [OK in 9.22s] 15:13:37 15:13:37 Finished running 1 table model in 10.20s. 15:13:37 15:13:37 Completed successfully 15:13:37 15:13:37 Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1 Confirm the creation of the table imdb_dbt.actor_summary: SHOW CREATE TABLE imdb_dbt.actor_summary; You should the table with the appropriate data types: +---------------------------------------- |statement +---------------------------------------- |CREATE TABLE imdb_dbt.actor_summary |( |`id` UInt32, |`first_name` String, |`last_name` String, |`num_movies` UInt64, |`updated_at` DateTime |) |ENGINE = MergeTree |ORDER BY (id, first_name, last_name) |SETTINGS index_granularity = 8192 +---------------------------------------- Confirm the results from this table are consistent with previous responses. Notice an appreciable improvement in the response time now that the model is a table: SELECT * FROM imdb_dbt.actor_summary ORDER BY num_movies DESC LIMIT 5; +------+------------+----------+------------------+------+---------+-------------------+ |id |name |num_movies|avg_rank |genres|directors|updated_at | +------+------------+----------+------------------+------+---------+-------------------+ |45332 |Mel Blanc |832 |6.175853582979779 |18 |84 |2022-04-26 15:26:55| |621468|Bess Flowers|659 |5.57727638854796 |19 |293 |2022-04-26 15:26:57| |372839|Lee Phelps |527 |5.032976449684617 |18 |261 |2022-04-26 15:26:56| |283127|Tom London |525 |2.8721716524875673|17 |203 |2022-04-26 15:26:56| |356804|Bud Osborne |515 |2.0389507108727773|15 |149 |2022-04-26 15:26:56| +------+------------+----------+------------------+------+---------+-------------------+ Feel free to issue other queries against this model. For example, which actors have the highest ranking movies with more than 5 appearances? SELECT * FROM imdb_dbt.actor_summary WHERE num_movies &gt; 5 ORDER BY avg_rank DESC LIMIT 10;  "},{"title":"Creating a Simple View Materialization","type":0,"sectionRef":"#","url":"en/integrations/dbt/dbt-view-model","content":"Creating a Simple View Materialization When using the view materialization, a model is rebuilt as a view on each run, via a CREATE VIEW AS statement in ClickHouse. This doesn't require any additional storage of data but will be slower to query than table materializations. From the imdb folder, delete the directory models/example: clickhouse-user@clickhouse:~/imdb$ rm -rf models/example Create a new file in the actors within the models folder. Here we create files that each represent an actor model: clickhouse-user@clickhouse:~/imdb$ mkdir models/actors Create the files schema.yml and actor_summary.sql in the models/actors folder. Add the following contents: clickhouse-user@clickhouse:~/imdb$ touch models/actors/actor_summary.sql clickhouse-user@clickhouse:~/imdb$ touch models/actors/schema.yml The schema.yml defines our tables. These will subsequently be available for use in macros. version: 2 sources: - name: imdb tables: - name: directors - name: actors - name: roles - name: movies - name: genres - name: movie_directors The actors_summary.sql defines our actual model. Note in the config function we also request the model be materialized as a view in ClickHouse. Our tables are referenced from the schema.yml file via the function source e.g. source('imdb', 'movies') refers to the movies table in the imdb database. {{ config(materialized='view') }} with actor_summary as ( SELECT id, any(actor_name) as name, uniqExact(movie_id) as num_movies, avg(rank) as avg_rank, uniqExact(genre) as genres, uniqExact(director_name) as directors, max(created_at) as updated_at FROM ( SELECT {{ source('imdb', 'actors') }}.id as id, concat({{ source('imdb', 'actors') }}.first_name, ' ', {{ source('imdb', 'actors') }}.last_name) as actor_name, {{ source('imdb', 'movies') }}.id as movie_id, {{ source('imdb', 'movies') }}.rank as rank, genre, concat({{ source('imdb', 'directors') }}.first_name, ' ', {{ source('imdb', 'directors') }}.last_name) as director_name, created_at FROM {{ source('imdb', 'actors') }} JOIN {{ source('imdb', 'roles') }} ON {{ source('imdb', 'roles') }}.actor_id = {{ source('imdb', 'actors') }}.id LEFT OUTER JOIN {{ source('imdb', 'movies') }} ON {{ source('imdb', 'movies') }}.id = {{ source('imdb', 'roles') }}.movie_id LEFT OUTER JOIN {{ source('imdb', 'genres') }} ON {{ source('imdb', 'genres') }}.movie_id = {{ source('imdb', 'movies') }}.id LEFT OUTER JOIN {{ source('imdb', 'movie_directors') }} ON {{ source('imdb', 'movie_directors') }}.movie_id = {{ source('imdb', 'movies') }}.id LEFT OUTER JOIN {{ source('imdb', 'directors') }} ON {{ source('imdb', 'directors') }}.id = {{ source('imdb', 'movie_directors') }}.director_id ) GROUP BY id ) select * from actor_summary Note how we include the column updated_at in our final actor_summary. We use this later for incremental materializations. From the imdb directory execute the command dbt run. clickhouse-user@clickhouse:~/imdb$ dbt run 15:05:35 Running with dbt=1.0.4 15:05:35 Found 1 model, 0 tests, 1 snapshot, 0 analyses, 181 macros, 0 operations, 0 seed files, 6 sources, 0 exposures, 0 metrics 15:05:35 15:05:36 Concurrency: 1 threads (target='dev') 15:05:36 15:05:36 1 of 1 START view model imdb_dbt.actor_summary.................................. [RUN] 15:05:37 1 of 1 OK created view model imdb_dbt.actor_summary............................. [OK in 1.00s] 15:05:37 15:05:37 Finished running 1 view model in 1.97s. 15:05:37 15:05:37 Completed successfully 15:05:37 15:05:37 Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1 dbt will represent the model as a view in ClickHouse as requested. We can now query this view directly. This view will have been created in the imdb_dbt database - this is determined by the schema parameter in the file ~/.dbt/profiles.yml under the clickhouse_imdb profile. SHOW DATABASES; +------------------+ |name | +------------------+ |INFORMATION_SCHEMA| |default | |imdb | |imdb_dbt | &lt;---created by dbt! |information_schema| |system | +------------------+ Querying this view, we can replicate the results of our earlier query with a simpler syntax: SELECT * FROM imdb_dbt.actor_summary ORDER BY num_movies DESC LIMIT 5; +------+------------+----------+------------------+------+---------+-------------------+ |id |name |num_movies|avg_rank |genres|directors|updated_at | +------+------------+----------+------------------+------+---------+-------------------+ |45332 |Mel Blanc |832 |6.175853582979779 |18 |84 |2022-04-26 15:26:55| |621468|Bess Flowers|659 |5.57727638854796 |19 |293 |2022-04-26 15:26:57| |372839|Lee Phelps |527 |5.032976449684617 |18 |261 |2022-04-26 15:26:56| |283127|Tom London |525 |2.8721716524875673|17 |203 |2022-04-26 15:26:56| |356804|Bud Osborne |515 |2.0389507108727773|15 |149 |2022-04-26 15:26:56| +------+------------+----------+------------------+------+---------+-------------------+ ","keywords":""},{"title":"Connecting ClickHouse to external data sources with JDBC","type":0,"sectionRef":"#","url":"en/integrations/jdbc/jdbc-with-clickhouse","content":"","keywords":"clickhouse jdbc connect integrate"},{"title":"Install the ClickHouse JDBC Bridge locally​","type":1,"pageTitle":"Connecting ClickHouse to external data sources with JDBC","url":"en/integrations/jdbc/jdbc-with-clickhouse#install-the-clickhouse-jdbc-bridge-locally","content":"The easiest way to use the ClickHouse JDBC Bridge is to install and run it on the same host where also ClickHouse is running: Let's start by connecting to the Unix shell on the machine where ClickHouse is running and create a local folder where we will later install the ClickHouse JDBC Bridge into (feel free to name the folder anything you like and put it anywhere you like): mkdir ~/clickhouse-jdbc-bridge  Now we download the current version of the ClickHouse JDBC Bridge into that folder: cd ~/clickhouse-jdbc-bridge wget https://github.com/ClickHouse/clickhouse-jdbc-bridge/releases/download/v2.0.7/clickhouse-jdbc-bridge-2.0.7-shaded.jar  In order to be able to connect to MySQL we are creating a named data source: cd ~/clickhouse-jdbc-bridge mkdir -p config/datasources touch config/datasources/mysql8.json  You can now copy and paste the following configuration into the file ~/clickhouse-jdbc-bridge/config/datasources/mysql8.json: { &quot;mysql8&quot;: { &quot;driverUrls&quot;: [ &quot;https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.28/mysql-connector-java-8.0.28.jar&quot; ], &quot;jdbcUrl&quot;: &quot;jdbc:mysql://&lt;host&gt;:&lt;port&gt;&quot;, &quot;username&quot;: &quot;&lt;username&gt;&quot;, &quot;password&quot;: &quot;&lt;password&gt;&quot; } }  note in the config file above you are free to use any name you like for the datasource, we used mysql8in the value for the jdbcUrl you need to replace &lt;host&gt;, and &lt;port&gt; with appropriate values according to your running MySQL instance, e.g. &quot;jdbc:mysql://localhost:3306&quot;you need to replace &lt;username&gt; and &lt;password&gt; with your MySQL credentials, if you don't use a password, you can delete the &quot;password&quot;: &quot;&lt;password&gt;&quot; line in the config file abovein the value for driverUrls we just specified a URL from which the current version of the MySQL JDBC driver can be downloaded. That's all we have to do, and the ClickHouse JDBC Bridge will automatically download that JDBC driver (into a OS specific directory).  Now we are ready to start the ClickHouse JDBC Bridge: cd ~/clickhouse-jdbc-bridge java -jar clickhouse-jdbc-bridge-2.0.7-shaded.jar  note We started the ClickHouse JDBC Bridge in foreground mode. In order to stop the Bridge you can bring the Unix shell window from above in foreground and press CTRL+C. "},{"title":"Use the JDBC connection from within ClickHouse​","type":1,"pageTitle":"Connecting ClickHouse to external data sources with JDBC","url":"en/integrations/jdbc/jdbc-with-clickhouse#use-the-jdbc-connection-from-within-clickhouse","content":"ClickHouse can now access MySQL data by either using the jdbc Table Function or the JDBC Table Engine. The easiest way to execute the following examples is to copy and paste them into the native ClickHouse command-line client or into the ClickHouse play HTTP Interface. jdbc Table Function: SELECT * FROM jdbc('mysql8', 'mydatabase', 'mytable'); note As the first paramter for the jdbc table funtion we are using the name of the named data source that we configured above. JDBC Table Engine: CREATE TABLE mytable ( &lt;column&gt; &lt;column_type&gt;, ... ) ENGINE = JDBC('mysql8', 'mydatabase', 'mytable'); SELECT * FROM mytable; note As the first paramter for the jdbc engine clause we are using the name of the named data source that we configured above The schema of the ClickHouse JDBC engine table and schema of the connected MySQL table must be aligned, e.g. the column names and order must be the same, and the column data types must be compatible "},{"title":"Install the ClickHouse JDBC Bridge externally​","type":1,"pageTitle":"Connecting ClickHouse to external data sources with JDBC","url":"en/integrations/jdbc/jdbc-with-clickhouse#install-the-clickhouse-jdbc-bridge-externally","content":"For a distributed ClickHouse cluster (a cluster with more than one ClickHouse host) it makes sense to install and run the ClickHouse JDBC Bridge externally on its own host:  This has the advantage that each ClickHouse host can access the JDBC Bridge. Otherwise the JDBC Bridge would need to be installed locally for each ClickHouse instance that is supposed to access external data sources via the Bridge. In order to install the ClickHouse JDBC Bridge externally, we do the following steps: We install, configure and run the ClickHouse JDBC Bridge on a dedicated host by following the steps described in section 1 of this guide. On each ClickHouse Host we add the following configuration block to the ClickHouse server configuration (depending on your chosen configuration format, use either the XML or YAML version): XMLYAML &lt;jdbc_bridge&gt; &lt;host&gt;JDBC-Bridge-Host&lt;/host&gt; &lt;port&gt;9019&lt;/port&gt; &lt;/jdbc_bridge&gt;  note you need to replace JDBC-Bridge-Host with the hostname or ip address of the dedicated ClickHouse JDBC Bridge hostwe specified the default ClickHouse JDBC Bridge port 9019, if you are using a different port for the JDBC Bridge then you must adapt the configuration above accordingly  "},{"title":"Connecting Applications to ClickHouse with JDBC","type":0,"sectionRef":"#","url":"en/integrations/jdbc/jdbc-with-clickhouse-2","content":"Connecting Applications to ClickHouse with JDBC Overview: The ClickHouse JDBC driver enables a Java application to interact with ClickHouse: In this lesson we will create a minimal Java application that uses the ClickHouse JDBC driver for querying a ClickHouse database. Let's get started! Prerequisites You have access to a machine that has: a Unix shell and internet access wget installeda current version of Java (e.g. OpenJDK Version &gt;= 17) installeda current version of ClickHouse installed and running Let's start by connecting to a Unix shell on your machine where Java is installed and create a project directory for our minimal Java application (feel free to name the folder anything you like and put it anywhere you like): mkdir ~/hello-clickhouse-java-app Now we download the current version of the ClickHouse JDBC driver into a subfolder of the project directory: cd ~/hello-clickhouse-java-app mkdir lib wget -P lib https://repo1.maven.org/maven2/com/clickhouse/clickhouse-jdbc/0.3.2-patch7/clickhouse-jdbc-0.3.2-patch7-shaded.jar Next we create a file for the Java main class of our minimal Java application in a subdirectory structure: cd ~/hello-clickhouse-java-app mkdir -p src/main/java/helloclickhouse touch src/main/java/helloclickhouse/HelloClickHouse.java You can now copy and paste the following Java code into the file ~/hello-clickhouse-java-app/src/main/java/helloclickhouse/HelloClickHouse.java: import com.clickhouse.jdbc.*; import java.sql.*; import java.util.*; public class HelloClickHouse { public static void main(String[] args) throws Exception { String url = &quot;jdbc:ch://&lt;host&gt;:&lt;port&gt;&quot;; Properties properties = new Properties(); // properties.setProperty(&quot;ssl&quot;, &quot;true&quot;); // properties.setProperty(&quot;sslmode&quot;, &quot;NONE&quot;); // NONE to trust all servers; STRICT for trusted only ClickHouseDataSource dataSource = new ClickHouseDataSource(url, properties); try (Connection connection = dataSource.getConnection(&lt;username&gt;, &lt;password&gt;); Statement statement = connection.createStatement(); ResultSet resultSet = statement.executeQuery(&quot;select * from system.tables limit 10&quot;)) { ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); int columns = resultSetMetaData.getColumnCount(); while (resultSet.next()) { for (int c = 1; c &lt;= columns; c++) { System.out.print(resultSetMetaData.getColumnName(c) + &quot;:&quot; + resultSet.getString(c) + (c &lt; columns ? &quot;, &quot; : &quot;\\n&quot;)); } } } } } note in the Java class file above in the first code line inside the main method you need to replace &lt;host&gt;, and &lt;port&gt; with values matching your running ClickHouse instance, e.g. &quot;jdbc:ch://localhost:8123&quot;you also need to replace &lt;username&gt; and &lt;password&gt; with your ClickHouse instance credentials, if you don't use a password, you can replace &lt;password&gt; with null That was all! Now we are ready to start our minimal Java application from the Unix shell: cd ~/hello-clickhouse-java-app java -classpath lib/clickhouse-jdbc-0.3.2-patch7-shaded.jar src/main/java/helloclickhouse/HelloClickHouse.java ","keywords":"clickhouse jdbc connect integrate"},{"title":"Kafka Samples","type":0,"sectionRef":"#","url":"en/integrations/kafka/code/","content":"Kafka Samples Supporting configurations and scripts for the ClickHouse-Kafka documentation. Producer scriptSample Kafka Connect configurations","keywords":""},{"title":"Kafka Connect Configurations","type":0,"sectionRef":"#","url":"en/integrations/kafka/code/connectors/","content":"Kafka Connect Configurations Kafka Connect configurations supporting ClickHouse documentation on Kafka. Configuration files support the Github dataset. These assume Kafka Connect is run in standalone mode and the use of Confluent Cloud. JDBC Sink ConfigurationHTTP Sink Configuration Configurations include comments regards settings which require environment specific modification.","keywords":""},{"title":"Kafka Producer","type":0,"sectionRef":"#","url":"en/integrations/kafka/code/producer/","content":"","keywords":""},{"title":"Requirements​","type":1,"pageTitle":"Kafka Producer","url":"en/integrations/kafka/code/producer/#requirements","content":"Python 3.8.10+Kafka instance v7+. Easiest solution is to create a Kafka cluster in Confluent Cloud - which offers an adequate free tier.Ndjson file. A sample github ndjson file can be found here with accompanying config for the script here. See Larger Datasets if a larger test file is required. "},{"title":"Setup​","type":1,"pageTitle":"Kafka Producer","url":"en/integrations/kafka/code/producer/#setup","content":"pip install -r requirements.txt "},{"title":"Usage​","type":1,"pageTitle":"Kafka Producer","url":"en/integrations/kafka/code/producer/#usage","content":"Prepare a configuration. See github.config for examples. Any target topic will be automatically created if it doesn't exist.(Optional) Prepare a JSON schema file for your ndjson and specify this in the config from (1) via input.schema. To infer a schema automatically do not set this parameter. This will cause the schema to be inferred from the first 100 lines. This is best effort only (but works for the gitub dataset)!Run it! python producer.py -c &lt;config_file&gt; "},{"title":"Not in scope​","type":1,"pageTitle":"Kafka Producer","url":"en/integrations/kafka/code/producer/#not-in-scope","content":"Whilst all producer configuration parameters supported by the Kafka python client can be used - replace _ with . in the configuration, no work has been done regards testing these settings for optimal performance. "},{"title":"Large Datasets​","type":1,"pageTitle":"Kafka Producer","url":"en/integrations/kafka/code/producer/#large-datasets","content":"The sample Github dataset consists of events on the ClickHouse Github repository. This static files covers the period 2019-09-23 to 2022-01-05. Specifically, this file was generated from the following command executed against the ClickHouse play site: clickhouse-client --secure --host play.clickhouse.com --port 9440 --user explorer --query &quot;SELECT file_time, event_type, actor_login, repo_name, created_at, updated_at, action, comment_id, path, ref, ref_type, creator_user_login, number, title, labels, state, assignee, assignees, closed_at, merged_at, merge_commit_sha, requested_reviewers, merged_by, review_comments, member_login FROM github_events WHERE repo_name = 'ClickHouse/ClickHouse' ORDER BY created_at ASC LIMIT 200000 FORMAT JSONEachRow&quot; &gt; github_all_columns.ndjson  Note the upper limit 200k rows and restriction to the ClickHouse/ClickHouse repository. Feel free to use this command to generate larger datasets for testing, potentially exploring other repositories. If you experience quota limits, instructions for downloading and transforming the data can be found here. "},{"title":"Choosing an option","type":0,"sectionRef":"#","url":"en/integrations/kafka/kafka-choosing-an-approach","content":"Choosing an option When integrating Kafka with ClickHouse, you will need to make early architectural decisions about the high-level approach used. We outline the most common approaches below: Kafka table engine - The Kafka table engine provides a Native ClickHouse integration. This table engine pulls data from the source system. This requires ClickHouse to have direct access to Kafka. (This approach is not currently supported in ClickHouse Cloud.)Kafka Connect - Kafka Connect is a free, open-source component of Apache Kafka® that works as a centralized data hub for simple data integration between Kafka and other data systems. Connectors provide a simple means of scalably and reliably streaming data to and from Kafka. Source Connectors inserts data to Kafka topics from other systems, whilst Sink Connectors delivers data from Kafka topics into other data stores such as ClickHouse. Vector - Vector is a vendor agnostic data pipeline. With the ability to read from Kafka, and send events to ClickHouse, this represents a robust integration option.Custom code - Custom code using respective client libraries for Kafka and ClickHouse may be appropriate cases where custom processing of events is required. This is beyond the scope of this documentation. Choosing an approach will come down to a few decision points: Connectivity - The Kafka table engine needs to be able to pull from Kafka if ClickHouse is the destination. This requires bi-directional connectivity. If there is a network separation, e.g. ClickHouse is in the Cloud and Kafka is self-managed, you may be hesitant to remove this for compliance and security reasons. (This approach is not currently supported in ClickHouse Cloud.) The Kafka table engine utilizes resources within ClickHouse itself, utilizing threads for the consumers. Placing this resource pressure on ClickHouse may not be possible due to resource constraints, or your architects may prefer a separation of concerns. In this case, tools such as Kafka Connect, which run as a separate process and can be deployed on different hardware may be preferable. This allows the process responsible for pulling Kafka data to be scaled independently of ClickHouse. External enrichment - Whilst messages can be manipulated before insertion into ClickHouse, through the use of functions in the select statement of the materialized view, users may prefer to move complex enrichment external to ClickHouse. Data flow direction - Vector only supports the transfer of data from Kafka to ClickHouse.","keywords":""},{"title":"HTTP Sink Connector","type":0,"sectionRef":"#","url":"en/integrations/kafka/kafka-connect-http","content":"","keywords":""},{"title":"Self-Managed​","type":1,"pageTitle":"HTTP Sink Connector","url":"en/integrations/kafka/kafka-connect-http#self-managed","content":""},{"title":"Steps​","type":1,"pageTitle":"HTTP Sink Connector","url":"en/integrations/kafka/kafka-connect-http#steps","content":"1. Install Kafka Connect and Connector​ Download the Confluent package and install it locally. Follow the installation instructions for installing the connector as documented here. If you use the confluent-hub installation method, your local configuration files will be updated. 2. Prepare Configuration​ Follow these instructions for setting up Connect relevant to your installation type, noting the differences between a standalone and distributed cluster. If using Confluent Cloud, the distributed setup is relevant. The most important parameter is the http.api.url. The HTTP interface for ClickHouse requires you to encode the INSERT statement as a parameter in the URL. This must include the format (JSONEachRow in this case) and target database. The format must be consistent with the Kafka data, which will be converted to a string in the HTTP payload. These parameters must be URL escaped. An example of this format for the Github dataset (assuming you are running ClickHouse locally) is shown below: &lt;protocol&gt;://&lt;clickhouse_host&gt;:&lt;clickhouse_port&gt;?query=INSERT%20INTO%20&lt;database&gt;.&lt;table&gt;%20FORMAT%20JSONEachRow http://localhost:8123?query=INSERT%20INTO%20default.github%20FORMAT%20JSONEachRow  This URL is error-prone. Ensure escaping is precise to avoid issues. The following additional parameters are relevant to using the HTTP Sink with ClickHouse. A complete parameter list can be found here: request.method - Set to POST**retry.on.status.codes - Set to 400-500 to retry on any error codes. Refine based expected errors in data.request.body.format - In most cases this will be JSON.auth.type - Set to BASIC if you security with ClickHouse. Other ClickHouse compatible authentication mechanisms are not currently supported.ssl.enabled - set to true if using SSL.headers - set to &quot;Content-Type: application/json&quot;connection.user - username for ClickHouse.connection.password - password for ClickHouse.batch.max.size - The number of rows to send in a single batch. Ensure this set is to an appropriately large number. Per ClickHouse recommendations a value of 1000 is should be considered a minimum.tasks.max - The HTTP Sink connector supports running one or more tasks. This can be used to increase performance. Along with batch size this represents your primary means of improving performance.key.converter - set according to the types of your keys.value.converter - set based on the type of data on your topic. This data does not need a schema. The format here must be consistent with the FORMAT specified in the parameter http.api.url. The simplest here is to use JSON and the org.apache.kafka.connect.json.JsonConverter converter. Treating the value as a string, via the converter org.apache.kafka.connect.storage.StringConverter, is also possible - although this will require the user to extract a value in the insert statement using functions. Avro format is also supported in ClickHouse if using the io.confluent.connect.avro.AvroConverter converter.  A full list of settings, including how to configure a proxy, retries, and advanced SSL, can be found here. Example configuration files for the Github sample data can be found here, assuming Connect is run in standalone mode and Kafka is hosted in Confluent Cloud. 3. Create the ClickHouse table​ Ensure the table has been created. An example for a minimal github dataset using a standard MergeTree is shown below. CREATE TABLE github ( file_time DateTime, event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4,'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), actor_login LowCardinality(String), repo_name LowCardinality(String), created_at DateTime, updated_at DateTime, action Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20), comment_id UInt64, path String, ref LowCardinality(String), ref_type Enum('none' = 0, 'branch' = 1, 'tag' = 2, 'repository' = 3, 'unknown' = 4), creator_user_login LowCardinality(String), number UInt32, title String, labels Array(LowCardinality(String)), state Enum('none' = 0, 'open' = 1, 'closed' = 2), assignee LowCardinality(String), assignees Array(LowCardinality(String)), closed_at DateTime, merged_at DateTime, merge_commit_sha String, requested_reviewers Array(LowCardinality(String)), merged_by LowCardinality(String), review_comments UInt32, member_login LowCardinality(String) ) ENGINE = MergeTree ORDER BY (event_type, repo_name, created_at)  4. Add data to Kafka​ Insert messages to Kafka. Below we use kcat to insert 10k messages. head -n 10000 github_all_columns.ndjson | kafkacat -b &lt;host&gt;:&lt;port&gt; -X security.protocol=sasl_ssl -X sasl.mechanisms=PLAIN -X sasl.username=&lt;username&gt; -X sasl.password=&lt;password&gt; -t github  A simple read on the target table “Github” should confirm the insertion of data. SELECT count() FROM default.github; | count\\(\\) | | :--- | | 10000 |  "},{"title":"Confluent Cloud​","type":1,"pageTitle":"HTTP Sink Connector","url":"en/integrations/kafka/kafka-connect-http#confluent-cloud","content":"A fully managed version of HTTP Sink is available for those using Confluent Cloud for their Kafka hosting. This requires your ClickHouse environment to be accessible from Confluent Cloud. We assume you have taken the appropriate measures to secure this. The instructions for creating an HTTP Sink in Confluent Cloud can be found here. The following settings are relevant if connecting to ClickHouse. If not specified, form defaults are applicable: Input messages - Depends on your source data but in most cases JSON or Avro. We assume JSON in the following settings.Kafka Cluster credentials - Confluent cloud allows you to generate these for the appropriate topic from which you wish to pull messages.HTTP server details - The connection details for ClickHouse. Specifically: HTTP Url - This should be of the same format as the self-managed configuration parameter http.api.url i.e. &amp;lt;protocol&gt;://&amp;lt;clickhouse_host&gt;:&amp;lt;clickhouse_port&gt;?query=INSERT%20INTO%20&amp;lt;database&gt;.&amp;lt;table&gt;%20FORMAT%20JSONEachRowHTTP Request Method - Set to POSTHTTP Headers - “Content Type: application/json” HTTP server batches Request Body Format - jsonBatch batch size - Per ClickHouse recommendations, set this to at least 1000. HTTP server authentication Endpoint Authentication type - BASICAuth username - ClickHouse usernameAuth password - ClickHouse password HTTP server retries - Settings here can be adjusted according to requirements. Timeouts specifically may need adjusting depending on latency. Retry on HTTP codes - 400-500 but adapt as required e.g. this may change if you have an HTTP proxy in front of ClickHouse.Maximum Reties - the default (10) is appropriate but feel to adjust for more robust retries.  "},{"title":"Using Kafka Connect","type":0,"sectionRef":"#","url":"en/integrations/kafka/kafka-connect-intro","content":"","keywords":""},{"title":"Pre-requisites​","type":1,"pageTitle":"Using Kafka Connect","url":"en/integrations/kafka/kafka-connect-intro#pre-requisites","content":"Download and install the Confluent platform [https://www.confluent.io/installation](https://www.confluent.io/installation). This main Confluent package contains the tested version of Kafka Connect v7.0.1. Java is required for the Confluent Platform. Refer to their documentation for the currently supported java versions.Ensure you have a ClickHouse instance available.Kafka instance - Confluent cloud is the easiest for this; otherwise, set up a self-managed instance using the above Confluent package. The setup of Kafka is beyond the scope of these docs.Test dataset. A small GitHub JSON-based dataset with an insertion script is provided for convenience here. This will automatically apply a Kafka schema to the data to ensure it is compatible with the JDBC connector.  "},{"title":"Assumptions​","type":1,"pageTitle":"Using Kafka Connect","url":"en/integrations/kafka/kafka-connect-intro#assumptions","content":"We assume you are familiar with the Confluent Platform, specifically Kafka Connect. We recommend the Getting Started guide for Kafka Connect and the Kafka Connect 101 guide.We assume your source data is in JSON. Other data formats and the relevant configuration parameters are highlighted. "},{"title":"JDBC Connector","type":0,"sectionRef":"#","url":"en/integrations/kafka/kafka-connect-jdbc","content":"","keywords":""},{"title":"Steps​","type":1,"pageTitle":"JDBC Connector","url":"en/integrations/kafka/kafka-connect-jdbc#steps","content":""},{"title":"1. Install Kafka Connect and Connector​","type":1,"pageTitle":"JDBC Connector","url":"en/integrations/kafka/kafka-connect-jdbc#1-install-kafka-connect-and-connector","content":"We assume you have downloaded the Confluent package and installed it locally. Follow the installation instructions for installing the connector as documented here. If you use the confluent-hub installation method, your local configuration files will be updated. For sending data to ClickHouse from Kafka, we use the Sink component of the connector. "},{"title":"2. Download and install the JDBC Driver​","type":1,"pageTitle":"JDBC Connector","url":"en/integrations/kafka/kafka-connect-jdbc#2-download-and-install-the-jdbc-driver","content":"Download and install the ClickHouse JDBC driver clickhouse-jdbc-&amp;lt;version&gt;-shaded.jar from here. Install this into Kafka Connect following the details here. Other drivers may work but have not been tested. note Common Issue: the docs suggest copying the jar to share/java/kafka-connect-jdbc/. If you experience issues with Connect finding the driver, copy the driver to share/confluent-hub-components/confluentinc-kafka-connect-jdbc/lib/. Or modify plugin.path to include the driver - see below. "},{"title":"3. Prepare Configuration​","type":1,"pageTitle":"JDBC Connector","url":"en/integrations/kafka/kafka-connect-jdbc#3-prepare-configuration","content":"Follow these instructions for setting up a Connect relevant to your installation type, noting the differences between a standalone and distributed cluster. If using Confluent Cloud the distributed setup is relevant. The following parameters are relevant to using the JDBC connector with ClickHouse. A full parameter list can be found here: _connection.url_ - this should take the form of jdbc:clickhouse://&amp;lt;clickhouse host&gt;:&amp;lt;clickhouse http port&gt;/&amp;lt;target database&gt;connection.user - a user with write access to the target databasetable.name.format- Clickhouse table to insert data. This must exist.batch.size - The number of rows to send in a single batch. Ensure this set is to an appropriately large number. Per ClickHouse recommendations a value of 1000 is should be considered a minimum.tasks.max - The JDBC Sink connector supports running one or more tasks. This can be used to increase performance. Along with batch size this represents your primary means of improving performance.value.converter.schemas.enable - Set to false if using a schema registry, true if you embed your schemas in the messages.value.converter - Set according to your datatype e.g. for JSON, “io.confluent.connect.json.JsonSchemaConverter”.key.converter - Set to “org.apache.kafka.connect.storage.StringConverter”. We utilise String keys.pk.mode - Not relevant to ClickHouse. Set to none.auto.create - Not supported and must be false.auto.evolve - We recommend false for this setting although it may be supported in the future.insert.mode - Set to “insert”. Other modes are not currently supported.key.converter - Set according to the types of your keys.value.converter - Set based on the type of data on your topic. This data must have a supported schema - JSON, Avro or Protobuf formats. If using our sample dataset for testing, ensure the following are set: value.converter.schemas.enable - Set to false as we utilize a schema registry. Set to true if you are embedding the schema in each message.key.converter - Set to “org.apache.kafka.connect.storage.StringConverter”. We utilise String keys.value.converter - Set “io.confluent.connect.json.JsonSchemaConverter”.value.converter.schema.registry.url - Set to the schema server url along with the credentials for the schema server via the parameter value.converter.schema.registry.basic.auth.user.info. Example configuration files for the Github sample data can be found here, assuming Connect is run in standalone mode and Kafka is hosted in Confluent Cloud. "},{"title":"4. Create the ClickHouse table​","type":1,"pageTitle":"JDBC Connector","url":"en/integrations/kafka/kafka-connect-jdbc#4-create-the-clickhouse-table","content":"Ensure the table has been created, dropping it if it already exists from previous examples. An example compatible with the reduced Github dataset is shown below. Not the absence of any Array or Map types that are not currently not supported: CREATE TABLE github ( file_time DateTime, event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), actor_login LowCardinality(String), repo_name LowCardinality(String), created_at DateTime, updated_at DateTime, action Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20), comment_id UInt64, path String, ref LowCardinality(String), ref_type Enum('none' = 0, 'branch' = 1, 'tag' = 2, 'repository' = 3, 'unknown' = 4), creator_user_login LowCardinality(String), number UInt32, title String, state Enum('none' = 0, 'open' = 1, 'closed' = 2), assignee LowCardinality(String), closed_at DateTime, merged_at DateTime, merge_commit_sha String, merged_by LowCardinality(String), review_comments UInt32, member_login LowCardinality(String) ) ENGINE = MergeTree ORDER BY (event_type, repo_name, created_at)  "},{"title":"5. Start Kafka Connect​","type":1,"pageTitle":"JDBC Connector","url":"en/integrations/kafka/kafka-connect-jdbc#5-start-kafka-connect","content":"Start Kafka Connect in either standalone or distributed mode. For standalone mode, using the sample configurations, this is as simple as: ./bin/connect-standalone connect.properties.ini github-jdbc-sink.properties.ini  "},{"title":"6. Add data to Kafka​","type":1,"pageTitle":"JDBC Connector","url":"en/integrations/kafka/kafka-connect-jdbc#6-add-data-to-kafka","content":"Insert messages to Kafka using the script and config provided. You will need to modify github.config to include your Kafka credentials. The script is currently configured for use with Confluent Cloud. python producer.py -c github.config  This script can be used to insert any ndjson file into a Kafka topic. This will attempt to infer a schema for you automatically. The sample config provided will only insert 10k messages - modify here if required. This configuration also removes any incompatible Array fields from the dataset during insertion to Kafka. This is required for the JDBC connector to convert messages to INSERT statements. If you are using your own data, ensure you either insert a schema with every message (setting _value.converter.schemas.enable _to true) or ensure your client publishes messages referencing a schema to the registry. Kafka Connect should begin consuming messages and inserting rows into ClickHouse. Note that warnings regards “[JDBC Compliant Mode] Transaction is not supported.” are expected and can be ignored. A simple read on the target table “Github” should confirm data insertion. SELECT count() FROM default.github;  | count\\(\\) | | :--- | | 10000 |  "},{"title":"Recommended Further Reading​","type":1,"pageTitle":"JDBC Connector","url":"en/integrations/kafka/kafka-connect-jdbc#recommended-further-reading","content":"Kafka Sink Configuration ParametersKafka Connect Deep Dive – JDBC Source ConnectorKafka Connect JDBC Sink deep-dive: Working with Primary KeysKafka Connect in Action: JDBC Sink - for those who prefer to watch over read.Kafka Connect Deep Dive – Converters and Serialization Explained "},{"title":"Connection Options","type":0,"sectionRef":"#","url":"en/integrations/kafka/kafka-connect-options","content":"Connection Options Kafka Connect uses Sink Connectors to deliver data from Kafka topics into other data stores such as ClickHouse. Two Sink connectors provided by Confluent are compatible with ClickHouse: JDBC Connector - This Connector is both a Sink and Source Connector (for pushing data to Kafka) via the JDBC interface.HTTP Sink Connector - A connector for pulling data from Kafka and inserting it via its HTTP interface. Limitations Each of these has benefits and limitations: The JDBC connector relies on the user providing a JDBC driver. This driver has several versions, including the official ClickHouse distribution. This version uses the HTTP interface, although native support is planned. Until the native interface is not supported, it provides no performance benefit over the HTTP Sink other than ease of configuration. Other drivers support the native protocol, but these have not been tested.The JDBC connector requires a Kafka schema defining the types of the fields. It uses this schema, defined in JSON schema, to formulate insert statements. Whilst this is effective on primitive types, the connector does not support ClickHouse specific types, e.g., Arrays and Maps. Furthermore, this connector will not support several configuration options which rely on DDL queries - highlighted in the section JDBC Connector below.The HTTP Sink Connector does not require a data schema. Our example assumes the data is in JSON format - although this approach should be compatible with any formats that the ClickHouse HTTP interface can consume. The HTTP Sink Connector is also deployed natively in Confluent Cloud and has been tested with ClickHouse Cloud, unlike the JDBC, which must be self-managed. We provide instructions for both scenarios below.The JDBC connector is not currently hosted in Confluent Cloud. This must be self-managed.Both connectors have at-least-once delivery semantics. Duplicates may therefore occur in ClickHouse. The JDBC Connector is distributed under the Confluent Community License. The HTTP Connector conversely requires a Confluent Enterprise License.","keywords":""},{"title":"Connecting Kafka","type":0,"sectionRef":"#","url":"en/integrations/kafka/kakfa-intro","content":"","keywords":""},{"title":"Assumptions​","type":1,"pageTitle":"Connecting Kafka","url":"en/integrations/kafka/kakfa-intro#assumptions","content":"You are familiar with the Kafka fundamentals, such as producers, consumers and topics.You have a topic prepared for these examples. We assume all data is stored in Kafka as JSON, although the principles remain the same if using Avro.We utilise the excellent kcat (formerly kafkacat) in our examples to publish and consume Kafka data.Whilst we reference some python scripts for loading sample data, feel free to adapt the examples to your dataset.You are broadly familiar with ClickHouse materialized views. "},{"title":"Connecting ClickHouse to MySQL using the MySQL Table Engine","type":0,"sectionRef":"#","url":"en/integrations/mysql/mysql-with-clickhouse","content":"","keywords":"clickhouse mysql connect integrate table engine"},{"title":"1. Configure MySQL​","type":1,"pageTitle":"Connecting ClickHouse to MySQL using the MySQL Table Engine","url":"en/integrations/mysql/mysql-with-clickhouse#1-configure-mysql","content":"Create a database in MySQL: CREATE DATABASE db1; Create a table: CREATE TABLE db1.table1 ( id INT, column1 VARCHAR(255) ); Insert sample rows: INSERT INTO db1.table1 (id, column1) VALUES (1, 'abc'), (2, 'def'), (3, 'ghi'); Create a user to connect from ClickHouse: CREATE USER 'mysql_clickhouse'@'%' IDENTIFIED BY 'Password123!'; Grant privileges as needed. (For demonstration purposes, the mysql_clickhouse user is granted admin prvileges.) GRANT ALL PRIVILEGES ON *.* TO 'mysql_clickhouse'@'%';  "},{"title":"2. Define a Table in ClickHouse​","type":1,"pageTitle":"Connecting ClickHouse to MySQL using the MySQL Table Engine","url":"en/integrations/mysql/mysql-with-clickhouse#2-define-a-table-in-clickhouse","content":"Now let's create a ClickHouse table that uses the MySQL table engine: CREATE TABLE mysql_table1 ( id UInt64, column1 String ) ENGINE = MySQL('mysql-host.domain.com','db1','table1','mysql_clickhouse','Password123!') The minimum parameters are: parameter\tDescription\texamplehost\thostname or IP\tmysql-host.domain.com database\tmysql database name\tdb1 table\tmysql table name\ttable1 user\tusername to connect to mysql\tmysql_clickhouse password\tpassword to connect to mysql\tPassword123! note View the MySQL table engine doc page for a complete list of parameters. "},{"title":"3. Test the Integration​","type":1,"pageTitle":"Connecting ClickHouse to MySQL using the MySQL Table Engine","url":"en/integrations/mysql/mysql-with-clickhouse#3-test-the-integration","content":"In MySQL, insert a sample row: INSERT INTO db1.table1 (id, column1) VALUES (4, 'jkl'); Notice the existing rows from the MySQL table are in the ClickHouse table, along with the new row you just added: SELECT id, column1 FROM mysql_table1 You should see 4 rows: Query id: 6d590083-841e-4e95-8715-ef37d3e95197 ┌─id─┬─column1─┐ │ 1 │ abc │ │ 2 │ def │ │ 3 │ ghi │ │ 4 │ jkl │ └────┴─────────┘ 4 rows in set. Elapsed: 0.044 sec. Let's add a row to the ClickHouse table: INSERT INTO mysql_table1 (id, column1) VALUES (5,'mno') Notice the new row appears in MySQL: mysql&gt; select id,column1 from db1.table1; You should see the new row: +------+---------+ | id | column1 | +------+---------+ | 1 | abc | | 2 | def | | 3 | ghi | | 4 | jkl | | 5 | mno | +------+---------+ 5 rows in set (0.01 sec)  "},{"title":"Summary​","type":1,"pageTitle":"Connecting ClickHouse to MySQL using the MySQL Table Engine","url":"en/integrations/mysql/mysql-with-clickhouse#summary","content":"The MySQL table engine allows you to connect ClickHouse to MySQL to exchange data back and forth. For more details, be sure to check out the documentation page for the MySQL table engine. "},{"title":"Replicate a MySQL Database in ClickHouse","type":0,"sectionRef":"#","url":"en/integrations/mysql/mysql-with-clickhouse-database-engine","content":"","keywords":"clickhouse mysql connect integrate replicate database MaterializedMySQL"},{"title":"1. Configure MySQL​","type":1,"pageTitle":"Replicate a MySQL Database in ClickHouse","url":"en/integrations/mysql/mysql-with-clickhouse-database-engine#1-configure-mysql","content":"Configure the MySQL database to allow for replication and native authentication. ClickHouse only works with native password authentication. Add the following entries to /etc/my.cnf: default-authentication-plugin = mysql_native_password gtid-mode = ON enforce-gtid-consistency = ON Create a user to connect from ClickHouse: CREATE USER clickhouse_user IDENTIFIED BY 'ClickHouse_123'; Grant the needed permissions to the new user. For demonstration purposes, full admin rights have been granted here: GRANT ALL PRIVILEGES ON *.* TO 'clickhouse_user'@'%'; note The minimal permissions needed for the MySQL user are RELOAD, REPLICATION SLAVE, REPLICATION CLIENT and SELECT PRIVILEGE. Create a database in MySQL: CREATE DATABASE db1; Create a table: CREATE TABLE db1.table_1 ( id INT, column1 VARCHAR(10), PRIMARY KEY (`id`) ) ENGINE = InnoDB; Insert a few sample rows: INSERT INTO db1.table_1 (id, column1) VALUES (1, 'abc'), (2, 'def'), (3, 'ghi');  "},{"title":"2. Configure ClickHouse​","type":1,"pageTitle":"Replicate a MySQL Database in ClickHouse","url":"en/integrations/mysql/mysql-with-clickhouse-database-engine#2-configure-clickhouse","content":"Set parameter to allow use of experimental feature: set allow_experimental_database_materialized_mysql = 1; Create a database that uses the MaterializedMySQL database engine: CREATE DATABASE db1_mysql ENGINE = MaterializedMySQL( 'mysql-host.domain.com:3306', 'db1', 'clickhouse_user', 'ClickHouse_123' ); The minimum parameters are: parameter\tDescription\texamplehost:port\thostname or IP and port\tmysql-host.domain.com database\tmysql database name\tdb1 user\tusername to connect to mysql\tclickhouse_user password\tpassword to connect to mysql\tClickHouse_123 note View the MaterializedMySQL database engine doc page for a complete list of parameters. "},{"title":"3. Test the Integration​","type":1,"pageTitle":"Replicate a MySQL Database in ClickHouse","url":"en/integrations/mysql/mysql-with-clickhouse-database-engine#3-test-the-integration","content":"In MySQL, insert a sample row: INSERT INTO db1.table_1 (id, column1) VALUES (4, 'jkl'); Notice the new row appears in the ClickHouse table: SELECT id, column1 FROM db1_mysql.table_1 The response looks like: Query id: d61a5840-63ca-4a3d-8fac-c93235985654 ┌─id─┬─column1─┐ │ 1 │ abc │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 4 │ jkl │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 2 │ def │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 3 │ ghi │ └────┴─────────┘ 4 rows in set. Elapsed: 0.030 sec. Suppose the table in MySQL is modified. Let's a column to db1.table_1 in MySQL: alter table db1.table_1 add column column2 varchar(10) after column1; Now let's insert a row to the modified table: INSERT INTO db1.table_1 (id, column1, column2) VALUES (5, 'mno', 'pqr'); Notice the talbe in ClickHouse now has the new column and the new row: SELECT id, column1, column2 FROM db1_mysql.table_1 The previous rows will have NULL for column2: Query id: 2c32fd15-3c83-480b-9bfc-cba5d932d674 Connecting to localhost:9000 as user default. Connected to ClickHouse server version 22.2.2 revision 54455. ┌─id─┬─column1─┬─column2─┐ │ 3 │ ghi │ ᴺᵁᴸᴸ │ └────┴─────────┴─────────┘ ┌─id─┬─column1─┬─column2─┐ │ 2 │ def │ ᴺᵁᴸᴸ │ └────┴─────────┴─────────┘ ┌─id─┬─column1─┬─column2─┐ │ 1 │ abc │ ᴺᵁᴸᴸ │ │ 5 │ mno │ pqr │ └────┴─────────┴─────────┘ ┌─id─┬─column1─┬─column2─┐ │ 4 │ jkl │ ᴺᵁᴸᴸ │ └────┴─────────┴─────────┘ 5 rows in set. Elapsed: 0.017 sec.  "},{"title":"Summary​","type":1,"pageTitle":"Replicate a MySQL Database in ClickHouse","url":"en/integrations/mysql/mysql-with-clickhouse-database-engine#summary","content":"That's it! The MaterializedMySQL database engine will keep the MySQL database synced on ClickHouse. There are a few details and limitations, so be sure to read the doc page for MaterializedMySQL for more details. note If you just want to move data between MySQL and ClickHouse, check out the MySQL table engine. "},{"title":"Connecting ClickHouse to PostgreSQL using the PostgreSQL Table Engine","type":0,"sectionRef":"#","url":"en/integrations/postgresql/postgres-with-clickhouse","content":"","keywords":"clickhouse postgres postgresql connect integrate table engine"},{"title":"1. Setting up PostgreSQL​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the PostgreSQL Table Engine","url":"en/integrations/postgresql/postgres-with-clickhouse#1-setting-up-postgresql","content":"In postgresql.conf, add the following entry to enable PostgreSQL to listen on the network interfaces: listen_addresses = '*' Create a user to connect from ClickHouse. For demonstration purposes, this example grants full superuser rights. CREATE ROLE clickhouse_user SUPERUSER LOGIN PASSWORD 'ClickHouse_123'; Create a new database in PostgreSQL: CREATE DATABASE db_in_psg; Create a new table: CREATE TABLE table1 ( id integer primary key, column1 varchar(10) ); Let's add a few rows for testing: INSERT INTO table1 (id, column1) VALUES (1, 'abc'), (2, 'def'); To configure PostgreSQL to allow connections to the new database with the new user for replication, add the following entry to the pg_hba.conf file. Update the address line with either the subnet or IP address of your PostgreSQL server: # TYPE DATABASE USER ADDRESS METHOD host db_in_psg clickhouse_user 192.168.1.0/24 password Reload the pg_hba.conf configuration (adjust this command depending on your version): /usr/pgsql-12/bin/pg_ctl reload Verify the new clickhouse_user can login: psql -U clickhouse_user -W -d db_in_psg -h &lt;your_postgresql_host&gt;  "},{"title":"2. Define a Table in ClickHouse​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the PostgreSQL Table Engine","url":"en/integrations/postgresql/postgres-with-clickhouse#2-define-a-table-in-clickhouse","content":"Login to the clickhouse-client: clickhouse-client --user default --password ClickHouse123! Let's create a new database: CREATE DATABASE db_in_ch; Create a table that uses the PostgreSQL: CREATE TABLE db_in_ch.table1 ( id UInt64, column1 String ) ENGINE = PostgreSQL('postgres-host.domain.com:5432', 'db_in_psg', 'table1', 'clickhouse_user', 'ClickHouse_123'); The minimum parameters needed are: parameter\tDescription\texamplehost:port\thostname or IP and port\tpostgres-host.domain.com:5432 database\tPostgreSQL database name\tdb_in_psg user\tusername to connect to postgres\tclickhouse_user password\tpassword to connect to postgres\tClickHouse_123 note View the PostgreSQL table engine doc page for a complete list of parameters. "},{"title":"3 Test the Integration​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the PostgreSQL Table Engine","url":"en/integrations/postgresql/postgres-with-clickhouse#3-test-the-integration","content":"In ClickHouse, view initial rows: SELECT * FROM db_in_ch.table1 The ClickHouse table should automatically be populated with the two rows that already existed in the table in PostgreSQL: Query id: 34193d31-fe21-44ac-a182-36aaefbd78bf ┌─id─┬─column1─┐ │ 1 │ abc │ │ 2 │ def │ └────┴─────────┘ Back in PostgreSQL, add a couple of rows to the table: INSERT INTO table1 (id, column1) VALUES (3, 'ghi'), (4, 'jkl'); Those two new rows should appear in your ClickHouse table: SELECT * FROM db_in_ch.table1 The response should be: Query id: 86fa2c62-d320-4e47-b564-47ebf3d5d27b ┌─id─┬─column1─┐ │ 1 │ abc │ │ 2 │ def │ │ 3 │ ghi │ │ 4 │ jkl │ └────┴─────────┘ Let's see what happens when you add rows to the ClickHouse table: INSERT INTO db_in_ch.table1 (id, column1) VALUES (5, 'mno'), (6, 'pqr'); The rows added in ClickHouse should appear in the table in PostgreSQL: db_in_psg=# SELECT * FROM table1; id | column1 ----+--------- 1 | abc 2 | def 3 | ghi 4 | jkl 5 | mno 6 | pqr (6 rows)  "},{"title":"Summary​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the PostgreSQL Table Engine","url":"en/integrations/postgresql/postgres-with-clickhouse#summary","content":"This example demonstrated the basic integration between PostgreSQL and ClickHouse using the PostrgeSQL table engine. Check out the doc page for the PostgreSQL table engine for more features, such as specifying schemas, returning only a subset of columns, and connecting to multiple replicas. "},{"title":"Connecting ClickHouse to PostgreSQL using the MaterializedPostgreSQL database engine","type":0,"sectionRef":"#","url":"en/integrations/postgresql/postgres-with-clickhouse-database-engine","content":"","keywords":"clickhouse postgres postgresql connect integrate"},{"title":"1. In PostgreSQL​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the MaterializedPostgreSQL database engine","url":"en/integrations/postgresql/postgres-with-clickhouse-database-engine#1-in-postgresql","content":"In postgresql.conf, set minimum listen levels, replication wal level and replication slots: add the following entries: listen_addresses = '*' max_replication_slots = 10 wal_level = logical  *ClickHouse needs minimum of logical wal level and minimum 2 replication slots Using an admin account, create a user to connect from ClickHouse: CREATE ROLE clickhouse_user SUPERUSER LOGIN PASSWORD 'ClickHouse_123';  *for demonstration purposes, full superuser rights have been granted. create a new database: CREATE DATABASE db1;  connect to the new database in psql: \\connect db1  create a new table: CREATE TABLE table1 ( id integer primary key, column1 varchar(10) );  add initial rows: INSERT INTO table1 (id, column1) VALUES (1, 'abc'), (2, 'def');  Configure the PostgreSQLto allow connections to the new database with the new user for replication: below is the minimum entry to add to the pg_hba.conf file: # TYPE DATABASE USER ADDRESS METHOD host db1 clickhouse_user 192.168.1.0/24 password  *for demonstration purposes, this is using clear text password authentication method. update the address line with either the subnet or the address of the server per PostgreSQL documentation reload the pg_hba.conf configuration with something like this (adjust for your version): /usr/pgsql-12/bin/pg_ctl reload  Test the login with new clickhouse_user:  psql -U clickhouse_user -W -d db1 -h &lt;your_postgresql_host&gt;  "},{"title":"2. In ClickHouse​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the MaterializedPostgreSQL database engine","url":"en/integrations/postgresql/postgres-with-clickhouse-database-engine#2-in-clickhouse","content":"log into the ClickHouse CLI clickhouse-client --user default --password ClickHouse123!  Enable the PosgreSQL experimental feature for the database engine: SET allow_experimental_database_materialized_postgresql=1  Create the new database to be replicated and define the intitial table: CREATE DATABASE db1_postgres ENGINE = MaterializedPostgreSQL('postgres-host.domain.com:5432', 'db1', 'clickhouse_user', 'ClickHouse_123') SETTINGS materialized_postgresql_tables_list = 'table1';  minimum options: parameter\tDescription\texamplehost:port\thostname or IP and port\tpostgres-host.domain.com:5432 database\tPostgreSQL database name\tdb1 user\tusername to connect to postgres\tclickhouse_user password\tpassword to connect to postgres\tClickHouse_123 settings\tadditional settings for the engine\tmaterialized_postgresql_tables_list = 'table1' For complete guide to the PostgreSQL database engine, refer to: https://clickhouse.com/docs/en/engines/database-engines/materialized-postgresql/#settings Verify initial table has data ch_env_2 :) select * from db1_postgres.table1; SELECT * FROM db1_postgres.table1 Query id: df2381ac-4e30-4535-b22e-8be3894aaafc ┌─id─┬─column1─┐ │ 1 │ abc │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 2 │ def │ └────┴─────────┘  "},{"title":"3. Test basic replication​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the MaterializedPostgreSQL database engine","url":"en/integrations/postgresql/postgres-with-clickhouse-database-engine#3-test-basic-replication","content":"In PostgreSQL, add new rows: INSERT INTO table1 (id, column1) VALUES (3, 'ghi'), (4, 'jkl');  In ClickHouse, verify new rows are visible: ch_env_2 :) select * from db1_postgres.table1; SELECT * FROM db1_postgres.table1 Query id: b0729816-3917-44d3-8d1a-fed912fb59ce ┌─id─┬─column1─┐ │ 1 │ abc │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 4 │ jkl │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 3 │ ghi │ └────┴─────────┘ ┌─id─┬─column1─┐ │ 2 │ def │ └────┴─────────┘  "},{"title":"4. Summary​","type":1,"pageTitle":"Connecting ClickHouse to PostgreSQL using the MaterializedPostgreSQL database engine","url":"en/integrations/postgresql/postgres-with-clickhouse-database-engine#4-summary","content":"This integration guide focused on a simple example on how to replicate a database with a table, however, there exist more advanced options which include replicating the whole database or adding new tables and schemas to the existing replications. Although DDL commands are not supported for this replication, the engine can be set to detect changes and reload the tables when there are structural changes made. For more features available for advanced options, please see the reference documenation:https://clickhouse.com/docs/en/engines/database-engines/materialized-postgresql/ "},{"title":"Migrating Data from Redshift to ClickHouse","type":0,"sectionRef":"#","url":"en/integrations/redshift/migrate-redshift-to-clickhouse","content":"Migrating Data from Redshift to ClickHouse Amazon Redshift is a popular cloud data warehousing solution that is part of the Amazon Web Services offerings. This guide presents different approaches to migrating data from a Redshift instance to ClickHouse. We will cover three options: From the ClickHouse instance standpoint, you can either: PUSH data to ClickHouse using a third party ETL/ELT tool or service PULL data from ClickHouse leveraging the ClickHouse JDBC Bridge PIVOT using S3 object storage using an “Unload then load” logic note We used Redshift as a data source in this tutorial. However, the migration approaches presented here are not exclusive to Redshift, and similar steps can be derived for any compatible data source.","keywords":""},{"title":"Pivot Data from Redshift to ClickHouse using S3","type":0,"sectionRef":"#","url":"en/integrations/redshift/redshift-pivot-to-clickhouse","content":"","keywords":""},{"title":"Tutorial​","type":1,"pageTitle":"Pivot Data from Redshift to ClickHouse using S3","url":"en/integrations/redshift/redshift-pivot-to-clickhouse#tutorial","content":"Using Redshift's UNLOAD feature, export the data into a an existing private S3 bucket: It will generate part files containing the raw data in S3 Load the S3 files into ClickHouse using an INSERT INTO ... SELECT statement: INSERT INTO users_imported (*) SELECT * FROM s3('https://ryadh-bucket.s3.amazonaws.com/unload/users/*', '&lt;aws_access_key&gt;', '&lt;aws_secret_access_key&gt;', 'CSV', 'username String, firstname String, lastname String') Query id: 2e7e219a-6124-461c-8d75-e4f5002c8557 Ok. 0 rows in set. Elapsed: 0.545 sec. Processed 49.99 thousand rows, 2.34 MB (91.72 thousand rows/s., 4.30 MB/s.)  note This example used CSV as the pivot format. However, for production workloads we recommend Apache Parquet as the best option for large migrations since it comes with compression and can save some storage costs while reducing transfer times. (By default, each row group is compressed using SNAPPY). ClickHouse also leverages Parquet's column orientation to speed up data ingestion. "},{"title":"Pull Data from Redshift to ClickHouse","type":0,"sectionRef":"#","url":"en/integrations/redshift/redshift-pull-to-clickhouse","content":"","keywords":""},{"title":"Tutorial​","type":1,"pageTitle":"Pull Data from Redshift to ClickHouse","url":"en/integrations/redshift/redshift-pull-to-clickhouse#tutorial","content":"To use this option, you need to set up a ClickHouse JDBC Bridge. ClickHouse JDBC Bridge is a standalone Java application that handles JDBC connectivity and acts as a proxy between the ClickHouse instance and the datasources. For this tutorial, we used a pre-populated Redshift instance with a sample database. Deploy the ClickHouse JDBC Bridge. For more details, see our user guide on JDBC for External Datasources Configure your Redshift datasource for ClickHouse JDBC Bridge. For example, /etc/clickhouse-jdbc-bridge/config/datasources/redshift.json { &quot;redshift-server&quot;: { &quot;aliases&quot;: [ &quot;redshift&quot; ], &quot;driverUrls&quot;: [ &quot;https://s3.amazonaws.com/redshift-downloads/drivers/jdbc/2.1.0.4/redshift-jdbc42-2.1.0.4.jar&quot; ], &quot;driverClassName&quot;: &quot;com.amazon.redshift.jdbc.Driver&quot;, &quot;jdbcUrl&quot;: &quot;jdbc:redshift://redshift-cluster-1.ckubnplpz1uv.us-east-1.redshift.amazonaws.com:5439/dev&quot;, &quot;username&quot;: &quot;awsuser&quot;, &quot;password&quot;: &quot;&lt;password&gt;&quot;, &quot;maximumPoolSize&quot;: 5 } } Once ClickHouse JDBC Bridge deployed and running, you can start querying your Redshift instance from ClickHouse SELECT * FROM jdbc('redshift', 'select username, firstname, lastname from users limit 5') Query id: 1b7de211-c0f6-4117-86a2-276484f9f4c0 ┌─username─┬─firstname─┬─lastname─┐ │ PGL08LJI │ Vladimir │ Humphrey │ │ XDZ38RDD │ Barry │ Roy │ │ AEB55QTM │ Reagan │ Hodge │ │ OWY35QYB │ Tamekah │ Juarez │ │ MSD36KVR │ Mufutau │ Watkins │ └──────────┴───────────┴──────────┘ 5 rows in set. Elapsed: 0.438 sec. SELECT * FROM jdbc('redshift', 'select count(*) from sales') Query id: 2d0f957c-8f4e-43b2-a66a-cc48cc96237b ┌──count─┐ │ 172456 │ └────────┘ 1 rows in set. Elapsed: 0.304 sec.  In the following, we display importing data using an INSERT INTO ... SELECT statement # TABLE CREATION with 3 columns CREATE TABLE users_imported ( `username` String, `firstname` String, `lastname` String ) ENGINE = MergeTree ORDER BY firstname Query id: c7c4c44b-cdb2-49cf-b319-4e569976ab05 Ok. 0 rows in set. Elapsed: 0.233 sec. # IMPORTING DATA INSERT INTO users_imported (*) SELECT * FROM jdbc('redshift', 'select username, firstname, lastname from users') Query id: 9d3a688d-b45a-40f4-a7c7-97d93d7149f1 Ok. 0 rows in set. Elapsed: 4.498 sec. Processed 49.99 thousand rows, 2.49 MB (11.11 thousand rows/s., 554.27 KB/s.)  "},{"title":"Push Data from Redshift to ClickHouse","type":0,"sectionRef":"#","url":"en/integrations/redshift/redshift-push-to-clickhouse","content":"Push Data from Redshift to ClickHouse In the push scenario, the idea is to leverage a third-party tool or service (either custom code or an ETL/ELT) to send your data to your ClickHouse instance. For example, you can use a software like Airbyte to move data between your Redshift instance (as a source) and ClickHouse as a destination (see our integration guide for Airbyte) Pros​ It can leverage the existing catalog of connectors from the ETL/ELT software.Built-in capabilities to keep data in sync (append/overwrite/increment logic).Enable data transformation scenarios (for example, see our integration guide for dbt). Cons​ Users need to set up and maintain an ETL/ELT infrastructure.Introduces a third-party element in the architecture which can turn into a potential scalability bottleneck.","keywords":""},{"title":"Connnecting S3","type":0,"sectionRef":"#","url":"en/integrations/s3/s3-intro","content":"Connnecting S3 Amazon S3 or Amazon Simple Storage Service is a service offered by Amazon Web Services (AWS) that provides object storage through a web service interface. Users can insert S3 based data into ClickHouse and use S3 as an export destination, thus allowing interaction with “Data Lake” architectures. Furthermore, s3 can provide “cold” storage tiers and assist with separating storage and compute. Below we outline the approach for these use cases: identifying key configuration parameters and any current limitations and providing hints on optimizing performance. We utilize a subset of the new york taxi public dataset for read-orientated examples. We assume you have s3 buckets available for insert examples into which data can be written.","keywords":""},{"title":"Using Vector with Kafka and ClickHouse","type":0,"sectionRef":"#","url":"en/integrations/kafka/kafka-vector","content":"","keywords":""},{"title":"Steps​","type":1,"pageTitle":"Using Vector with Kafka and ClickHouse","url":"en/integrations/kafka/kafka-vector#steps","content":"Create the Kafka github topic and insert the Github dataset. cat /opt/data/github/github_all_columns.ndjson | kafkacat -b &lt;host&gt;:&lt;port&gt; -X security.protocol=sasl_ssl -X sasl.mechanisms=PLAIN -X sasl.username=&lt;username&gt; -X sasl.password=&lt;password&gt; -t github  This dataset consists of 200,000 rows focused on the ClickHouse/ClickHouse repository. Ensure the target table is created. Below we use the default database.  CREATE TABLE github ( file_time DateTime, event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), actor_login LowCardinality(String), repo_name LowCardinality(String), created_at DateTime, updated_at DateTime, action Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20), comment_id UInt64, path String, ref LowCardinality(String), ref_type Enum('none' = 0, 'branch' = 1, 'tag' = 2, 'repository' = 3, 'unknown' = 4), creator_user_login LowCardinality(String), number UInt32, title String, labels Array(LowCardinality(String)), state Enum('none' = 0, 'open' = 1, 'closed' = 2), assignee LowCardinality(String), assignees Array(LowCardinality(String)), closed_at DateTime, merged_at DateTime, merge_commit_sha String, requested_reviewers Array(LowCardinality(String)), merged_by LowCardinality(String), review_comments UInt32, member_login LowCardinality(String) ) ENGINE = MergeTree ORDER BY (event_type, repo_name, created_at);  Download and install Vector. Create a kafka.toml configuration file and modify the values for your Kafka and ClickHouse instances.  [sources.github] type = &quot;kafka&quot; auto_offset_reset = &quot;smallest&quot; bootstrap_servers = &quot;&lt;kafka_host&gt;:&lt;kafka_port&gt;&quot; group_id = &quot;vector&quot; topics = [ &quot;github&quot; ] tls.enabled = true sasl.enabled = true sasl.mechanism = &quot;PLAIN&quot; sasl.username = &quot;&lt;username&gt;&quot; sasl.password = &quot;&lt;password&gt;&quot; decoding.codec = &quot;json&quot; [sinks.clickhouse] type = &quot;clickhouse&quot; inputs = [&quot;github&quot;] endpoint = &quot;http://localhost:8123&quot; database = &quot;default&quot; table = &quot;github&quot; skip_unknown_fields = true auth.strategy = &quot;basic&quot; auth.user = &quot;username&quot; auth.password = &quot;password&quot; buffer.max_events = 10000 batch.timeout_secs = 1  A few important notes on this configuration and behavior of Vector: This example has been tested against Confluent Cloud. Therefore, the sasl.* and ssl.enabled security options may not be appropriate in self-managed cases.A protocol prefix is not required for the configuration parameter bootstrap_servers e.g. pkc-2396y.us-east-1.aws.confluent.cloud:9092The source parameter decoding.codec = &quot;json&quot; ensures the message is passed to the ClickHouse sink as a single JSON object. If handling messages as Strings and using the default bytes value, the contents of the message will be appended to a field message. In most cases this will require processing in ClickHouse as described in the Vector getting started guide.Vector adds a number of fields to the messages. In our example, we ignore these fields in the ClickHouse sink via the configuration parameter skip_unknown_fields = true. This ignores fields that are not part of the target table schema. Feel free to adjust your schema to ensure these meta fields such as offset are added.Notice how the sink references of the source of events via the parameter inputs.Note the behavior of the ClickHouse sink as described here. For optimal throughput, users may wish to tune the buffer.max_events, batch.timeout_secs and batch.max_bytes parameters. Per ClickHouse recommendations a value of 1000 is should be considered a minimum for the number of events in any single batch. For uniform high throughput use cases, users may increase the parameter buffer.max_events. More variable throughputs may require changes in the parameter batch.timeout_secsThe parameter auto_offset_reset = &quot;smallest&quot; forces the Kafka source to start from the start of the topic - thus ensuring we consume the messages published in step (1). Users may require different behavior. See here for further details. Start Vector vector --config ./kafka.toml  By default, a health check is required before insertions begin to ClickHouse. This ensures connectivity can be established and the schema read. Prepending VECTOR_LOG=debug can be helpful to obtain further logging should you encounter issues. Confirm the insertion of the data. SELECT count() as count FROM github;  count200000 "},{"title":"Using MinIO","type":0,"sectionRef":"#","url":"en/integrations/s3/s3-minio","content":"Using MinIO All S3 functions and tables and compatible with MinIO. Users may experience superior throughput on self-hosted MinIO stores, especially in the event of optimal network locality.","keywords":""},{"title":"Optimizing for Performance","type":0,"sectionRef":"#","url":"en/integrations/s3/s3-optimizing-performance","content":"","keywords":""},{"title":"Measuring Performance​","type":1,"pageTitle":"Optimizing for Performance","url":"en/integrations/s3/s3-optimizing-performance#measuring-performance","content":"Before making any changes to improve performance, ensure you measure appropriately. As S3 API calls are sensitive to latency and may impact client timings, use the query log for performance metrics, i.e., system.query_log. If measuring the performance of SELECT queries, where large volumes of data are returned to the client, either utilize the null format for queries or direct results to the Null engine. This should avoid the client being overwhelmed with data and network saturation. "},{"title":"Region Locality​","type":1,"pageTitle":"Optimizing for Performance","url":"en/integrations/s3/s3-optimizing-performance#region-locality","content":"Ensure your buckets are located in the same region as your ClickHouse instances. This simple optimization can dramatically improve throughput performance, especially if you deploy your ClickHouse instances on AWS infrastructure. "},{"title":"Using Threads​","type":1,"pageTitle":"Optimizing for Performance","url":"en/integrations/s3/s3-optimizing-performance#using-threads","content":"Read performance on s3 will scale linearly with the number of cores, provided you are not limited by network bandwidth or local IO. Increasing the number of threads also has memory overhead permutations that users should be aware of. The following can be modified to improve throughput performance potentially: Usually, the default value of max_threads is sufficient, i.e., the number of cores. If the amount of memory used for a query is high, and this needs to be reduced, or the LIMIT on results is low, this value can be set lower. Users with plenty of memory may wish to experiment with increasing this value for possible higher read throughput from s3. Typically this is only beneficial on machines with lower core counts, i.e., &lt; 10. The benefit from further parallelization typically diminishes as other resources act as a bottleneck, e.g., network.If performing an INSERT INTO x SELECT request, note that the number of threads will be set to 1 as dictated by the setting max_insert_threads. Provided max_threads is greater than 1 (confirm with SELECT * FROM system.settings WHERE name='max_threads'), increasing this will improve insert performance at the expense of memory. Increase with caution due to memory consumption overheads. This value should not be as high as the max_threads as resources are consumed on background merges. Furthermore, not all target engines (MergeTree does) support parallel inserts. Finally, parallel inserts invariably cause more parts, slowing down subsequent reads. Increase with caution.For low memory scenarios, consider lowering max_insert_delayed_streams_for_parallel_write if inserting into s3.Versions of ClickHouse before 22.3.1 only parallelized reads across multiple files when using the s3 function or s3 table engine. This required the user to ensure files were split into chunks on s3 and read using a glob pattern to achieve optimal read performance. Later versions now parallelize downloads within a file. Assuming sufficient memory (test!), increasing min_insert_block_size_rows can improve insert throughput.In low thread count scenarios, users may benefit from setting remote_filesystem_read_method to &quot;read&quot; to cause the synchronous reading of files from s3. "},{"title":"Formats​","type":1,"pageTitle":"Optimizing for Performance","url":"en/integrations/s3/s3-optimizing-performance#formats","content":"ClickHouse can read files stored in s3 buckets in the supported formats using the s3 function and s3 engine. If reading raw files, some of these formats have distinct advantages: Formats with encoded column names such as Native, Parquet, CSVWithNames, and TabSeparatedWithNames will be less verbose to query since the user will not be required to specify the column name is the s3 function. The column names allow this information to be inferred.Formats will differ in performance with respect to read and write throughputs. Native and parquet represent the most optimal formats for read performance since they are already column orientated and more compact. The native format additionally benefits from alignment with how ClickHouse stores data in memory - thus reducing processing overhead as data is streamed into ClickHouse.The block size will often impact the latency of reads on large files. This is very apparent if you only sample the data, e.g., returning the top N rows. In the case of formats such as CSV and TSV, files must be parsed to return a set of rows. Formats such as Native and Parquet will allow faster sampling as a result.Each compression format brings pros and cons, often balancing the compression level for speed and biasing compression or decompression performance. If compressing raw files such as CSV or TSV, lz4 offers the fastest decompression performance, sacrificing the compression level. Gzip typically compresses better at the expense of slightly slower read speeds. Xz takes this further by usually offering the best compression with the slowest compression and decompression performance. If exporting, Gz and lz4 offer comparable compression speeds. Balance this against your connection speeds. Any gains from faster decompression or compression will be easily negated by a slower connection to your s3 buckets.Formats such as native or parquet do not typically justify the overhead of compression. Any savings in data size are likely to be minimal since these formats are inherently compact. The time spent compressing and decompressing will rarely offset network transfer times - especially since s3 is globally available with higher network bandwidth. Internally the ClickHouse merge tree uses two primary storage formats: Wide and Compact. While the current implementation uses the default behavior of ClickHouse - controlled through the settings min_bytes_for_wide_part and min_rows_for_wide_part; we expect behavior to diverge for s3 in the future releases, e.g., a larger default value of min_bytes_for_wide_part encouraging a more Compact format and thus fewer files. Users may now wish to tune these settings when using exclusively s3 storage. "},{"title":"Scaling with Nodes​","type":1,"pageTitle":"Optimizing for Performance","url":"en/integrations/s3/s3-optimizing-performance#scaling-with-nodes","content":"Users will often have more than one node of ClickHouse available. While users can scale vertically, improving s3 throughput linearly with the number of cores, horizontal scaling is often necessary due to hardware availability and cost-efficiency. The replication of an s3 backed Merge Tree is supported through zero copy replication. Utilizing a cluster for s3 reads requires using the s3Cluster function as described in Utilizing Clusters. While this allows reads to be distributed across nodes, thread settings will not currently be sent to all nodes as of 22.3.1. For example, if the following query was executed against a node, only the receiving initiator node will respect the max_insert_threads setting. INSERT INTO default.trips_all SELECT * FROM s3Cluster('events', 'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames') SETTINGS max_insert_threads=8;  To ensure this setting is used, the following would need to be added to each nodes config.xml file (or under conf.d): &lt;clickhouse&gt; &lt;profiles&gt; &lt;default&gt; &lt;max_insert_threads&gt;8&lt;/max_insert_threads&gt; &lt;/default&gt; &lt;/profiles&gt; &lt;/clickhouse&gt;  "},{"title":"JDBC Driver","type":0,"sectionRef":"#","url":"en/interfaces/jdbc","content":"JDBC Driver Use the official JDBC driver (and Java client) to access ClickHouse from your Java applications. Third-party drivers: ClickHouse-Native-JDBCclickhouse4j Original article","keywords":""},{"title":"ODBC Driver","type":0,"sectionRef":"#","url":"en/interfaces/odbc","content":"ODBC Driver Use the official ODBC driver for accessing ClickHouse as a data source. Original article","keywords":""},{"title":"MySQL Interface","type":0,"sectionRef":"#","url":"en/interfaces/mysql","content":"MySQL Interface ClickHouse supports MySQL wire protocol. It can be enabled by mysql_port setting in configuration file: &lt;mysql_port&gt;9004&lt;/mysql_port&gt; Example of connecting using command-line tool mysql: $ mysql --protocol tcp -u default -P 9004 Output if a connection succeeded: Welcome to the MySQL monitor. Commands end with ; or \\g. Your MySQL connection id is 4 Server version: 20.2.1.1-ClickHouse Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type 'help;' or '\\h' for help. Type '\\c' to clear the current input statement. mysql&gt; For compatibility with all MySQL clients, it is recommended to specify user password with double SHA1 in configuration file. If user password is specified using SHA256, some clients won’t be able to authenticate (mysqljs and old versions of command-line tool MySQL and MariaDB). Restrictions: prepared queries are not supported some data types are sent as strings To cancel a long query use KILL QUERY connection_id statement (it is replaced with KILL QUERY WHERE query_id = connection_id while proceeding). For example: $ mysql --protocol tcp -h mysql_server -P 9004 default -u default --password=123 -e &quot;KILL QUERY 123456;&quot; Original article","keywords":""},{"title":"C++ Client Library","type":0,"sectionRef":"#","url":"en/interfaces/cpp","content":"C++ Client Library See README at clickhouse-cpp repository. Original article","keywords":""},{"title":"Native Interface (TCP)","type":0,"sectionRef":"#","url":"en/interfaces/tcp","content":"Native Interface (TCP) The native protocol is used in the command-line client, for inter-server communication during distributed query processing, and also in other C++ programs. Unfortunately, native ClickHouse protocol does not have formal specification yet, but it can be reverse-engineered from ClickHouse source code (starting around here) and/or by intercepting and analyzing TCP traffic. Original article","keywords":""},{"title":"gRPC Interface","type":0,"sectionRef":"#","url":"en/interfaces/grpc","content":"","keywords":""},{"title":"Introduction​","type":1,"pageTitle":"gRPC Interface","url":"en/interfaces/grpc#grpc-interface-introduction","content":"ClickHouse supports gRPC interface. It is an open source remote procedure call system that uses HTTP/2 and Protocol Buffers. The implementation of gRPC in ClickHouse supports: SSL; authentication; sessions; compression; parallel queries through the same channel; cancellation of queries; getting progress and logs; external tables. The specification of the interface is described in clickhouse_grpc.proto. "},{"title":"gRPC Configuration​","type":1,"pageTitle":"gRPC Interface","url":"en/interfaces/grpc#grpc-interface-configuration","content":"To use the gRPC interface set grpc_port in the main server configuration. Other configuration options see in the following example: &lt;grpc_port&gt;9100&lt;/grpc_port&gt; &lt;grpc&gt; &lt;enable_ssl&gt;false&lt;/enable_ssl&gt; &lt;!-- The following two files are used only if SSL is enabled --&gt; &lt;ssl_cert_file&gt;/path/to/ssl_cert_file&lt;/ssl_cert_file&gt; &lt;ssl_key_file&gt;/path/to/ssl_key_file&lt;/ssl_key_file&gt; &lt;!-- Whether server requests client for a certificate --&gt; &lt;ssl_require_client_auth&gt;false&lt;/ssl_require_client_auth&gt; &lt;!-- The following file is used only if ssl_require_client_auth=true --&gt; &lt;ssl_ca_cert_file&gt;/path/to/ssl_ca_cert_file&lt;/ssl_ca_cert_file&gt; &lt;!-- Default compression algorithm (applied if client doesn't specify another algorithm, see result_compression in QueryInfo). Supported algorithms: none, deflate, gzip, stream_gzip --&gt; &lt;compression&gt;deflate&lt;/compression&gt; &lt;!-- Default compression level (applied if client doesn't specify another level, see result_compression in QueryInfo). Supported levels: none, low, medium, high --&gt; &lt;compression_level&gt;medium&lt;/compression_level&gt; &lt;!-- Send/receive message size limits in bytes. -1 means unlimited --&gt; &lt;max_send_message_size&gt;-1&lt;/max_send_message_size&gt; &lt;max_receive_message_size&gt;-1&lt;/max_receive_message_size&gt; &lt;!-- Enable if you want to get detailed logs --&gt; &lt;verbose_logs&gt;false&lt;/verbose_logs&gt; &lt;/grpc&gt;  "},{"title":"Built-in Client​","type":1,"pageTitle":"gRPC Interface","url":"en/interfaces/grpc#grpc-client","content":"You can write a client in any of the programming languages supported by gRPC using the provided specification. Or you can use a built-in Python client. It is placed in utils/grpc-client/clickhouse-grpc-client.py in the repository. The built-in client requires grpcio and grpcio-tools Python modules. The client supports the following arguments: --help – Shows a help message and exits.--host HOST, -h HOST – A server name. Default value: localhost. You can use IPv4 or IPv6 addresses also.--port PORT – A port to connect to. This port should be enabled in the ClickHouse server configuration (see grpc_port). Default value: 9100.--user USER_NAME, -u USER_NAME – A user name. Default value: default.--password PASSWORD – A password. Default value: empty string.--query QUERY, -q QUERY – A query to process when using non-interactive mode.--database DATABASE, -d DATABASE – A default database. If not specified, the current database set in the server settings is used (default by default).--format OUTPUT_FORMAT, -f OUTPUT_FORMAT – A result output format. Default value for interactive mode: PrettyCompact.--debug – Enables showing debug information. To run the client in an interactive mode call it without --query argument. In a batch mode query data can be passed via stdin. Client Usage Example In the following example a table is created and loaded with data from a CSV file. Then the content of the table is queried. ./clickhouse-grpc-client.py -q &quot;CREATE TABLE grpc_example_table (id UInt32, text String) ENGINE = MergeTree() ORDER BY id;&quot; echo &quot;0,Input data for&quot; &gt; a.txt ; echo &quot;1,gRPC protocol example&quot; &gt;&gt; a.txt cat a.txt | ./clickhouse-grpc-client.py -q &quot;INSERT INTO grpc_example_table FORMAT CSV&quot; ./clickhouse-grpc-client.py --format PrettyCompact -q &quot;SELECT * FROM grpc_example_table;&quot;  Result: ┌─id─┬─text──────────────────┐ │ 0 │ Input data for │ │ 1 │ gRPC protocol example │ └────┴───────────────────────┘  "},{"title":"Third-Party Interfaces","type":0,"sectionRef":"#","url":"en/interfaces/third-party/","content":"Third-Party Interfaces This is a collection of links to third-party tools that provide some sort of interface to ClickHouse. It can be either visual interface, command-line interface or an API: Client librariesIntegrationsGUIProxies note Generic tools that support common API like ODBC or JDBC usually can work with ClickHouse as well, but are not listed here because there are way too many of them.","keywords":""},{"title":"Client Libraries from Third-party Developers","type":0,"sectionRef":"#","url":"en/interfaces/third-party/client-libraries","content":"Client Libraries from Third-party Developers warning ClickHouse Inc does not maintain the libraries listed below and hasn’t done any extensive testing to ensure their quality. Python infi.clickhouse_ormclickhouse-driverclickhouse-clientaiochclientasynch PHP smi2/phpclickhouse8bitov/clickhouse-php-clientbozerkins/clickhouse-clientsimpod/clickhouse-clientseva-code/php-click-house-clientSeasClick C++ clientone-ckglushkovds/phpclickhouse-laravelkolya7k ClickHouse PHP extension Go clickhousego-clickhousechconnmailrugo-clickhousegolang-clickhouse Swift ClickHouseNIOClickHouseVapor ORM NodeJs clickhouse (NodeJs)node-clickhousenestjs-clickhouseclickhouse-client Perl perl-DBD-ClickHouseHTTP-ClickHouseAnyEvent-ClickHouse Ruby ClickHouse (Ruby)clickhouse-activerecord Rust Klickhouse R clickhouse-rRClickHouse Java clickhouse-client-javaclickhouse-client Scala clickhouse-scala-client Kotlin AORM C# Octonica.ClickHouseClientClickHouse.AdoClickHouse.ClientClickHouse.Net Elixir clickhousexpillar Nim nim-clickhouse Haskell hdbc-clickhouse Original article","keywords":""},{"title":"PostgreSQL Interface","type":0,"sectionRef":"#","url":"en/interfaces/postgresql","content":"","keywords":""},{"title":"Connect psql to ClickHouse​","type":1,"pageTitle":"PostgreSQL Interface","url":"en/interfaces/postgresql#connect-psql-to-clickhouse","content":"The following command demonstrates how to connect the PostgreSQL client psql to ClickHouse: psql -p [port] -h [hostname] -U [username] [database_name]  For example: psql -p 9005 -h 127.0.0.1 -U alice default  note The psql client requires a login with a password, so you will not be able connect using the default user with no password. Either assign a password to the default user, or login as a different user. The psql client prompts for the password: Password for user alice: psql (14.2, server 22.3.1.1) WARNING: psql major version 14, server major version 22. Some psql features might not work. Type &quot;help&quot; for help. default=&gt;  And that's it! You now have a PostgreSQL client connected to ClickHouse, and all commands and queries are executed on ClickHouse. caution The PostgreSQL protocol currently only supports plain-text passwords. "},{"title":"Using SSL​","type":1,"pageTitle":"PostgreSQL Interface","url":"en/interfaces/postgresql#using-ssl","content":"If you have SSL/TLS configured on your ClickHouse instance, then postgresql_port will use the same settings (the port is shared for both secure and unsecure clients). Each client has their own method of how to connect using SSL. The following command demonstrates how to pass in the certificates and key to securely connect psql to ClickHouse: psql &quot;port=9005 host=127.0.0.1 user=alice dbname=default sslcert=/path/to/certificate.pem sslkey=/path/to/key.pem sslrootcert=/path/to/rootcert.pem sslmode=verify-ca&quot;  View the PostgreSQL docs for more details on their SSL settings. Original article "},{"title":"Command-line Client","type":0,"sectionRef":"#","url":"en/interfaces/cli","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"Command-line Client","url":"en/interfaces/cli#cli_usage","content":"The client can be used in interactive and non-interactive (batch) mode. To use batch mode, specify the ‘query’ parameter, or send data to ‘stdin’ (it verifies that ‘stdin’ is not a terminal), or both. Similar to the HTTP interface, when using the ‘query’ parameter and sending data to ‘stdin’, the request is a concatenation of the ‘query’ parameter, a line feed, and the data in ‘stdin’. This is convenient for large INSERT queries. Example of using the client to insert data: $ echo -ne &quot;1, 'some text', '2016-08-14 00:00:00'\\n2, 'some more text', '2016-08-14 00:00:01'&quot; | clickhouse-client --database=test --query=&quot;INSERT INTO test FORMAT CSV&quot;; $ cat &lt;&lt;_EOF | clickhouse-client --database=test --query=&quot;INSERT INTO test FORMAT CSV&quot;; 3, 'some text', '2016-08-14 00:00:00' 4, 'some more text', '2016-08-14 00:00:01' _EOF $ cat file.csv | clickhouse-client --database=test --query=&quot;INSERT INTO test FORMAT CSV&quot;;  In batch mode, the default data format is TabSeparated. You can set the format in the FORMAT clause of the query. By default, you can only process a single query in batch mode. To make multiple queries from a “script,” use the --multiquery parameter. This works for all queries except INSERT. Query results are output consecutively without additional separators. Similarly, to process a large number of queries, you can run ‘clickhouse-client’ for each query. Note that it may take tens of milliseconds to launch the ‘clickhouse-client’ program. In interactive mode, you get a command line where you can enter queries. If ‘multiline’ is not specified (the default): To run the query, press Enter. The semicolon is not necessary at the end of the query. To enter a multiline query, enter a backslash \\ before the line feed. After you press Enter, you will be asked to enter the next line of the query. If multiline is specified: To run a query, end it with a semicolon and press Enter. If the semicolon was omitted at the end of the entered line, you will be asked to enter the next line of the query. Only a single query is run, so everything after the semicolon is ignored. You can specify \\G instead of or after the semicolon. This indicates Vertical format. In this format, each value is printed on a separate line, which is convenient for wide tables. This unusual feature was added for compatibility with the MySQL CLI. The command line is based on ‘replxx’ (similar to ‘readline’). In other words, it uses the familiar keyboard shortcuts and keeps a history. The history is written to ~/.clickhouse-client-history. By default, the format used is PrettyCompact. You can change the format in the FORMAT clause of the query, or by specifying \\G at the end of the query, using the --format or --vertical argument in the command line, or using the client configuration file. To exit the client, press Ctrl+D, or enter one of the following instead of a query: “exit”, “quit”, “logout”, “exit;”, “quit;”, “logout;”, “q”, “Q”, “:q” When processing a query, the client shows: Progress, which is updated no more than 10 times per second (by default). For quick queries, the progress might not have time to be displayed.The formatted query after parsing, for debugging.The result in the specified format.The number of lines in the result, the time passed, and the average speed of query processing. You can cancel a long query by pressing Ctrl+C. However, you will still need to wait for a little for the server to abort the request. It is not possible to cancel a query at certain stages. If you do not wait and press Ctrl+C a second time, the client will exit. The command-line client allows passing external data (external temporary tables) for querying. For more information, see the section “External data for query processing”. "},{"title":"Queries with Parameters​","type":1,"pageTitle":"Command-line Client","url":"en/interfaces/cli#cli-queries-with-parameters","content":"You can create a query with parameters and pass values to them from client application. This allows to avoid formatting query with specific dynamic values on client side. For example: $ clickhouse-client --param_parName=&quot;[1, 2]&quot; -q &quot;SELECT * FROM table WHERE a = {parName:Array(UInt16)}&quot;  Query Syntax​ Format a query as usual, then place the values that you want to pass from the app parameters to the query in braces in the following format: {&lt;name&gt;:&lt;data type&gt;}  name — Placeholder identifier. In the console client it should be used in app parameters as --param_&lt;name&gt; = value.data type — Data type of the app parameter value. For example, a data structure like (integer, ('string', integer)) can have the Tuple(UInt8, Tuple(String, UInt8)) data type (you can also use another integer types). It's also possible to pass table, database, column names as a parameter, in that case you would need to use Identifier as a data type. Example​ $ clickhouse-client --param_tuple_in_tuple=&quot;(10, ('dt', 10))&quot; -q &quot;SELECT * FROM table WHERE val = {tuple_in_tuple:Tuple(UInt8, Tuple(String, UInt8))}&quot; $ clickhouse-client --param_tbl=&quot;numbers&quot; --param_db=&quot;system&quot; --param_col=&quot;number&quot; --query &quot;SELECT {col:Identifier} FROM {db:Identifier}.{tbl:Identifier} LIMIT 10&quot;  "},{"title":"Configuring​","type":1,"pageTitle":"Command-line Client","url":"en/interfaces/cli#interfaces_cli_configuration","content":"You can pass parameters to clickhouse-client (all parameters have a default value) using: From the Command Line Command-line options override the default values and settings in configuration files. Configuration files. Settings in the configuration files override the default values. "},{"title":"Command Line Options​","type":1,"pageTitle":"Command-line Client","url":"en/interfaces/cli#command-line-options","content":"--host, -h – The server name, ‘localhost’ by default. You can use either the name or the IPv4 or IPv6 address.--port – The port to connect to. Default value: 9000. Note that the HTTP interface and the native interface use different ports.--user, -u – The username. Default value: default.--password – The password. Default value: empty string.--query, -q – The query to process when using non-interactive mode. You must specify either query or queries-file option.--queries-file, -qf – file path with queries to execute. You must specify either query or queries-file option.--database, -d – Select the current default database. Default value: the current database from the server settings (‘default’ by default).--multiline, -m – If specified, allow multiline queries (do not send the query on Enter).--multiquery, -n – If specified, allow processing multiple queries separated by semicolons.--format, -f – Use the specified default format to output the result.--vertical, -E – If specified, use the Vertical format by default to output the result. This is the same as –format=Vertical. In this format, each value is printed on a separate line, which is helpful when displaying wide tables.--time, -t – If specified, print the query execution time to ‘stderr’ in non-interactive mode.--stacktrace – If specified, also print the stack trace if an exception occurs.--config-file – The name of the configuration file.--secure – If specified, will connect to server over secure connection (TLS). You might need to configure your CA certificates in the configuration file. The available configuration settings are the same as for server-side TLS configuration.--history_file — Path to a file containing command history.--param_&lt;name&gt; — Value for a query with parameters.--hardware-utilization — Print hardware utilization information in progress bar.--print-profile-events – Print ProfileEvents packets.--profile-events-delay-ms – Delay between printing ProfileEvents packets (-1 - print only totals, 0 - print every single packet). Since version 20.5, clickhouse-client has automatic syntax highlighting (always enabled). "},{"title":"Configuration Files​","type":1,"pageTitle":"Command-line Client","url":"en/interfaces/cli#configuration_files","content":"clickhouse-client uses the first existing file of the following: Defined in the --config-file parameter../clickhouse-client.xml~/.clickhouse-client/config.xml/etc/clickhouse-client/config.xml Example of a config file: &lt;config&gt; &lt;user&gt;username&lt;/user&gt; &lt;password&gt;password&lt;/password&gt; &lt;secure&gt;true&lt;/secure&gt; &lt;openSSL&gt; &lt;client&gt; &lt;caConfig&gt;/etc/ssl/cert.pem&lt;/caConfig&gt; &lt;/client&gt; &lt;/openSSL&gt; &lt;/config&gt;  "},{"title":"Query ID Format​","type":1,"pageTitle":"Command-line Client","url":"en/interfaces/cli#query-id-format","content":"In interactive mode clickhouse-client shows query ID for every query. By default, the ID is formatted like this: Query id: 927f137d-00f1-4175-8914-0dd066365e96  A custom format may be specified in a configuration file inside a query_id_formats tag. {query_id} placeholder in the format string is replaced with the ID of a query. Several format strings are allowed inside the tag. This feature can be used to generate URLs to facilitate profiling of queries. Example &lt;config&gt; &lt;query_id_formats&gt; &lt;speedscope&gt;http://speedscope-host/#profileURL=qp%3Fid%3D{query_id}&lt;/speedscope&gt; &lt;/query_id_formats&gt; &lt;/config&gt;  If the configuration above is applied, the ID of a query is shown in the following format: speedscope:http://speedscope-host/#profileURL=qp%3Fid%3Dc8ecc783-e753-4b38-97f1-42cddfb98b7d  "},{"title":"What Is ClickHouse?","type":0,"sectionRef":"#","url":"en/intro","content":"","keywords":""},{"title":"Key Properties of OLAP Scenario​","type":1,"pageTitle":"What Is ClickHouse?","url":"en/intro#key-properties-of-olap-scenario","content":"The vast majority of requests are for read access.Data is updated in fairly large batches (&gt; 1000 rows), not by single rows; or it is not updated at all.Data is added to the DB but is not modified.For reads, quite a large number of rows are extracted from the DB, but only a small subset of columns.Tables are “wide,” meaning they contain a large number of columns.Queries are relatively rare (usually hundreds of queries per server or less per second).For simple queries, latencies around 50 ms are allowed.Column values are fairly small: numbers and short strings (for example, 60 bytes per URL).Requires high throughput when processing a single query (up to billions of rows per second per server).Transactions are not necessary.Low requirements for data consistency.There is one large table per query. All tables are small, except for one.A query result is significantly smaller than the source data. In other words, data is filtered or aggregated, so the result fits in a single server’s RAM. It is easy to see that the OLAP scenario is very different from other popular scenarios (such as OLTP or Key-Value access). So it does not make sense to try to use OLTP or a Key-Value DB for processing analytical queries if you want to get decent performance. For example, if you try to use MongoDB or Redis for analytics, you will get very poor performance compared to OLAP databases. "},{"title":"Why Column-Oriented Databases Work Better in the OLAP Scenario​","type":1,"pageTitle":"What Is ClickHouse?","url":"en/intro#why-column-oriented-databases-work-better-in-the-olap-scenario","content":"Column-oriented databases are better suited to OLAP scenarios: they are at least 100 times faster in processing most queries. The reasons are explained in detail below, but the fact is easier to demonstrate visually: Row-oriented DBMS  Column-oriented DBMS  See the difference? "},{"title":"Input/output​","type":1,"pageTitle":"What Is ClickHouse?","url":"en/intro#inputoutput","content":"For an analytical query, only a small number of table columns need to be read. In a column-oriented database, you can read just the data you need. For example, if you need 5 columns out of 100, you can expect a 20-fold reduction in I/O.Since data is read in packets, it is easier to compress. Data in columns is also easier to compress. This further reduces the I/O volume.Due to the reduced I/O, more data fits in the system cache. For example, the query “count the number of records for each advertising platform” requires reading one “advertising platform ID” column, which takes up 1 byte uncompressed. If most of the traffic was not from advertising platforms, you can expect at least 10-fold compression of this column. When using a quick compression algorithm, data decompression is possible at a speed of at least several gigabytes of uncompressed data per second. In other words, this query can be processed at a speed of approximately several billion rows per second on a single server. This speed is actually achieved in practice. "},{"title":"CPU​","type":1,"pageTitle":"What Is ClickHouse?","url":"en/intro#cpu","content":"Since executing a query requires processing a large number of rows, it helps to dispatch all operations for entire vectors instead of for separate rows, or to implement the query engine so that there is almost no dispatching cost. If you do not do this, with any half-decent disk subsystem, the query interpreter inevitably stalls the CPU. It makes sense to both store data in columns and process it, when possible, by columns. There are two ways to do this: A vector engine. All operations are written for vectors, instead of for separate values. This means you do not need to call operations very often, and dispatching costs are negligible. Operation code contains an optimized internal cycle. Code generation. The code generated for the query has all the indirect calls in it. This is not done in “normal” databases, because it does not make sense when running simple queries. However, there are exceptions. For example, MemSQL uses code generation to reduce latency when processing SQL queries. (For comparison, analytical DBMSs require optimization of throughput, not latency.) Note that for CPU efficiency, the query language must be declarative (SQL or MDX), or at least a vector (J, K). The query should only contain implicit loops, allowing for optimization. "},{"title":"Proxy Servers from Third-party Developers","type":0,"sectionRef":"#","url":"en/interfaces/third-party/proxy","content":"","keywords":""},{"title":"chproxy​","type":1,"pageTitle":"Proxy Servers from Third-party Developers","url":"en/interfaces/third-party/proxy#chproxy","content":"chproxy, is an HTTP proxy and load balancer for ClickHouse database. Features: Per-user routing and response caching.Flexible limits.Automatic SSL certificate renewal. Implemented in Go. "},{"title":"KittenHouse​","type":1,"pageTitle":"Proxy Servers from Third-party Developers","url":"en/interfaces/third-party/proxy#kittenhouse","content":"KittenHouse is designed to be a local proxy between ClickHouse and application server in case it’s impossible or inconvenient to buffer INSERT data on your application side. Features: In-memory and on-disk data buffering.Per-table routing.Load-balancing and health checking. Implemented in Go. "},{"title":"ClickHouse-Bulk​","type":1,"pageTitle":"Proxy Servers from Third-party Developers","url":"en/interfaces/third-party/proxy#clickhouse-bulk","content":"ClickHouse-Bulk is a simple ClickHouse insert collector. Features: Group requests and send by threshold or interval.Multiple remote servers.Basic authentication. Implemented in Go. Original article "},{"title":"Access Control and Account Management","type":0,"sectionRef":"#","url":"en/operations/access-rights","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"Access Control and Account Management","url":"en/operations/access-rights#access-control-usage","content":"By default, the ClickHouse server provides the default user account which is not allowed using SQL-driven access control and account management but has all the rights and permissions. The default user account is used in any cases when the username is not defined, for example, at login from client or in distributed queries. In distributed query processing a default user account is used, if the configuration of the server or cluster does not specify the user and password properties. If you just started using ClickHouse, consider the following scenario: Enable SQL-driven access control and account management for the default user.Log in to the default user account and create all the required users. Don’t forget to create an administrator account (GRANT ALL ON *.* TO admin_user_account WITH GRANT OPTION).Restrict permissions for the default user and disable SQL-driven access control and account management for it. "},{"title":"Properties of Current Solution​","type":1,"pageTitle":"Access Control and Account Management","url":"en/operations/access-rights#access-control-properties","content":"You can grant permissions for databases and tables even if they do not exist.If a table was deleted, all the privileges that correspond to this table are not revoked. This means that even if you create a new table with the same name later, all the privileges remain valid. To revoke privileges corresponding to the deleted table, you need to execute, for example, the REVOKE ALL PRIVILEGES ON db.table FROM ALL query.There are no lifetime settings for privileges. "},{"title":"User Account​","type":1,"pageTitle":"Access Control and Account Management","url":"en/operations/access-rights#user-account-management","content":"A user account is an access entity that allows to authorize someone in ClickHouse. A user account contains: Identification information.Privileges that define a scope of queries the user can execute.Hosts allowed to connect to the ClickHouse server.Assigned and default roles.Settings with their constraints applied by default at user login.Assigned settings profiles. Privileges can be granted to a user account by the GRANT query or by assigning roles. To revoke privileges from a user, ClickHouse provides the REVOKE query. To list privileges for a user, use the SHOW GRANTS statement. Management queries: CREATE USERALTER USERDROP USERSHOW CREATE USERSHOW USERS "},{"title":"Settings Applying​","type":1,"pageTitle":"Access Control and Account Management","url":"en/operations/access-rights#access-control-settings-applying","content":"Settings can be configured differently: for a user account, in its granted roles and in settings profiles. At user login, if a setting is configured for different access entities, the value and constraints of this setting are applied as follows (from higher to lower priority): User account settings.The settings of default roles of the user account. If a setting is configured in some roles, then order of the setting application is undefined.The settings from settings profiles assigned to a user or to its default roles. If a setting is configured in some profiles, then order of setting application is undefined.Settings applied to all the server by default or from the default profile. "},{"title":"Role​","type":1,"pageTitle":"Access Control and Account Management","url":"en/operations/access-rights#role-management","content":"Role is a container for access entities that can be granted to a user account. Role contains: PrivilegesSettings and constraintsList of assigned roles Management queries: CREATE ROLEALTER ROLEDROP ROLESET ROLESET DEFAULT ROLESHOW CREATE ROLESHOW ROLES Privileges can be granted to a role by the GRANT query. To revoke privileges from a role ClickHouse provides the REVOKE query. "},{"title":"Row Policy​","type":1,"pageTitle":"Access Control and Account Management","url":"en/operations/access-rights#row-policy-management","content":"Row policy is a filter that defines which of the rows are available to a user or a role. Row policy contains filters for one particular table, as well as a list of roles and/or users which should use this row policy. warning Row policies makes sense only for users with readonly access. If user can modify table or copy partitions between tables, it defeats the restrictions of row policies. Management queries: CREATE ROW POLICYALTER ROW POLICYDROP ROW POLICYSHOW CREATE ROW POLICYSHOW POLICIES "},{"title":"Settings Profile​","type":1,"pageTitle":"Access Control and Account Management","url":"en/operations/access-rights#settings-profiles-management","content":"Settings profile is a collection of settings. Settings profile contains settings and constraints, as well as a list of roles and/or users to which this profile is applied. Management queries: CREATE SETTINGS PROFILEALTER SETTINGS PROFILEDROP SETTINGS PROFILESHOW CREATE SETTINGS PROFILESHOW PROFILES "},{"title":"Quota​","type":1,"pageTitle":"Access Control and Account Management","url":"en/operations/access-rights#quotas-management","content":"Quota limits resource usage. See Quotas. Quota contains a set of limits for some durations, as well as a list of roles and/or users which should use this quota. Management queries: CREATE QUOTAALTER QUOTADROP QUOTASHOW CREATE QUOTASHOW QUOTASHOW QUOTAS "},{"title":"Enabling SQL-driven Access Control and Account Management​","type":1,"pageTitle":"Access Control and Account Management","url":"en/operations/access-rights#enabling-access-control","content":"Setup a directory for configurations storage. ClickHouse stores access entity configurations in the folder set in the access_control_path server configuration parameter. Enable SQL-driven access control and account management for at least one user account. By default, SQL-driven access control and account management is disabled for all users. You need to configure at least one user in the users.xml configuration file and set the value of the access_management setting to 1. Original article "},{"title":"S3 Backed MergeTree","type":0,"sectionRef":"#","url":"en/integrations/s3/s3-merge-tree","content":"","keywords":""},{"title":"Storage Tiers​","type":1,"pageTitle":"S3 Backed MergeTree","url":"en/integrations/s3/s3-merge-tree#storage-tiers","content":"ClickHouse storage volumes allow physical disks to be abstracted from the MergeTree table engine. Any single volume can be composed of an ordered set of disks. Whilst principally allowing multiple block devices to be potentially used for data storage, this abstraction also allows other storage types, including S3. ClickHouse data parts can be moved between volumes and fill rates according to storage policies, thus creating the concept of storage tiers. Storage tiers unlock hot-cold architectures where the most recent data, which is typically also the most queried, requires only a small amount of space on high-performing storage, e.g., NVMe SSDs. As the data ages, SLAs for query times increase, as does query frequency. This fat tail of data can be stored on slower, less performant storage such as HDD or object storage such as S3. "},{"title":"Creating a Disk​","type":1,"pageTitle":"S3 Backed MergeTree","url":"en/integrations/s3/s3-merge-tree#creating-a-disk","content":"To utilize an S3 bucket as a disk, we must first declare it within the ClickHouse configuration file. Either extend config.xml or preferably provide a new file under conf.d. An example of an S3 disk declaration is shown below: &lt;clickhouse&gt; &lt;storage_configuration&gt; ... &lt;disks&gt; &lt;s3&gt; &lt;type&gt;s3&lt;/type&gt; &lt;endpoint&gt;https://sample-bucket.s3.us-east-2.amazonaws.com/tables/&lt;/endpoint&gt; &lt;access_key_id&gt;your_access_key_id&lt;/access_key_id&gt; &lt;secret_access_key&gt;your_secret_access_key&lt;/secret_access_key&gt; &lt;region&gt;&lt;/region&gt; &lt;metadata_path&gt;/var/lib/clickhouse/disks/s3/&lt;/metadata_path&gt; &lt;cache_enabled&gt;true&lt;/cache_enabled&gt; &lt;data_cache_enabled&gt;true&lt;/data_cache_enabled&gt; &lt;cache_path&gt;/var/lib/clickhouse/disks/s3/cache/&lt;/cache_path&gt; &lt;/s3&gt; &lt;/disks&gt; ... &lt;/storage_configuration&gt; &lt;/clickhouse&gt;  A complete list of settings relevant to this disk declaration can be found here. Note that credentials can be managed here using the same approaches described in Managing credentials, i.e., the use_environment_credentials can be set to true in the above settings block to use IAM roles. Further information on the cache settings can be found under Internals. "},{"title":"Creating a Storage Policy​","type":1,"pageTitle":"S3 Backed MergeTree","url":"en/integrations/s3/s3-merge-tree#creating-a-storage-policy","content":"Once configured, this “disk” can be used by a storage volume declared within a policy. For the example below, we assume s3 is our only storage. This ignores more complex hot-cold architectures where data can be relocated based on TTLs and fill rates. &lt;clickhouse&gt; &lt;storage_configuration&gt; &lt;disks&gt; &lt;s3&gt; ... &lt;/s3&gt; &lt;/disks&gt; &lt;policies&gt; &lt;s3_main&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;s3&lt;/disk&gt; &lt;/main&gt; &lt;/volumes&gt; &lt;/s3_main&gt; &lt;/policies&gt; &lt;/storage_configuration&gt; &lt;/clickhouse&gt;  "},{"title":"Creating a table​","type":1,"pageTitle":"S3 Backed MergeTree","url":"en/integrations/s3/s3-merge-tree#creating-a-table","content":"Assuming you have configured your disk to use a bucket with write access, you should be able to create a table such as in the example below. For purposes of brevity, we use a subset of the NYC taxi columns and stream data directly to the s3 backed table: CREATE TABLE trips_s3 ( `trip_id` UInt32, `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_datetime` DateTime, `pickup_longitude` Float64, `pickup_latitude` Float64, `dropoff_longitude` Float64, `dropoff_latitude` Float64, `passenger_count` UInt8, `trip_distance` Float64, `tip_amount` Float32, `total_amount` Float32, `payment_type` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4) ) ENGINE = MergeTree PARTITION BY toYYYYMM(pickup_date) ORDER BY pickup_datetime SETTINGS index_granularity = 8192, storage_policy='s3_main'  INSERT INTO trips_s3 SELECT trip_id, pickup_date, pickup_datetime, dropoff_datetime, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count, trip_distance, tip_amount, total_amount, payment_type FROM s3('https://ch-nyc-taxi.s3.eu-west-3.amazonaws.com/tsv/trips_{0..9}.tsv.gz', 'TabSeparatedWithNames') LIMIT 1000000;  Depending on the hardware, this latter insert of 1m rows may take a few minutes to execute. You can confirm the progress via the system.processes table. Feel free to adjust the row count up to the limit of 10m and explore some sample queries. SELECT passenger_count, avg(tip_amount) as avg_tip, avg(total_amount) as avg_amount FROM trips_s3 GROUP BY passenger_count;  "},{"title":"Modifying a Table​","type":1,"pageTitle":"S3 Backed MergeTree","url":"en/integrations/s3/s3-merge-tree#modifying-a-table","content":"Occasionally users may need to modify the storage policy of a specific table. Whilst this is possible, it comes with limitations. The new target policy must contain all of the disks and volumes of the previous policy, i.e., data will not be migrated to satisfy a policy change. When validating these constraints, volumes and disks will be identified by their name, with attempts to violate resulting in an error. However, assuming you use the previous examples, the following changes are valid. &lt;policies&gt; &lt;s3_main&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;s3&lt;/disk&gt; &lt;/main&gt; &lt;/volumes&gt; &lt;/s3_main&gt; &lt;s3_tiered&gt; &lt;volumes&gt; &lt;hot&gt; &lt;disk&gt;default&lt;/disk&gt; &lt;/hot&gt; &lt;main&gt; &lt;disk&gt;s3&lt;/disk&gt; &lt;/main&gt; &lt;/volumes&gt; &lt;move_factor&gt;0.2&lt;/move_factor&gt; &lt;/s3_tiered&gt; &lt;/policies&gt;  ALTER TABLE trips_s3 MODIFY SETTING storage_policy='s3_tiered'  Here we reuse the main volume in our new s3_tiered policy and introduce a new hot volume. This uses the default disk, which consists of only one disk configured via the parameter &lt;path&gt;. Note that our volume names and disks do not change. New inserts to our table will reside on the default disk until this reaches move_factor * disk_size - at which data will be relocated to S3. "},{"title":"Handling Replication​","type":1,"pageTitle":"S3 Backed MergeTree","url":"en/integrations/s3/s3-merge-tree#handling-replication","content":"For traditional disk-backed tables, we rely on ClickHouse to handle data replication via the ReplicatedTableEngine. Whilst for S3, this replication is inherently handled at the storage layer, local files are still held for the table on disk. Specifically, ClickHouse stores metadata data files on disk (see Internals) for further details. These files will be replicated if using a ReplicatedMergeTree in a process known as Zero Copy Replication. This is enabled by default through the setting allow_remote_fs_zero_copy_replication. This is best illustrated below where the table exists on 2 ClickHouse nodes:  "},{"title":"Internals​","type":1,"pageTitle":"S3 Backed MergeTree","url":"en/integrations/s3/s3-merge-tree#internals","content":""},{"title":"Read & Writes​","type":1,"pageTitle":"S3 Backed MergeTree","url":"en/integrations/s3/s3-merge-tree#read--writes","content":"The following notes cover the implementation of S3 interactions with ClickHouse. Whilst generally only informative, it may help the readers when Optimizing for Performance: By default, the maximum number of query processing threads used by any stage of the query processing pipeline is equal to the number of cores. Some stages are more parallelizable than others, so this value provides an upper bound. Multiple query stages may execute at once since data is streamed from the disk. The exact number of threads used for a query may thus exceed this. Modify through the setting max_threads.Reads on S3 are asynchronous by default. This behavior is determined by setting remote_filesystem_read_method, set to the value threadpool by default. When serving a request, ClickHouse reads granules in stripes. Each of these stripes potentially contain many columns. A thread will read the columns for their granules one by one. Rather than doing this synchronously, a prefetch is made for all columns before waiting for the data. This offers significant performance improvements over synchronous waits on each column. Users will not need to change this setting in most cases - see Optimizing for Performance.For the s3 function and table, parallel downloading is determined by the values max_download_threads and max_download_buffer_size. Files will only be downloaded in parallel if their size is greater than the total buffer size combined across all threads. This is only available on versions &gt; 22.3.1.Writes are performed in parallel, with a maximum of 100 concurrent file writing threads. max_insert_delayed_streams_for_parallel_write, which has a default value of 1000, controls the number of S3 blobs written in parallel. Since a buffer is required for each file being written (~1MB), this effectively limits the memory consumption of an INSERT. It may be appropriate to lower this value in low server memory scenarios. For further information on tuning threads, see Optimizing for Performance. Important: as of 22.3.1, there are two settings to enable the cache data_cache_enabled and enable_filesystem_cache. We recommend setting both of these 1 to enable the new cache behavior described, which supports the eviction of index files. To disable the eviction of index and mark files from the cache, we also recommend setting cache_enabled=1. To accelerate reads, s3 files are cached on the local filesystem by breaking files into segments. Any contiguous read segments are saved in the cache, with overlapping segments reused. Later versions will optionally allow writes resulting from INSERTs or merges to be stored in the cache via the option enable_filesystem_cache_on_write_operations. Where possible, the cache is reused for file reads. ClickHouse’s linear reads lend themselves to this caching strategy. Should a contiguous read result in a cache miss, the segment is downloaded and cached. Eviction occurs on an LRU basis per segment. The removal of a file also causes its removal from the cache. The setting read_from_cache_if_exists_otherwise_bypass_cache can be set to 1 for specific queries which you know are not cache efficient. These queries might be known to be unfriendly to the cache and result in heavy evictions. The metadata for the cache (entries and last used time) is held in memory for fast access. On restarts of ClickHouse, this metadata is reconstructed from the files on disk with the loss of the last used time. In this case, the value is set to 0, causing random eviction until the values are fully populated. The max cache size can be specified in bytes through the setting max_cache_size. This defaults to 1GB (subject to change). Index and mark files can be evicted from the cache. The FS page cache can efficiently cache all files. Enabling the cache can speed up first-time queries for which the data is not resident in the cache. If a query needs to re-access data that has been cached as part of its execution, the fs page cache can be utilized - thus avoiding re-reads from s3. Finally, merges on data residing in s3 are potentially a performant bottleneck if not performed intelligently. Cached versions of files minimize merges performed directly on the remote storage. "},{"title":"Interfaces","type":0,"sectionRef":"#","url":"en/interfaces/overview","content":"Interfaces ClickHouse provides three network interfaces (they can be optionally wrapped in TLS for additional security): HTTP, which is documented and easy to use directly.Native TCP, which has less overhead.gRPC. In most cases it is recommended to use an appropriate tool or library instead of interacting with those directly. The following are officially supported by ClickHouse: Command-line clientJDBC driverODBC driverC++ client library There are also a wide range of third-party libraries for working with ClickHouse: Client librariesIntegrationsVisual interfaces","keywords":"clickhouse network interfaces http tcp grpc command-line client jdbc odbc driver"},{"title":"Data Backup","type":0,"sectionRef":"#","url":"en/operations/backup","content":"","keywords":""},{"title":"Duplicating Source Data Somewhere Else​","type":1,"pageTitle":"Data Backup","url":"en/operations/backup#duplicating-source-data-somewhere-else","content":"Often data that is ingested into ClickHouse is delivered through some sort of persistent queue, such as Apache Kafka. In this case it is possible to configure an additional set of subscribers that will read the same data stream while it is being written to ClickHouse and store it in cold storage somewhere. Most companies already have some default recommended cold storage, which could be an object store or a distributed filesystem like HDFS. "},{"title":"Filesystem Snapshots​","type":1,"pageTitle":"Data Backup","url":"en/operations/backup#filesystem-snapshots","content":"Some local filesystems provide snapshot functionality (for example, ZFS), but they might not be the best choice for serving live queries. A possible solution is to create additional replicas with this kind of filesystem and exclude them from the Distributed tables that are used for SELECT queries. Snapshots on such replicas will be out of reach of any queries that modify data. As a bonus, these replicas might have special hardware configurations with more disks attached per server, which would be cost-effective. "},{"title":"clickhouse-copier​","type":1,"pageTitle":"Data Backup","url":"en/operations/backup#clickhouse-copier","content":"clickhouse-copier is a versatile tool that was initially created to re-shard petabyte-sized tables. It can also be used for backup and restore purposes because it reliably copies data between ClickHouse tables and clusters. For smaller volumes of data, a simple INSERT INTO ... SELECT ... to remote tables might work as well. "},{"title":"Manipulations with Parts​","type":1,"pageTitle":"Data Backup","url":"en/operations/backup#manipulations-with-parts","content":"ClickHouse allows using the ALTER TABLE ... FREEZE PARTITION ... query to create a local copy of table partitions. This is implemented using hardlinks to the /var/lib/clickhouse/shadow/ folder, so it usually does not consume extra disk space for old data. The created copies of files are not handled by ClickHouse server, so you can just leave them there: you will have a simple backup that does not require any additional external system, but it will still be prone to hardware issues. For this reason, it’s better to remotely copy them to another location and then remove the local copies. Distributed filesystems and object stores are still a good options for this, but normal attached file servers with a large enough capacity might work as well (in this case the transfer will occur via the network filesystem or maybe rsync). Data can be restored from backup using the ALTER TABLE ... ATTACH PARTITION ... For more information about queries related to partition manipulations, see the ALTER documentation. A third-party tool is available to automate this approach: clickhouse-backup. Original article "},{"title":"S3 Table Engines","type":0,"sectionRef":"#","url":"en/integrations/s3/s3-table-engine","content":"","keywords":""},{"title":"Reading Data​","type":1,"pageTitle":"S3 Table Engines","url":"en/integrations/s3/s3-table-engine#reading-data","content":"In the following example, we create a table trips_raw using the first ten tsv files located within the nyc-taxi bucket. Each of these contains 1m rows each. CREATE TABLE trips_raw ( `trip_id` UInt32, `vendor_id` Enum8('1' = 1, '2' = 2, '3' = 3, '4' = 4, 'CMT' = 5, 'VTS' = 6, 'DDS' = 7, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14, '' = 15), `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_date` Date, `dropoff_datetime` DateTime, `store_and_fwd_flag` UInt8, `rate_code_id` UInt8, `pickup_longitude` Float64, `pickup_latitude` Float64, `dropoff_longitude` Float64, `dropoff_latitude` Float64, `passenger_count` UInt8, `trip_distance` Float64, `fare_amount` Float32, `extra` Float32, `mta_tax` Float32, `tip_amount` Float32, `tolls_amount` Float32, `ehail_fee` Float32, `improvement_surcharge` Float32, `total_amount` Float32, `payment_type_` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4), `trip_type` UInt8, `pickup` FixedString(25), `dropoff` FixedString(25), `cab_type` Enum8('yellow' = 1, 'green' = 2, 'uber' = 3), `pickup_nyct2010_gid` Int8, `pickup_ctlabel` Float32, `pickup_borocode` Int8, `pickup_ct2010` String, `pickup_boroct2010` FixedString(7), `pickup_cdeligibil` String, `pickup_ntacode` FixedString(4), `pickup_ntaname` String, `pickup_puma` UInt16, `dropoff_nyct2010_gid` UInt8, `dropoff_ctlabel` Float32, `dropoff_borocode` UInt8, `dropoff_ct2010` String, `dropoff_boroct2010` FixedString(7), `dropoff_cdeligibil` String, `dropoff_ntacode` FixedString(4), `dropoff_ntaname` String, `dropoff_puma` UInt16 ) ENGINE = S3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_{0..9}.tsv.gz', 'TabSeparatedWithNames', 'gzip');  Notice the use of the {0..9} pattern to limit to the first ten files. Once created, we can query this table like any other e.g. _path\t_file\ttrip_id\tpickup_date\ttotal_amountdatasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999902\t2015-07-07\t19.56 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999919\t2015-07-07\t10.3 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999944\t2015-07-07\t24.3 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999969\t2015-07-07\t9.95 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999990\t2015-07-08\t9.8 SELECT payment_type, max(tip_amount) as max_tip FROM trips_raw GROUP BY payment_type;  payment_type\tmax_tipUNK\t0 CSH\t800 CRE\t53.06 NOC\t100 DIS\t100 "},{"title":"Inserting Data​","type":1,"pageTitle":"S3 Table Engines","url":"en/integrations/s3/s3-table-engine#inserting-data","content":"Whilst this table engine supports parallel reads, writes are only supported if the table definition does not contain glob patterns. The above table, therefore, would block writes. To illustrate writes, create the following table: CREATE TABLE trips_dest ( `trip_id` UInt32, `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_datetime` DateTime, `tip_amount` Float32, `total_amount` Float32 ) ENGINE = S3('&lt;bucket path&gt;/trips.bin', 'Native');  This query requires write access to the bucket INSERT INTO trips_dest SELECT trip_id, pickup_date, pickup_datetime, dropoff_datetime, tip_amount, total_amount FROM trips LIMIT 10;  SELECT * FROM trips_dest LIMIT 5;  trip_id\tpickup_date\tpickup_datetime\tdropoff_datetime\ttip_amount\ttotal_amount14\t2013-08-02\t2013-08-02 09:43:58\t2013-08-02 09:44:13\t0\t2 15\t2013-08-02\t2013-08-02 09:44:43\t2013-08-02 09:45:15\t0\t2 21\t2013-08-02\t2013-08-02 11:30:00\t2013-08-02 17:08:00\t0\t172 21\t2013-08-02\t2013-08-02 12:30:00\t2013-08-02 18:08:00\t0\t172 23\t2013-08-02\t2013-08-02 18:00:50\t2013-08-02 18:01:55\t0\t6.5 Note that rows can only be inserted into new files. There are no merge cycles or file split operations. Once a file is written, subsequent inserts will fail. Users have two options here: Specify the setting s3_create_new_file_on_insert=1. This will cause the creation of new files on each insert. A numeric suffix will be appended to the end of each file that will monotonically increase for each insert operation. For the above example, a subsequent insert would cause the creation of a trips_1.bin file.Specify the setting s3_truncate_on_insert=1. This will cause a truncation of the file, i.e. it will only contain the newly inserted rows once complete. Both of these settings default to 0 - thus forcing the user to set one of them. s3_truncate_on_insert will take precedence if both are set. "},{"title":"Miscellaneous​","type":1,"pageTitle":"S3 Table Engines","url":"en/integrations/s3/s3-table-engine#miscellaneous","content":"Unlike a traditional merge tree family table, dropping an s3 table will not delete the underlying data. Full settings for this table type can be found here. Be aware of the following caveats when using this engine: ALTER queries are not supportedSAMPLE operations are not supportedThere is no notion of indexes, i.e. primary or skip. "},{"title":"Managing Credentials​","type":1,"pageTitle":"S3 Table Engines","url":"en/integrations/s3/s3-table-engine#managing-credentials","content":"In the previous examples, we have passed credentials in the s3 function or table definition. Whilst this may be acceptable for occasional usage, users require less explicit authentication mechanisms in production. To address this, ClickHouse has several options: In the previous examples, we have passed credentials in the s3 function or table definition. Whilst this may be acceptable for occasional usage, users require less explicit authentication mechanisms in production. To address this, ClickHouse has several options: Specify the connection details in the config.xml or an equivalent configuration file under conf.d. The contents of an example file are shown below, assuming installation using the debian package. ubuntu@single-node-clickhouse:/etc/clickhouse-server/config.d$ cat s3.xml &lt;clickhouse&gt; &lt;s3&gt; &lt;endpoint-name&gt; &lt;endpoint&gt;https://dalem-files.s3.amazonaws.com/test/&lt;/endpoint&gt; &lt;access_key_id&gt;key&lt;/access_key_id&gt; &lt;secret_access_key&gt;secret&lt;/secret_access_key&gt; &lt;!-- &lt;use_environment_credentials&gt;false&lt;/use_environment_credentials&gt; --&gt; &lt;!-- &lt;header&gt;Authorization: Bearer SOME-TOKEN&lt;/header&gt; --&gt; &lt;/endpoint-name&gt; &lt;/s3&gt; &lt;/clickhouse&gt; These credentials will be used for any requests where the endpoint above is an exact prefix match for the requested URL. Also, note the ability in this example to declare an authorization header as an alternative to access and secret keys. A complete list of supported settings can be found here. The example above highlights the availability of the configuration parameter use_environment_credentials. This configuration parameter can also be set globally at the s3 level i.e. &lt;clickhouse&gt; &lt;s3&gt; &lt;use_environment_credentials&gt;true&lt;/use_environment_credentials&gt; &lt;s3&gt; &lt;/clickhouse&gt; This setting turns on an attempt to retrieve s3 credentials from the environment, thus allowing access through IAM roles. Specifically, the following order of retrieval is performed: * A lookup for the environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and AWS_SESSION_TOKEN * Check performed in $HOME/.aws * Temporary credentials obtained via the AWS Security Token Service - i.e. vi[a AssumeRole](https://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html) API * Checks for credentials in the ECS environment variables AWS_CONTAINER_CREDENTIALS_RELATIVE_URI or AWS_CONTAINER_CREDENTIALS_FULL_URI and AWS_ECS_CONTAINER_AUTHORIZATION_TOKEN. * Obtains the credentials via [Amazon EC2 instance metadata](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-metadata.html) provided [AWS_EC2_METADATA_DISABLED](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-envvars.html#envvars-list-AWS_EC2_METADATA_DISABLED) is not set to true.  These same settings can also be set for a specific endpoint, using the same prefix matching rule. "},{"title":"S3 Table Functions","type":0,"sectionRef":"#","url":"en/integrations/s3/s3-table-functions","content":"","keywords":""},{"title":"Preparation​","type":1,"pageTitle":"S3 Table Functions","url":"en/integrations/s3/s3-table-functions#preparation","content":"To interact with our s3 based dataset, we prepare a standard merge tree table as our destination. The statement below creates this table under the default database. CREATE TABLE trips ( `trip_id` UInt32, `vendor_id` Enum8('1' = 1, '2' = 2, '3' = 3, '4' = 4, 'CMT' = 5, 'VTS' = 6, 'DDS' = 7, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14, '' = 15), `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_date` Date, `dropoff_datetime` DateTime, `store_and_fwd_flag` UInt8, `rate_code_id` UInt8, `pickup_longitude` Float64, `pickup_latitude` Float64, `dropoff_longitude` Float64, `dropoff_latitude` Float64, `passenger_count` UInt8, `trip_distance` Float64, `fare_amount` Float32, `extra` Float32, `mta_tax` Float32, `tip_amount` Float32, `tolls_amount` Float32, `ehail_fee` Float32, `improvement_surcharge` Float32, `total_amount` Float32, `payment_type` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4), `trip_type` UInt8, `pickup` FixedString(25), `dropoff` FixedString(25), `cab_type` Enum8('yellow' = 1, 'green' = 2, 'uber' = 3), `pickup_nyct2010_gid` Int8, `pickup_ctlabel` Float32, `pickup_borocode` Int8, `pickup_ct2010` String, `pickup_boroct2010` String, `pickup_cdeligibil` String, `pickup_ntacode` FixedString(4), `pickup_ntaname` String, `pickup_puma` UInt16, `dropoff_nyct2010_gid` UInt8, `dropoff_ctlabel` Float32, `dropoff_borocode` UInt8, `dropoff_ct2010` String, `dropoff_boroct2010` String, `dropoff_cdeligibil` String, `dropoff_ntacode` FixedString(4), `dropoff_ntaname` String, `dropoff_puma` UInt16 ) ENGINE = MergeTree PARTITION BY toYYYYMM(pickup_date) ORDER BY pickup_datetime SETTINGS index_granularity = 8192  Note the use of partitioning on the pickup_date field. Whilst usually a technique to assist with data management, we can later use this key to parallelize writes to s3. Each entry in our taxi dataset contains a taxi trip. This anonymized data consists of 20m compressed in the s3 bucket https://datasets-documentation.s3.eu-west-3.amazonaws.com/ under the folder nyc-taxi. We offer this data in tsv format with approximately 1m rows per file. "},{"title":"Reading Data from s3​","type":1,"pageTitle":"S3 Table Functions","url":"en/integrations/s3/s3-table-functions#reading-data-from-s3","content":"We can query s3 data as a source without requiring persistence in ClickHouse. In the following query, we sample 10 rows. Note the absence of credentials here as the bucket is publicly accessible: SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames') LIMIT 10;  Note that we are not required to list the columns since the TabSeparatedWithNames format encodes the column names in the first row. Other formats, such as plain CSV or TSV, will return auto-generated columns for this query, e.g., c1, c2, c3 etc. Queries additionally support the virtual columns _path and _file that provide information regards the bucket path and filename respectively e.g. SELECT _path, _file, trip_id FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_0.gz', 'TabSeparatedWithNames') LIMIT 5;  _path\t_file\ttrip_iddatasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999902 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999919 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999944 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999969 datasets-documentation/nyc-taxi/trips_0.gz\ttrips_0.gz\t1199999990 Confirm the number of rows in this sample dataset. Note the use of wildcards for file expansion, so we consider all twenty files. This query will take around 10s depending on the number of cores on the ClickHouse instance: SELECT count() as count FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames');  count20000000 Whilst useful for sampling data and executing one-off exploratory queries, reading data directly from s3 is unlikely to perform on larger datasets. "},{"title":"Using clickHouse-local​","type":1,"pageTitle":"S3 Table Functions","url":"en/integrations/s3/s3-table-functions#using-clickhouse-local","content":"The clickhouse-local program enables you to perform fast processing on local files without deploying and configuring the ClickHouse server. Any queries using the s3 table function can be performed with this utility. For example, clickhouse-local --query &quot;SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames') LIMIT 10&quot;  "},{"title":"Inserting Data from s3​","type":1,"pageTitle":"S3 Table Functions","url":"en/integrations/s3/s3-table-functions#inserting-data-from-s3","content":"To exploit the full capabilities of ClickHouse, we next read and insert the data into our instance. We combine our s3 function with a simple INSERT statement to achieve this. Note that we aren’t required to list our columns as our target table provides the required structure. This requires the columns to appear in the order specified in the table DDL statement: columns are mapped according to their position in the SELECT clause. The insertion of all 10m rows can take a few minutes depending on the ClickHouse instance. Below we insert 1m to ensure a prompt response. Adjust the LIMIT clause or column selection to import subsets as required: INSERT INTO trips SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames') LIMIT 1000000;  "},{"title":"Remote Insert using ClickHouse Local​","type":1,"pageTitle":"S3 Table Functions","url":"en/integrations/s3/s3-table-functions#remote-insert-using-clickhouse-local","content":"If network security policies prevent your ClickHouse cluster from making outbound connections, you can potentially insert s3 data using the ClickHouse local. In the example below, we read from an s3 bucket and insert to ClickHouse using the remote function. clickhouse-local --query &quot;INSERT INTO TABLE FUNCTION remote('localhost:9000', 'default.trips', 'username', 'password') (*) SELECT * FROM s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames') LIMIT 10&quot;  To execute this over a secure SSL connection, utilize the remoteSecure function. This approach offers inferior performance to direct inserts on the cluster and is for use in restricted scenarios only. "},{"title":"Exporting Data​","type":1,"pageTitle":"S3 Table Functions","url":"en/integrations/s3/s3-table-functions#exporting-data","content":"We assume you have a bucket to write data in the following examples. This will require appropriate permissions. We pass the credentials needed in the request. For further options, see Managing Credentials. In the simple example below, we use the table function as a destination instead of a source. Here we stream 10k rows from the trips table to a bucket, specifying lz4 compression and output type of CSV. INSERT INTO FUNCTION s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/csv/trips.csv.lz4', 's3_key', 's3_secret', 'CSV') SELECT * FROM trips LIMIT 10000;  note This query requires write access to the bucket. Note here how the format of the file is inferred from the extension. We also don’t need to specify the columns in the s3 function - this can be inferred from the SELECT. "},{"title":"Splitting Large Files​","type":1,"pageTitle":"S3 Table Functions","url":"en/integrations/s3/s3-table-functions#splitting-large-files","content":"It is unlikely you will want to export your data as a single file. Most tools, including ClickHouse, will achieve higher throughput performance when reading and writing to multiple files due to the possibility of parallelism. We could execute our INSERT command multiple times, targeting a subset of the data. ClickHouse offers a means of automatic splitting files using a PARTITION key. In the example below, we create ten files using a modulus of the rand() function. Notice how the resulting partition id is referenced in the filename. This results in ten files with a numerical suffix, e.g. trips_0.csv.lz4, trips_1.csv.lz4 etc...: INSERT INTO FUNCTION s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/csv/trips_{_partition_id}.csv.lz4', 's3_key', 's3_secret', 'CSV') PARTITION BY rand() % 10 SELECT * FROM trips LIMIT 100000;  note This query requires write access to the bucket. Alternatively, we can reference a field in the data. For this dataset, the payment_type provides a natural partitioning key with a cardinality of 5. INSERT INTO FUNCTION s3('https://datasets-documentation.s3.eu-west-3.amazonaws.com/csv/trips_{_partition_id}.csv.lz4', 's3_key', 's3_secret', 'CSV') PARTITION BY payment_type SELECT * FROM trips LIMIT 100000;  note This query requires write access to the bucket. "},{"title":"Utilizing Clusters​","type":1,"pageTitle":"S3 Table Functions","url":"en/integrations/s3/s3-table-functions#utilizing-clusters","content":"The above functions are all limited to execution on a single node. Read speeds will scale linearly with CPU cores until other resources (typically network) are saturated, allowing users to vertically scale. However, this approach has its limitations. While users can alleviate some resource pressure by inserting into a distributed table when performing an INSERT INTO SELECT query, this still leaves a single node reading, parsing, and processing the data. To address this challenge and allow us to scale reads horizontally, we have the s3Cluster function. The node which receives the query, known as the initiator, creates a connection to every node in the cluster. The glob pattern determining which files need to be read is resolved to a set of files. The initiator distributes files to the nodes in the cluster, which act as workers. These workers, in turn, request files to process as they complete reads. This process ensures that we can scale reads horizontally. The s3Cluster function takes the same format as the single node variants, except that a target cluster is required to denote the worker nodes. s3Cluster(cluster_name, source, [access_key_id, secret_access_key,] format, structure)  where, cluster_name — Name of a cluster that is used to build a set of addresses and connection parameters to remote and local servers.source — URL to a file or a bunch of files. Supports following wildcards in read-only mode: *, ?, {'abc','def'} and {N..M} where N, M — numbers, abc, def — strings. For more information see Wildcards In Path.access_key_id and secret_access_key — Keys that specify credentials to use with the given endpoint. Optional.format — The format of the file.structure — Structure of the table. Format 'column1_name column1_type, column2_name column2_type, ...'. Like any s3 functions, the credentials are optional if the bucket is insecure or you define security through the environment, e.g., IAM roles. Unlike the s3 function, however, the structure must be specified in the request as of 22.3.1, i.e., the schema is not inferred. This function will be used as part of an INSERT INTO SELECT in most cases. In this case, you will often be inserting a distributed table. We illustrate a simple example below where trips_all is a distributed table. Whilst this table uses the events cluster, the consistency of the nodes used for reads and writes is not a requirement: INSERT INTO default.trips_all SELECT * FROM s3Cluster('events', 'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_*.gz', 'TabSeparatedWithNames')  note This query requires fixes to support schema inference present in 22.3.1 and later. Note that as of 22.3.1, inserts will occur against the initiator node. This means that whilst reads will occur on each node, the resulting rows will be routed to the initiator for distribution. In high throughput scenarios, this may prove a bottleneck. To address this, the s3Cluster function will work with the parameter parallel_distributed_insert_select in future versions. See Optimizing for Performance for further details on ensuring the s3cluster function achieves optimal performance. "},{"title":"Other Formats & Increasing Throughput​","type":1,"pageTitle":"S3 Table Functions","url":"en/integrations/s3/s3-table-functions#other-formats--increasing-throughput","content":"See Optimizing for Performance. "},{"title":"Integrating Vector with ClickHouse","type":0,"sectionRef":"#","url":"en/integrations/vector-to-clickhouse","content":"","keywords":""},{"title":"1. Create a database and table​","type":1,"pageTitle":"Integrating Vector with ClickHouse","url":"en/integrations/vector-to-clickhouse#1-create-a-database-and-table","content":"Let's define a table to store the log events: We will start with a new database named nginxdb: CREATE DATABASE IF NOT EXISTS nginxdb For starters, we are just going to insert the entire log event as a single string. Obviously this is not a great format for performing analytics on the log data, but we will figure that part out below using materialized views. CREATE TABLE IF NOT EXISTS nginxdb.access_logs ( message String ) ENGINE = MergeTree() ORDER BY tuple() note There is not really a need for a primary key yet, so that is why ORDER BY is set to tuple(). "},{"title":"2. Configure Nginx​","type":1,"pageTitle":"Integrating Vector with ClickHouse","url":"en/integrations/vector-to-clickhouse#2--configure-nginx","content":"We certainly do not want to spend too much time explaining Nginx, but we also do not want to hide all the details, so in this step we will provide you with enough details to get Nginx logging configured. The following access_log property sends logs to /var/log/nginx/my_access.log in the combined format. This value goes in the http section of your nginx.conf file: http { include /etc/nginx/mime.types; default_type application/octet-stream; access_log /var/log/nginx/my_access.log combined; sendfile on; keepalive_timeout 65; include /etc/nginx/conf.d/*.conf; } Be sure to restart Nginx if you had to modify nginx.conf. Generate some log events in the access log by visiting pages on your web server. Logs in the combined format have the following format: 192.168.208.1 - - [12/Oct/2021:03:31:44 +0000] &quot;GET / HTTP/1.1&quot; 200 615 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36&quot; 192.168.208.1 - - [12/Oct/2021:03:31:44 +0000] &quot;GET /favicon.ico HTTP/1.1&quot; 404 555 &quot;http://localhost/&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36&quot; 192.168.208.1 - - [12/Oct/2021:03:31:49 +0000] &quot;GET / HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36&quot;  "},{"title":"3. Configure Vector​","type":1,"pageTitle":"Integrating Vector with ClickHouse","url":"en/integrations/vector-to-clickhouse#3-configure-vector","content":"Vector collects, transforms and routes logs, metrics, and traces (referred to as sources) to lots of different vendors (referred to as sinks), including out-of-the-box compatibility with ClickHouse. Sources and sinks are defined in a configuration file named vector.toml. The following vector.toml defines a source of type file that tails the end of my_access.log, and it also defines a sink as the access_logs table defined above: [sources.nginx_logs] type = &quot;file&quot; include = [ &quot;/var/log/nginx/my_access.log&quot; ] read_from = &quot;end&quot; [sinks.clickhouse] type = &quot;clickhouse&quot; inputs = [&quot;nginx_logs&quot;] endpoint = &quot;http://clickhouse-server:8123&quot; database = &quot;nginxdb&quot; table = &quot;access_logs&quot; skip_unknown_fields = true Start up Vector using the configuration above. Visit the Vector documentation for more details on defining sources and sinks. Verify the access logs are being inserted into ClickHouse. Run the following query and you should see the access logs in your table: SELECT * FROM nginxdb.access_logs  "},{"title":"4. Parse the Logs​","type":1,"pageTitle":"Integrating Vector with ClickHouse","url":"en/integrations/vector-to-clickhouse#4-parse-the-logs","content":"Having the logs in ClickHouse is great, but storing each event as a single string does not allow for much data analysis. Let's see how to parse the log events using a materialized view. A materialized view (MV, for short) is a new table based on an existing table, and when inserts are made to the existing table, the new data is also added to the materialized view. Let's see how to define a MV that contains a parsed representation of the log events in access_logs, in other words: 192.168.208.1 - - [12/Oct/2021:15:32:43 +0000] &quot;GET / HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36&quot; There are various functions in ClickHouse to parse the string, but for starters let's take a look at splitByWhitespace - which parses a string by whitespace and returns each token in an array. To demonstrate, run the following command: SELECT splitByWhitespace('192.168.208.1 - - [12/Oct/2021:15:32:43 +0000] &quot;GET / HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36&quot;') Notice the response is pretty close to what we want! A few of the strings have some extra characters, and the user agent (the browser details) did not need to be parsed, but we will resolve that in the next step: [&quot;192.168.208.1&quot;,&quot;-&quot;,&quot;-&quot;,&quot;[12/Oct/2021:15:32:43&quot;,&quot;+0000]&quot;,&quot;\\&quot;GET&quot;,&quot;/&quot;,&quot;HTTP/1.1\\&quot;&quot;,&quot;304&quot;,&quot;0&quot;,&quot;\\&quot;-\\&quot;&quot;,&quot;\\&quot;Mozilla/5.0&quot;,&quot;(Macintosh;&quot;,&quot;Intel&quot;,&quot;Mac&quot;,&quot;OS&quot;,&quot;X&quot;,&quot;10_15_7)&quot;,&quot;AppleWebKit/537.36&quot;,&quot;(KHTML,&quot;,&quot;like&quot;,&quot;Gecko)&quot;,&quot;Chrome/93.0.4577.63&quot;,&quot;Safari/537.36\\&quot;&quot;] Similar to splitByWhitespace, the splitByRegexp function splits a string into an array based on a regular expression. Run the following command, which returns two strings. SELECT splitByRegexp('\\S \\d+ &quot;([^&quot;]*)&quot;', '192.168.208.1 - - [12/Oct/2021:15:32:43 +0000] &quot;GET / HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36&quot;') Notice the second string returned is the user agent successfully parsed from the log: [&quot;192.168.208.1 - - [12/Oct/2021:15:32:43 +0000] \\&quot;GET / HTTP/1.1\\&quot; 30&quot;,&quot; \\&quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\\&quot;&quot;] Before looking at the final CREATE MATERIALIZED VIEW command, let's view a couple more functions used to cleanup the data. For example, the RequestMethod looks like &quot;GET with an unwanted double-quote. Run the following trim function, which removes the double quote: SELECT trim(LEADING '&quot;' FROM '&quot;GET') The time string has a leading square bracket, and also is not in a format that ClickHouse can parse into a date. However, if we change the separater from a colon (:) to a comma (,) then the parsing works great: SELECT parseDateTimeBestEffort(replaceOne(trim(LEADING '[' FROM '[12/Oct/2021:15:32:43'), ':', ' ')) We are now ready to define our materialized view. Our definition includes POPULATE, which means the existing rows in access_logs will be processed and inserted right away. Run the following SQL statement: CREATE MATERIALIZED VIEW nginxdb.access_logs_view ( RemoteAddr String, Client String, RemoteUser String, TimeLocal DateTime, RequestMethod String, Request String, HttpVersion String, Status Int32, BytesSent Int64, UserAgent String ) ENGINE = MergeTree() ORDER BY RemoteAddr POPULATE AS WITH splitByWhitespace(message) as split, splitByRegexp('\\S \\d+ &quot;([^&quot;]*)&quot;', message) as referer SELECT split[1] AS RemoteAddr, split[2] AS Client, split[3] AS RemoteUser, parseDateTimeBestEffort(replaceOne(trim(LEADING '[' FROM split[4]), ':', ' ')) AS TimeLocal, trim(LEADING '&quot;' FROM split[6]) AS RequestMethod, split[7] AS Request, trim(TRAILING '&quot;' FROM split[8]) AS HttpVersion, split[9] AS Status, split[10] AS BytesSent, trim(BOTH '&quot;' from referer[2]) AS UserAgent FROM (SELECT message FROM nginxdb.access_logs) Now verify it worked. You should see the access logs nicely parsed into columns: SELECT * FROM nginxdb.access_logs_view note The lesson above stored the data in two tables, but you could change the initial nginxdb.access_logs table to use the Null table engine - the parsed data will still end up in the nginxdb.access_logs_view table, but the raw data will not be stored in a table. Summary: By using Vector, which only required a simple install and quick configuration, we can send logs from an Nginx server to a table in ClickHouse. By using a clever materialized view, we can parse those logs into columns for easier analytics. "},{"title":"Visual Interfaces from Third-party Developers","type":0,"sectionRef":"#","url":"en/interfaces/third-party/gui","content":"","keywords":""},{"title":"Open-Source​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#open-source","content":""},{"title":"Tabix​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#tabix","content":"Web interface for ClickHouse in the Tabix project. Features: Works with ClickHouse directly from the browser, without the need to install additional software.Query editor with syntax highlighting.Auto-completion of commands.Tools for graphical analysis of query execution.Colour scheme options. Tabix documentation. "},{"title":"HouseOps​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#houseops","content":"HouseOps is a UI/IDE for OSX, Linux and Windows. Features: Query builder with syntax highlighting. View the response in a table or JSON view.Export query results as CSV or JSON.List of processes with descriptions. Write mode. Ability to stop (KILL) a process.Database graph. Shows all tables and their columns with additional information.A quick view of the column size.Server configuration. The following features are planned for development: Database management.User management.Real-time data analysis.Cluster monitoring.Cluster management.Monitoring replicated and Kafka tables. "},{"title":"LightHouse​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#lighthouse","content":"LightHouse is a lightweight web interface for ClickHouse. Features: Table list with filtering and metadata.Table preview with filtering and sorting.Read-only queries execution. "},{"title":"Redash​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#redash","content":"Redash is a platform for data visualization. Supports for multiple data sources including ClickHouse, Redash can join results of queries from different data sources into one final dataset. Features: Powerful editor of queries.Database explorer.Visualization tools, that allow you to represent data in different forms. "},{"title":"Grafana​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#grafana","content":"Grafana is a platform for monitoring and visualization. &quot;Grafana allows you to query, visualize, alert on and understand your metrics no matter where they are stored. Create, explore, and share dashboards with your team and foster a data driven culture. Trusted and loved by the community&quot; — grafana.com. ClickHouse datasource plugin provides a support for ClickHouse as a backend database. "},{"title":"DBeaver​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#dbeaver","content":"DBeaver - universal desktop database client with ClickHouse support. Features: Query development with syntax highlight and autocompletion.Table list with filters and metadata search.Table data preview.Full-text search. By default, DBeaver does not connect using a session (the CLI for example does). If you require session support (for example to set settings for your session), edit the driver connection properties and set session_id to a random string (it uses the http connection under the hood). Then you can use any setting from the query window. "},{"title":"clickhouse-cli​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#clickhouse-cli","content":"clickhouse-cli is an alternative command-line client for ClickHouse, written in Python 3. Features: Autocompletion.Syntax highlighting for the queries and data output.Pager support for the data output.Custom PostgreSQL-like commands. "},{"title":"clickhouse-flamegraph​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#clickhouse-flamegraph","content":"clickhouse-flamegraph is a specialized tool to visualize the system.trace_log as flamegraph. "},{"title":"clickhouse-plantuml​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#clickhouse-plantuml","content":"cickhouse-plantuml is a script to generate PlantUML diagram of tables’ schemes. "},{"title":"xeus-clickhouse​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#xeus-clickhouse","content":"xeus-clickhouse is a Jupyter kernal for ClickHouse, which supports query CH data using SQL in Jupyter. "},{"title":"MindsDB Studio​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#mindsdb","content":"MindsDB is an open-source AI layer for databases including ClickHouse that allows you to effortlessly develop, train and deploy state-of-the-art machine learning models. MindsDB Studio(GUI) allows you to train new models from database, interpret predictions made by the model, identify potential data biases, and evaluate and visualize model accuracy using the Explainable AI function to adapt and tune your Machine Learning models faster. "},{"title":"DBM​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#dbm","content":"DBM DBM is a visual management tool for ClickHouse! Features: Support query history (pagination, clear all, etc.)Support selected sql clauses querySupport terminating querySupport table management (metadata, delete, preview)Support database management (delete, create)Support custom querySupport multiple data sources management(connection test, monitoring)Support monitor (processor, connection, query)Support migrate data "},{"title":"Bytebase​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#bytebase","content":"Bytebase is a web-based, open source schema change and version control tool for teams. It supports various databases including ClickHouse. Features: Schema review between developers and DBAs.Database-as-Code, version control the schema in VCS such GitLab and trigger the deployment upon code commit.Streamlined deployment with per-environment policy.Full migration history.Schema drift detection.Backup and restore.RBAC. "},{"title":"Zeppelin-Interpreter-for-ClickHouse​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#zeppelin-interpreter-for-clickhouse","content":"Zeppelin-Interpreter-for-ClickHouse is a Zeppelin interpreter for ClickHouse. Compared with JDBC interpreter, it can provide better timeout control for long running queries. "},{"title":"Commercial​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#commercial","content":""},{"title":"DataGrip​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#datagrip","content":"DataGrip is a database IDE from JetBrains with dedicated support for ClickHouse. It is also embedded in other IntelliJ-based tools: PyCharm, IntelliJ IDEA, GoLand, PhpStorm and others. Features: Very fast code completion.ClickHouse syntax highlighting.Support for features specific to ClickHouse, for example, nested columns, table engines.Data Editor.Refactorings.Search and Navigation. "},{"title":"Yandex DataLens​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#yandex-datalens","content":"Yandex DataLens is a service of data visualization and analytics. Features: Wide range of available visualizations, from simple bar charts to complex dashboards.Dashboards could be made publicly available.Support for multiple data sources including ClickHouse.Storage for materialized data based on ClickHouse. DataLens is available for free for low-load projects, even for commercial use. DataLens documentation.Tutorial on visualizing data from a ClickHouse database. "},{"title":"Holistics Software​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#holistics-software","content":"Holistics is a full-stack data platform and business intelligence tool. Features: Automated email, Slack and Google Sheet schedules of reports.SQL editor with visualizations, version control, auto-completion, reusable query components and dynamic filters.Embedded analytics of reports and dashboards via iframe.Data preparation and ETL capabilities.SQL data modelling support for relational mapping of data. "},{"title":"Looker​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#looker","content":"Looker is a data platform and business intelligence tool with support for 50+ database dialects including ClickHouse. Looker is available as a SaaS platform and self-hosted. Users can use Looker via the browser to explore data, build visualizations and dashboards, schedule reports, and share their insights with colleagues. Looker provides a rich set of tools to embed these features in other applications, and an API to integrate data with other applications. Features: Easy and agile development using LookML, a language which supports curatedData Modeling to support report writers and end-users.Powerful workflow integration via Looker’s Data Actions. How to configure ClickHouse in Looker. "},{"title":"SeekTable​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#seektable","content":"SeekTable is a self-service BI tool for data exploration and operational reporting. It is available both as a cloud service and a self-hosted version. Reports from SeekTable may be embedded into any web-app. Features: Business users-friendly reports builder.Powerful report parameters for SQL filtering and report-specific query customizations.Can connect to ClickHouse both with a native TCP/IP endpoint and a HTTP(S) interface (2 different drivers).It is possible to use all power of ClickHouse SQL dialect in dimensions/measures definitions.Web API for automated reports generation.Supports reports development flow with account data backup/restore; data models (cubes) / reports configuration is a human-readable XML and can be stored under version control system. SeekTable is free for personal/individual usage. How to configure ClickHouse connection in SeekTable. "},{"title":"Chadmin​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#chadmin","content":"Chadmin is a simple UI where you can visualize your currently running queries on your ClickHouse cluster and info about them and kill them if you want. "},{"title":"TABLUM.IO​","type":1,"pageTitle":"Visual Interfaces from Third-party Developers","url":"en/interfaces/third-party/gui#tablum_io","content":"TABLUM.IO — an online query and analytics tool for ETL and visualization. It allows connecting to ClickHouse, query data via a versatile SQL console as well as to load data from static files and 3rd party services. TABLUM.IO can visualize data results as charts and tables. Features: ETL: data loading from popular databases, local and remote files, API invocations.Versatile SQL console with syntax highlight and visual query builder.Data visualization as charts and tables.Data materialization and sub-queries.Data reporting to Slack, Telegram or email.Data pipelining via proprietary API.Data export in JSON, CSV, SQL, HTML formats.Web-based interface. TABLUM.IO can be run as a self-hosted solution (as a docker image) or in the cloud. License: commercial product with 3-month free period. Try it out for free in the cloud. Learn more about the product at TABLUM.IO Original article "},{"title":"Cache Types","type":0,"sectionRef":"#","url":"en/operations/caches","content":"Cache Types When performing queries, ClichHouse uses different caches. Main cache types: mark_cache — Cache of marks used by table engines of the MergeTree family.uncompressed_cache — Cache of uncompressed data used by table engines of the MergeTree family. Additional cache types: DNS cache.Regexp cache.Compiled expressions cache.Avro format schemas cache.Dictionaries data cache. Indirectly used: OS page cache. To drop cache, use SYSTEM DROP ... CACHE statements. Original article","keywords":""},{"title":"Integration Libraries from Third-party Developers","type":0,"sectionRef":"#","url":"en/interfaces/third-party/integrations","content":"","keywords":""},{"title":"Infrastructure Products​","type":1,"pageTitle":"Integration Libraries from Third-party Developers","url":"en/interfaces/third-party/integrations#infrastructure-products","content":"Relational database management systems MySQL mysql2chProxySQLclickhouse-mysql-data-readerhorgh-replicator PostgreSQL clickhousedb_fdwinfi.clickhouse_fdw (uses infi.clickhouse_orm)pg2chclickhouse_fdw MSSQL ClickHouseMigrator Message queues Kafka clickhouse_sinker (uses Go client)stream-loader-clickhouse Stream processing Flink flink-clickhouse-sink Object storages S3 clickhouse-backup Container orchestration Kubernetes clickhouse-operator Configuration management puppet innogames/clickhousemfedotov/clickhouse Monitoring Graphite graphousecarbon-clickhousegraphite-clickhousegraphite-ch-optimizer - optimizes staled partitions in *GraphiteMergeTree if rules from rollup configuration could be applied Grafana clickhouse-grafana Prometheus clickhouse_exporterPromHouseclickhouse_exporter (uses Go client) Nagios check_clickhousecheck_clickhouse.py Zabbix clickhouse-zabbix-template Sematext clickhouse integration Logging rsyslog omclickhouse fluentd loghouse (for Kubernetes) logagent logagent output-plugin-clickhouse Geo MaxMind clickhouse-maxmind-geoip AutoML MindsDB MindsDB - Predictive AI layer for ClickHouse database. "},{"title":"Programming Language Ecosystems​","type":1,"pageTitle":"Integration Libraries from Third-party Developers","url":"en/interfaces/third-party/integrations#programming-language-ecosystems","content":"Python SQLAlchemy sqlalchemy-clickhouse (uses infi.clickhouse_orm) pandas pandahouse PHP Doctrine dbal-clickhouse R dplyr RClickHouse (uses clickhouse-cpp) Java Hadoop clickhouse-hdfs-loader (uses JDBC) Scala Akka clickhouse-scala-client C# ADO.NET ClickHouse.AdoClickHouse.ClientClickHouse.NetClickHouse.Net.Migrations Elixir Ecto clickhouse_ecto Ruby Ruby on Rails activecubeActiveRecord GraphQL activecube-graphql Original article "},{"title":"ClickHouse Keeper","type":0,"sectionRef":"#","url":"en/operations/clickhouse-keeper","content":"","keywords":""},{"title":"Implementation details​","type":1,"pageTitle":"ClickHouse Keeper","url":"en/operations/clickhouse-keeper#implementation-details","content":"ZooKeeper is one of the first well-known open-source coordination systems. It's implemented in Java, has quite a simple and powerful data model. ZooKeeper's coordination algorithm called ZAB (ZooKeeper Atomic Broadcast) doesn't provide linearizability guarantees for reads, because each ZooKeeper node serves reads locally. Unlike ZooKeeper ClickHouse Keeper is written in C++ and uses RAFT algorithm implementation. This algorithm allows to have linearizability for reads and writes, has several open-source implementations in different languages. By default, ClickHouse Keeper provides the same guarantees as ZooKeeper (linearizable writes, non-linearizable reads). It has a compatible client-server protocol, so any standard ZooKeeper client can be used to interact with ClickHouse Keeper. Snapshots and logs have an incompatible format with ZooKeeper, but clickhouse-keeper-converter tool allows to convert ZooKeeper data to ClickHouse Keeper snapshot. Interserver protocol in ClickHouse Keeper is also incompatible with ZooKeeper so mixed ZooKeeper / ClickHouse Keeper cluster is impossible. ClickHouse Keeper supports Access Control List (ACL) the same way as ZooKeeper does. ClickHouse Keeper supports the same set of permissions and has the identical built-in schemes: world, auth, digest, host and ip. Digest authentication scheme uses pair username:password. Password is encoded in Base64. note External integrations are not supported. "},{"title":"Configuration​","type":1,"pageTitle":"ClickHouse Keeper","url":"en/operations/clickhouse-keeper#configuration","content":"ClickHouse Keeper can be used as a standalone replacement for ZooKeeper or as an internal part of the ClickHouse server, but in both cases configuration is almost the same .xml file. The main ClickHouse Keeper configuration tag is &lt;keeper_server&gt;. Keeper configuration has the following parameters: tcp_port — Port for a client to connect (default for ZooKeeper is 2181).tcp_port_secure — Secure port for an SSL connection between client and keeper-server.server_id — Unique server id, each participant of the ClickHouse Keeper cluster must have a unique number (1, 2, 3, and so on).log_storage_path — Path to coordination logs, better to store logs on the non-busy device (same for ZooKeeper).snapshot_storage_path — Path to coordination snapshots. Other common parameters are inherited from the ClickHouse server config (listen_host, logger, and so on). Internal coordination settings are located in &lt;keeper_server&gt;.&lt;coordination_settings&gt; section: operation_timeout_ms — Timeout for a single client operation (ms) (default: 10000).min_session_timeout_ms — Min timeout for client session (ms) (default: 10000).session_timeout_ms — Max timeout for client session (ms) (default: 100000).dead_session_check_period_ms — How often ClickHouse Keeper check dead sessions and remove them (ms) (default: 500).heart_beat_interval_ms — How often a ClickHouse Keeper leader will send heartbeats to followers (ms) (default: 500).election_timeout_lower_bound_ms — If the follower didn't receive heartbeats from the leader in this interval, then it can initiate leader election (default: 1000).election_timeout_upper_bound_ms — If the follower didn't receive heartbeats from the leader in this interval, then it must initiate leader election (default: 2000).rotate_log_storage_interval — How many log records to store in a single file (default: 100000).reserved_log_items — How many coordination log records to store before compaction (default: 100000).snapshot_distance — How often ClickHouse Keeper will create new snapshots (in the number of records in logs) (default: 100000).snapshots_to_keep — How many snapshots to keep (default: 3).stale_log_gap — Threshold when leader considers follower as stale and sends the snapshot to it instead of logs (default: 10000).fresh_log_gap — When node became fresh (default: 200).max_requests_batch_size - Max size of batch in requests count before it will be sent to RAFT (default: 100).force_sync — Call fsync on each write to coordination log (default: true).quorum_reads — Execute read requests as writes through whole RAFT consensus with similar speed (default: false).raft_logs_level — Text logging level about coordination (trace, debug, and so on) (default: system default).auto_forwarding — Allow to forward write requests from followers to the leader (default: true).shutdown_timeout — Wait to finish internal connections and shutdown (ms) (default: 5000).startup_timeout — If the server doesn't connect to other quorum participants in the specified timeout it will terminate (ms) (default: 30000).four_letter_word_white_list — White list of 4lw commands (default: &quot;conf,cons,crst,envi,ruok,srst,srvr,stat,wchc,wchs,dirs,mntr,isro&quot;). Quorum configuration is located in &lt;keeper_server&gt;.&lt;raft_configuration&gt; section and contain servers description. The only parameter for the whole quorum is secure, which enables encrypted connection for communication between quorum participants. The parameter can be set true if SSL connection is required for internal communication between nodes, or left unspecified otherwise. The main parameters for each &lt;server&gt; are: id — Server identifier in a quorum.hostname — Hostname where this server is placed.port — Port where this server listens for connections. note In the case of a change in the topology of your ClickHouse Keeper cluster (eg. replacing a server), please make sure to keep the mapping server_id to hostname consistent and avoid shuffling or reusing an existing server_id for different servers (eg. it can happen if your rely on automation scripts to deploy ClickHouse Keeper) Examples of configuration for quorum with three nodes can be found in integration tests with test_keeper_ prefix. Example configuration for server #1: &lt;keeper_server&gt; &lt;tcp_port&gt;2181&lt;/tcp_port&gt; &lt;server_id&gt;1&lt;/server_id&gt; &lt;log_storage_path&gt;/var/lib/clickhouse/coordination/log&lt;/log_storage_path&gt; &lt;snapshot_storage_path&gt;/var/lib/clickhouse/coordination/snapshots&lt;/snapshot_storage_path&gt; &lt;coordination_settings&gt; &lt;operation_timeout_ms&gt;10000&lt;/operation_timeout_ms&gt; &lt;session_timeout_ms&gt;30000&lt;/session_timeout_ms&gt; &lt;raft_logs_level&gt;trace&lt;/raft_logs_level&gt; &lt;/coordination_settings&gt; &lt;raft_configuration&gt; &lt;server&gt; &lt;id&gt;1&lt;/id&gt; &lt;hostname&gt;zoo1&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;2&lt;/id&gt; &lt;hostname&gt;zoo2&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;/server&gt; &lt;server&gt; &lt;id&gt;3&lt;/id&gt; &lt;hostname&gt;zoo3&lt;/hostname&gt; &lt;port&gt;9444&lt;/port&gt; &lt;/server&gt; &lt;/raft_configuration&gt; &lt;/keeper_server&gt;  "},{"title":"How to run​","type":1,"pageTitle":"ClickHouse Keeper","url":"en/operations/clickhouse-keeper#how-to-run","content":"ClickHouse Keeper is bundled into the ClickHouse server package, just add configuration of &lt;keeper_server&gt; and start ClickHouse server as always. If you want to run standalone ClickHouse Keeper you can start it in a similar way with: clickhouse-keeper --config /etc/your_path_to_config/config.xml  If you don't have the symlink (clickhouse-keeper) you can create it or specify keeper as argument: clickhouse keeper --config /etc/your_path_to_config/config.xml  "},{"title":"Four Letter Word Commands​","type":1,"pageTitle":"ClickHouse Keeper","url":"en/operations/clickhouse-keeper#four-letter-word-commands","content":"ClickHouse Keeper also provides 4lw commands which are almost the same with Zookeeper. Each command is composed of four letters such as mntr, stat etc. There are some more interesting commands: stat gives some general information about the server and connected clients, while srvr and cons give extended details on server and connections respectively. The 4lw commands has a white list configuration four_letter_word_white_list which has default value &quot;conf,cons,crst,envi,ruok,srst,srvr,stat,wchc,wchs,dirs,mntr,isro&quot;. You can issue the commands to ClickHouse Keeper via telnet or nc, at the client port. echo mntr | nc localhost 9181  Bellow is the detailed 4lw commands: ruok: Tests if server is running in a non-error state. The server will respond with imok if it is running. Otherwise it will not respond at all. A response of &quot;imok&quot; does not necessarily indicate that the server has joined the quorum, just that the server process is active and bound to the specified client port. Use &quot;stat&quot; for details on state wrt quorum and client connection information. imok  mntr: Outputs a list of variables that could be used for monitoring the health of the cluster. zk_version v21.11.1.1-prestable-7a4a0b0edef0ad6e0aa662cd3b90c3f4acf796e7 zk_avg_latency 0 zk_max_latency 0 zk_min_latency 0 zk_packets_received 68 zk_packets_sent 68 zk_num_alive_connections 1 zk_outstanding_requests 0 zk_server_state leader zk_znode_count 4 zk_watch_count 1 zk_ephemerals_count 0 zk_approximate_data_size 723 zk_open_file_descriptor_count 310 zk_max_file_descriptor_count 10240 zk_followers 0 zk_synced_followers 0  srvr: Lists full details for the server. ClickHouse Keeper version: v21.11.1.1-prestable-7a4a0b0edef0ad6e0aa662cd3b90c3f4acf796e7 Latency min/avg/max: 0/0/0 Received: 2 Sent : 2 Connections: 1 Outstanding: 0 Zxid: 34 Mode: leader Node count: 4  stat: Lists brief details for the server and connected clients. ClickHouse Keeper version: v21.11.1.1-prestable-7a4a0b0edef0ad6e0aa662cd3b90c3f4acf796e7 Clients: 192.168.1.1:52852(recved=0,sent=0) 192.168.1.1:52042(recved=24,sent=48) Latency min/avg/max: 0/0/0 Received: 4 Sent : 4 Connections: 1 Outstanding: 0 Zxid: 36 Mode: leader Node count: 4  srst: Reset server statistics. The command will affect the result of srvr, mntr and stat. Server stats reset.  conf: Print details about serving configuration. server_id=1 tcp_port=2181 four_letter_word_white_list=* log_storage_path=./coordination/logs snapshot_storage_path=./coordination/snapshots max_requests_batch_size=100 session_timeout_ms=30000 operation_timeout_ms=10000 dead_session_check_period_ms=500 heart_beat_interval_ms=500 election_timeout_lower_bound_ms=1000 election_timeout_upper_bound_ms=2000 reserved_log_items=1000000000000000 snapshot_distance=10000 auto_forwarding=true shutdown_timeout=5000 startup_timeout=240000 raft_logs_level=information snapshots_to_keep=3 rotate_log_storage_interval=100000 stale_log_gap=10000 fresh_log_gap=200 max_requests_batch_size=100 quorum_reads=false force_sync=false compress_logs=true compress_snapshots_with_zstd_format=true configuration_change_tries_count=20  cons: List full connection/session details for all clients connected to this server. Includes information on numbers of packets received/sent, session id, operation latencies, last operation performed, etc...  192.168.1.1:52163(recved=0,sent=0,sid=0xffffffffffffffff,lop=NA,est=1636454787393,to=30000,lzxid=0xffffffffffffffff,lresp=0,llat=0,minlat=0,avglat=0,maxlat=0) 192.168.1.1:52042(recved=9,sent=18,sid=0x0000000000000001,lop=List,est=1636454739887,to=30000,lcxid=0x0000000000000005,lzxid=0x0000000000000005,lresp=1636454739892,llat=0,minlat=0,avglat=0,maxlat=0)  crst: Reset connection/session statistics for all connections. Connection stats reset.  envi: Print details about serving environment Environment: clickhouse.keeper.version=v21.11.1.1-prestable-7a4a0b0edef0ad6e0aa662cd3b90c3f4acf796e7 host.name=ZBMAC-C02D4054M.local os.name=Darwin os.arch=x86_64 os.version=19.6.0 cpu.count=12 user.name=root user.home=/Users/JackyWoo/ user.dir=/Users/JackyWoo/project/jd/clickhouse/cmake-build-debug/programs/ user.tmp=/var/folders/b4/smbq5mfj7578f2jzwn602tt40000gn/T/  dirs: Shows the total size of snapshot and log files in bytes snapshot_dir_size: 0 log_dir_size: 3875  isro: Tests if server is running in read-only mode. The server will respond with &quot;ro&quot; if in read-only mode or &quot;rw&quot; if not in read-only mode. rw  wchs: Lists brief information on watches for the server. 1 connections watching 1 paths Total watches:1  wchc: Lists detailed information on watches for the server, by session. This outputs a list of sessions (connections) with associated watches (paths). Note, depending on the number of watches this operation may be expensive (ie impact server performance), use it carefully. 0x0000000000000001 /clickhouse/task_queue/ddl  wchp: Lists detailed information on watches for the server, by path. This outputs a list of paths (znodes) with associated sessions. Note, depending on the number of watches this operation may be expensive (i. e. impact server performance), use it carefully. /clickhouse/task_queue/ddl 0x0000000000000001  dump: Lists the outstanding sessions and ephemeral nodes. This only works on the leader. Sessions dump (2): 0x0000000000000001 0x0000000000000002 Sessions with Ephemerals (1): 0x0000000000000001 /clickhouse/task_queue/ddl  "},{"title":"[experimental] Migration from ZooKeeper​","type":1,"pageTitle":"ClickHouse Keeper","url":"en/operations/clickhouse-keeper#migration-from-zookeeper","content":"Seamlessly migration from ZooKeeper to ClickHouse Keeper is impossible you have to stop your ZooKeeper cluster, convert data and start ClickHouse Keeper. clickhouse-keeper-converter tool allows converting ZooKeeper logs and snapshots to ClickHouse Keeper snapshot. It works only with ZooKeeper &gt; 3.4. Steps for migration: Stop all ZooKeeper nodes. Optional, but recommended: find ZooKeeper leader node, start and stop it again. It will force ZooKeeper to create a consistent snapshot. Run clickhouse-keeper-converter on a leader, for example: clickhouse-keeper-converter --zookeeper-logs-dir /var/lib/zookeeper/version-2 --zookeeper-snapshots-dir /var/lib/zookeeper/version-2 --output-dir /path/to/clickhouse/keeper/snapshots  Copy snapshot to ClickHouse server nodes with a configured keeper or start ClickHouse Keeper instead of ZooKeeper. The snapshot must persist on all nodes, otherwise, empty nodes can be faster and one of them can become a leader. Original article "},{"title":"Recovering after losing quorum​","type":1,"pageTitle":"ClickHouse Keeper","url":"en/operations/clickhouse-keeper#recovering-after-losing-quorum","content":"Because Clickhouse Keeper uses Raft it can tolerate certain amount of node crashes depending on the cluster size. \\ E.g. for a 3-node cluster, it will continue working correctly if only 1 node crashes. Cluster configuration can be dynamically configured but there are some limitations. Reconfiguration relies on Raft also so to add/remove a node from the cluster you need to have a quorum. If you lose too many nodes in your cluster at the same time without any chance of starting them again, Raft will stop working and not allow you to reconfigure your cluster using the convenvtional way. Nevertheless, Clickhouse Keeper has a recovery mode which allows you to forcfully reconfigure your cluster with only 1 node. This should be done only as your last resort if you cannot start your nodes again, or start a new instance on the same endpoint. Important things to note before continuing: Make sure that the failed nodes cannot connect to the cluster again.Do not start any of the new nodes until it's specified in the steps. After making sure that the above things are true, you need to do following: Pick a single Keeper node to be your new leader. Be aware that the data of that node will be used for the entire cluster so we recommend to use a node with the most up to date state.Before doing anything else, make a backup of the log_storage_path and snapshot_storage_path folders of the picked node.Reconfigure the cluster on all of the nodes you want to use.Send the four letter command rcvr to the node you picked which will move the node to the recovery mode OR stop Keeper instance on the picked node and start it again with the --force-recovery argument.One by one, start Keeper instances on the new nodes making sure that mntr returns follower for the zk_server_state before starting the next one.While in the recovery mode, the leader node will return error message for mntr command until it achieves quorum with the new nodes and refuse any requests from the client and the followers.After quorum is achieved, the leader node will return to the normal mode of operation, accepting all the requests using Raft - verify with mntr which should return leader for the zk_server_state. "},{"title":"Configuration Files","type":0,"sectionRef":"#","url":"en/operations/configuration-files","content":"","keywords":""},{"title":"Override​","type":1,"pageTitle":"Configuration Files","url":"en/operations/configuration-files#override","content":"Some settings specified in the main configuration file can be overridden in other configuration files: The replace or remove attributes can be specified for the elements of these configuration files.If neither is specified, it combines the contents of elements recursively, replacing values of duplicate children.If replace is specified, it replaces the entire element with the specified one.If remove is specified, it deletes the element. You can also declare attributes as coming from environment variables by using from_env=&quot;VARIABLE_NAME&quot;: &lt;clickhouse&gt; &lt;macros&gt; &lt;replica from_env=&quot;REPLICA&quot; /&gt; &lt;layer from_env=&quot;LAYER&quot; /&gt; &lt;shard from_env=&quot;SHARD&quot; /&gt; &lt;/macros&gt; &lt;/clickhouse&gt;  "},{"title":"Substitution​","type":1,"pageTitle":"Configuration Files","url":"en/operations/configuration-files#substitution","content":"The config can also define “substitutions”. If an element has the incl attribute, the corresponding substitution from the file will be used as the value. By default, the path to the file with substitutions is /etc/metrika.xml. This can be changed in the include_from element in the server config. The substitution values are specified in /clickhouse/substitution_name elements in this file. If a substitution specified in incl does not exist, it is recorded in the log. To prevent ClickHouse from logging missing substitutions, specify the optional=&quot;true&quot; attribute (for example, settings for macros). If you want to replace an entire element with a substitution use include as element name. XML substitution example: &lt;clickhouse&gt; &lt;!-- Appends XML subtree found at `/profiles-in-zookeeper` ZK path to `&lt;profiles&gt;` element. --&gt; &lt;profiles from_zk=&quot;/profiles-in-zookeeper&quot; /&gt; &lt;users&gt; &lt;!-- Replaces `include` element with the subtree found at `/users-in-zookeeper` ZK path. --&gt; &lt;include from_zk=&quot;/users-in-zookeeper&quot; /&gt; &lt;include from_zk=&quot;/other-users-in-zookeeper&quot; /&gt; &lt;/users&gt; &lt;/clickhouse&gt;  Substitutions can also be performed from ZooKeeper. To do this, specify the attribute from_zk = &quot;/path/to/node&quot;. The element value is replaced with the contents of the node at /path/to/node in ZooKeeper. You can also put an entire XML subtree on the ZooKeeper node and it will be fully inserted into the source element. "},{"title":"User Settings​","type":1,"pageTitle":"Configuration Files","url":"en/operations/configuration-files#user-settings","content":"The config.xml file can specify a separate config with user settings, profiles, and quotas. The relative path to this config is set in the users_config element. By default, it is users.xml. If users_config is omitted, the user settings, profiles, and quotas are specified directly in config.xml. Users configuration can be splitted into separate files similar to config.xml and config.d/. Directory name is defined as users_config setting without .xml postfix concatenated with .d. Directory users.d is used by default, as users_config defaults to users.xml. Note that configuration files are first merged taking into account Override settings and includes are processed after that. "},{"title":"XML example​","type":1,"pageTitle":"Configuration Files","url":"en/operations/configuration-files#example","content":"For example, you can have separate config file for each user like this: $ cat /etc/clickhouse-server/users.d/alice.xml  &lt;clickhouse&gt; &lt;users&gt; &lt;alice&gt; &lt;profile&gt;analytics&lt;/profile&gt; &lt;networks&gt; &lt;ip&gt;::/0&lt;/ip&gt; &lt;/networks&gt; &lt;password_sha256_hex&gt;...&lt;/password_sha256_hex&gt; &lt;quota&gt;analytics&lt;/quota&gt; &lt;/alice&gt; &lt;/users&gt; &lt;/clickhouse&gt;  "},{"title":"YAML examples​","type":1,"pageTitle":"Configuration Files","url":"en/operations/configuration-files#example","content":"Here you can see default config written in YAML: config.yaml.example. There are some differences between YAML and XML formats in terms of ClickHouse configurations. Here are some tips for writing a configuration in YAML format. You should use a Scalar node to write a key-value pair: key: value  To create a node, containing other nodes you should use a Map: map_key: key1: val1 key2: val2 key3: val3  To create a list of values or nodes assigned to one tag you should use a Sequence: seq_key: - val1 - val2 - key1: val3 - map: key2: val4 key3: val5  If you want to write an attribute for a Sequence or Map node, you should use a @ prefix before the attribute key. Note, that @ is reserved by YAML standard, so you should also to wrap it into double quotes: map: &quot;@attr1&quot;: value1 &quot;@attr2&quot;: value2 key: 123  From that Map we will get these XML nodes: &lt;map attr1=&quot;value1&quot; attr2=&quot;value2&quot;&gt; &lt;key&gt;123&lt;/key&gt; &lt;/map&gt;  You can also set attributes for Sequence: seq: - &quot;@attr1&quot;: value1 - &quot;@attr2&quot;: value2 - 123 - abc  So, we can get YAML config equal to this XML one: &lt;seq attr1=&quot;value1&quot; attr2=&quot;value2&quot;&gt;123&lt;/seq&gt; &lt;seq attr1=&quot;value1&quot; attr2=&quot;value2&quot;&gt;abc&lt;/seq&gt;  "},{"title":"Implementation Details​","type":1,"pageTitle":"Configuration Files","url":"en/operations/configuration-files#implementation-details","content":"For each config file, the server also generates file-preprocessed.xml files when starting. These files contain all the completed substitutions and overrides, and they are intended for informational use. If ZooKeeper substitutions were used in the config files but ZooKeeper is not available on the server start, the server loads the configuration from the preprocessed file. The server tracks changes in config files, as well as files and ZooKeeper nodes that were used when performing substitutions and overrides, and reloads the settings for users and clusters on the fly. This means that you can modify the cluster, users, and their settings without restarting the server. Original article "},{"title":"External User Authenticators and Directories","type":0,"sectionRef":"#","url":"en/operations/external-authenticators/","content":"External User Authenticators and Directories ClickHouse supports authenticating and managing users using external services. The following external authenticators and directories are supported: LDAP Authenticator and DirectoryKerberos AuthenticatorSSL X.509 authentication Original article","keywords":""},{"title":"Using the Kafka table engine","type":0,"sectionRef":"#","url":"en/integrations/kafka/kafka-table-engine","content":"","keywords":""},{"title":"Kafka to ClickHouse​","type":1,"pageTitle":"Using the Kafka table engine","url":"en/integrations/kafka/kafka-table-engine#kafka-to-clickhouse","content":"To utilise the Kafka table engine, the reader should be broadly familiar with ClickHouse materialized views. "},{"title":"Overview​","type":1,"pageTitle":"Using the Kafka table engine","url":"en/integrations/kafka/kafka-table-engine#overview","content":"Initially, we focus on the most common use case: using the Kafka table engine to insert data into ClickHouse from Kafka. The Kafka table engine allows ClickHouse to read from a Kafka topic directly. Whilst useful for viewing messages on a topic, the engine by design only permits one-time retrieval, i.e. when a query is issued to the table, it consumes data from the queue and increases the consumer offset before returning results to the caller. Data cannot, in effect, be re-read without resetting these offsets. To persist this data from a read of the table engine, we need a means of capturing the data and inserting it into another table. Trigger-based materialized views natively provide this functionality. A materialized view initiates a read on the table engine, receiving batches of documents. The TO clause determines the destination of the data - typically a table of the Merge Tree family. This process is visualized below:  "},{"title":"Steps​","type":1,"pageTitle":"Using the Kafka table engine","url":"en/integrations/kafka/kafka-table-engine#steps","content":"1. Prepare​ If you have data populated on a target topic, you can adapt the following for use in your dataset. Alternatively, a sample Github dataset is provided here. This dataset is used in the examples below and uses a reduced schema and subset of the rows (specifically, we limit to Github events concerning the ClickHouse repository), compared to the full dataset available here, for brevity. This is still sufficient for most of the queries published with the dataset to work. 2. Configure ClickHouse​ This step is required if you are connecting to a secure Kafka. These settings cannot be passed through the SQL DDL commands and must be configured in the ClickHouse config.xml. We assume you are connecting to a SASL secured instance. This is the simplest method when interacting with Confluent Cloud. &lt;clickhouse&gt; &lt;kafka&gt; &lt;sasl_username&gt;username&lt;/sasl_username&gt; &lt;sasl_password&gt;password&lt;/sasl_password&gt; &lt;security_protocol&gt;sasl_ssl&lt;/security_protocol&gt; &lt;sasl_mechanisms&gt;PLAIN&lt;/sasl_mechanisms&gt; &lt;/kafka&gt; &lt;/clickhouse&gt;  Either place the above snippet inside a new file under your conf.d/ directory or merge it into existing configuration files. For settings that can be configured, see here. 3. Create the destination table​ Prepare your destination table. In the example below we use the reduced GitHub schema for purposes of brevity. Note that although we use a MergeTree table engine, this example could easily be adapted for any member of the MergeTree family. CREATE TABLE github ( file_time DateTime, event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), actor_login LowCardinality(String), repo_name LowCardinality(String), created_at DateTime, updated_at DateTime, action Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20), comment_id UInt64, path String, ref LowCardinality(String), ref_type Enum('none' = 0, 'branch' = 1, 'tag' = 2, 'repository' = 3, 'unknown' = 4), creator_user_login LowCardinality(String), number UInt32, title String, labels Array(LowCardinality(String)), state Enum('none' = 0, 'open' = 1, 'closed' = 2), assignee LowCardinality(String), assignees Array(LowCardinality(String)), closed_at DateTime, merged_at DateTime, merge_commit_sha String, requested_reviewers Array(LowCardinality(String)), merged_by LowCardinality(String), review_comments UInt32, member_login LowCardinality(String) ) ENGINE = MergeTree ORDER BY (event_type, repo_name, created_at)  4. Create and populate the topic​ Kcat is recommended as a simple means of publishing data to a topic. Using the provided dataset with Confluent Cloud is as simple as modifying the configuration file and running the below example. The following assumes you have created the topic “github”. cat github_all_columns.ndjson | kafkacat -b &lt;host&gt;:&lt;port&gt; -X security.protocol=sasl_ssl -X sasl.mechanisms=PLAIN -X sasl.username=&lt;username&gt; -X sasl.password=&lt;password&gt; -t github  Note that this dataset is deliberately small, with only 200,000 rows. This should take only a few seconds to insert on most Kafka clusters, although this may depend on network connectivity. We include instructions to produce larger datasets should you need e.g. for performance testing. 5. Create the Kafka table engine​ The below example creates a table engine with the same schema as the merge tree table. Note that this isn’t required, e.g. you can have an alias or ephemeral columns in the target table. The settings are important; however - note the use of JSONEachRow as the data type for consuming JSON from a Kafka topic. The values “github” and “clickhouse” represent the name of the topic and consumer group names, respectively. Note that the topics can actually be a list of values. CREATE TABLE default.github_queue ( file_time DateTime, event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), actor_login LowCardinality(String), repo_name LowCardinality(String), created_at DateTime, updated_at DateTime, action Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20), comment_id UInt64, path String, ref LowCardinality(String), ref_type Enum('none' = 0, 'branch' = 1, 'tag' = 2, 'repository' = 3, 'unknown' = 4), creator_user_login LowCardinality(String), number UInt32, title String, labels Array(LowCardinality(String)), state Enum('none' = 0, 'open' = 1, 'closed' = 2), assignee LowCardinality(String), assignees Array(LowCardinality(String)), closed_at DateTime, merged_at DateTime, merge_commit_sha String, requested_reviewers Array(LowCardinality(String)), merged_by LowCardinality(String), review_comments UInt32, member_login LowCardinality(String) ) ENGINE = Kafka('kafka_host:9092', 'github', 'clickhouse', 'JSONEachRow') settings kafka_thread_per_consumer = 0, kafka_num_consumers = 1;  We discuss engine settings and performance tuning below. At this point, a simple select on the table github_queue should read some rows. Note that this will move the consumer offsets forward, preventing these rows from being re-read without a reset. Note the limit and required parameter stream_like_engine_allow_direct_select. 6. Create the materialized view​ The materialized view will connect the two previously created tables, reading data from the Kafka table engine and inserting it into the target merge tree table. We can do a number of data transformations. We will do a simple read and insert. The use of * assumes column names are identical (case sensitive). CREATE MATERIALIZED VIEW default.github_mv TO default.github AS SELECT * FROM default.github_queue;  At the point of creation, the materialized view connects to the Kafka engine and commences reading: inserting rows into the target table. This process will continue indefinitely, with subsequent message inserts into Kafka being consumed. Feel free to re-run the insertion script to insert further messages to Kafka. 7. Confirm rows have been inserted​ Confirm data exists in the target table: SELECT count() FROM default.github;  You should see 200,000 rows: | count\\(\\) | | :--- | | 200000 |  "},{"title":"Common Operations​","type":1,"pageTitle":"Using the Kafka table engine","url":"en/integrations/kafka/kafka-table-engine#common-operations","content":"Stopping &amp; restarting message consumption​ To stop message consumption, simply detach the Kafka engine table e.g. DETACH TABLE github_queue;  Note that this will not impact the offsets of the consumer group. To restart consumption, and continue from the previous offset, simply reattach the table. ATTACH TABLE github_queue;  We can use this operation to make setting and schema changes - see below. Adding Kafka Metadata​ It is not uncommon for users to need to identify the coordinates of the original Kafka messages for the rows in ClickHouse. For example, we may want to know how much of a specific topic or partition we have consumed. For this purpose, the Kafka table engine exposes several virtual columns. These can be persisted as columns in our target table by modifying our schema and materialized view’s select statement. First, we perform the stop operation described above before adding columns to our target table. DETACH TABLE github_queue;  Below we add information columns to identify the source topic and the partition from which the row originated. ALTER TABLE github ADD COLUMN topic String, ADD COLUMN partition UInt64;  Next, we need to ensure virtual columns are mapped as required. This requires us to drop and recreate our materialized view. Note those prefixed with _. A complete listing of virtual columns can be found here. DROP VIEW default.github_mv; CREATE MATERIALIZED VIEW default.github_mv TO default.github AS SELECT *, _topic as topic, _partition as partition FROM default.github_queue;  Finally, we are good to reattach our Kafka engine table github_queue and restart message consumption. ATTACH TABLE github_queue;  Newly consumed rows should have the metadata. SELECT actor_login, event_type, created_at, topic, partition FROM default.github LIMIT 10;  The result looks like: actor_login\tevent_type\tcreated_at\ttopic\tpartitionIgorMinar\tCommitCommentEvent\t2011-02-12 02:22:00\tgithub\t0 queeup\tCommitCommentEvent\t2011-02-12 02:23:23\tgithub\t0 IgorMinar\tCommitCommentEvent\t2011-02-12 02:23:24\tgithub\t0 IgorMinar\tCommitCommentEvent\t2011-02-12 02:24:50\tgithub\t0 IgorMinar\tCommitCommentEvent\t2011-02-12 02:25:20\tgithub\t0 dapi\tCommitCommentEvent\t2011-02-12 06:18:36\tgithub\t0 sourcerebels\tCommitCommentEvent\t2011-02-12 06:34:10\tgithub\t0 jamierumbelow\tCommitCommentEvent\t2011-02-12 12:21:40\tgithub\t0 jpn\tCommitCommentEvent\t2011-02-12 12:24:31\tgithub\t0 Oxonium\tCommitCommentEvent\t2011-02-12 12:31:28\tgithub\t0 Modify Kafka Engine Settings​ We recommend dropping the Kafka engine table and recreating it with the new settings. The materialized view does not need to be modified during this process - message consumption will resume once the Kafka engine table is recreated. Debugging Issues​ Errors such as authentication issues are not reported in responses to Kafka engine DDL. For diagnosing issues, we recommend using the main ClickHouse log file clickhouse-server.err.log. Further trace logging for the underlying Kafka client library librdkafka can be enabled through configuration. &lt;kafka&gt; &lt;debug&gt;all&lt;/debug&gt; &lt;/kafka&gt;  Handling malformed messages​ Kafka is often used as a &quot;dumping ground&quot; for data. This leads to topics containing mixed message formats and inconsistent field names. Avoid this and utilize Kafka features such Kafka Streams or ksqlDB to ensure messages are well-formed and consistent before insertion into Kafka. If these options are not possible, we can assist: Treat the message field as strings. Functions can be used in the materialized view statement to perform cleansing and casting if required. This should not represent a production solution but might assist in one-off ingestions.If you’re consuming JSON from a topic, using the JSONEachRow format, consider the setting input_format_skip_unknown_fields. Normally, when writing data, ClickHouse throws an exception if input data contains columns that do not exist in the target table. If this option is enabled, these excess columns will be ignored. Again this is not a production-level solution and might confuse others.Consider the setting kafka_skip_broken_messages. This requires the user to specify the level of tolerance per block for malformed messages - considered in the context of kafka_max_block_size. If this tolerance is exceeded (measured in absolute messages) the usual exception behaviour will revert, and other messages will be skipped.  Delivery Semantics and challenges with duplicates​ The Kafka table engine has at-least-once semantics. Duplicates are possible in several known rare circumstances. For example, messages could be read from Kafka and successfully inserted into ClickHouse. Before the new offset can be committed, the connection to Kafka is lost. A retry of the block in this situation is required. The block may be de-duplicated using a distributed table or ReplicatedMergeTree as the target table. While this reduces the chance of duplicate rows, it relies on identical blocks. Events such as a Kafka rebalancing may invalidate this assumption, causing duplicates in rare circumstances. Quorum based Inserts​ Users often need quorum-based inserts for cases where higher delivery guarantees are required in ClickHouse. This can’t be set on the materialized view or the target table. It can, however, be set for user profiles e.g. &lt;profiles&gt; &lt;default&gt; &lt;insert_quorum&gt;2&lt;/insert_quorum&gt; &lt;/default&gt; &lt;/profiles&gt;  "},{"title":"ClickHouse to Kafka​","type":1,"pageTitle":"Using the Kafka table engine","url":"en/integrations/kafka/kafka-table-engine#clickhouse-to-kafka","content":"Although a rarer use case, ClickHouse data can also be persisted in Kafka. For example, we will insert rows manually into a Kafka table engine. This data will be read by the same Kafka engine, whose materialized view will place the data into a Merge Tree table. Finally, we demonstrate the application of materialized views in inserts to Kafka to read tables from existing source tables. "},{"title":"Steps​","type":1,"pageTitle":"Using the Kafka table engine","url":"en/integrations/kafka/kafka-table-engine#steps-1","content":"Our initial objective is best illustrated:  We assume you have the tables and views created under steps for Kafka to ClickHouse and that the topic has been fully consumed. 1. Inserting rows directly​ First, confirm the count of the target table. SELECT count() FROM default.github;  You should have 200,000 rows: | count\\(\\) | | :--- | | 200000 |  Now insert rows from the GitHub target table back into the Kafka table engine github_queue. Note how we utilize JSONEachRow format and LIMIT the select to 100. INSERT INTO default.github_queue (*) SELECT file_time, event_type, actor_login, repo_name, created_at, updated_at, action, comment_id, path, ref, ref_type, creator_user_login, number, title, labels, state, assignee, assignees, closed_at, merged_at, merge_commit_sha, requested_reviewers, merged_by, review_comments, member_login FROM default.github LIMIT 100 FORMAT JSONEachRow; Recount the row in GitHub to confirm it has increased by 100. As shown in the above diagram, rows have been inserted into Kafka via the Kafka table engine before being re-read by the same engine and inserted into the GitHub target table by our materialized view! SELECT count() FROM default.github;  You should see 100 additional rows: | count\\(\\) | | :--- | | 200100 |  2. Utilizing materialized views​ We can utilize materialized views to push messages to a Kafka engine (and a topic) when documents are inserted into a table. When rows are inserted into the GitHub table, a materialized view is triggered, which causes the rows to be inserted back into a Kafka engine and into a new topic. Again this is best illustrated:  Create a new Kafka topic github_out or equivalent. Ensure a Kafka table engine github_out_queue points to this topic. ( file_time DateTime, event_type Enum('CommitCommentEvent' = 1, 'CreateEvent' = 2, 'DeleteEvent' = 3, 'ForkEvent' = 4, 'GollumEvent' = 5, 'IssueCommentEvent' = 6, 'IssuesEvent' = 7, 'MemberEvent' = 8, 'PublicEvent' = 9, 'PullRequestEvent' = 10, 'PullRequestReviewCommentEvent' = 11, 'PushEvent' = 12, 'ReleaseEvent' = 13, 'SponsorshipEvent' = 14, 'WatchEvent' = 15, 'GistEvent' = 16, 'FollowEvent' = 17, 'DownloadEvent' = 18, 'PullRequestReviewEvent' = 19, 'ForkApplyEvent' = 20, 'Event' = 21, 'TeamAddEvent' = 22), actor_login LowCardinality(String), repo_name LowCardinality(String), created_at DateTime, updated_at DateTime, action Enum('none' = 0, 'created' = 1, 'added' = 2, 'edited' = 3, 'deleted' = 4, 'opened' = 5, 'closed' = 6, 'reopened' = 7, 'assigned' = 8, 'unassigned' = 9, 'labeled' = 10, 'unlabeled' = 11, 'review_requested' = 12, 'review_request_removed' = 13, 'synchronize' = 14, 'started' = 15, 'published' = 16, 'update' = 17, 'create' = 18, 'fork' = 19, 'merged' = 20), comment_id UInt64, path String, ref LowCardinality(String), ref_type Enum('none' = 0, 'branch' = 1, 'tag' = 2, 'repository' = 3, 'unknown' = 4), creator_user_login LowCardinality(String), number UInt32, title String, labels Array(LowCardinality(String)), state Enum('none' = 0, 'open' = 1, 'closed' = 2), assignee LowCardinality(String), assignees Array(LowCardinality(String)), closed_at DateTime, merged_at DateTime, merge_commit_sha String, requested_reviewers Array(LowCardinality(String)), merged_by LowCardinality(String), review_comments UInt32, member_login LowCardinality(String) ) ENGINE = Kafka('host:port', 'github_out', 'clickhouse_out', 'JSONEachRow') settings kafka_thread_per_consumer = 0, kafka_num_consumers = 1;  Now create a new materialized view github_out_mv to point at the GitHub table, inserting rows to the above engine when it triggers. Additions to the GitHub table will, as a result, be pushed to our new Kafka topic. CREATE MATERIALIZED VIEW default.github_out_mv TO default.github_out_queue AS SELECT file_time, event_type, actor_login, repo_name, created_at, updated_at, action, comment_id, path, ref, ref_type, creator_user_login, number, title, labels, state, assignee, assignees, closed_at, merged_at, merge_commit_sha, requested_reviewers, merged_by, review_comments, member_login FROM default.github FORMAT JsonEachRow;  Should you insert into the original github topic, created as part of Kafka to ClickHouse, documents will magically appear in the “github_clickhouse” topic. Confirm this with native Kafka tooling. For example, below, we insert 100 rows onto the github topic using kcat for a Confluent Cloud hosted topic: head -n 10 github_all_columns.ndjson | kafkacat -b &lt;host&gt;:&lt;port&gt; -X security.protocol=sasl_ssl -X sasl.mechanisms=PLAIN -X sasl.username=&lt;username&gt; -X sasl.password=&lt;password&gt; -t github  A read on the github_out topic should confirm delivery of the messages. kafkacat -b &lt;host&gt;:&lt;port&gt; -X security.protocol=sasl_ssl -X sasl.mechanisms=PLAIN -X sasl.username=&lt;username&gt; -X sasl.password=&lt;password&gt; -t github_out -C -e -q | wc -l  Although an elaborate example, this illustrates the power of materialized views when used in conjunction with the Kafka engine. "},{"title":"Clusters and Performance​","type":1,"pageTitle":"Using the Kafka table engine","url":"en/integrations/kafka/kafka-table-engine#clusters-and-performance","content":""},{"title":"Working with ClickHouse Clusters​","type":1,"pageTitle":"Using the Kafka table engine","url":"en/integrations/kafka/kafka-table-engine#working-with-clickhouse-clusters","content":"Through Kafka consumer groups, multiple ClickHouse instances can potentially read from the same topic. Each consumer will be assigned to a topic partition in a 1:1 mapping. When scaling ClickHouse consumption using the Kafka table engine, consider that the total number of consumers within a cluster cannot exceed the number of partitions on the topic. Therefore ensure partitioning is appropriately configured for the topic in advance. Multiple ClickHouse instances can all be configured to read from a topic using the same consumer group id - specified during the Kafka table engine creation. Therefore, each instance will read from one or more partitions, inserting segments to their local target table. The target tables can, in turn, be configured to use a ReplicatedMergeTree to handle duplication of the data. This approach allows Kafka reads to be scaled with the ClickHouse cluster, provided there are sufficient Kafka partitions.  "},{"title":"Tuning Performance​","type":1,"pageTitle":"Using the Kafka table engine","url":"en/integrations/kafka/kafka-table-engine#tuning-performance","content":"Consider the following when looking to increase Kafka Engine table throughput performance: The performance will vary depending on the message size, format, and target table types. 100k rows/sec on a single table engine should be considered obtainable. By default, messages are read in blocks, controlled by the parameter kafka_max_block_size. By default, this is set to the max_block_size, defaulting to 65,536. Unless messages are extremely large, this should nearly always be increased. Values between 500k to 1M are not uncommon. Test and evaluate the effect on throughput performance.The number of consumers for a table engine can be increased using kafka_num_consumers. However, by default, inserts will be linearized in a single thread unless kafka_thread_per_consumer is changed from the default value of 1. Set this to 1 to ensure flushes are performed in parallel. Note that creating a Kafka engine table with N consumers (and kafka_thread_per_consumer=1) is logically equivalent to creating N Kafka engines, each with a materialized view and kafka_thread_per_consumer=0.Increasing consumers is not a free operation. Each consumer maintains its own buffers and threads, increasing the overhead on the server. Be conscious of the overhead of consumers and scale linearly across your cluster first and if possible.If the throughput of Kafka messages is variable and delays are acceptable, consider increasing the stream_flush_interval_ms to ensure larger blocks are flushed. background_schedule_pool_size sets the number of threads performing background tasks. These threads are used for Kafka streaming. This setting is applied at the ClickHouse server start and can’t be changed in a user session, defaulting to 128. It is unlikely you should ever need to change this as sufficient threads are available for the number of Kafka engines you will create on a single host. If you see timeouts in the logs, it may be appropriate to increase this.For communication with Kafka, the librdkafka library is used, which itself creates threads. Large numbers of Kafka tables, or consumers, can thus result in large numbers of context switches. Either distribute this load across the cluster, only replicating the target tables if possible, or consider using a table engine to read from multiple topics - a list of values is supported. Multiple materialized views can be read from a single table, each filtering to the data from a specific topic. Any settings changes should be tested. We recommend monitoring Kafka consumer lags to ensure you are properly scaled. "},{"title":"Additional Settings​","type":1,"pageTitle":"Using the Kafka table engine","url":"en/integrations/kafka/kafka-table-engine#additional-settings","content":"Aside from the settings discussed above, the following may be of interest: Kafka_max_wait_ms - The wait time in milliseconds for reading messages from Kafka before retry. Set at a user profile level and defaults to 5000. All settings from the underlying librdkafka can also be placed in the ClickHouse configuration files inside a kafka element - setting names should be XML elements with periods replaced with underscores e.g. &lt;clickhouse&gt; &lt;kafka&gt; &lt;enable_ssl_certificate_verification&gt;false&lt;/enable_ssl_certificate_verification&gt; &lt;/kafka&gt; &lt;/clickhouse&gt;  These are expert settings for which the user is referred to the Kafka documentation. "},{"title":"Kerberos","type":0,"sectionRef":"#","url":"en/operations/external-authenticators/kerberos","content":"","keywords":""},{"title":"Enabling Kerberos in ClickHouse​","type":1,"pageTitle":"Kerberos","url":"en/operations/external-authenticators/kerberos#enabling-kerberos-in-clickhouse","content":"To enable Kerberos, one should include kerberos section in config.xml. This section may contain additional parameters. Parameters:​ principal - canonical service principal name that will be acquired and used when accepting security contexts. This parameter is optional, if omitted, the default principal will be used. realm - a realm, that will be used to restrict authentication to only those requests whose initiator's realm matches it. This parameter is optional, if omitted, no additional filtering by realm will be applied. Example (goes into config.xml): &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;kerberos /&gt; &lt;/clickhouse&gt;  With principal specification: &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;kerberos&gt; &lt;principal&gt;HTTP/clickhouse.example.com@EXAMPLE.COM&lt;/principal&gt; &lt;/kerberos&gt; &lt;/clickhouse&gt;  With filtering by realm: &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;kerberos&gt; &lt;realm&gt;EXAMPLE.COM&lt;/realm&gt; &lt;/kerberos&gt; &lt;/clickhouse&gt;  warning You can define only one kerberos section. The presence of multiple kerberos sections will force ClickHouse to disable Kerberos authentication. warning principal and realm sections cannot be specified at the same time. The presence of both principal and realm sections will force ClickHouse to disable Kerberos authentication. "},{"title":"Kerberos as an external authenticator for existing users​","type":1,"pageTitle":"Kerberos","url":"en/operations/external-authenticators/kerberos#kerberos-as-an-external-authenticator-for-existing-users","content":"Kerberos can be used as a method for verifying the identity of locally defined users (users defined in users.xml or in local access control paths). Currently, only requests over the HTTP interface can be kerberized (via GSS-SPNEGO mechanism). Kerberos principal name format usually follows this pattern: primary/instance@REALM The /instance part may occur zero or more times. The primary part of the canonical principal name of the initiator is expected to match the kerberized user name for authentication to succeed. "},{"title":"Enabling Kerberos in users.xml​","type":1,"pageTitle":"Kerberos","url":"en/operations/external-authenticators/kerberos#enabling-kerberos-in-users-xml","content":"In order to enable Kerberos authentication for the user, specify kerberos section instead of password or similar sections in the user definition. Parameters: realm - a realm that will be used to restrict authentication to only those requests whose initiator's realm matches it. This parameter is optional, if omitted, no additional filtering by realm will be applied. Example (goes into users.xml): &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;users&gt; &lt;!- ... --&gt; &lt;my_user&gt; &lt;!- ... --&gt; &lt;kerberos&gt; &lt;realm&gt;EXAMPLE.COM&lt;/realm&gt; &lt;/kerberos&gt; &lt;/my_user&gt; &lt;/users&gt; &lt;/clickhouse&gt;  warning Note that Kerberos authentication cannot be used alongside with any other authentication mechanism. The presence of any other sections like password alongside kerberos will force ClickHouse to shutdown. Reminder Note, that now, once user my_user uses kerberos, Kerberos must be enabled in the main config.xml file as described previously. "},{"title":"Enabling Kerberos using SQL​","type":1,"pageTitle":"Kerberos","url":"en/operations/external-authenticators/kerberos#enabling-kerberos-using-sql","content":"When SQL-driven Access Control and Account Management is enabled in ClickHouse, users identified by Kerberos can also be created using SQL statements. CREATE USER my_user IDENTIFIED WITH kerberos REALM 'EXAMPLE.COM'  ...or, without filtering by realm: CREATE USER my_user IDENTIFIED WITH kerberos  "},{"title":"LDAP","type":0,"sectionRef":"#","url":"en/operations/external-authenticators/ldap","content":"","keywords":""},{"title":"LDAP Server Definition​","type":1,"pageTitle":"LDAP","url":"en/operations/external-authenticators/ldap#ldap-server-definition","content":"To define LDAP server you must add ldap_servers section to the config.xml. Example &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;ldap_servers&gt; &lt;!- Typical LDAP server. --&gt; &lt;my_ldap_server&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;636&lt;/port&gt; &lt;bind_dn&gt;uid={user_name},ou=users,dc=example,dc=com&lt;/bind_dn&gt; &lt;verification_cooldown&gt;300&lt;/verification_cooldown&gt; &lt;enable_tls&gt;yes&lt;/enable_tls&gt; &lt;tls_minimum_protocol_version&gt;tls1.2&lt;/tls_minimum_protocol_version&gt; &lt;tls_require_cert&gt;demand&lt;/tls_require_cert&gt; &lt;tls_cert_file&gt;/path/to/tls_cert_file&lt;/tls_cert_file&gt; &lt;tls_key_file&gt;/path/to/tls_key_file&lt;/tls_key_file&gt; &lt;tls_ca_cert_file&gt;/path/to/tls_ca_cert_file&lt;/tls_ca_cert_file&gt; &lt;tls_ca_cert_dir&gt;/path/to/tls_ca_cert_dir&lt;/tls_ca_cert_dir&gt; &lt;tls_cipher_suite&gt;ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:AES256-GCM-SHA384&lt;/tls_cipher_suite&gt; &lt;/my_ldap_server&gt; &lt;!- Typical Active Directory with configured user DN detection for further role mapping. --&gt; &lt;my_ad_server&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;389&lt;/port&gt; &lt;bind_dn&gt;EXAMPLE\\{user_name}&lt;/bind_dn&gt; &lt;user_dn_detection&gt; &lt;base_dn&gt;CN=Users,DC=example,DC=com&lt;/base_dn&gt; &lt;search_filter&gt;(&amp;amp;(objectClass=user)(sAMAccountName={user_name}))&lt;/search_filter&gt; &lt;/user_dn_detection&gt; &lt;enable_tls&gt;no&lt;/enable_tls&gt; &lt;/my_ad_server&gt; &lt;/ldap_servers&gt; &lt;/clickhouse&gt;  Note, that you can define multiple LDAP servers inside the ldap_servers section using distinct names. Parameters host — LDAP server hostname or IP, this parameter is mandatory and cannot be empty.port — LDAP server port, default is 636 if enable_tls is set to true, 389 otherwise.bind_dn — Template used to construct the DN to bind to. The resulting DN will be constructed by replacing all {user_name} substrings of the template with the actual user name during each authentication attempt. user_dn_detection — Section with LDAP search parameters for detecting the actual user DN of the bound user. This is mainly used in search filters for further role mapping when the server is Active Directory. The resulting user DN will be used when replacing {user_dn} substrings wherever they are allowed. By default, user DN is set equal to bind DN, but once search is performed, it will be updated with to the actual detected user DN value. base_dn — Template used to construct the base DN for the LDAP search. The resulting DN will be constructed by replacing all {user_name} and {bind_dn} substrings of the template with the actual user name and bind DN during the LDAP search. scope — Scope of the LDAP search. Accepted values are: base, one_level, children, subtree (the default). search_filter — Template used to construct the search filter for the LDAP search. The resulting filter will be constructed by replacing all {user_name}, {bind_dn}, and {base_dn} substrings of the template with the actual user name, bind DN, and base DN during the LDAP search.Note, that the special characters must be escaped properly in XML. verification_cooldown — A period of time, in seconds, after a successful bind attempt, during which the user will be assumed to be successfully authenticated for all consecutive requests without contacting the LDAP server. Specify 0 (the default) to disable caching and force contacting the LDAP server for each authentication request. enable_tls — A flag to trigger the use of the secure connection to the LDAP server. Specify no for plain text ldap:// protocol (not recommended).Specify yes for LDAP over SSL/TLS ldaps:// protocol (recommended, the default).Specify starttls for legacy StartTLS protocol (plain text ldap:// protocol, upgraded to TLS). tls_minimum_protocol_version — The minimum protocol version of SSL/TLS. Accepted values are: ssl2, ssl3, tls1.0, tls1.1, tls1.2 (the default). tls_require_cert — SSL/TLS peer certificate verification behavior. Accepted values are: never, allow, try, demand (the default). tls_cert_file — Path to certificate file.tls_key_file — Path to certificate key file.tls_ca_cert_file — Path to CA certificate file.tls_ca_cert_dir — Path to the directory containing CA certificates.tls_cipher_suite — Allowed cipher suite (in OpenSSL notation). "},{"title":"LDAP External Authenticator​","type":1,"pageTitle":"LDAP","url":"en/operations/external-authenticators/ldap#ldap-external-authenticator","content":"A remote LDAP server can be used as a method for verifying passwords for locally defined users (users defined in users.xml or in local access control paths). To achieve this, specify previously defined LDAP server name instead of password or similar sections in the user definition. At each login attempt, ClickHouse tries to &quot;bind&quot; to the specified DN defined by the bind_dn parameter in the LDAP server definition using the provided credentials, and if successful, the user is considered authenticated. This is often called a &quot;simple bind&quot; method. Example &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;users&gt; &lt;!- ... --&gt; &lt;my_user&gt; &lt;!- ... --&gt; &lt;ldap&gt; &lt;server&gt;my_ldap_server&lt;/server&gt; &lt;/ldap&gt; &lt;/my_user&gt; &lt;/users&gt; &lt;/clickhouse&gt;  Note, that user my_user refers to my_ldap_server. This LDAP server must be configured in the main config.xml file as described previously. When SQL-driven Access Control and Account Management is enabled, users that are authenticated by LDAP servers can also be created using the CREATE USER statement. Query: CREATE USER my_user IDENTIFIED WITH ldap SERVER 'my_ldap_server';  "},{"title":"LDAP Exernal User Directory​","type":1,"pageTitle":"LDAP","url":"en/operations/external-authenticators/ldap#ldap-external-user-directory","content":"In addition to the locally defined users, a remote LDAP server can be used as a source of user definitions. To achieve this, specify previously defined LDAP server name (see LDAP Server Definition) in the ldap section inside the users_directories section of the config.xml file. At each login attempt, ClickHouse tries to find the user definition locally and authenticate it as usual. If the user is not defined, ClickHouse will assume the definition exists in the external LDAP directory and will try to &quot;bind&quot; to the specified DN at the LDAP server using the provided credentials. If successful, the user will be considered existing and authenticated. The user will be assigned roles from the list specified in the roles section. Additionally, LDAP &quot;search&quot; can be performed and results can be transformed and treated as role names and then be assigned to the user if the role_mapping section is also configured. All this implies that the SQL-driven Access Control and Account Management is enabled and roles are created using the CREATE ROLE statement. Example Goes into config.xml. &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;user_directories&gt; &lt;!- Typical LDAP server. --&gt; &lt;ldap&gt; &lt;server&gt;my_ldap_server&lt;/server&gt; &lt;roles&gt; &lt;my_local_role1 /&gt; &lt;my_local_role2 /&gt; &lt;/roles&gt; &lt;role_mapping&gt; &lt;base_dn&gt;ou=groups,dc=example,dc=com&lt;/base_dn&gt; &lt;scope&gt;subtree&lt;/scope&gt; &lt;search_filter&gt;(&amp;amp;(objectClass=groupOfNames)(member={bind_dn}))&lt;/search_filter&gt; &lt;attribute&gt;cn&lt;/attribute&gt; &lt;prefix&gt;clickhouse_&lt;/prefix&gt; &lt;/role_mapping&gt; &lt;/ldap&gt; &lt;!- Typical Active Directory with role mapping that relies on the detected user DN. --&gt; &lt;ldap&gt; &lt;server&gt;my_ad_server&lt;/server&gt; &lt;role_mapping&gt; &lt;base_dn&gt;CN=Users,DC=example,DC=com&lt;/base_dn&gt; &lt;attribute&gt;CN&lt;/attribute&gt; &lt;scope&gt;subtree&lt;/scope&gt; &lt;search_filter&gt;(&amp;amp;(objectClass=group)(member={user_dn}))&lt;/search_filter&gt; &lt;prefix&gt;clickhouse_&lt;/prefix&gt; &lt;/role_mapping&gt; &lt;/ldap&gt; &lt;/user_directories&gt; &lt;/clickhouse&gt;  Note that my_ldap_server referred in the ldap section inside the user_directories section must be a previously defined LDAP server that is configured in the config.xml (see LDAP Server Definition). Parameters server — One of LDAP server names defined in the ldap_servers config section above. This parameter is mandatory and cannot be empty.roles — Section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server. If no roles are specified here or assigned during role mapping (below), user will not be able to perform any actions after authentication. role_mapping — Section with LDAP search parameters and mapping rules. When a user authenticates, while still bound to LDAP, an LDAP search is performed using search_filter and the name of the logged-in user. For each entry found during that search, the value of the specified attribute is extracted. For each attribute value that has the specified prefix, the prefix is removed, and the rest of the value becomes the name of a local role defined in ClickHouse, which is expected to be created beforehand by the CREATE ROLE statement.There can be multiple role_mapping sections defined inside the same ldap section. All of them will be applied. base_dn — Template used to construct the base DN for the LDAP search. The resulting DN will be constructed by replacing all {user_name}, {bind_dn}, and {user_dn} substrings of the template with the actual user name, bind DN, and user DN during each LDAP search. scope — Scope of the LDAP search. Accepted values are: base, one_level, children, subtree (the default). search_filter — Template used to construct the search filter for the LDAP search. The resulting filter will be constructed by replacing all {user_name}, {bind_dn}, {user_dn}, and {base_dn} substrings of the template with the actual user name, bind DN, user DN, and base DN during each LDAP search.Note, that the special characters must be escaped properly in XML. attribute — Attribute name whose values will be returned by the LDAP search. cn, by default.prefix — Prefix, that will be expected to be in front of each string in the original list of strings returned by the LDAP search. The prefix will be removed from the original strings and the resulting strings will be treated as local role names. Empty by default. Original article "},{"title":"SSL X.509 certificate authentication","type":0,"sectionRef":"#","url":"en/operations/external-authenticators/ssl-x509","content":"SSL X.509 certificate authentication SSL 'strict' option enables mandatory certificate validation for the incoming connections. In this case, only connections with trusted certificates can be established. Connections with untrusted certificates will be rejected. Thus, certificate validation allows to uniquely authenticate an incoming connection. Common Name field of the certificate is used to identify connected user. This allows to associate multiple certificates with the same user. Additionally, reissuing and revoking of the certificates does not affect the ClickHouse configuration. To enable SSL certificate authentication, a list of Common Name's for each ClickHouse user must be specified in the settings file users.xml : Example &lt;clickhouse&gt; &lt;!- ... --&gt; &lt;users&gt; &lt;user_name&gt; &lt;ssl_certificates&gt; &lt;common_name&gt;host.domain.com:example_user&lt;/common_name&gt; &lt;common_name&gt;host.domain.com:example_user_dev&lt;/common_name&gt; &lt;!-- More names --&gt; &lt;/ssl_certificates&gt; &lt;!-- Other settings --&gt; &lt;/user_name&gt; &lt;/users&gt; &lt;/clickhouse&gt; For the SSL chain of trust to work correctly, it is also important to make sure that the caConfig parameter is configured properly.","keywords":""},{"title":"Monitoring","type":0,"sectionRef":"#","url":"en/operations/monitoring","content":"","keywords":""},{"title":"Resource Utilization​","type":1,"pageTitle":"Monitoring","url":"en/operations/monitoring#resource-utilization","content":"ClickHouse does not monitor the state of hardware resources by itself. It is highly recommended to set up monitoring for: Load and temperature on processors. You can use dmesg, turbostat or other instruments. Utilization of storage system, RAM and network. "},{"title":"ClickHouse Server Metrics​","type":1,"pageTitle":"Monitoring","url":"en/operations/monitoring#clickhouse-server-metrics","content":"ClickHouse server has embedded instruments for self-state monitoring. To track server events use server logs. See the logger section of the configuration file. ClickHouse collects: Different metrics of how the server uses computational resources.Common statistics on query processing. You can find metrics in the system.metrics, system.events, and system.asynchronous_metrics tables. You can configure ClickHouse to export metrics to Graphite. See the Graphite section in the ClickHouse server configuration file. Before configuring export of metrics, you should set up Graphite by following their official guide. You can configure ClickHouse to export metrics to Prometheus. See the Prometheus section in the ClickHouse server configuration file. Before configuring export of metrics, you should set up Prometheus by following their official guide. Additionally, you can monitor server availability through the HTTP API. Send the HTTP GET request to /ping. If the server is available, it responds with 200 OK. To monitor servers in a cluster configuration, you should set the max_replica_delay_for_distributed_queries parameter and use the HTTP resource /replicas_status. A request to /replicas_status returns 200 OK if the replica is available and is not delayed behind the other replicas. If a replica is delayed, it returns 503 HTTP_SERVICE_UNAVAILABLE with information about the gap. "},{"title":"Storing details for connecting to external sources in configuration files","type":0,"sectionRef":"#","url":"en/operations/named-collections","content":"","keywords":""},{"title":"Named connections for accessing S3.​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"en/operations/named-collections#named-connections-for-accessing-s3","content":"The description of parameters see s3 Table Function. Example of configuration: &lt;clickhouse&gt; &lt;named_collections&gt; &lt;s3_mydata&gt; &lt;access_key_id&gt;AKIAIOSFODNN7EXAMPLE&lt;/access_key_id&gt; &lt;secret_access_key&gt; wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY&lt;/secret_access_key&gt; &lt;format&gt;CSV&lt;/format&gt; &lt;url&gt;https://s3.us-east-1.amazonaws.com/yourbucket/mydata/&lt;/url&gt; &lt;/s3_mydata&gt; &lt;/named_collections&gt; &lt;/clickhouse&gt;  "},{"title":"Example of using named connections with the s3 function​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"en/operations/named-collections#example-of-using-named-connections-with-the-s3-function","content":"INSERT INTO FUNCTION s3(s3_mydata, filename = 'test_file.tsv.gz', format = 'TSV', structure = 'number UInt64', compression_method = 'gzip') SELECT * FROM numbers(10000); SELECT count() FROM s3(s3_mydata, filename = 'test_file.tsv.gz') ┌─count()─┐ │ 10000 │ └─────────┘ 1 rows in set. Elapsed: 0.279 sec. Processed 10.00 thousand rows, 90.00 KB (35.78 thousand rows/s., 322.02 KB/s.)  "},{"title":"Example of using named connections with an S3 table​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"en/operations/named-collections#example-of-using-named-connections-with-an-s3-table","content":"CREATE TABLE s3_engine_table (number Int64) ENGINE=S3(s3_mydata, url='https://s3.us-east-1.amazonaws.com/yourbucket/mydata/test_file.tsv.gz', format = 'TSV') SETTINGS input_format_with_names_use_header = 0; SELECT * FROM s3_engine_table LIMIT 3; ┌─number─┐ │ 0 │ │ 1 │ │ 2 │ └────────┘  "},{"title":"Named connections for accessing MySQL database​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"en/operations/named-collections#named-connections-for-accessing-mysql-database","content":"The description of parameters see mysql. Example of configuration: &lt;clickhouse&gt; &lt;named_collections&gt; &lt;mymysql&gt; &lt;user&gt;myuser&lt;/user&gt; &lt;password&gt;mypass&lt;/password&gt; &lt;host&gt;127.0.0.1&lt;/host&gt; &lt;port&gt;3306&lt;/port&gt; &lt;database&gt;test&lt;/database&gt; &lt;connection_pool_size&gt;8&lt;/connection_pool_size&gt; &lt;on_duplicate_clause&gt;1&lt;/on_duplicate_clause&gt; &lt;replace_query&gt;1&lt;/replace_query&gt; &lt;/mymysql&gt; &lt;/named_collections&gt; &lt;/clickhouse&gt;  "},{"title":"Example of using named connections with the mysql function​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"en/operations/named-collections#example-of-using-named-connections-with-the-mysql-function","content":"SELECT count() FROM mysql(mymysql, table = 'test'); ┌─count()─┐ │ 3 │ └─────────┘  "},{"title":"Example of using named connections with an MySQL table​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"en/operations/named-collections#example-of-using-named-connections-with-an-mysql-table","content":"CREATE TABLE mytable(A Int64) ENGINE = MySQL(mymysql, table = 'test', connection_pool_size=3, replace_query=0); SELECT count() FROM mytable; ┌─count()─┐ │ 3 │ └─────────┘  "},{"title":"Example of using named connections with database with engine MySQL​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"en/operations/named-collections#example-of-using-named-connections-with-database-with-engine-mysql","content":"CREATE DATABASE mydatabase ENGINE = MySQL(mymysql); SHOW TABLES FROM mydatabase; ┌─name───┐ │ source │ │ test │ └────────┘  "},{"title":"Example of using named connections with an external dictionary with source MySQL​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"en/operations/named-collections#example-of-using-named-connections-with-an-external-dictionary-with-source-mysql","content":"CREATE DICTIONARY dict (A Int64, B String) PRIMARY KEY A SOURCE(MYSQL(NAME mymysql TABLE 'source')) LIFETIME(MIN 1 MAX 2) LAYOUT(HASHED()); SELECT dictGet('dict', 'B', 2); ┌─dictGet('dict', 'B', 2)─┐ │ two │ └─────────────────────────┘  "},{"title":"Named connections for accessing PostgreSQL database​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"en/operations/named-collections#named-connections-for-accessing-postgresql-database","content":"The description of parameters see postgresql. Example of configuration: &lt;clickhouse&gt; &lt;named_collections&gt; &lt;mypg&gt; &lt;user&gt;pguser&lt;/user&gt; &lt;password&gt;jw8s0F4&lt;/password&gt; &lt;host&gt;127.0.0.1&lt;/host&gt; &lt;port&gt;5432&lt;/port&gt; &lt;database&gt;test&lt;/database&gt; &lt;schema&gt;test_schema&lt;/schema&gt; &lt;connection_pool_size&gt;8&lt;/connection_pool_size&gt; &lt;/mypg&gt; &lt;/named_collections&gt; &lt;/clickhouse&gt;  "},{"title":"Example of using named connections with the postgresql function​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"en/operations/named-collections#example-of-using-named-connections-with-the-postgresql-function","content":"SELECT * FROM postgresql(mypg, table = 'test'); ┌─a─┬─b───┐ │ 2 │ two │ │ 1 │ one │ └───┴─────┘ SELECT * FROM postgresql(mypg, table = 'test', schema = 'public'); ┌─a─┐ │ 1 │ │ 2 │ │ 3 │ └───┘  "},{"title":"Example of using named connections with database with engine PostgreSQL​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"en/operations/named-collections#example-of-using-named-connections-with-database-with-engine-postgresql","content":"CREATE TABLE mypgtable (a Int64) ENGINE = PostgreSQL(mypg, table = 'test', schema = 'public'); SELECT * FROM mypgtable; ┌─a─┐ │ 1 │ │ 2 │ │ 3 │ └───┘  "},{"title":"Example of using named connections with database with engine PostgreSQL​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"en/operations/named-collections#example-of-using-named-connections-with-database-with-engine-postgresql-1","content":"CREATE DATABASE mydatabase ENGINE = PostgreSQL(mypg); SHOW TABLES FROM mydatabase ┌─name─┐ │ test │ └──────┘  "},{"title":"Example of using named connections with an external dictionary with source POSTGRESQL​","type":1,"pageTitle":"Storing details for connecting to external sources in configuration files","url":"en/operations/named-collections#example-of-using-named-connections-with-an-external-dictionary-with-source-postgresql","content":"CREATE DICTIONARY dict (a Int64, b String) PRIMARY KEY a SOURCE(POSTGRESQL(NAME mypg TABLE test)) LIFETIME(MIN 1 MAX 2) LAYOUT(HASHED()); SELECT dictGet('dict', 'b', 2); ┌─dictGet('dict', 'b', 2)─┐ │ two │ └─────────────────────────┘  "},{"title":"[experimental] OpenTelemetry Support","type":0,"sectionRef":"#","url":"en/operations/opentelemetry","content":"","keywords":""},{"title":"Supplying Trace Context to ClickHouse​","type":1,"pageTitle":"[experimental] OpenTelemetry Support","url":"en/operations/opentelemetry#supplying-trace-context-to-clickhouse","content":"ClickHouse accepts trace context HTTP headers, as described by the W3C recommendation. It also accepts trace context over a native protocol that is used for communication between ClickHouse servers or between the client and server. For manual testing, trace context headers conforming to the Trace Context recommendation can be supplied to clickhouse-client using --opentelemetry-traceparent and --opentelemetry-tracestate flags. If no parent trace context is supplied or the provided trace context does not comply with W3C standard above, ClickHouse can start a new trace, with probability controlled by the opentelemetry_start_trace_probability setting. "},{"title":"Propagating the Trace Context​","type":1,"pageTitle":"[experimental] OpenTelemetry Support","url":"en/operations/opentelemetry#propagating-the-trace-context","content":"The trace context is propagated to downstream services in the following cases: Queries to remote ClickHouse servers, such as when using Distributed table engine. url table function. Trace context information is sent in HTTP headers. "},{"title":"Tracing the ClickHouse Itself​","type":1,"pageTitle":"[experimental] OpenTelemetry Support","url":"en/operations/opentelemetry#tracing-the-clickhouse-itself","content":"ClickHouse creates trace spans for each query and some of the query execution stages, such as query planning or distributed queries. To be useful, the tracing information has to be exported to a monitoring system that supports OpenTelemetry, such as Jaeger or Prometheus. ClickHouse avoids a dependency on a particular monitoring system, instead only providing the tracing data through a system table. OpenTelemetry trace span information required by the standard is stored in the system.opentelemetry_span_log table. The table must be enabled in the server configuration, see the opentelemetry_span_log element in the default config file config.xml. It is enabled by default. The tags or attributes are saved as two parallel arrays, containing the keys and values. Use ARRAY JOIN to work with them. "},{"title":"Integration with monitoring systems​","type":1,"pageTitle":"[experimental] OpenTelemetry Support","url":"en/operations/opentelemetry#integration-with-monitoring-systems","content":"At the moment, there is no ready tool that can export the tracing data from ClickHouse to a monitoring system. For testing, it is possible to setup the export using a materialized view with the URL engine over the system.opentelemetry_span_log table, which would push the arriving log data to an HTTP endpoint of a trace collector. For example, to push the minimal span data to a Zipkin instance running at http://localhost:9411, in Zipkin v2 JSON format: CREATE MATERIALIZED VIEW default.zipkin_spans ENGINE = URL('http://127.0.0.1:9411/api/v2/spans', 'JSONEachRow') SETTINGS output_format_json_named_tuples_as_objects = 1, output_format_json_array_of_rows = 1 AS SELECT lower(hex(trace_id)) AS traceId, case when parent_span_id = 0 then '' else lower(hex(parent_span_id)) end AS parentId, lower(hex(span_id)) AS id, operation_name AS name, start_time_us AS timestamp, finish_time_us - start_time_us AS duration, cast(tuple('clickhouse'), 'Tuple(serviceName text)') AS localEndpoint, cast(tuple( attribute.values[indexOf(attribute.names, 'db.statement')]), 'Tuple(&quot;db.statement&quot; text)') AS tags FROM system.opentelemetry_span_log  In case of any errors, the part of the log data for which the error has occurred will be silently lost. Check the server log for error messages if the data does not arrive. Original article "},{"title":"Optimizing Performance","type":0,"sectionRef":"#","url":"en/operations/optimizing-performance/","content":"Optimizing Performance Sampling query profiler","keywords":""},{"title":"Sampling Query Profiler","type":0,"sectionRef":"#","url":"en/operations/optimizing-performance/sampling-query-profiler","content":"","keywords":""},{"title":"Example​","type":1,"pageTitle":"Sampling Query Profiler","url":"en/operations/optimizing-performance/sampling-query-profiler#example","content":"In this example we: Filtering trace_log data by a query identifier and the current date. Aggregating by stack trace. Using introspection functions, we will get a report of: Names of symbols and corresponding source code functions.Source code locations of these functions. SELECT count(), arrayStringConcat(arrayMap(x -&gt; concat(demangle(addressToSymbol(x)), '\\n ', addressToLine(x)), trace), '\\n') AS sym FROM system.trace_log WHERE (query_id = 'ebca3574-ad0a-400a-9cbc-dca382f5998c') AND (event_date = today()) GROUP BY trace ORDER BY count() DESC LIMIT 10  {% include &quot;examples/sampling_query_profiler_result.txt&quot; %}  "},{"title":"How to Test Your Hardware with ClickHouse","type":0,"sectionRef":"#","url":"en/operations/performance-test","content":"","keywords":""},{"title":"Automated Run​","type":1,"pageTitle":"How to Test Your Hardware with ClickHouse","url":"en/operations/performance-test#automated-run","content":"You can run benchmark with a single script. Download the script. wget https://raw.githubusercontent.com/ClickHouse/ClickHouse/master/benchmark/hardware.sh  Run the script. chmod a+x ./hardware.sh ./hardware.sh  Copy the output and send it to feedback@clickhouse.com All the results are published here: https://clickhouse.com/benchmark/hardware/ "},{"title":"Manual Run​","type":1,"pageTitle":"How to Test Your Hardware with ClickHouse","url":"en/operations/performance-test#manual-run","content":"Alternatively you can perform benchmark in the following steps. ssh to the server and download the binary with wget: # For amd64: wget https://builds.clickhouse.com/master/amd64/clickhouse # For aarch64: wget https://builds.clickhouse.com/master/aarch64/clickhouse # For powerpc64le: wget https://builds.clickhouse.com/master/powerpc64le/clickhouse # For freebsd: wget https://builds.clickhouse.com/master/freebsd/clickhouse # For freebsd-aarch64: wget https://builds.clickhouse.com/master/freebsd-aarch64/clickhouse # For freebsd-powerpc64le: wget https://builds.clickhouse.com/master/freebsd-powerpc64le/clickhouse # For macos: wget https://builds.clickhouse.com/master/macos/clickhouse # For macos-aarch64: wget https://builds.clickhouse.com/master/macos-aarch64/clickhouse # Then do: chmod a+x clickhouse  Download benchmark files: wget https://raw.githubusercontent.com/ClickHouse/ClickHouse/master/benchmark/clickhouse/benchmark-new.sh chmod a+x benchmark-new.sh wget https://raw.githubusercontent.com/ClickHouse/ClickHouse/master/benchmark/clickhouse/queries.sql  Download the web analytics dataset (“hits” table containing 100 million rows). wget https://datasets.clickhouse.com/hits/partitions/hits_100m_obfuscated_v1.tar.xz tar xvf hits_100m_obfuscated_v1.tar.xz -C . mv hits_100m_obfuscated_v1/* .  Run the server: ./clickhouse server  Check the data: ssh to the server in another terminal ./clickhouse client --query &quot;SELECT count() FROM hits_100m_obfuscated&quot; 100000000  Run the benchmark: ./benchmark-new.sh hits_100m_obfuscated  Send the numbers and the info about your hardware configuration to feedback@clickhouse.com All the results are published here: https://clickhouse.com/benchmark/hardware/ "},{"title":"Quotas","type":0,"sectionRef":"#","url":"en/operations/quotas","content":"Quotas Quotas allow you to limit resource usage over a period of time or track the use of resources. Quotas are set up in the user config, which is usually ‘users.xml’. The system also has a feature for limiting the complexity of a single query. See the section Restrictions on query complexity. In contrast to query complexity restrictions, quotas: Place restrictions on a set of queries that can be run over a period of time, instead of limiting a single query.Account for resources spent on all remote servers for distributed query processing. Let’s look at the section of the ‘users.xml’ file that defines quotas. &lt;!-- Quotas --&gt; &lt;quotas&gt; &lt;!-- Quota name. --&gt; &lt;default&gt; &lt;!-- Restrictions for a time period. You can set many intervals with different restrictions. --&gt; &lt;interval&gt; &lt;!-- Length of the interval. --&gt; &lt;duration&gt;3600&lt;/duration&gt; &lt;!-- Unlimited. Just collect data for the specified time interval. --&gt; &lt;queries&gt;0&lt;/queries&gt; &lt;query_selects&gt;0&lt;/query_selects&gt; &lt;query_inserts&gt;0&lt;/query_inserts&gt; &lt;errors&gt;0&lt;/errors&gt; &lt;result_rows&gt;0&lt;/result_rows&gt; &lt;read_rows&gt;0&lt;/read_rows&gt; &lt;execution_time&gt;0&lt;/execution_time&gt; &lt;/interval&gt; &lt;/default&gt; By default, the quota tracks resource consumption for each hour, without limiting usage. The resource consumption calculated for each interval is output to the server log after each request. &lt;statbox&gt; &lt;!-- Restrictions for a time period. You can set many intervals with different restrictions. --&gt; &lt;interval&gt; &lt;!-- Length of the interval. --&gt; &lt;duration&gt;3600&lt;/duration&gt; &lt;queries&gt;1000&lt;/queries&gt; &lt;query_selects&gt;100&lt;/query_selects&gt; &lt;query_inserts&gt;100&lt;/query_inserts&gt; &lt;errors&gt;100&lt;/errors&gt; &lt;result_rows&gt;1000000000&lt;/result_rows&gt; &lt;read_rows&gt;100000000000&lt;/read_rows&gt; &lt;execution_time&gt;900&lt;/execution_time&gt; &lt;/interval&gt; &lt;interval&gt; &lt;duration&gt;86400&lt;/duration&gt; &lt;queries&gt;10000&lt;/queries&gt; &lt;query_selects&gt;10000&lt;/query_selects&gt; &lt;query_inserts&gt;10000&lt;/query_inserts&gt; &lt;errors&gt;1000&lt;/errors&gt; &lt;result_rows&gt;5000000000&lt;/result_rows&gt; &lt;read_rows&gt;500000000000&lt;/read_rows&gt; &lt;execution_time&gt;7200&lt;/execution_time&gt; &lt;/interval&gt; &lt;/statbox&gt; For the ‘statbox’ quota, restrictions are set for every hour and for every 24 hours (86,400 seconds). The time interval is counted, starting from an implementation-defined fixed moment in time. In other words, the 24-hour interval does not necessarily begin at midnight. When the interval ends, all collected values are cleared. For the next hour, the quota calculation starts over. Here are the amounts that can be restricted: queries – The total number of requests. query_selects – The total number of select requests. query_inserts – The total number of insert requests. errors – The number of queries that threw an exception. result_rows – The total number of rows given as a result. read_rows – The total number of source rows read from tables for running the query on all remote servers. execution_time – The total query execution time, in seconds (wall time). If the limit is exceeded for at least one time interval, an exception is thrown with a text about which restriction was exceeded, for which interval, and when the new interval begins (when queries can be sent again). Quotas can use the “quota key” feature to report on resources for multiple keys independently. Here is an example of this: &lt;!-- For the global reports designer. --&gt; &lt;web_global&gt; &lt;!-- keyed – The quota_key &quot;key&quot; is passed in the query parameter, and the quota is tracked separately for each key value. For example, you can pass a username as the key, so the quota will be counted separately for each username. Using keys makes sense only if quota_key is transmitted by the program, not by a user. You can also write &lt;keyed_by_ip /&gt;, so the IP address is used as the quota key. (But keep in mind that users can change the IPv6 address fairly easily.) --&gt; &lt;keyed /&gt; The quota is assigned to users in the ‘users’ section of the config. See the section “Access rights”. For distributed query processing, the accumulated amounts are stored on the requestor server. So if the user goes to another server, the quota there will “start over”. When the server is restarted, quotas are reset. Original article","keywords":""},{"title":"Requirements","type":0,"sectionRef":"#","url":"en/operations/requirements","content":"","keywords":""},{"title":"CPU​","type":1,"pageTitle":"Requirements","url":"en/operations/requirements#cpu","content":"For installation from prebuilt deb packages, use a CPU with x86_64 architecture and support for SSE 4.2 instructions. To run ClickHouse with processors that do not support SSE 4.2 or have AArch64 or PowerPC64LE architecture, you should build ClickHouse from sources. ClickHouse implements parallel data processing and uses all the hardware resources available. When choosing a processor, take into account that ClickHouse works more efficiently at configurations with a large number of cores but a lower clock rate than at configurations with fewer cores and a higher clock rate. For example, 16 cores with 2600 MHz is preferable to 8 cores with 3600 MHz. It is recommended to use Turbo Boost and hyper-threading technologies. It significantly improves performance with a typical workload. "},{"title":"RAM​","type":1,"pageTitle":"Requirements","url":"en/operations/requirements#ram","content":"We recommend using a minimum of 4GB of RAM to perform non-trivial queries. The ClickHouse server can run with a much smaller amount of RAM, but it requires memory for processing queries. The required volume of RAM depends on: The complexity of queries.The amount of data that is processed in queries. To calculate the required volume of RAM, you should estimate the size of temporary data for GROUP BY, DISTINCT, JOIN and other operations you use. ClickHouse can use external memory for temporary data. See GROUP BY in External Memory for details. "},{"title":"Swap File​","type":1,"pageTitle":"Requirements","url":"en/operations/requirements#swap-file","content":"Disable the swap file for production environments. "},{"title":"Storage Subsystem​","type":1,"pageTitle":"Requirements","url":"en/operations/requirements#storage-subsystem","content":"You need to have 2GB of free disk space to install ClickHouse. The volume of storage required for your data should be calculated separately. Assessment should include: Estimation of the data volume. You can take a sample of the data and get the average size of a row from it. Then multiply the value by the number of rows you plan to store. The data compression coefficient. To estimate the data compression coefficient, load a sample of your data into ClickHouse, and compare the actual size of the data with the size of the table stored. For example, clickstream data is usually compressed by 6-10 times. To calculate the final volume of data to be stored, apply the compression coefficient to the estimated data volume. If you plan to store data in several replicas, then multiply the estimated volume by the number of replicas. "},{"title":"Network​","type":1,"pageTitle":"Requirements","url":"en/operations/requirements#network","content":"If possible, use networks of 10G or higher class. The network bandwidth is critical for processing distributed queries with a large amount of intermediate data. Besides, network speed affects replication processes. "},{"title":"Software​","type":1,"pageTitle":"Requirements","url":"en/operations/requirements#software","content":"ClickHouse is developed primarily for the Linux family of operating systems. The recommended Linux distribution is Ubuntu. The tzdata package should be installed in the system. ClickHouse can also work in other operating system families. See details in the install guide section of the documentation. "},{"title":"HTTP Interface","type":0,"sectionRef":"#","url":"en/interfaces/http","content":"","keywords":""},{"title":"Compression​","type":1,"pageTitle":"HTTP Interface","url":"en/interfaces/http#compression","content":"You can use compression to reduce network traffic when transmitting a large amount of data or for creating dumps that are immediately compressed. You can use the internal ClickHouse compression format when transmitting data. The compressed data has a non-standard format, and you need clickhouse-compressor program to work with it. It is installed with the clickhouse-client package. To increase the efficiency of data insertion, you can disable server-side checksum verification by using the http_native_compression_disable_checksumming_on_decompress setting. If you specify compress=1 in the URL, the server will compress the data it sends to you. If you specify decompress=1 in the URL, the server will decompress the data which you pass in the POST method. You can also choose to use HTTP compression. ClickHouse supports the following compression methods: gzipbrdeflatexz To send a compressed POST request, append the request header Content-Encoding: compression_method. In order for ClickHouse to compress the response, enable compression with enable_http_compression setting and append Accept-Encoding: compression_method header to the request. You can configure the data compression level in the http_zlib_compression_level setting for all compression methods. info Some HTTP clients might decompress data from the server by default (with gzip and deflate) and you might get decompressed data even if you use the compression settings correctly. Examples # Sending compressed data to the server $ echo &quot;SELECT 1&quot; | gzip -c | \\ curl -sS --data-binary @- -H 'Content-Encoding: gzip' 'http://localhost:8123/'  # Receiving compressed data archive from the server $ curl -vsS &quot;http://localhost:8123/?enable_http_compression=1&quot; \\ -H 'Accept-Encoding: gzip' --output result.gz -d 'SELECT number FROM system.numbers LIMIT 3' $ zcat result.gz 0 1 2  # Receiving compressed data from the server and using the gunzip to receive decompressed data $ curl -sS &quot;http://localhost:8123/?enable_http_compression=1&quot; \\ -H 'Accept-Encoding: gzip' -d 'SELECT number FROM system.numbers LIMIT 3' | gunzip - 0 1 2  "},{"title":"Default Database​","type":1,"pageTitle":"HTTP Interface","url":"en/interfaces/http#default-database","content":"You can use the ‘database’ URL parameter or the ‘X-ClickHouse-Database’ header to specify the default database. $ echo 'SELECT number FROM numbers LIMIT 10' | curl 'http://localhost:8123/?database=system' --data-binary @- 0 1 2 3 4 5 6 7 8 9  By default, the database that is registered in the server settings is used as the default database. By default, this is the database called ‘default’. Alternatively, you can always specify the database using a dot before the table name. The username and password can be indicated in one of three ways: Using HTTP Basic Authentication. Example: $ echo 'SELECT 1' | curl 'http://user:password@localhost:8123/' -d @-  In the ‘user’ and ‘password’ URL parameters. Example: $ echo 'SELECT 1' | curl 'http://localhost:8123/?user=user&amp;password=password' -d @-  Using ‘X-ClickHouse-User’ and ‘X-ClickHouse-Key’ headers. Example: $ echo 'SELECT 1' | curl -H 'X-ClickHouse-User: user' -H 'X-ClickHouse-Key: password' 'http://localhost:8123/' -d @-  If the user name is not specified, the default name is used. If the password is not specified, the empty password is used. You can also use the URL parameters to specify any settings for processing a single query or entire profiles of settings. Example:http://localhost:8123/?profile=web&amp;max_rows_to_read=1000000000&amp;query=SELECT+1 For more information, see the Settings section. $ echo 'SELECT number FROM system.numbers LIMIT 10' | curl 'http://localhost:8123/?' --data-binary @- 0 1 2 3 4 5 6 7 8 9  For information about other parameters, see the section “SET”. Similarly, you can use ClickHouse sessions in the HTTP protocol. To do this, you need to add the session_id GET parameter to the request. You can use any string as the session ID. By default, the session is terminated after 60 seconds of inactivity. To change this timeout, modify the default_session_timeout setting in the server configuration, or add the session_timeout GET parameter to the request. To check the session status, use the session_check=1 parameter. Only one query at a time can be executed within a single session. You can receive information about the progress of a query in X-ClickHouse-Progress response headers. To do this, enable send_progress_in_http_headers. Example of the header sequence: X-ClickHouse-Progress: {&quot;read_rows&quot;:&quot;2752512&quot;,&quot;read_bytes&quot;:&quot;240570816&quot;,&quot;total_rows_to_read&quot;:&quot;8880128&quot;} X-ClickHouse-Progress: {&quot;read_rows&quot;:&quot;5439488&quot;,&quot;read_bytes&quot;:&quot;482285394&quot;,&quot;total_rows_to_read&quot;:&quot;8880128&quot;} X-ClickHouse-Progress: {&quot;read_rows&quot;:&quot;8783786&quot;,&quot;read_bytes&quot;:&quot;819092887&quot;,&quot;total_rows_to_read&quot;:&quot;8880128&quot;}  Possible header fields: read_rows — Number of rows read.read_bytes — Volume of data read in bytes.total_rows_to_read — Total number of rows to be read.written_rows — Number of rows written.written_bytes — Volume of data written in bytes. Running requests do not stop automatically if the HTTP connection is lost. Parsing and data formatting are performed on the server-side, and using the network might be ineffective. The optional ‘query_id’ parameter can be passed as the query ID (any string). For more information, see the section “Settings, replace_running_query”. The optional ‘quota_key’ parameter can be passed as the quota key (any string). For more information, see the section “Quotas”. The HTTP interface allows passing external data (external temporary tables) for querying. For more information, see the section “External data for query processing”. "},{"title":"Response Buffering​","type":1,"pageTitle":"HTTP Interface","url":"en/interfaces/http#response-buffering","content":"You can enable response buffering on the server-side. The buffer_size and wait_end_of_query URL parameters are provided for this purpose. buffer_size determines the number of bytes in the result to buffer in the server memory. If a result body is larger than this threshold, the buffer is written to the HTTP channel, and the remaining data is sent directly to the HTTP channel. To ensure that the entire response is buffered, set wait_end_of_query=1. In this case, the data that is not stored in memory will be buffered in a temporary server file. Example: $ curl -sS 'http://localhost:8123/?max_result_bytes=4000000&amp;buffer_size=3000000&amp;wait_end_of_query=1' -d 'SELECT toUInt8(number) FROM system.numbers LIMIT 9000000 FORMAT RowBinary'  Use buffering to avoid situations where a query processing error occurred after the response code and HTTP headers were sent to the client. In this situation, an error message is written at the end of the response body, and on the client-side, the error can only be detected at the parsing stage. "},{"title":"Queries with Parameters​","type":1,"pageTitle":"HTTP Interface","url":"en/interfaces/http#cli-queries-with-parameters","content":"You can create a query with parameters and pass values for them from the corresponding HTTP request parameters. For more information, see Queries with Parameters for CLI. "},{"title":"Example​","type":1,"pageTitle":"HTTP Interface","url":"en/interfaces/http#example","content":"$ curl -sS &quot;&lt;address&gt;?param_id=2&amp;param_phrase=test&quot; -d &quot;SELECT * FROM table WHERE int_column = {id:UInt8} and string_column = {phrase:String}&quot;  "},{"title":"Predefined HTTP Interface​","type":1,"pageTitle":"HTTP Interface","url":"en/interfaces/http#predefined_http_interface","content":"ClickHouse supports specific queries through the HTTP interface. For example, you can write data to a table as follows: $ echo '(4),(5),(6)' | curl 'http://localhost:8123/?query=INSERT%20INTO%20t%20VALUES' --data-binary @-  ClickHouse also supports Predefined HTTP Interface which can help you more easily integrate with third-party tools like Prometheus exporter. Example: First of all, add this section to server configuration file: &lt;http_handlers&gt; &lt;rule&gt; &lt;url&gt;/predefined_query&lt;/url&gt; &lt;methods&gt;POST,GET&lt;/methods&gt; &lt;handler&gt; &lt;type&gt;predefined_query_handler&lt;/type&gt; &lt;query&gt;SELECT * FROM system.metrics LIMIT 5 FORMAT Template SETTINGS format_template_resultset = 'prometheus_template_output_format_resultset', format_template_row = 'prometheus_template_output_format_row', format_template_rows_between_delimiter = '\\n'&lt;/query&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;rule&gt;...&lt;/rule&gt; &lt;rule&gt;...&lt;/rule&gt; &lt;/http_handlers&gt;  You can now request the URL directly for data in the Prometheus format: $ curl -v 'http://localhost:8123/predefined_query' * Trying ::1... * Connected to localhost (::1) port 8123 (#0) &gt; GET /predefined_query HTTP/1.1 &gt; Host: localhost:8123 &gt; User-Agent: curl/7.47.0 &gt; Accept: */* &gt; &lt; HTTP/1.1 200 OK &lt; Date: Tue, 28 Apr 2020 08:52:56 GMT &lt; Connection: Keep-Alive &lt; Content-Type: text/plain; charset=UTF-8 &lt; X-ClickHouse-Server-Display-Name: i-mloy5trc &lt; Transfer-Encoding: chunked &lt; X-ClickHouse-Query-Id: 96fe0052-01e6-43ce-b12a-6b7370de6e8a &lt; X-ClickHouse-Format: Template &lt; X-ClickHouse-Timezone: Asia/Shanghai &lt; Keep-Alive: timeout=3 &lt; X-ClickHouse-Summary: {&quot;read_rows&quot;:&quot;0&quot;,&quot;read_bytes&quot;:&quot;0&quot;,&quot;written_rows&quot;:&quot;0&quot;,&quot;written_bytes&quot;:&quot;0&quot;,&quot;total_rows_to_read&quot;:&quot;0&quot;} &lt; # HELP &quot;Query&quot; &quot;Number of executing queries&quot; # TYPE &quot;Query&quot; counter &quot;Query&quot; 1 # HELP &quot;Merge&quot; &quot;Number of executing background merges&quot; # TYPE &quot;Merge&quot; counter &quot;Merge&quot; 0 # HELP &quot;PartMutation&quot; &quot;Number of mutations (ALTER DELETE/UPDATE)&quot; # TYPE &quot;PartMutation&quot; counter &quot;PartMutation&quot; 0 # HELP &quot;ReplicatedFetch&quot; &quot;Number of data parts being fetched from replica&quot; # TYPE &quot;ReplicatedFetch&quot; counter &quot;ReplicatedFetch&quot; 0 # HELP &quot;ReplicatedSend&quot; &quot;Number of data parts being sent to replicas&quot; # TYPE &quot;ReplicatedSend&quot; counter &quot;ReplicatedSend&quot; 0 * Connection #0 to host localhost left intact * Connection #0 to host localhost left intact  As you can see from the example if http_handlers is configured in the config.xml file and http_handlers can contain many rules. ClickHouse will match the HTTP requests received to the predefined type in rule and the first matched runs the handler. Then ClickHouse will execute the corresponding predefined query if the match is successful. Now rule can configure method, headers, url, handler: method is responsible for matching the method part of the HTTP request. method fully conforms to the definition of method in the HTTP protocol. It is an optional configuration. If it is not defined in the configuration file, it does not match the method portion of the HTTP request. url is responsible for matching the URL part of the HTTP request. It is compatible with RE2’s regular expressions. It is an optional configuration. If it is not defined in the configuration file, it does not match the URL portion of the HTTP request. headers are responsible for matching the header part of the HTTP request. It is compatible with RE2’s regular expressions. It is an optional configuration. If it is not defined in the configuration file, it does not match the header portion of the HTTP request. handler contains the main processing part. Now handler can configure type, status, content_type, response_content, query, query_param_name.type currently supports three types: predefined_query_handler, dynamic_query_handler, static. query — use with predefined_query_handler type, executes query when the handler is called. query_param_name — use with dynamic_query_handler type, extracts and executes the value corresponding to the query_param_name value in HTTP request params. status — use with static type, response status code. content_type — use with any type, response content-type. response_content — use with static type, response content sent to client, when using the prefix ‘file://’ or ‘config://’, find the content from the file or configuration sends to client. Next are the configuration methods for different type. "},{"title":"predefined_query_handler​","type":1,"pageTitle":"HTTP Interface","url":"en/interfaces/http#predefined_query_handler","content":"predefined_query_handler supports setting Settings and query_params values. You can configure query in the type of predefined_query_handler. query value is a predefined query of predefined_query_handler, which is executed by ClickHouse when an HTTP request is matched and the result of the query is returned. It is a must configuration. The following example defines the values of max_threads and max_final_threads settings, then queries the system table to check whether these settings were set successfully. warning To keep the default handlers such as query, play, ping, add the &lt;defaults/&gt; rule. Example: &lt;http_handlers&gt; &lt;rule&gt; &lt;url&gt;&lt;![CDATA[/query_param_with_url/\\w+/(?P&lt;name_1&gt;[^/]+)(/(?P&lt;name_2&gt;[^/]+))?]]&gt;&lt;/url&gt; &lt;methods&gt;GET&lt;/methods&gt; &lt;headers&gt; &lt;XXX&gt;TEST_HEADER_VALUE&lt;/XXX&gt; &lt;PARAMS_XXX&gt;&lt;![CDATA[(?P&lt;name_1&gt;[^/]+)(/(?P&lt;name_2&gt;[^/]+))?]]&gt;&lt;/PARAMS_XXX&gt; &lt;/headers&gt; &lt;handler&gt; &lt;type&gt;predefined_query_handler&lt;/type&gt; &lt;query&gt;SELECT value FROM system.settings WHERE name = {name_1:String}&lt;/query&gt; &lt;query&gt;SELECT name, value FROM system.settings WHERE name = {name_2:String}&lt;/query&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;defaults/&gt; &lt;/http_handlers&gt;  $ curl -H 'XXX:TEST_HEADER_VALUE' -H 'PARAMS_XXX:max_threads' 'http://localhost:8123/query_param_with_url/1/max_threads/max_final_threads?max_threads=1&amp;max_final_threads=2' 1 max_final_threads 2  warning In one predefined_query_handler only supports one query of an insert type. "},{"title":"dynamic_query_handler​","type":1,"pageTitle":"HTTP Interface","url":"en/interfaces/http#dynamic_query_handler","content":"In dynamic_query_handler, the query is written in the form of param of the HTTP request. The difference is that in predefined_query_handler, the query is written in the configuration file. You can configure query_param_name in dynamic_query_handler. ClickHouse extracts and executes the value corresponding to the query_param_name value in the URL of the HTTP request. The default value of query_param_name is /query . It is an optional configuration. If there is no definition in the configuration file, the param is not passed in. To experiment with this functionality, the example defines the values of max_threads and max_final_threads and queries whether the settings were set successfully. Example: &lt;http_handlers&gt; &lt;rule&gt; &lt;headers&gt; &lt;XXX&gt;TEST_HEADER_VALUE_DYNAMIC&lt;/XXX&gt; &lt;/headers&gt; &lt;handler&gt; &lt;type&gt;dynamic_query_handler&lt;/type&gt; &lt;query_param_name&gt;query_param&lt;/query_param_name&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;defaults/&gt; &lt;/http_handlers&gt;  $ curl -H 'XXX:TEST_HEADER_VALUE_DYNAMIC' 'http://localhost:8123/own?max_threads=1&amp;max_final_threads=2&amp;param_name_1=max_threads&amp;param_name_2=max_final_threads&amp;query_param=SELECT%20name,value%20FROM%20system.settings%20where%20name%20=%20%7Bname_1:String%7D%20OR%20name%20=%20%7Bname_2:String%7D' max_threads 1 max_final_threads 2  "},{"title":"static​","type":1,"pageTitle":"HTTP Interface","url":"en/interfaces/http#static","content":"static can return content_type, status and response_content. response_content can return the specified content. Example: Return a message. &lt;http_handlers&gt; &lt;rule&gt; &lt;methods&gt;GET&lt;/methods&gt; &lt;headers&gt;&lt;XXX&gt;xxx&lt;/XXX&gt;&lt;/headers&gt; &lt;url&gt;/hi&lt;/url&gt; &lt;handler&gt; &lt;type&gt;static&lt;/type&gt; &lt;status&gt;402&lt;/status&gt; &lt;content_type&gt;text/html; charset=UTF-8&lt;/content_type&gt; &lt;response_content&gt;Say Hi!&lt;/response_content&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;defaults/&gt; &lt;/http_handlers&gt;  $ curl -vv -H 'XXX:xxx' 'http://localhost:8123/hi' * Trying ::1... * Connected to localhost (::1) port 8123 (#0) &gt; GET /hi HTTP/1.1 &gt; Host: localhost:8123 &gt; User-Agent: curl/7.47.0 &gt; Accept: */* &gt; XXX:xxx &gt; &lt; HTTP/1.1 402 Payment Required &lt; Date: Wed, 29 Apr 2020 03:51:26 GMT &lt; Connection: Keep-Alive &lt; Content-Type: text/html; charset=UTF-8 &lt; Transfer-Encoding: chunked &lt; Keep-Alive: timeout=3 &lt; X-ClickHouse-Summary: {&quot;read_rows&quot;:&quot;0&quot;,&quot;read_bytes&quot;:&quot;0&quot;,&quot;written_rows&quot;:&quot;0&quot;,&quot;written_bytes&quot;:&quot;0&quot;,&quot;total_rows_to_read&quot;:&quot;0&quot;} &lt; * Connection #0 to host localhost left intact Say Hi!%  Find the content from the configuration send to client. &lt;get_config_static_handler&gt;&lt;![CDATA[&lt;html ng-app=&quot;SMI2&quot;&gt;&lt;head&gt;&lt;base href=&quot;http://ui.tabix.io/&quot;&gt;&lt;/head&gt;&lt;body&gt;&lt;div ui-view=&quot;&quot; class=&quot;content-ui&quot;&gt;&lt;/div&gt;&lt;script src=&quot;http://loader.tabix.io/master.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;]]&gt;&lt;/get_config_static_handler&gt; &lt;http_handlers&gt; &lt;rule&gt; &lt;methods&gt;GET&lt;/methods&gt; &lt;headers&gt;&lt;XXX&gt;xxx&lt;/XXX&gt;&lt;/headers&gt; &lt;url&gt;/get_config_static_handler&lt;/url&gt; &lt;handler&gt; &lt;type&gt;static&lt;/type&gt; &lt;response_content&gt;config://get_config_static_handler&lt;/response_content&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;/http_handlers&gt;  $ curl -v -H 'XXX:xxx' 'http://localhost:8123/get_config_static_handler' * Trying ::1... * Connected to localhost (::1) port 8123 (#0) &gt; GET /get_config_static_handler HTTP/1.1 &gt; Host: localhost:8123 &gt; User-Agent: curl/7.47.0 &gt; Accept: */* &gt; XXX:xxx &gt; &lt; HTTP/1.1 200 OK &lt; Date: Wed, 29 Apr 2020 04:01:24 GMT &lt; Connection: Keep-Alive &lt; Content-Type: text/plain; charset=UTF-8 &lt; Transfer-Encoding: chunked &lt; Keep-Alive: timeout=3 &lt; X-ClickHouse-Summary: {&quot;read_rows&quot;:&quot;0&quot;,&quot;read_bytes&quot;:&quot;0&quot;,&quot;written_rows&quot;:&quot;0&quot;,&quot;written_bytes&quot;:&quot;0&quot;,&quot;total_rows_to_read&quot;:&quot;0&quot;} &lt; * Connection #0 to host localhost left intact &lt;html ng-app=&quot;SMI2&quot;&gt;&lt;head&gt;&lt;base href=&quot;http://ui.tabix.io/&quot;&gt;&lt;/head&gt;&lt;body&gt;&lt;div ui-view=&quot;&quot; class=&quot;content-ui&quot;&gt;&lt;/div&gt;&lt;script src=&quot;http://loader.tabix.io/master.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;%  Find the content from the file send to client. &lt;http_handlers&gt; &lt;rule&gt; &lt;methods&gt;GET&lt;/methods&gt; &lt;headers&gt;&lt;XXX&gt;xxx&lt;/XXX&gt;&lt;/headers&gt; &lt;url&gt;/get_absolute_path_static_handler&lt;/url&gt; &lt;handler&gt; &lt;type&gt;static&lt;/type&gt; &lt;content_type&gt;text/html; charset=UTF-8&lt;/content_type&gt; &lt;response_content&gt;file:///absolute_path_file.html&lt;/response_content&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;rule&gt; &lt;methods&gt;GET&lt;/methods&gt; &lt;headers&gt;&lt;XXX&gt;xxx&lt;/XXX&gt;&lt;/headers&gt; &lt;url&gt;/get_relative_path_static_handler&lt;/url&gt; &lt;handler&gt; &lt;type&gt;static&lt;/type&gt; &lt;content_type&gt;text/html; charset=UTF-8&lt;/content_type&gt; &lt;response_content&gt;file://./relative_path_file.html&lt;/response_content&gt; &lt;/handler&gt; &lt;/rule&gt; &lt;/http_handlers&gt;  $ user_files_path='/var/lib/clickhouse/user_files' $ sudo echo &quot;&lt;html&gt;&lt;body&gt;Relative Path File&lt;/body&gt;&lt;/html&gt;&quot; &gt; $user_files_path/relative_path_file.html $ sudo echo &quot;&lt;html&gt;&lt;body&gt;Absolute Path File&lt;/body&gt;&lt;/html&gt;&quot; &gt; $user_files_path/absolute_path_file.html $ curl -vv -H 'XXX:xxx' 'http://localhost:8123/get_absolute_path_static_handler' * Trying ::1... * Connected to localhost (::1) port 8123 (#0) &gt; GET /get_absolute_path_static_handler HTTP/1.1 &gt; Host: localhost:8123 &gt; User-Agent: curl/7.47.0 &gt; Accept: */* &gt; XXX:xxx &gt; &lt; HTTP/1.1 200 OK &lt; Date: Wed, 29 Apr 2020 04:18:16 GMT &lt; Connection: Keep-Alive &lt; Content-Type: text/html; charset=UTF-8 &lt; Transfer-Encoding: chunked &lt; Keep-Alive: timeout=3 &lt; X-ClickHouse-Summary: {&quot;read_rows&quot;:&quot;0&quot;,&quot;read_bytes&quot;:&quot;0&quot;,&quot;written_rows&quot;:&quot;0&quot;,&quot;written_bytes&quot;:&quot;0&quot;,&quot;total_rows_to_read&quot;:&quot;0&quot;} &lt; &lt;html&gt;&lt;body&gt;Absolute Path File&lt;/body&gt;&lt;/html&gt; * Connection #0 to host localhost left intact $ curl -vv -H 'XXX:xxx' 'http://localhost:8123/get_relative_path_static_handler' * Trying ::1... * Connected to localhost (::1) port 8123 (#0) &gt; GET /get_relative_path_static_handler HTTP/1.1 &gt; Host: localhost:8123 &gt; User-Agent: curl/7.47.0 &gt; Accept: */* &gt; XXX:xxx &gt; &lt; HTTP/1.1 200 OK &lt; Date: Wed, 29 Apr 2020 04:18:31 GMT &lt; Connection: Keep-Alive &lt; Content-Type: text/html; charset=UTF-8 &lt; Transfer-Encoding: chunked &lt; Keep-Alive: timeout=3 &lt; X-ClickHouse-Summary: {&quot;read_rows&quot;:&quot;0&quot;,&quot;read_bytes&quot;:&quot;0&quot;,&quot;written_rows&quot;:&quot;0&quot;,&quot;written_bytes&quot;:&quot;0&quot;,&quot;total_rows_to_read&quot;:&quot;0&quot;} &lt; &lt;html&gt;&lt;body&gt;Relative Path File&lt;/body&gt;&lt;/html&gt; * Connection #0 to host localhost left intact  "},{"title":"Server Configuration Parameters","type":0,"sectionRef":"#","url":"en/operations/server-configuration-parameters/","content":"Server Configuration Parameters This section contains descriptions of server settings that cannot be changed at the session or query level. These settings are stored in the config.xml file on the ClickHouse server. Other settings are described in the “Settings” section. Before studying the settings, read the Configuration files section and note the use of substitutions (the incl and optional attributes). Original article","keywords":""},{"title":"Settings Overview","type":0,"sectionRef":"#","url":"en/operations/settings/","content":"","keywords":""},{"title":"Custom Settings​","type":1,"pageTitle":"Settings Overview","url":"en/operations/settings/#custom_settings","content":"In addition to the common settings, users can define custom settings. A custom setting name must begin with one of predefined prefixes. The list of these prefixes must be declared in the custom_settings_prefixes parameter in the server configuration file. &lt;custom_settings_prefixes&gt;custom_&lt;/custom_settings_prefixes&gt;  To define a custom setting use SET command: SET custom_a = 123;  To get the current value of a custom setting use getSetting() function: SELECT getSetting('custom_a');  See Also Server Configuration Settings Original article "},{"title":"Constraints on Settings","type":0,"sectionRef":"#","url":"en/operations/settings/constraints-on-settings","content":"Constraints on Settings The constraints on settings can be defined in the profiles section of the user.xml configuration file and prohibit users from changing some of the settings with the SET query. The constraints are defined as the following: &lt;profiles&gt; &lt;user_name&gt; &lt;constraints&gt; &lt;setting_name_1&gt; &lt;min&gt;lower_boundary&lt;/min&gt; &lt;/setting_name_1&gt; &lt;setting_name_2&gt; &lt;max&gt;upper_boundary&lt;/max&gt; &lt;/setting_name_2&gt; &lt;setting_name_3&gt; &lt;min&gt;lower_boundary&lt;/min&gt; &lt;max&gt;upper_boundary&lt;/max&gt; &lt;/setting_name_3&gt; &lt;setting_name_4&gt; &lt;readonly/&gt; &lt;/setting_name_4&gt; &lt;/constraints&gt; &lt;/user_name&gt; &lt;/profiles&gt; If the user tries to violate the constraints an exception is thrown and the setting isn’t changed. There are supported three types of constraints: min, max, readonly. The min and max constraints specify upper and lower boundaries for a numeric setting and can be used in combination. The readonly constraint specifies that the user cannot change the corresponding setting at all. Example: Let users.xml includes lines: &lt;profiles&gt; &lt;default&gt; &lt;max_memory_usage&gt;10000000000&lt;/max_memory_usage&gt; &lt;force_index_by_date&gt;0&lt;/force_index_by_date&gt; ... &lt;constraints&gt; &lt;max_memory_usage&gt; &lt;min&gt;5000000000&lt;/min&gt; &lt;max&gt;20000000000&lt;/max&gt; &lt;/max_memory_usage&gt; &lt;force_index_by_date&gt; &lt;readonly/&gt; &lt;/force_index_by_date&gt; &lt;/constraints&gt; &lt;/default&gt; &lt;/profiles&gt; The following queries all throw exceptions: SET max_memory_usage=20000000001; SET max_memory_usage=4999999999; SET force_index_by_date=1; Code: 452, e.displayText() = DB::Exception: Setting max_memory_usage should not be greater than 20000000000. Code: 452, e.displayText() = DB::Exception: Setting max_memory_usage should not be less than 5000000000. Code: 452, e.displayText() = DB::Exception: Setting force_index_by_date should not be changed. Note: the default profile has special handling: all the constraints defined for the default profile become the default constraints, so they restrict all the users until they’re overridden explicitly for these users. Original article","keywords":""},{"title":"Memory overcommit","type":0,"sectionRef":"#","url":"en/operations/settings/memory-overcommit","content":"","keywords":""},{"title":"User overcommit tracker​","type":1,"pageTitle":"Memory overcommit","url":"en/operations/settings/memory-overcommit#user-overcommit-tracker","content":"User overcommit tracker finds a query with the biggest overcommit ratio in the user's query list. Overcommit ratio for a query is computed as number of allocated bytes divided by value of memory_overcommit_ratio_denominator setting. If memory_overcommit_ratio_denominator for the query is equals to zero, overcommit tracker won't choose this query. Waiting timeout is set by memory_usage_overcommit_max_wait_microseconds setting. Example SELECT number FROM numbers(1000) GROUP BY number SETTINGS memory_overcommit_ratio_denominator=4000, memory_usage_overcommit_max_wait_microseconds=500  "},{"title":"Global overcommit tracker​","type":1,"pageTitle":"Memory overcommit","url":"en/operations/settings/memory-overcommit#global-overcommit-tracker","content":"Global overcommit tracker finds a query with the biggest overcommit ratio in the list of all queries. In this case overcommit ratio is computed as number of allocated bytes divided by value of memory_overcommit_ratio_denominator_for_user setting. If memory_overcommit_ratio_denominator_for_user for the query is equals to zero, overcommit tracker won't choose this query. Waiting timeout is set by global_memory_usage_overcommit_max_wait_microseconds parameter in the configuration file. "},{"title":"MergeTree tables settings","type":0,"sectionRef":"#","url":"en/operations/settings/merge-tree-settings","content":"","keywords":""},{"title":"parts_to_throw_insert​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#parts-to-throw-insert","content":"If the number of active parts in a single partition exceeds the parts_to_throw_insert value, INSERT is interrupted with the Too many parts (N). Merges are processing significantly slower than inserts exception. Possible values: Any positive integer. Default value: 300. To achieve maximum performance of SELECT queries, it is necessary to minimize the number of parts processed, see Merge Tree. You can set a larger value to 600 (1200), this will reduce the probability of the Too many parts error, but at the same time SELECT performance might degrade. Also in case of a merge issue (for example, due to insufficient disk space) you will notice it later than it could be with the original 300. "},{"title":"parts_to_delay_insert​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#parts-to-delay-insert","content":"If the number of active parts in a single partition exceeds the parts_to_delay_insert value, an INSERT artificially slows down. Possible values: Any positive integer. Default value: 150. ClickHouse artificially executes INSERT longer (adds ‘sleep’) so that the background merge process can merge parts faster than they are added. "},{"title":"inactive_parts_to_throw_insert​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#inactive-parts-to-throw-insert","content":"If the number of inactive parts in a single partition more than the inactive_parts_to_throw_insert value, INSERT is interrupted with the &quot;Too many inactive parts (N). Parts cleaning are processing significantly slower than inserts&quot; exception. Possible values: Any positive integer. Default value: 0 (unlimited). "},{"title":"inactive_parts_to_delay_insert​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#inactive-parts-to-delay-insert","content":"If the number of inactive parts in a single partition in the table at least that many the inactive_parts_to_delay_insert value, an INSERT artificially slows down. It is useful when a server fails to clean up parts quickly enough. Possible values: Any positive integer. Default value: 0 (unlimited). "},{"title":"max_delay_to_insert​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#max-delay-to-insert","content":"The value in seconds, which is used to calculate the INSERT delay, if the number of active parts in a single partition exceeds the parts_to_delay_insert value. Possible values: Any positive integer. Default value: 1. The delay (in milliseconds) for INSERT is calculated by the formula: max_k = parts_to_throw_insert - parts_to_delay_insert k = 1 + parts_count_in_partition - parts_to_delay_insert delay_milliseconds = pow(max_delay_to_insert * 1000, k / max_k)  For example if a partition has 299 active parts and parts_to_throw_insert = 300, parts_to_delay_insert = 150, max_delay_to_insert = 1, INSERT is delayed for pow( 1 * 1000, (1 + 299 - 150) / (300 - 150) ) = 1000 milliseconds. "},{"title":"max_parts_in_total​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#max-parts-in-total","content":"If the total number of active parts in all partitions of a table exceeds the max_parts_in_total value INSERT is interrupted with the Too many parts (N) exception. Possible values: Any positive integer. Default value: 100000. A large number of parts in a table reduces performance of ClickHouse queries and increases ClickHouse boot time. Most often this is a consequence of an incorrect design (mistakes when choosing a partitioning strategy - too small partitions). "},{"title":"replicated_deduplication_window​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#replicated-deduplication-window","content":"The number of most recently inserted blocks for which Zookeeper stores hash sums to check for duplicates. Possible values: Any positive integer.0 (disable deduplication) Default value: 100. The Insert command creates one or more blocks (parts). When inserting into Replicated tables, ClickHouse for insert deduplication writes the hash sums of the created parts into Zookeeper. Hash sums are stored only for the most recent replicated_deduplication_window blocks. The oldest hash sums are removed from Zookeeper. A large number of replicated_deduplication_window slows down Inserts because it needs to compare more entries. The hash sum is calculated from the composition of the field names and types and the data of the inserted part (stream of bytes). "},{"title":"non_replicated_deduplication_window​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#non-replicated-deduplication-window","content":"The number of the most recently inserted blocks in the non-replicated MergeTree table for which hash sums are stored to check for duplicates. Possible values: Any positive integer.0 (disable deduplication). Default value: 0. A deduplication mechanism is used, similar to replicated tables (see replicated_deduplication_window setting). The hash sums of the created parts are written to a local file on a disk. "},{"title":"replicated_deduplication_window_seconds​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#replicated-deduplication-window-seconds","content":"The number of seconds after which the hash sums of the inserted blocks are removed from Zookeeper. Possible values: Any positive integer. Default value: 604800 (1 week). Similar to replicated_deduplication_window, replicated_deduplication_window_seconds specifies how long to store hash sums of blocks for insert deduplication. Hash sums older than replicated_deduplication_window_seconds are removed from Zookeeper, even if they are less than replicated_deduplication_window. "},{"title":"replicated_fetches_http_connection_timeout​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#replicated_fetches_http_connection_timeout","content":"HTTP connection timeout (in seconds) for part fetch requests. Inherited from default profile http_connection_timeout if not set explicitly. Possible values: Any positive integer.0 - Use value of http_connection_timeout. Default value: 0. "},{"title":"replicated_fetches_http_send_timeout​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#replicated_fetches_http_send_timeout","content":"HTTP send timeout (in seconds) for part fetch requests. Inherited from default profile http_send_timeout if not set explicitly. Possible values: Any positive integer.0 - Use value of http_send_timeout. Default value: 0. "},{"title":"replicated_fetches_http_receive_timeout​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#replicated_fetches_http_receive_timeout","content":"HTTP receive timeout (in seconds) for fetch part requests. Inherited from default profile http_receive_timeout if not set explicitly. Possible values: Any positive integer.0 - Use value of http_receive_timeout. Default value: 0. "},{"title":"max_replicated_fetches_network_bandwidth​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#max_replicated_fetches_network_bandwidth","content":"Limits the maximum speed of data exchange over the network in bytes per second for replicated fetches. This setting is applied to a particular table, unlike the max_replicated_fetches_network_bandwidth_for_server setting, which is applied to the server. You can limit both server network and network for a particular table, but for this the value of the table-level setting should be less than server-level one. Otherwise the server considers only the max_replicated_fetches_network_bandwidth_for_server setting. The setting isn't followed perfectly accurately. Possible values: Positive integer.0 — Unlimited. Default value: 0. Usage Could be used for throttling speed when replicating data to add or replace new nodes. "},{"title":"max_replicated_sends_network_bandwidth​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#max_replicated_sends_network_bandwidth","content":"Limits the maximum speed of data exchange over the network in bytes per second for replicated sends. This setting is applied to a particular table, unlike the max_replicated_sends_network_bandwidth_for_server setting, which is applied to the server. You can limit both server network and network for a particular table, but for this the value of the table-level setting should be less than server-level one. Otherwise the server considers only the max_replicated_sends_network_bandwidth_for_server setting. The setting isn't followed perfectly accurately. Possible values: Positive integer.0 — Unlimited. Default value: 0. Usage Could be used for throttling speed when replicating data to add or replace new nodes. "},{"title":"old_parts_lifetime​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#old-parts-lifetime","content":"The time (in seconds) of storing inactive parts to protect against data loss during spontaneous server reboots. Possible values: Any positive integer. Default value: 480. After merging several parts into a new part, ClickHouse marks the original parts as inactive and deletes them only after old_parts_lifetime seconds. Inactive parts are removed if they are not used by current queries, i.e. if the refcount of the part is zero. fsync is not called for new parts, so for some time new parts exist only in the server's RAM (OS cache). If the server is rebooted spontaneously, new parts can be lost or damaged. To protect data inactive parts are not deleted immediately. During startup ClickHouse checks the integrity of the parts. If the merged part is damaged ClickHouse returns the inactive parts to the active list, and later merges them again. Then the damaged part is renamed (the broken_ prefix is added) and moved to the detached folder. If the merged part is not damaged, then the original inactive parts are renamed (the ignored_ prefix is added) and moved to the detached folder. The default dirty_expire_centisecs value (a Linux kernel setting) is 30 seconds (the maximum time that written data is stored only in RAM), but under heavy loads on the disk system data can be written much later. Experimentally, a value of 480 seconds was chosen for old_parts_lifetime, during which a new part is guaranteed to be written to disk. "},{"title":"max_bytes_to_merge_at_max_space_in_pool​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#max-bytes-to-merge-at-max-space-in-pool","content":"The maximum total parts size (in bytes) to be merged into one part, if there are enough resources available.max_bytes_to_merge_at_max_space_in_pool -- roughly corresponds to the maximum possible part size created by an automatic background merge. Possible values: Any positive integer. Default value: 161061273600 (150 GB). The merge scheduler periodically analyzes the sizes and number of parts in partitions, and if there is enough free resources in the pool, it starts background merges. Merges occur until the total size of the source parts is larger than max_bytes_to_merge_at_max_space_in_pool. Merges initiated by OPTIMIZE FINAL ignore max_bytes_to_merge_at_max_space_in_pool and merge parts only taking into account available resources (free disk's space) until one part remains in the partition. "},{"title":"max_bytes_to_merge_at_min_space_in_pool​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#max-bytes-to-merge-at-min-space-in-pool","content":"The maximum total part size (in bytes) to be merged into one part, with the minimum available resources in the background pool. Possible values: Any positive integer. Default value: 1048576 (1 MB) max_bytes_to_merge_at_min_space_in_pool defines the maximum total size of parts which can be merged despite the lack of available disk space (in pool). This is necessary to reduce the number of small parts and the chance of Too many parts errors. Merges book disk space by doubling the total merged parts sizes. Thus, with a small amount of free disk space, a situation may happen that there is free space, but this space is already booked by ongoing large merges, so other merges unable to start, and the number of small parts grows with every insert. "},{"title":"merge_max_block_size​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#merge-max-block-size","content":"The number of rows that are read from the merged parts into memory. Possible values: Any positive integer. Default value: 8192 Merge reads rows from parts in blocks of merge_max_block_size rows, then merges and writes the result into a new part. The read block is placed in RAM, so merge_max_block_size affects the size of the RAM required for the merge. Thus, merges can consume a large amount of RAM for tables with very wide rows (if the average row size is 100kb, then when merging 10 parts, (100kb 10 8192) = ~ 8GB of RAM). By decreasing merge_max_block_size, you can reduce the amount of RAM required for a merge but slow down a merge. "},{"title":"max_part_loading_threads​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#max-part-loading-threads","content":"The maximum number of threads that read parts when ClickHouse starts. Possible values: Any positive integer. Default value: auto (number of CPU cores). During startup ClickHouse reads all parts of all tables (reads files with metadata of parts) to build a list of all parts in memory. In some systems with a large number of parts this process can take a long time, and this time might be shortened by increasing max_part_loading_threads (if this process is not CPU and disk I/O bound). "},{"title":"max_partitions_to_read​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#max-partitions-to-read","content":"Limits the maximum number of partitions that can be accessed in one query. The setting value specified when the table is created can be overridden via query-level setting. Possible values: Any positive integer. Default value: -1 (unlimited). "},{"title":"allow_floating_point_partition_key​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#allow_floating_point_partition_key","content":"Enables to allow floating-point number as a partition key. Possible values: 0 — Floating-point partition key not allowed.1 — Floating-point partition key allowed. Default value: 0. "},{"title":"check_sample_column_is_correct​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#check_sample_column_is_correct","content":"Enables the check at table creation, that the data type of a column for sampling or sampling expression is correct. The data type must be one of unsigned integer types: UInt8, UInt16, UInt32, UInt64. Possible values: true — The check is enabled.false — The check is disabled at table creation. Default value: true. By default, the ClickHouse server checks at table creation the data type of a column for sampling or sampling expression. If you already have tables with incorrect sampling expression and do not want the server to raise an exception during startup, set check_sample_column_is_correct to false. "},{"title":"min_bytes_to_rebalance_partition_over_jbod​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#min-bytes-to-rebalance-partition-over-jbod","content":"Sets minimal amount of bytes to enable balancing when distributing new big parts over volume disks JBOD. Possible values: Positive integer.0 — Balancing is disabled. Default value: 0. Usage The value of the min_bytes_to_rebalance_partition_over_jbod setting should not be less than the value of the max_bytes_to_merge_at_max_space_in_pool / 1024. Otherwise, ClickHouse throws an exception. "},{"title":"detach_not_byte_identical_parts​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#detach_not_byte_identical_parts","content":"Enables or disables detaching a data part on a replica after a merge or a mutation, if it is not byte-identical to data parts on other replicas. If disabled, the data part is removed. Activate this setting if you want to analyze such parts later. The setting is applicable to MergeTree tables with enabled data replication. Possible values: 0 — Parts are removed.1 — Parts are detached. Default value: 0. "},{"title":"merge_tree_clear_old_temporary_directories_interval_seconds​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#setting-merge-tree-clear-old-temporary-directories-interval-seconds","content":"Sets the interval in seconds for ClickHouse to execute the cleanup of old temporary directories. Possible values: Any positive integer. Default value: 60 seconds. "},{"title":"merge_tree_clear_old_parts_interval_seconds​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#setting-merge-tree-clear-old-parts-interval-seconds","content":"Sets the interval in seconds for ClickHouse to execute the cleanup of old parts, WALs, and mutations. Possible values: Any positive integer. Default value: 1 second. "},{"title":"max_concurrent_queries​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#max-concurrent-queries","content":"Max number of concurrently executed queries related to the MergeTree table. Queries will still be limited by other max_concurrent_queries settings. Possible values: Positive integer.0 — No limit. Default value: 0 (no limit). Example &lt;max_concurrent_queries&gt;50&lt;/max_concurrent_queries&gt;  "},{"title":"min_marks_to_honor_max_concurrent_queries​","type":1,"pageTitle":"MergeTree tables settings","url":"en/operations/settings/merge-tree-settings#min-marks-to-honor-max-concurrent-queries","content":"The minimal number of marks read by the query for applying the max_concurrent_queries setting. Note that queries will still be limited by other max_concurrent_queries settings. Possible values: Positive integer.0 — Disabled (max_concurrent_queries limit applied to no queries). Default value: 0 (limit never applied). Example &lt;min_marks_to_honor_max_concurrent_queries&gt;10&lt;/min_marks_to_honor_max_concurrent_queries&gt;  "},{"title":"Permissions for Queries","type":0,"sectionRef":"#","url":"en/operations/settings/permissions-for-queries","content":"","keywords":""},{"title":"readonly​","type":1,"pageTitle":"Permissions for Queries","url":"en/operations/settings/permissions-for-queries#settings_readonly","content":"Restricts permissions for reading data, write data and change settings queries. See how the queries are divided into types above. Possible values: 0 — All queries are allowed.1 — Only read data queries are allowed.2 — Read data and change settings queries are allowed. After setting readonly = 1, the user can’t change readonly and allow_ddl settings in the current session. When using the GET method in the HTTP interface, readonly = 1 is set automatically. To modify data, use the POST method. Setting readonly = 1 prohibit the user from changing all the settings. There is a way to prohibit the user from changing only specific settings, for details see constraints on settings. Default value: 0 "},{"title":"allow_ddl​","type":1,"pageTitle":"Permissions for Queries","url":"en/operations/settings/permissions-for-queries#settings_allow_ddl","content":"Allows or denies DDL queries. See how the queries are divided into types above. Possible values: 0 — DDL queries are not allowed.1 — DDL queries are allowed. You can’t execute SET allow_ddl = 1 if allow_ddl = 0 for the current session. Default value: 1 Original article "},{"title":"Restrictions on Query Complexity","type":0,"sectionRef":"#","url":"en/operations/settings/query-complexity","content":"","keywords":""},{"title":"max_memory_usage​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#settings_max_memory_usage","content":"The maximum amount of RAM to use for running a query on a single server. In the default configuration file, the maximum is 10 GB. The setting does not consider the volume of available memory or the total volume of memory on the machine. The restriction applies to a single query within a single server. You can use SHOW PROCESSLIST to see the current memory consumption for each query. Besides, the peak memory consumption is tracked for each query and written to the log. Memory usage is not monitored for the states of certain aggregate functions. Memory usage is not fully tracked for states of the aggregate functions min, max, any, anyLast, argMin, argMax from String and Array arguments. Memory consumption is also restricted by the parameters max_memory_usage_for_user and max_server_memory_usage. "},{"title":"max_memory_usage_for_user​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-memory-usage-for-user","content":"The maximum amount of RAM to use for running a user’s queries on a single server. Default values are defined in Settings.h. By default, the amount is not restricted (max_memory_usage_for_user = 0). See also the description of max_memory_usage. "},{"title":"max_rows_to_read​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-rows-to-read","content":"The following restrictions can be checked on each block (instead of on each row). That is, the restrictions can be broken a little. A maximum number of rows that can be read from a table when running a query. "},{"title":"max_bytes_to_read​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-bytes-to-read","content":"A maximum number of bytes (uncompressed data) that can be read from a table when running a query. "},{"title":"read_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#read-overflow-mode","content":"What to do when the volume of data read exceeds one of the limits: ‘throw’ or ‘break’. By default, throw. "},{"title":"max_rows_to_read_leaf​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-rows-to-read-leaf","content":"The following restrictions can be checked on each block (instead of on each row). That is, the restrictions can be broken a little. A maximum number of rows that can be read from a local table on a leaf node when running a distributed query. While distributed queries can issue a multiple sub-queries to each shard (leaf) - this limit will be checked only on the read stage on the leaf nodes and ignored on results merging stage on the root node. For example, cluster consists of 2 shards and each shard contains a table with 100 rows. Then distributed query which suppose to read all the data from both tables with setting max_rows_to_read=150 will fail as in total it will be 200 rows. While query with max_rows_to_read_leaf=150 will succeed since leaf nodes will read 100 rows at max. "},{"title":"max_bytes_to_read_leaf​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-bytes-to-read-leaf","content":"A maximum number of bytes (uncompressed data) that can be read from a local table on a leaf node when running a distributed query. While distributed queries can issue a multiple sub-queries to each shard (leaf) - this limit will be checked only on the read stage on the leaf nodes and ignored on results merging stage on the root node. For example, cluster consists of 2 shards and each shard contains a table with 100 bytes of data. Then distributed query which suppose to read all the data from both tables with setting max_bytes_to_read=150 will fail as in total it will be 200 bytes. While query with max_bytes_to_read_leaf=150 will succeed since leaf nodes will read 100 bytes at max. "},{"title":"read_overflow_mode_leaf​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#read-overflow-mode-leaf","content":"What to do when the volume of data read exceeds one of the leaf limits: ‘throw’ or ‘break’. By default, throw. "},{"title":"max_rows_to_group_by​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#settings-max-rows-to-group-by","content":"A maximum number of unique keys received from aggregation. This setting lets you limit memory consumption when aggregating. "},{"title":"group_by_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#group-by-overflow-mode","content":"What to do when the number of unique keys for aggregation exceeds the limit: ‘throw’, ‘break’, or ‘any’. By default, throw. Using the ‘any’ value lets you run an approximation of GROUP BY. The quality of this approximation depends on the statistical nature of the data. "},{"title":"max_bytes_before_external_group_by​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#settings-max_bytes_before_external_group_by","content":"Enables or disables execution of GROUP BY clauses in external memory. See GROUP BY in external memory. Possible values: Maximum volume of RAM (in bytes) that can be used by the single GROUP BY operation.0 — GROUP BY in external memory disabled. Default value: 0. "},{"title":"max_rows_to_sort​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-rows-to-sort","content":"A maximum number of rows before sorting. This allows you to limit memory consumption when sorting. "},{"title":"max_bytes_to_sort​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-bytes-to-sort","content":"A maximum number of bytes before sorting. "},{"title":"sort_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#sort-overflow-mode","content":"What to do if the number of rows received before sorting exceeds one of the limits: ‘throw’ or ‘break’. By default, throw. "},{"title":"max_result_rows​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#setting-max_result_rows","content":"Limit on the number of rows in the result. Also checked for subqueries, and on remote servers when running parts of a distributed query. "},{"title":"max_result_bytes​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-result-bytes","content":"Limit on the number of bytes in the result. The same as the previous setting. "},{"title":"result_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#result-overflow-mode","content":"What to do if the volume of the result exceeds one of the limits: ‘throw’ or ‘break’. By default, throw. Using ‘break’ is similar to using LIMIT. Break interrupts execution only at the block level. This means that amount of returned rows is greater than max_result_rows, multiple of max_block_size and depends on max_threads. Example: SET max_threads = 3, max_block_size = 3333; SET max_result_rows = 3334, result_overflow_mode = 'break'; SELECT * FROM numbers_mt(100000) FORMAT Null;  Result: 6666 rows in set. ...  "},{"title":"max_execution_time​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-execution-time","content":"Maximum query execution time in seconds. At this time, it is not checked for one of the sorting stages, or when merging and finalizing aggregate functions. "},{"title":"timeout_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#timeout-overflow-mode","content":"What to do if the query is run longer than ‘max_execution_time’: ‘throw’ or ‘break’. By default, throw. "},{"title":"min_execution_speed​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#min-execution-speed","content":"Minimal execution speed in rows per second. Checked on every data block when ‘timeout_before_checking_execution_speed’ expires. If the execution speed is lower, an exception is thrown. "},{"title":"min_execution_speed_bytes​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#min-execution-speed-bytes","content":"A minimum number of execution bytes per second. Checked on every data block when ‘timeout_before_checking_execution_speed’ expires. If the execution speed is lower, an exception is thrown. "},{"title":"max_execution_speed​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-execution-speed","content":"A maximum number of execution rows per second. Checked on every data block when ‘timeout_before_checking_execution_speed’ expires. If the execution speed is high, the execution speed will be reduced. "},{"title":"max_execution_speed_bytes​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-execution-speed-bytes","content":"A maximum number of execution bytes per second. Checked on every data block when ‘timeout_before_checking_execution_speed’ expires. If the execution speed is high, the execution speed will be reduced. "},{"title":"timeout_before_checking_execution_speed​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#timeout-before-checking-execution-speed","content":"Checks that execution speed is not too slow (no less than ‘min_execution_speed’), after the specified time in seconds has expired. "},{"title":"max_columns_to_read​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-columns-to-read","content":"A maximum number of columns that can be read from a table in a single query. If a query requires reading a greater number of columns, it throws an exception. "},{"title":"max_temporary_columns​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-temporary-columns","content":"A maximum number of temporary columns that must be kept in RAM at the same time when running a query, including constant columns. If there are more temporary columns than this, it throws an exception. "},{"title":"max_temporary_non_const_columns​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-temporary-non-const-columns","content":"The same thing as ‘max_temporary_columns’, but without counting constant columns. Note that constant columns are formed fairly often when running a query, but they require approximately zero computing resources. "},{"title":"max_subquery_depth​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-subquery-depth","content":"Maximum nesting depth of subqueries. If subqueries are deeper, an exception is thrown. By default, 100. "},{"title":"max_pipeline_depth​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-pipeline-depth","content":"Maximum pipeline depth. Corresponds to the number of transformations that each data block goes through during query processing. Counted within the limits of a single server. If the pipeline depth is greater, an exception is thrown. By default, 1000. "},{"title":"max_ast_depth​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-ast-depth","content":"Maximum nesting depth of a query syntactic tree. If exceeded, an exception is thrown. At this time, it isn’t checked during parsing, but only after parsing the query. That is, a syntactic tree that is too deep can be created during parsing, but the query will fail. By default, 1000. "},{"title":"max_ast_elements​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-ast-elements","content":"A maximum number of elements in a query syntactic tree. If exceeded, an exception is thrown. In the same way as the previous setting, it is checked only after parsing the query. By default, 50,000. "},{"title":"max_rows_in_set​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-rows-in-set","content":"A maximum number of rows for a data set in the IN clause created from a subquery. "},{"title":"max_bytes_in_set​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-bytes-in-set","content":"A maximum number of bytes (uncompressed data) used by a set in the IN clause created from a subquery. "},{"title":"set_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#set-overflow-mode","content":"What to do when the amount of data exceeds one of the limits: ‘throw’ or ‘break’. By default, throw. "},{"title":"max_rows_in_distinct​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-rows-in-distinct","content":"A maximum number of different rows when using DISTINCT. "},{"title":"max_bytes_in_distinct​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-bytes-in-distinct","content":"A maximum number of bytes used by a hash table when using DISTINCT. "},{"title":"distinct_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#distinct-overflow-mode","content":"What to do when the amount of data exceeds one of the limits: ‘throw’ or ‘break’. By default, throw. "},{"title":"max_rows_to_transfer​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-rows-to-transfer","content":"A maximum number of rows that can be passed to a remote server or saved in a temporary table when using GLOBAL IN. "},{"title":"max_bytes_to_transfer​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-bytes-to-transfer","content":"A maximum number of bytes (uncompressed data) that can be passed to a remote server or saved in a temporary table when using GLOBAL IN. "},{"title":"transfer_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#transfer-overflow-mode","content":"What to do when the amount of data exceeds one of the limits: ‘throw’ or ‘break’. By default, throw. "},{"title":"max_rows_in_join​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#settings-max_rows_in_join","content":"Limits the number of rows in the hash table that is used when joining tables. This settings applies to SELECT … JOIN operations and the Join table engine. If a query contains multiple joins, ClickHouse checks this setting for every intermediate result. ClickHouse can proceed with different actions when the limit is reached. Use the join_overflow_mode setting to choose the action. Possible values: Positive integer.0 — Unlimited number of rows. Default value: 0. "},{"title":"max_bytes_in_join​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#settings-max_bytes_in_join","content":"Limits the size in bytes of the hash table used when joining tables. This settings applies to SELECT … JOIN operations and Join table engine. If the query contains joins, ClickHouse checks this setting for every intermediate result. ClickHouse can proceed with different actions when the limit is reached. Use join_overflow_mode settings to choose the action. Possible values: Positive integer.0 — Memory control is disabled. Default value: 0. "},{"title":"join_overflow_mode​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#settings-join_overflow_mode","content":"Defines what action ClickHouse performs when any of the following join limits is reached: max_bytes_in_joinmax_rows_in_join Possible values: THROW — ClickHouse throws an exception and breaks operation.BREAK — ClickHouse breaks operation and does not throw an exception. Default value: THROW. See Also JOIN clauseJoin table engine "},{"title":"max_partitions_per_insert_block​","type":1,"pageTitle":"Restrictions on Query Complexity","url":"en/operations/settings/query-complexity#max-partitions-per-insert-block","content":"Limits the maximum number of partitions in a single inserted block. Positive integer.0 — Unlimited number of partitions. Default value: 100. Details When inserting data, ClickHouse calculates the number of partitions in the inserted block. If the number of partitions is more than max_partitions_per_insert_block, ClickHouse throws an exception with the following text: “Too many partitions for single INSERT block (more than” + toString(max_parts) + “). The limit is controlled by ‘max_partitions_per_insert_block’ setting. A large number of partitions is a common misconception. It will lead to severe negative performance impact, including slow server startup, slow INSERT queries and slow SELECT queries. Recommended total number of partitions for a table is under 1000..10000. Please note, that partitioning is not intended to speed up SELECT queries (ORDER BY key is sufficient to make range queries fast). Partitions are intended for data manipulation (DROP PARTITION, etc).” Original article "},{"title":"Settings Profiles","type":0,"sectionRef":"#","url":"en/operations/settings/settings-profiles","content":"Settings Profiles A settings profile is a collection of settings grouped under the same name. note ClickHouse also supports SQL-driven workflow for managing settings profiles. We recommend using it. The profile can have any name. You can specify the same profile for different users. The most important thing you can write in the settings profile is readonly=1, which ensures read-only access. Settings profiles can inherit from each other. To use inheritance, indicate one or multiple profile settings before the other settings that are listed in the profile. In case when one setting is defined in different profiles, the latest defined is used. To apply all the settings in a profile, set the profile setting. Example: Install the web profile. SET profile = 'web' Settings profiles are declared in the user config file. This is usually users.xml. Example: &lt;!-- Settings profiles --&gt; &lt;profiles&gt; &lt;!-- Default settings --&gt; &lt;default&gt; &lt;!-- The maximum number of threads when running a single query. --&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;/default&gt; &lt;!-- Settings for quries from the user interface --&gt; &lt;web&gt; &lt;max_rows_to_read&gt;1000000000&lt;/max_rows_to_read&gt; &lt;max_bytes_to_read&gt;100000000000&lt;/max_bytes_to_read&gt; &lt;max_rows_to_group_by&gt;1000000&lt;/max_rows_to_group_by&gt; &lt;group_by_overflow_mode&gt;any&lt;/group_by_overflow_mode&gt; &lt;max_rows_to_sort&gt;1000000&lt;/max_rows_to_sort&gt; &lt;max_bytes_to_sort&gt;1000000000&lt;/max_bytes_to_sort&gt; &lt;max_result_rows&gt;100000&lt;/max_result_rows&gt; &lt;max_result_bytes&gt;100000000&lt;/max_result_bytes&gt; &lt;result_overflow_mode&gt;break&lt;/result_overflow_mode&gt; &lt;max_execution_time&gt;600&lt;/max_execution_time&gt; &lt;min_execution_speed&gt;1000000&lt;/min_execution_speed&gt; &lt;timeout_before_checking_execution_speed&gt;15&lt;/timeout_before_checking_execution_speed&gt; &lt;max_columns_to_read&gt;25&lt;/max_columns_to_read&gt; &lt;max_temporary_columns&gt;100&lt;/max_temporary_columns&gt; &lt;max_temporary_non_const_columns&gt;50&lt;/max_temporary_non_const_columns&gt; &lt;max_subquery_depth&gt;2&lt;/max_subquery_depth&gt; &lt;max_pipeline_depth&gt;25&lt;/max_pipeline_depth&gt; &lt;max_ast_depth&gt;50&lt;/max_ast_depth&gt; &lt;max_ast_elements&gt;100&lt;/max_ast_elements&gt; &lt;readonly&gt;1&lt;/readonly&gt; &lt;/web&gt; &lt;/profiles&gt; The example specifies two profiles: default and web. The default profile has a special purpose: it must always be present and is applied when starting the server. In other words, the default profile contains default settings. The web profile is a regular profile that can be set using the SET query or using a URL parameter in an HTTP query. Original article","keywords":""},{"title":"User Settings","type":0,"sectionRef":"#","url":"en/operations/settings/settings-users","content":"","keywords":""},{"title":"user_name/password​","type":1,"pageTitle":"User Settings","url":"en/operations/settings/settings-users#user-namepassword","content":"Password can be specified in plaintext or in SHA256 (hex format). To assign a password in plaintext (not recommended), place it in a password element. For example, &lt;password&gt;qwerty&lt;/password&gt;. The password can be left blank.  To assign a password using its SHA256 hash, place it in a password_sha256_hex element. For example, &lt;password_sha256_hex&gt;65e84be33532fb784c48129675f9eff3a682b27168c0ea744b2cf58ee02337c5&lt;/password_sha256_hex&gt;. Example of how to generate a password from shell: PASSWORD=$(base64 &lt; /dev/urandom | head -c8); echo &quot;$PASSWORD&quot;; echo -n &quot;$PASSWORD&quot; | sha256sum | tr -d '-' The first line of the result is the password. The second line is the corresponding SHA256 hash.  For compatibility with MySQL clients, password can be specified in double SHA1 hash. Place it in password_double_sha1_hex element. For example, &lt;password_double_sha1_hex&gt;08b4a0f1de6ad37da17359e592c8d74788a83eb0&lt;/password_double_sha1_hex&gt;. Example of how to generate a password from shell: PASSWORD=$(base64 &lt; /dev/urandom | head -c8); echo &quot;$PASSWORD&quot;; echo -n &quot;$PASSWORD&quot; | sha1sum | tr -d '-' | xxd -r -p | sha1sum | tr -d '-' The first line of the result is the password. The second line is the corresponding double SHA1 hash. "},{"title":"access_management​","type":1,"pageTitle":"User Settings","url":"en/operations/settings/settings-users#access_management-user-setting","content":"This setting enables or disables using of SQL-driven access control and account management for the user. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"user_name/networks​","type":1,"pageTitle":"User Settings","url":"en/operations/settings/settings-users#user-namenetworks","content":"List of networks from which the user can connect to the ClickHouse server. Each element of the list can have one of the following forms: &lt;ip&gt; — IP address or network mask. Examples: 213.180.204.3, 10.0.0.1/8, 10.0.0.1/255.255.255.0, 2a02:6b8::3, 2a02:6b8::3/64, 2a02:6b8::3/ffff:ffff:ffff:ffff::. &lt;host&gt; — Hostname. Example: example01.host.ru. To check access, a DNS query is performed, and all returned IP addresses are compared to the peer address. &lt;host_regexp&gt; — Regular expression for hostnames. Example, ^example\\d\\d-\\d\\d-\\d\\.host\\.ru$ To check access, a DNS PTR query is performed for the peer address and then the specified regexp is applied. Then, another DNS query is performed for the results of the PTR query and all the received addresses are compared to the peer address. We strongly recommend that regexp ends with $. All results of DNS requests are cached until the server restarts. Examples To open access for user from any network, specify: &lt;ip&gt;::/0&lt;/ip&gt;  warning It’s insecure to open access from any network unless you have a firewall properly configured or the server is not directly connected to Internet. To open access only from localhost, specify: &lt;ip&gt;::1&lt;/ip&gt; &lt;ip&gt;127.0.0.1&lt;/ip&gt;  "},{"title":"user_name/profile​","type":1,"pageTitle":"User Settings","url":"en/operations/settings/settings-users#user-nameprofile","content":"You can assign a settings profile for the user. Settings profiles are configured in a separate section of the users.xml file. For more information, see Profiles of Settings. "},{"title":"user_name/quota​","type":1,"pageTitle":"User Settings","url":"en/operations/settings/settings-users#user-namequota","content":"Quotas allow you to track or limit resource usage over a period of time. Quotas are configured in the quotassection of the users.xml configuration file. You can assign a quotas set for the user. For a detailed description of quotas configuration, see Quotas. "},{"title":"user_name/databases​","type":1,"pageTitle":"User Settings","url":"en/operations/settings/settings-users#user-namedatabases","content":"In this section, you can limit rows that are returned by ClickHouse for SELECT queries made by the current user, thus implementing basic row-level security. Example The following configuration forces that user user1 can only see the rows of table1 as the result of SELECT queries, where the value of the id field is 1000. &lt;user1&gt; &lt;databases&gt; &lt;database_name&gt; &lt;table1&gt; &lt;filter&gt;id = 1000&lt;/filter&gt; &lt;/table1&gt; &lt;/database_name&gt; &lt;/databases&gt; &lt;/user1&gt;  The filter can be any expression resulting in a UInt8-type value. It usually contains comparisons and logical operators. Rows from database_name.table1 where filter results to 0 are not returned for this user. The filtering is incompatible with PREWHERE operations and disables WHERE→PREWHERE optimization. Original article "},{"title":"Optional secured communication between ClickHouse and Zookeeper","type":0,"sectionRef":"#","url":"en/operations/ssl-zookeeper","content":"Optional secured communication between ClickHouse and Zookeeper You should specify ssl.keyStore.location, ssl.keyStore.password and ssl.trustStore.location, ssl.trustStore.password for communication with ClickHouse client over SSL. These options are available from Zookeeper version 3.5.2. You can add zookeeper.crt to trusted certificates. sudo cp zookeeper.crt /usr/local/share/ca-certificates/zookeeper.crt sudo update-ca-certificates Client section in config.xml will look like: &lt;client&gt; &lt;certificateFile&gt;/etc/clickhouse-server/client.crt&lt;/certificateFile&gt; &lt;privateKeyFile&gt;/etc/clickhouse-server/client.key&lt;/privateKeyFile&gt; &lt;loadDefaultCAFile&gt;true&lt;/loadDefaultCAFile&gt; &lt;cacheSessions&gt;true&lt;/cacheSessions&gt; &lt;disableProtocols&gt;sslv2,sslv3&lt;/disableProtocols&gt; &lt;preferServerCiphers&gt;true&lt;/preferServerCiphers&gt; &lt;invalidCertificateHandler&gt; &lt;name&gt;RejectCertificateHandler&lt;/name&gt; &lt;/invalidCertificateHandler&gt; &lt;/client&gt; Add Zookeeper to ClickHouse config with some cluster and macros: &lt;clickhouse&gt; &lt;zookeeper&gt; &lt;node&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;2281&lt;/port&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;/node&gt; &lt;/zookeeper&gt; &lt;/clickhouse&gt; Start clickhouse-server. In logs you should see: &lt;Trace&gt; ZooKeeper: initialized, hosts: secure://localhost:2281 Prefix secure:// indicates that connection is secured by SSL. To ensure traffic is encrypted run tcpdump on secured port: tcpdump -i any dst port 2281 -nnXS And query in clickhouse-client: SELECT * FROM system.zookeeper WHERE path = '/'; On unencrypted connection you will see in tcpdump output something like this: ..../zookeeper/quota. On encrypted connection you should not see this.","keywords":""},{"title":"External Disks for Storing Data","type":0,"sectionRef":"#","url":"en/operations/storing-data","content":"","keywords":""},{"title":"Configuring HDFS​","type":1,"pageTitle":"External Disks for Storing Data","url":"en/operations/storing-data#configuring-hdfs","content":"MergeTree and Log family table engines can store data to HDFS using a disk with type HDFS. Configuration markup: &lt;clickhouse&gt; &lt;storage_configuration&gt; &lt;disks&gt; &lt;hdfs&gt; &lt;type&gt;hdfs&lt;/type&gt; &lt;endpoint&gt;hdfs://hdfs1:9000/clickhouse/&lt;/endpoint&gt; &lt;/hdfs&gt; &lt;/disks&gt; &lt;policies&gt; &lt;hdfs&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;hdfs&lt;/disk&gt; &lt;/main&gt; &lt;/volumes&gt; &lt;/hdfs&gt; &lt;/policies&gt; &lt;/storage_configuration&gt; &lt;merge_tree&gt; &lt;min_bytes_for_wide_part&gt;0&lt;/min_bytes_for_wide_part&gt; &lt;/merge_tree&gt; &lt;/clickhouse&gt;  Required parameters: endpoint — HDFS endpoint URL in path format. Endpoint URL should contain a root path to store data. Optional parameters: min_bytes_for_seek — The minimal number of bytes to use seek operation instead of sequential read. Default value: 1 Mb. "},{"title":"Using Virtual File System for Data Encryption​","type":1,"pageTitle":"External Disks for Storing Data","url":"en/operations/storing-data#encrypted-virtual-file-system","content":"You can encrypt the data stored on S3, or HDFS external disks, or on a local disk. To turn on the encryption mode, in the configuration file you must define a disk with the type encrypted and choose a disk on which the data will be saved. An encrypted disk ciphers all written files on the fly, and when you read files from an encrypted disk it deciphers them automatically. So you can work with an encrypted disk like with a normal one. Example of disk configuration: &lt;disks&gt; &lt;disk1&gt; &lt;type&gt;local&lt;/type&gt; &lt;path&gt;/path1/&lt;/path&gt; &lt;/disk1&gt; &lt;disk2&gt; &lt;type&gt;encrypted&lt;/type&gt; &lt;disk&gt;disk1&lt;/disk&gt; &lt;path&gt;path2/&lt;/path&gt; &lt;key&gt;_16_ascii_chars_&lt;/key&gt; &lt;/disk2&gt; &lt;/disks&gt;  For example, when ClickHouse writes data from some table to a file store/all_1_1_0/data.bin to disk1, then in fact this file will be written to the physical disk along the path /path1/store/all_1_1_0/data.bin. When writing the same file to disk2, it will actually be written to the physical disk at the path /path1/path2/store/all_1_1_0/data.bin in encrypted mode. Required parameters: type — encrypted. Otherwise the encrypted disk is not created.disk — Type of disk for data storage.key — The key for encryption and decryption. Type: Uint64. You can use key_hex parameter to encrypt in hexadecimal form. You can specify multiple keys using the id attribute (see example above). Optional parameters: path — Path to the location on the disk where the data will be saved. If not specified, the data will be saved in the root directory.current_key_id — The key used for encryption. All the specified keys can be used for decryption, and you can always switch to another key while maintaining access to previously encrypted data.algorithm — Algorithm for encryption. Possible values: AES_128_CTR, AES_192_CTR or AES_256_CTR. Default value: AES_128_CTR. The key length depends on the algorithm: AES_128_CTR — 16 bytes, AES_192_CTR — 24 bytes, AES_256_CTR — 32 bytes. Example of disk configuration: &lt;clickhouse&gt; &lt;storage_configuration&gt; &lt;disks&gt; &lt;disk_s3&gt; &lt;type&gt;s3&lt;/type&gt; &lt;endpoint&gt;... &lt;/disk_s3&gt; &lt;disk_s3_encrypted&gt; &lt;type&gt;encrypted&lt;/type&gt; &lt;disk&gt;disk_s3&lt;/disk&gt; &lt;algorithm&gt;AES_128_CTR&lt;/algorithm&gt; &lt;key_hex id=&quot;0&quot;&gt;00112233445566778899aabbccddeeff&lt;/key_hex&gt; &lt;key_hex id=&quot;1&quot;&gt;ffeeddccbbaa99887766554433221100&lt;/key_hex&gt; &lt;current_key_id&gt;1&lt;/current_key_id&gt; &lt;/disk_s3_encrypted&gt; &lt;/disks&gt; &lt;/storage_configuration&gt; &lt;/clickhouse&gt;  "},{"title":"Storing Data on Web Server​","type":1,"pageTitle":"External Disks for Storing Data","url":"en/operations/storing-data#storing-data-on-webserver","content":"There is a tool clickhouse-static-files-uploader, which prepares a data directory for a given table (SELECT data_paths FROM system.tables WHERE name = 'table_name'). For each table you need, you get a directory of files. These files can be uploaded to, for example, a web server with static files. After this preparation, you can load this table into any ClickHouse server via DiskWeb. This is a read-only disk. Its data is only read and never modified. A new table is loaded to this disk via ATTACH TABLE query (see example below). Local disk is not actually used, each SELECT query will result in a http request to fetch required data. All modification of the table data will result in an exception, i.e. the following types of queries are not allowed: CREATE TABLE, ALTER TABLE, RENAME TABLE, DETACH TABLE and TRUNCATE TABLE. Web server storage is supported only for the MergeTree and Log engine families. To access the data stored on a web disk, use the storage_policy setting when executing the query. For example, ATTACH TABLE table_web UUID '{}' (id Int32) ENGINE = MergeTree() ORDER BY id SETTINGS storage_policy = 'web'. A ready test case. You need to add this configuration to config: &lt;clickhouse&gt; &lt;storage_configuration&gt; &lt;disks&gt; &lt;web&gt; &lt;type&gt;web&lt;/type&gt; &lt;endpoint&gt;https://clickhouse-datasets.s3.yandex.net/disk-with-static-files-tests/test-hits/&lt;/endpoint&gt; &lt;/web&gt; &lt;/disks&gt; &lt;policies&gt; &lt;web&gt; &lt;volumes&gt; &lt;main&gt; &lt;disk&gt;web&lt;/disk&gt; &lt;/main&gt; &lt;/volumes&gt; &lt;/web&gt; &lt;/policies&gt; &lt;/storage_configuration&gt; &lt;/clickhouse&gt;  And then execute this query: ATTACH TABLE test_hits UUID '1ae36516-d62d-4218-9ae3-6516d62da218' ( WatchID UInt64, JavaEnable UInt8, Title String, GoodEvent Int16, EventTime DateTime, EventDate Date, CounterID UInt32, ClientIP UInt32, ClientIP6 FixedString(16), RegionID UInt32, UserID UInt64, CounterClass Int8, OS UInt8, UserAgent UInt8, URL String, Referer String, URLDomain String, RefererDomain String, Refresh UInt8, IsRobot UInt8, RefererCategories Array(UInt16), URLCategories Array(UInt16), URLRegions Array(UInt32), RefererRegions Array(UInt32), ResolutionWidth UInt16, ResolutionHeight UInt16, ResolutionDepth UInt8, FlashMajor UInt8, FlashMinor UInt8, FlashMinor2 String, NetMajor UInt8, NetMinor UInt8, UserAgentMajor UInt16, UserAgentMinor FixedString(2), CookieEnable UInt8, JavascriptEnable UInt8, IsMobile UInt8, MobilePhone UInt8, MobilePhoneModel String, Params String, IPNetworkID UInt32, TraficSourceID Int8, SearchEngineID UInt16, SearchPhrase String, AdvEngineID UInt8, IsArtifical UInt8, WindowClientWidth UInt16, WindowClientHeight UInt16, ClientTimeZone Int16, ClientEventTime DateTime, SilverlightVersion1 UInt8, SilverlightVersion2 UInt8, SilverlightVersion3 UInt32, SilverlightVersion4 UInt16, PageCharset String, CodeVersion UInt32, IsLink UInt8, IsDownload UInt8, IsNotBounce UInt8, FUniqID UInt64, HID UInt32, IsOldCounter UInt8, IsEvent UInt8, IsParameter UInt8, DontCountHits UInt8, WithHash UInt8, HitColor FixedString(1), UTCEventTime DateTime, Age UInt8, Sex UInt8, Income UInt8, Interests UInt16, Robotness UInt8, GeneralInterests Array(UInt16), RemoteIP UInt32, RemoteIP6 FixedString(16), WindowName Int32, OpenerName Int32, HistoryLength Int16, BrowserLanguage FixedString(2), BrowserCountry FixedString(2), SocialNetwork String, SocialAction String, HTTPError UInt16, SendTiming Int32, DNSTiming Int32, ConnectTiming Int32, ResponseStartTiming Int32, ResponseEndTiming Int32, FetchTiming Int32, RedirectTiming Int32, DOMInteractiveTiming Int32, DOMContentLoadedTiming Int32, DOMCompleteTiming Int32, LoadEventStartTiming Int32, LoadEventEndTiming Int32, NSToDOMContentLoadedTiming Int32, FirstPaintTiming Int32, RedirectCount Int8, SocialSourceNetworkID UInt8, SocialSourcePage String, ParamPrice Int64, ParamOrderID String, ParamCurrency FixedString(3), ParamCurrencyID UInt16, GoalsReached Array(UInt32), OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID UInt8, RefererHash UInt64, URLHash UInt64, CLID UInt32, YCLID UInt64, ShareService String, ShareURL String, ShareTitle String, ParsedParams Nested( Key1 String, Key2 String, Key3 String, Key4 String, Key5 String, ValueDouble Float64), IslandID FixedString(16), RequestNum UInt32, RequestTry UInt8 ) ENGINE = MergeTree() PARTITION BY toYYYYMM(EventDate) ORDER BY (CounterID, EventDate, intHash32(UserID)) SAMPLE BY intHash32(UserID) SETTINGS storage_policy='web';  Required parameters: type — web. Otherwise the disk is not created.endpoint — The endpoint URL in path format. Endpoint URL must contain a root path to store data, where they were uploaded. Optional parameters: min_bytes_for_seek — The minimal number of bytes to use seek operation instead of sequential read. Default value: 1 Mb.remote_fs_read_backoff_threashold — The maximum wait time when trying to read data for remote disk. Default value: 10000 seconds.remote_fs_read_backoff_max_tries — The maximum number of attempts to read with backoff. Default value: 5. If a query fails with an exception DB:Exception Unreachable URL, then you can try to adjust the settings: http_connection_timeout, http_receive_timeout, keep_alive_timeout. To get files for upload run:clickhouse static-files-disk-uploader --metadata-path &lt;path&gt; --output-dir &lt;dir&gt; (--metadata-path can be found in query SELECT data_paths FROM system.tables WHERE name = 'table_name'). When loading files by endpoint, they must be loaded into &lt;endpoint&gt;/store/ path, but config must contain only endpoint. If URL is not reachable on disk load when the server is starting up tables, then all errors are caught. If in this case there were errors, tables can be reloaded (become visible) via DETACH TABLE table_name -&gt; ATTACH TABLE table_name. If metadata was successfully loaded at server startup, then tables are available straight away. Use http_max_single_read_retries setting to limit the maximum number of retries during a single HTTP read. "},{"title":"Zero-copy Replication (not ready for production)​","type":1,"pageTitle":"External Disks for Storing Data","url":"en/operations/storing-data#zero-copy","content":"ClickHouse supports zero-copy replication for S3 and HDFS disks, which means that if the data is stored remotely on several machines and needs to be synchronized, then only the metadata is replicated (paths to the data parts), but not the data itself. "},{"title":"Formats for Input and Output Data","type":0,"sectionRef":"#","url":"en/interfaces/formats","content":"","keywords":""},{"title":"TabSeparated​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#tabseparated","content":"In TabSeparated format, data is written by row. Each row contains values separated by tabs. Each value is followed by a tab, except the last value in the row, which is followed by a line feed. Strictly Unix line feeds are assumed everywhere. The last row also must contain a line feed at the end. Values are written in text format, without enclosing quotation marks, and with special characters escaped. This format is also available under the name TSV. The TabSeparated format is convenient for processing data using custom programs and scripts. It is used by default in the HTTP interface, and in the command-line client’s batch mode. This format also allows transferring data between different DBMSs. For example, you can get a dump from MySQL and upload it to ClickHouse, or vice versa. The TabSeparated format supports outputting total values (when using WITH TOTALS) and extreme values (when ‘extremes’ is set to 1). In these cases, the total values and extremes are output after the main data. The main result, total values, and extremes are separated from each other by an empty line. Example: SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT TabSeparated  2014-03-17 1406958 2014-03-18 1383658 2014-03-19 1405797 2014-03-20 1353623 2014-03-21 1245779 2014-03-22 1031592 2014-03-23 1046491 1970-01-01 8873898 2014-03-17 1031592 2014-03-23 1406958  "},{"title":"Data Formatting​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data-formatting","content":"Integer numbers are written in decimal form. Numbers can contain an extra “+” character at the beginning (ignored when parsing, and not recorded when formatting). Non-negative numbers can’t contain the negative sign. When reading, it is allowed to parse an empty string as a zero, or (for signed types) a string consisting of just a minus sign as a zero. Numbers that do not fit into the corresponding data type may be parsed as a different number, without an error message. Floating-point numbers are written in decimal form. The dot is used as the decimal separator. Exponential entries are supported, as are ‘inf’, ‘+inf’, ‘-inf’, and ‘nan’. An entry of floating-point numbers may begin or end with a decimal point. During formatting, accuracy may be lost on floating-point numbers. During parsing, it is not strictly required to read the nearest machine-representable number. Dates are written in YYYY-MM-DD format and parsed in the same format, but with any characters as separators. Dates with times are written in the format YYYY-MM-DD hh:mm:ss and parsed in the same format, but with any characters as separators. This all occurs in the system time zone at the time the client or server starts (depending on which of them formats data). For dates with times, daylight saving time is not specified. So if a dump has times during daylight saving time, the dump does not unequivocally match the data, and parsing will select one of the two times. During a read operation, incorrect dates and dates with times can be parsed with natural overflow or as null dates and times, without an error message. As an exception, parsing dates with times is also supported in Unix timestamp format, if it consists of exactly 10 decimal digits. The result is not time zone-dependent. The formats YYYY-MM-DD hh:mm:ss and NNNNNNNNNN are differentiated automatically. Strings are output with backslash-escaped special characters. The following escape sequences are used for output: \\b, \\f, \\r, \\n, \\t, \\0, \\', \\\\. Parsing also supports the sequences \\a, \\v, and \\xHH (hex escape sequences) and any \\c sequences, where c is any character (these sequences are converted to c). Thus, reading data supports formats where a line feed can be written as \\n or \\, or as a line feed. For example, the string Hello world with a line feed between the words instead of space can be parsed in any of the following variations: Hello\\nworld Hello\\ world  The second variant is supported because MySQL uses it when writing tab-separated dumps. The minimum set of characters that you need to escape when passing data in TabSeparated format: tab, line feed (LF) and backslash. Only a small set of symbols are escaped. You can easily stumble onto a string value that your terminal will ruin in output. Arrays are written as a list of comma-separated values in square brackets. Number items in the array are formatted as normally. Date and DateTime types are written in single quotes. Strings are written in single quotes with the same escaping rules as above. NULL is formatted according to setting format_tsv_null_representation (default value is \\N). If setting input_format_tsv_empty_as_default is enabled, empty input fields are replaced with default values. For complex default expressions input_format_defaults_for_omitted_fields must be enabled too. Each element of Nested structures is represented as array. In input data, ENUM values can be represented as names or as ids. First, we try to match the input value to the ENUM name. If we fail and the input value is a number, we try to match this number to ENUM id. If input data contains only ENUM ids, it's recommended to enable the setting input_format_tsv_enum_as_number to optimize ENUM parsing. For example: CREATE TABLE nestedt ( `id` UInt8, `aux` Nested( a UInt8, b String ) ) ENGINE = TinyLog  INSERT INTO nestedt Values ( 1, [1], ['a'])  SELECT * FROM nestedt FORMAT TSV  1 [1] ['a']  "},{"title":"TabSeparatedRaw​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#tabseparatedraw","content":"Differs from TabSeparated format in that the rows are written without escaping. When parsing with this format, tabs or linefeeds are not allowed in each field. This format is also available under the name TSVRaw. "},{"title":"TabSeparatedWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#tabseparatedwithnames","content":"Differs from the TabSeparated format in that the column names are written in the first row. During parsing, the first row is expected to contain the column names. You can use column names to determine their position and to check their correctness. If setting input_format_with_names_use_header is set to 1, the columns from input data will be mapped to the columns from the table by their names, columns with unknown names will be skipped if setting input_format_skip_unknown_fields is set to 1. Otherwise, the first row will be skipped. This format is also available under the name TSVWithNames. "},{"title":"TabSeparatedWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#tabseparatedwithnamesandtypes","content":"Differs from the TabSeparated format in that the column names are written to the first row, while the column types are in the second row. The first row with names is processed the same way as in TabSeparatedWithNames format. If setting input_format_with_types_use_header is set to 1, the types from input data will be compared with the types of the corresponding columns from the table. Otherwise, the second row will be skipped. This format is also available under the name TSVWithNamesAndTypes. "},{"title":"TabSeparatedRawWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#tabseparatedrawwithnames","content":"Differs from TabSeparatedWithNames format in that the rows are written without escaping. When parsing with this format, tabs or linefeeds are not allowed in each field. This format is also available under the name TSVRawWithNames. "},{"title":"TabSeparatedRawWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#tabseparatedrawwithnamesandtypes","content":"Differs from TabSeparatedWithNamesAndTypes format in that the rows are written without escaping. When parsing with this format, tabs or linefeeds are not allowed in each field. This format is also available under the name TSVRawWithNamesAndNames. "},{"title":"Template​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#format-template","content":"This format allows specifying a custom format string with placeholders for values with a specified escaping rule. It uses settings format_template_resultset, format_template_row, format_template_rows_between_delimiter and some settings of other formats (e.g. output_format_json_quote_64bit_integers when using JSON escaping, see further) Setting format_template_row specifies path to file, which contains format string for rows with the following syntax: delimiter_1${column_1:serializeAs_1}delimiter_2${column_2:serializeAs_2} ... delimiter_N, where delimiter_i is a delimiter between values ($ symbol can be escaped as $$),column_i is a name or index of a column whose values are to be selected or inserted (if empty, then column will be skipped),serializeAs_i is an escaping rule for the column values. The following escaping rules are supported: CSV, JSON, XML (similarly to the formats of the same names)Escaped (similarly to TSV)Quoted (similarly to Values)Raw (without escaping, similarly to TSVRaw)None (no escaping rule, see further) If an escaping rule is omitted, then None will be used. XML is suitable only for output. So, for the following format string:  `Search phrase: ${SearchPhrase:Quoted}, count: ${c:Escaped}, ad price: $$${price:JSON};`  the values of SearchPhrase, c and price columns, which are escaped as Quoted, Escaped and JSON will be printed (for select) or will be expected (for insert) between Search phrase:, , count:, , ad price: $ and ; delimiters respectively. For example: Search phrase: 'bathroom interior design', count: 2166, ad price: $3; The format_template_rows_between_delimiter setting specifies delimiter between rows, which is printed (or expected) after every row except the last one (\\n by default) Setting format_template_resultset specifies the path to file, which contains a format string for resultset. Format string for resultset has the same syntax as a format string for row and allows to specify a prefix, a suffix and a way to print some additional information. It contains the following placeholders instead of column names: data is the rows with data in format_template_row format, separated by format_template_rows_between_delimiter. This placeholder must be the first placeholder in the format string.totals is the row with total values in format_template_row format (when using WITH TOTALS)min is the row with minimum values in format_template_row format (when extremes are set to 1)max is the row with maximum values in format_template_row format (when extremes are set to 1)rows is the total number of output rowsrows_before_limit is the minimal number of rows there would have been without LIMIT. Output only if the query contains LIMIT. If the query contains GROUP BY, rows_before_limit_at_least is the exact number of rows there would have been without a LIMIT.time is the request execution time in secondsrows_read is the number of rows has been readbytes_read is the number of bytes (uncompressed) has been read The placeholders data, totals, min and max must not have escaping rule specified (or None must be specified explicitly). The remaining placeholders may have any escaping rule specified. If the format_template_resultset setting is an empty string, ${data} is used as default value. For insert queries format allows skipping some columns or some fields if prefix or suffix (see example). Select example: SELECT SearchPhrase, count() AS c FROM test.hits GROUP BY SearchPhrase ORDER BY c DESC LIMIT 5 FORMAT Template SETTINGS format_template_resultset = '/some/path/resultset.format', format_template_row = '/some/path/row.format', format_template_rows_between_delimiter = '\\n '  /some/path/resultset.format: &lt;!DOCTYPE HTML&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Search phrases&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;table border=&quot;1&quot;&gt; &lt;caption&gt;Search phrases&lt;/caption&gt; &lt;tr&gt; &lt;th&gt;Search phrase&lt;/th&gt; &lt;th&gt;Count&lt;/th&gt; &lt;/tr&gt; ${data} &lt;/table&gt; &lt;table border=&quot;1&quot;&gt; &lt;caption&gt;Max&lt;/caption&gt; ${max} &lt;/table&gt; &lt;b&gt;Processed ${rows_read:XML} rows in ${time:XML} sec&lt;/b&gt; &lt;/body&gt; &lt;/html&gt;  /some/path/row.format: &lt;tr&gt; &lt;td&gt;${0:XML}&lt;/td&gt; &lt;td&gt;${1:XML}&lt;/td&gt; &lt;/tr&gt;  Result: &lt;!DOCTYPE HTML&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Search phrases&lt;/title&gt; &lt;/head&gt; &lt;body&gt; &lt;table border=&quot;1&quot;&gt; &lt;caption&gt;Search phrases&lt;/caption&gt; &lt;tr&gt; &lt;th&gt;Search phrase&lt;/th&gt; &lt;th&gt;Count&lt;/th&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;8267016&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;bathroom interior design&lt;/td&gt; &lt;td&gt;2166&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;clickhouse&lt;/td&gt; &lt;td&gt;1655&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;spring 2014 fashion&lt;/td&gt; &lt;td&gt;1549&lt;/td&gt; &lt;/tr&gt; &lt;tr&gt; &lt;td&gt;freeform photos&lt;/td&gt; &lt;td&gt;1480&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;table border=&quot;1&quot;&gt; &lt;caption&gt;Max&lt;/caption&gt; &lt;tr&gt; &lt;td&gt;&lt;/td&gt; &lt;td&gt;8873898&lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;b&gt;Processed 3095973 rows in 0.1569913 sec&lt;/b&gt; &lt;/body&gt; &lt;/html&gt;  Insert example: Some header Page views: 5, User id: 4324182021466249494, Useless field: hello, Duration: 146, Sign: -1 Page views: 6, User id: 4324182021466249494, Useless field: world, Duration: 185, Sign: 1 Total rows: 2  INSERT INTO UserActivity FORMAT Template SETTINGS format_template_resultset = '/some/path/resultset.format', format_template_row = '/some/path/row.format'  /some/path/resultset.format: Some header\\n${data}\\nTotal rows: ${:CSV}\\n  /some/path/row.format: Page views: ${PageViews:CSV}, User id: ${UserID:CSV}, Useless field: ${:CSV}, Duration: ${Duration:CSV}, Sign: ${Sign:CSV}  PageViews, UserID, Duration and Sign inside placeholders are names of columns in the table. Values after Useless field in rows and after \\nTotal rows: in suffix will be ignored. All delimiters in the input data must be strictly equal to delimiters in specified format strings. "},{"title":"TemplateIgnoreSpaces​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#templateignorespaces","content":"This format is suitable only for input. Similar to Template, but skips whitespace characters between delimiters and values in the input stream. However, if format strings contain whitespace characters, these characters will be expected in the input stream. Also allows to specify empty placeholders (${} or ${:None}) to split some delimiter into separate parts to ignore spaces between them. Such placeholders are used only for skipping whitespace characters. It’s possible to read JSON using this format, if values of columns have the same order in all rows. For example, the following request can be used for inserting data from output example of format JSON: INSERT INTO table_name FORMAT TemplateIgnoreSpaces SETTINGS format_template_resultset = '/some/path/resultset.format', format_template_row = '/some/path/row.format', format_template_rows_between_delimiter = ','  /some/path/resultset.format: {${}&quot;meta&quot;${}:${:JSON},${}&quot;data&quot;${}:${}[${data}]${},${}&quot;totals&quot;${}:${:JSON},${}&quot;extremes&quot;${}:${:JSON},${}&quot;rows&quot;${}:${:JSON},${}&quot;rows_before_limit_at_least&quot;${}:${:JSON}${}}  /some/path/row.format: {${}&quot;SearchPhrase&quot;${}:${}${phrase:JSON}${},${}&quot;c&quot;${}:${}${cnt:JSON}${}}  "},{"title":"TSKV​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#tskv","content":"Similar to TabSeparated, but outputs a value in name=value format. Names are escaped the same way as in TabSeparated format, and the = symbol is also escaped. SearchPhrase= count()=8267016 SearchPhrase=bathroom interior design count()=2166 SearchPhrase=clickhouse count()=1655 SearchPhrase=2014 spring fashion count()=1549 SearchPhrase=freeform photos count()=1480 SearchPhrase=angelina jolie count()=1245 SearchPhrase=omsk count()=1112 SearchPhrase=photos of dog breeds count()=1091 SearchPhrase=curtain designs count()=1064 SearchPhrase=baku count()=1000  NULL is formatted as \\N. SELECT * FROM t_null FORMAT TSKV  x=1 y=\\N  When there is a large number of small columns, this format is ineffective, and there is generally no reason to use it. Nevertheless, it is no worse than JSONEachRow in terms of efficiency. Both data output and parsing are supported in this format. For parsing, any order is supported for the values of different columns. It is acceptable for some values to be omitted – they are treated as equal to their default values. In this case, zeros and blank rows are used as default values. Complex values that could be specified in the table are not supported as defaults. Parsing allows the presence of the additional field tskv without the equal sign or a value. This field is ignored. "},{"title":"CSV​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#csv","content":"Comma Separated Values format (RFC). When formatting, rows are enclosed in double-quotes. A double quote inside a string is output as two double quotes in a row. There are no other rules for escaping characters. Date and date-time are enclosed in double-quotes. Numbers are output without quotes. Values are separated by a delimiter character, which is , by default. The delimiter character is defined in the setting format_csv_delimiter. Rows are separated using the Unix line feed (LF). Arrays are serialized in CSV as follows: first, the array is serialized to a string as in TabSeparated format, and then the resulting string is output to CSV in double-quotes. Tuples in CSV format are serialized as separate columns (that is, their nesting in the tuple is lost). $ clickhouse-client --format_csv_delimiter=&quot;|&quot; --query=&quot;INSERT INTO test.csv FORMAT CSV&quot; &lt; data.csv  *By default, the delimiter is ,. See the format_csv_delimiter setting for more information. When parsing, all values can be parsed either with or without quotes. Both double and single quotes are supported. Rows can also be arranged without quotes. In this case, they are parsed up to the delimiter character or line feed (CR or LF). In violation of the RFC, when parsing rows without quotes, the leading and trailing spaces and tabs are ignored. For the line feed, Unix (LF), Windows (CR LF) and Mac OS Classic (CR LF) types are all supported. If setting input_format_csv_empty_as_default is enabled, empty unquoted input values are replaced with default values. For complex default expressions input_format_defaults_for_omitted_fields must be enabled too. NULL is formatted according to setting format_csv_null_representation (default value is \\N). In input data, ENUM values can be represented as names or as ids. First, we try to match the input value to the ENUM name. If we fail and the input value is a number, we try to match this number to ENUM id. If input data contains only ENUM ids, it's recommended to enable the setting input_format_csv_enum_as_number to optimize ENUM parsing. The CSV format supports the output of totals and extremes the same way as TabSeparated. "},{"title":"CSVWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#csvwithnames","content":"Also prints the header row with column names, similar to TabSeparatedWithNames. "},{"title":"CSVWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#csvwithnamesandtypes","content":"Also prints two header rows with column names and types, similar to TabSeparatedWithNamesAndTypes. "},{"title":"CustomSeparated​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#format-customseparated","content":"Similar to Template, but it prints or reads all names and types of columns and uses escaping rule from format_custom_escaping_rule setting and delimiters from format_custom_field_delimiter, format_custom_row_before_delimiter, format_custom_row_after_delimiter, format_custom_row_between_delimiter, format_custom_result_before_delimiter and format_custom_result_after_delimiter settings, not from format strings. There is also CustomSeparatedIgnoreSpaces format, which is similar to TemplateIgnoreSpaces. "},{"title":"CustomSeparatedWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#customseparatedwithnames","content":"Also prints the header row with column names, similar to TabSeparatedWithNames. "},{"title":"CustomSeparatedWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#customseparatedwithnamesandtypes","content":"Also prints two header rows with column names and types, similar to TabSeparatedWithNamesAndTypes. "},{"title":"JSON​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#json","content":"Outputs data in JSON format. Besides data tables, it also outputs column names and types, along with some additional information: the total number of output rows, and the number of rows that could have been output if there weren’t a LIMIT. Example: SELECT SearchPhrase, count() AS c FROM test.hits GROUP BY SearchPhrase WITH TOTALS ORDER BY c DESC LIMIT 5 FORMAT JSON  { &quot;meta&quot;: [ { &quot;name&quot;: &quot;'hello'&quot;, &quot;type&quot;: &quot;String&quot; }, { &quot;name&quot;: &quot;multiply(42, number)&quot;, &quot;type&quot;: &quot;UInt64&quot; }, { &quot;name&quot;: &quot;range(5)&quot;, &quot;type&quot;: &quot;Array(UInt8)&quot; } ], &quot;data&quot;: [ { &quot;'hello'&quot;: &quot;hello&quot;, &quot;multiply(42, number)&quot;: &quot;0&quot;, &quot;range(5)&quot;: [0,1,2,3,4] }, { &quot;'hello'&quot;: &quot;hello&quot;, &quot;multiply(42, number)&quot;: &quot;42&quot;, &quot;range(5)&quot;: [0,1,2,3,4] }, { &quot;'hello'&quot;: &quot;hello&quot;, &quot;multiply(42, number)&quot;: &quot;84&quot;, &quot;range(5)&quot;: [0,1,2,3,4] } ], &quot;rows&quot;: 3, &quot;rows_before_limit_at_least&quot;: 3 }  The JSON is compatible with JavaScript. To ensure this, some characters are additionally escaped: the slash / is escaped as \\/; alternative line breaks U+2028 and U+2029, which break some browsers, are escaped as \\uXXXX. ASCII control characters are escaped: backspace, form feed, line feed, carriage return, and horizontal tab are replaced with \\b, \\f, \\n, \\r, \\t , as well as the remaining bytes in the 00-1F range using \\uXXXX sequences. Invalid UTF-8 sequences are changed to the replacement character � so the output text will consist of valid UTF-8 sequences. For compatibility with JavaScript, Int64 and UInt64 integers are enclosed in double-quotes by default. To remove the quotes, you can set the configuration parameter output_format_json_quote_64bit_integers to 0. rows – The total number of output rows. rows_before_limit_at_least The minimal number of rows there would have been without LIMIT. Output only if the query contains LIMIT. If the query contains GROUP BY, rows_before_limit_at_least is the exact number of rows there would have been without a LIMIT. totals – Total values (when using WITH TOTALS). extremes – Extreme values (when extremes are set to 1). This format is only appropriate for outputting a query result, but not for parsing (retrieving data to insert in a table). ClickHouse supports NULL, which is displayed as null in the JSON output. To enable +nan, -nan, +inf, -inf values in output, set the output_format_json_quote_denormals to 1. See Also JSONEachRow formatoutput_format_json_array_of_rows setting "},{"title":"JSONStrings​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsonstrings","content":"Differs from JSON only in that data fields are output in strings, not in typed JSON values. Example: { &quot;meta&quot;: [ { &quot;name&quot;: &quot;'hello'&quot;, &quot;type&quot;: &quot;String&quot; }, { &quot;name&quot;: &quot;multiply(42, number)&quot;, &quot;type&quot;: &quot;UInt64&quot; }, { &quot;name&quot;: &quot;range(5)&quot;, &quot;type&quot;: &quot;Array(UInt8)&quot; } ], &quot;data&quot;: [ { &quot;'hello'&quot;: &quot;hello&quot;, &quot;multiply(42, number)&quot;: &quot;0&quot;, &quot;range(5)&quot;: &quot;[0,1,2,3,4]&quot; }, { &quot;'hello'&quot;: &quot;hello&quot;, &quot;multiply(42, number)&quot;: &quot;42&quot;, &quot;range(5)&quot;: &quot;[0,1,2,3,4]&quot; }, { &quot;'hello'&quot;: &quot;hello&quot;, &quot;multiply(42, number)&quot;: &quot;84&quot;, &quot;range(5)&quot;: &quot;[0,1,2,3,4]&quot; } ], &quot;rows&quot;: 3, &quot;rows_before_limit_at_least&quot;: 3 }  "},{"title":"JSONAsString​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsonasstring","content":"In this format, a single JSON object is interpreted as a single value. If the input has several JSON objects (comma separated), they are interpreted as separate rows. If the input data is enclosed in square brackets, it is interpreted as an array of JSONs. This format can only be parsed for table with a single field of type String. The remaining columns must be set to DEFAULT or MATERIALIZED, or omitted. Once you collect whole JSON object to string you can use JSON functions to process it. Examples Query: DROP TABLE IF EXISTS json_as_string; CREATE TABLE json_as_string (json String) ENGINE = Memory; INSERT INTO json_as_string (json) FORMAT JSONAsString {&quot;foo&quot;:{&quot;bar&quot;:{&quot;x&quot;:&quot;y&quot;},&quot;baz&quot;:1}},{},{&quot;any json stucture&quot;:1} SELECT * FROM json_as_string;  Result: ┌─json──────────────────────────────┐ │ {&quot;foo&quot;:{&quot;bar&quot;:{&quot;x&quot;:&quot;y&quot;},&quot;baz&quot;:1}} │ │ {} │ │ {&quot;any json stucture&quot;:1} │ └───────────────────────────────────┘  An array of JSON objects Query: CREATE TABLE json_square_brackets (field String) ENGINE = Memory; INSERT INTO json_square_brackets FORMAT JSONAsString [{&quot;id&quot;: 1, &quot;name&quot;: &quot;name1&quot;}, {&quot;id&quot;: 2, &quot;name&quot;: &quot;name2&quot;}]; SELECT * FROM json_square_brackets;  Result: ┌─field──────────────────────┐ │ {&quot;id&quot;: 1, &quot;name&quot;: &quot;name1&quot;} │ │ {&quot;id&quot;: 2, &quot;name&quot;: &quot;name2&quot;} │ └────────────────────────────┘  "},{"title":"JSONCompact​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsoncompact","content":""},{"title":"JSONCompactStrings​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsoncompactstrings","content":"Differs from JSON only in that data rows are output in arrays, not in objects. Example: // JSONCompact { &quot;meta&quot;: [ { &quot;name&quot;: &quot;'hello'&quot;, &quot;type&quot;: &quot;String&quot; }, { &quot;name&quot;: &quot;multiply(42, number)&quot;, &quot;type&quot;: &quot;UInt64&quot; }, { &quot;name&quot;: &quot;range(5)&quot;, &quot;type&quot;: &quot;Array(UInt8)&quot; } ], &quot;data&quot;: [ [&quot;hello&quot;, &quot;0&quot;, [0,1,2,3,4]], [&quot;hello&quot;, &quot;42&quot;, [0,1,2,3,4]], [&quot;hello&quot;, &quot;84&quot;, [0,1,2,3,4]] ], &quot;rows&quot;: 3, &quot;rows_before_limit_at_least&quot;: 3 }  // JSONCompactStrings { &quot;meta&quot;: [ { &quot;name&quot;: &quot;'hello'&quot;, &quot;type&quot;: &quot;String&quot; }, { &quot;name&quot;: &quot;multiply(42, number)&quot;, &quot;type&quot;: &quot;UInt64&quot; }, { &quot;name&quot;: &quot;range(5)&quot;, &quot;type&quot;: &quot;Array(UInt8)&quot; } ], &quot;data&quot;: [ [&quot;hello&quot;, &quot;0&quot;, &quot;[0,1,2,3,4]&quot;], [&quot;hello&quot;, &quot;42&quot;, &quot;[0,1,2,3,4]&quot;], [&quot;hello&quot;, &quot;84&quot;, &quot;[0,1,2,3,4]&quot;] ], &quot;rows&quot;: 3, &quot;rows_before_limit_at_least&quot;: 3 }  "},{"title":"JSONEachRow​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsoneachrow","content":""},{"title":"JSONStringsEachRow​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsonstringseachrow","content":""},{"title":"JSONCompactEachRow​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsoncompacteachrow","content":""},{"title":"JSONCompactStringsEachRow​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsoncompactstringseachrow","content":"When using these formats, ClickHouse outputs rows as separated, newline-delimited JSON values, but the data as a whole is not valid JSON. {&quot;some_int&quot;:42,&quot;some_str&quot;:&quot;hello&quot;,&quot;some_tuple&quot;:[1,&quot;a&quot;]} // JSONEachRow [42,&quot;hello&quot;,[1,&quot;a&quot;]] // JSONCompactEachRow [&quot;42&quot;,&quot;hello&quot;,&quot;(2,'a')&quot;] // JSONCompactStringsEachRow  When inserting the data, you should provide a separate JSON value for each row. "},{"title":"JSONEachRowWithProgress​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsoneachrowwithprogress","content":""},{"title":"JSONStringsEachRowWithProgress​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsonstringseachrowwithprogress","content":"Differs from JSONEachRow/JSONStringsEachRow in that ClickHouse will also yield progress information as JSON values. {&quot;row&quot;:{&quot;'hello'&quot;:&quot;hello&quot;,&quot;multiply(42, number)&quot;:&quot;0&quot;,&quot;range(5)&quot;:[0,1,2,3,4]}} {&quot;row&quot;:{&quot;'hello'&quot;:&quot;hello&quot;,&quot;multiply(42, number)&quot;:&quot;42&quot;,&quot;range(5)&quot;:[0,1,2,3,4]}} {&quot;row&quot;:{&quot;'hello'&quot;:&quot;hello&quot;,&quot;multiply(42, number)&quot;:&quot;84&quot;,&quot;range(5)&quot;:[0,1,2,3,4]}} {&quot;progress&quot;:{&quot;read_rows&quot;:&quot;3&quot;,&quot;read_bytes&quot;:&quot;24&quot;,&quot;written_rows&quot;:&quot;0&quot;,&quot;written_bytes&quot;:&quot;0&quot;,&quot;total_rows_to_read&quot;:&quot;3&quot;}}  "},{"title":"JSONCompactEachRowWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsoncompacteachrowwithnames","content":"Differs from JSONCompactEachRow format in that it also prints the header row with column names, similar to TabSeparatedWithNames. "},{"title":"JSONCompactEachRowWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsoncompacteachrowwithnamesandtypes","content":"Differs from JSONCompactEachRow format in that it also prints two header rows with column names and types, similar to TabSeparatedWithNamesAndTypes. "},{"title":"JSONCompactStringsEachRowWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsoncompactstringseachrowwithnames","content":"Differs from JSONCompactStringsEachRow in that in that it also prints the header row with column names, similar to TabSeparatedWithNames. "},{"title":"JSONCompactStringsEachRowWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsoncompactstringseachrowwithnamesandtypes","content":"Differs from JSONCompactStringsEachRow in that it also prints two header rows with column names and types, similar to TabSeparatedWithNamesAndTypes. [&quot;'hello'&quot;, &quot;multiply(42, number)&quot;, &quot;range(5)&quot;] [&quot;String&quot;, &quot;UInt64&quot;, &quot;Array(UInt8)&quot;] [&quot;hello&quot;, &quot;0&quot;, [0,1,2,3,4]] [&quot;hello&quot;, &quot;42&quot;, [0,1,2,3,4]] [&quot;hello&quot;, &quot;84&quot;, [0,1,2,3,4]]  "},{"title":"Inserting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#inserting-data","content":"INSERT INTO UserActivity FORMAT JSONEachRow {&quot;PageViews&quot;:5, &quot;UserID&quot;:&quot;4324182021466249494&quot;, &quot;Duration&quot;:146,&quot;Sign&quot;:-1} {&quot;UserID&quot;:&quot;4324182021466249494&quot;,&quot;PageViews&quot;:6,&quot;Duration&quot;:185,&quot;Sign&quot;:1}  ClickHouse allows: Any order of key-value pairs in the object.Omitting some values. ClickHouse ignores spaces between elements and commas after the objects. You can pass all the objects in one line. You do not have to separate them with line breaks. Omitted values processing ClickHouse substitutes omitted values with the default values for the corresponding data types. If DEFAULT expr is specified, ClickHouse uses different substitution rules depending on the input_format_defaults_for_omitted_fields setting. Consider the following table: CREATE TABLE IF NOT EXISTS example_table ( x UInt32, a DEFAULT x * 2 ) ENGINE = Memory;  If input_format_defaults_for_omitted_fields = 0, then the default value for x and a equals 0 (as the default value for the UInt32 data type).If input_format_defaults_for_omitted_fields = 1, then the default value for x equals 0, but the default value of a equals x * 2. warning When inserting data with input_format_defaults_for_omitted_fields = 1, ClickHouse consumes more computational resources, compared to insertion with input_format_defaults_for_omitted_fields = 0. "},{"title":"Selecting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#selecting-data","content":"Consider the UserActivity table as an example: ┌──────────────UserID─┬─PageViews─┬─Duration─┬─Sign─┐ │ 4324182021466249494 │ 5 │ 146 │ -1 │ │ 4324182021466249494 │ 6 │ 185 │ 1 │ └─────────────────────┴───────────┴──────────┴──────┘  The query SELECT * FROM UserActivity FORMAT JSONEachRow returns: {&quot;UserID&quot;:&quot;4324182021466249494&quot;,&quot;PageViews&quot;:5,&quot;Duration&quot;:146,&quot;Sign&quot;:-1} {&quot;UserID&quot;:&quot;4324182021466249494&quot;,&quot;PageViews&quot;:6,&quot;Duration&quot;:185,&quot;Sign&quot;:1}  Unlike the JSON format, there is no substitution of invalid UTF-8 sequences. Values are escaped in the same way as for JSON. info Any set of bytes can be output in the strings. Use the JSONEachRow format if you are sure that the data in the table can be formatted as JSON without losing any information. "},{"title":"Usage of Nested Structures​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#jsoneachrow-nested","content":"If you have a table with Nested data type columns, you can insert JSON data with the same structure. Enable this feature with the input_format_import_nested_json setting. For example, consider the following table: CREATE TABLE json_each_row_nested (n Nested (s String, i Int32) ) ENGINE = Memory  As you can see in the Nested data type description, ClickHouse treats each component of the nested structure as a separate column (n.s and n.i for our table). You can insert data in the following way: INSERT INTO json_each_row_nested FORMAT JSONEachRow {&quot;n.s&quot;: [&quot;abc&quot;, &quot;def&quot;], &quot;n.i&quot;: [1, 23]}  To insert data as a hierarchical JSON object, set input_format_import_nested_json=1. { &quot;n&quot;: { &quot;s&quot;: [&quot;abc&quot;, &quot;def&quot;], &quot;i&quot;: [1, 23] } }  Without this setting, ClickHouse throws an exception. SELECT name, value FROM system.settings WHERE name = 'input_format_import_nested_json'  ┌─name────────────────────────────┬─value─┐ │ input_format_import_nested_json │ 0 │ └─────────────────────────────────┴───────┘  INSERT INTO json_each_row_nested FORMAT JSONEachRow {&quot;n&quot;: {&quot;s&quot;: [&quot;abc&quot;, &quot;def&quot;], &quot;i&quot;: [1, 23]}}  Code: 117. DB::Exception: Unknown field found while parsing JSONEachRow format: n: (at row 1)  SET input_format_import_nested_json=1 INSERT INTO json_each_row_nested FORMAT JSONEachRow {&quot;n&quot;: {&quot;s&quot;: [&quot;abc&quot;, &quot;def&quot;], &quot;i&quot;: [1, 23]}} SELECT * FROM json_each_row_nested  ┌─n.s───────────┬─n.i────┐ │ ['abc','def'] │ [1,23] │ └───────────────┴────────┘  "},{"title":"Native​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#native","content":"The most efficient format. Data is written and read by blocks in binary format. For each block, the number of rows, number of columns, column names and types, and parts of columns in this block are recorded one after another. In other words, this format is “columnar” – it does not convert columns to rows. This is the format used in the native interface for interaction between servers, for using the command-line client, and for C++ clients. You can use this format to quickly generate dumps that can only be read by the ClickHouse DBMS. It does not make sense to work with this format yourself. "},{"title":"Null​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#null","content":"Nothing is output. However, the query is processed, and when using the command-line client, data is transmitted to the client. This is used for tests, including performance testing. Obviously, this format is only appropriate for output, not for parsing. "},{"title":"Pretty​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#pretty","content":"Outputs data as Unicode-art tables, also using ANSI-escape sequences for setting colours in the terminal. A full grid of the table is drawn, and each row occupies two lines in the terminal. Each result block is output as a separate table. This is necessary so that blocks can be output without buffering results (buffering would be necessary in order to pre-calculate the visible width of all the values). NULL is output as ᴺᵁᴸᴸ. Example (shown for the PrettyCompact format): SELECT * FROM t_null  ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ └───┴──────┘  Rows are not escaped in Pretty* formats. Example is shown for the PrettyCompact format: SELECT 'String with \\'quotes\\' and \\t character' AS Escaping_test  ┌─Escaping_test────────────────────────┐ │ String with 'quotes' and character │ └──────────────────────────────────────┘  To avoid dumping too much data to the terminal, only the first 10,000 rows are printed. If the number of rows is greater than or equal to 10,000, the message “Showed first 10 000” is printed. This format is only appropriate for outputting a query result, but not for parsing (retrieving data to insert in a table). The Pretty format supports outputting total values (when using WITH TOTALS) and extremes (when ‘extremes’ is set to 1). In these cases, total values and extreme values are output after the main data, in separate tables. Example (shown for the PrettyCompact format): SELECT EventDate, count() AS c FROM test.hits GROUP BY EventDate WITH TOTALS ORDER BY EventDate FORMAT PrettyCompact  ┌──EventDate─┬───────c─┐ │ 2014-03-17 │ 1406958 │ │ 2014-03-18 │ 1383658 │ │ 2014-03-19 │ 1405797 │ │ 2014-03-20 │ 1353623 │ │ 2014-03-21 │ 1245779 │ │ 2014-03-22 │ 1031592 │ │ 2014-03-23 │ 1046491 │ └────────────┴─────────┘ Totals: ┌──EventDate─┬───────c─┐ │ 1970-01-01 │ 8873898 │ └────────────┴─────────┘ Extremes: ┌──EventDate─┬───────c─┐ │ 2014-03-17 │ 1031592 │ │ 2014-03-23 │ 1406958 │ └────────────┴─────────┘  "},{"title":"PrettyCompact​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#prettycompact","content":"Differs from Pretty in that the grid is drawn between rows and the result is more compact. This format is used by default in the command-line client in interactive mode. "},{"title":"PrettyCompactMonoBlock​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#prettycompactmonoblock","content":"Differs from PrettyCompact in that up to 10,000 rows are buffered, then output as a single table, not by blocks. "},{"title":"PrettyNoEscapes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#prettynoescapes","content":"Differs from Pretty in that ANSI-escape sequences aren’t used. This is necessary for displaying this format in a browser, as well as for using the ‘watch’ command-line utility. Example: $ watch -n1 &quot;clickhouse-client --query='SELECT event, value FROM system.events FORMAT PrettyCompactNoEscapes'&quot;  You can use the HTTP interface for displaying in the browser. "},{"title":"PrettyCompactNoEscapes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#prettycompactnoescapes","content":"The same as the previous setting. "},{"title":"PrettySpaceNoEscapes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#prettyspacenoescapes","content":"The same as the previous setting. "},{"title":"PrettySpace​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#prettyspace","content":"Differs from PrettyCompact in that whitespace (space characters) is used instead of the grid. "},{"title":"RowBinary​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#rowbinary","content":"Formats and parses data by row in binary format. Rows and values are listed consecutively, without separators. This format is less efficient than the Native format since it is row-based. Integers use fixed-length little-endian representation. For example, UInt64 uses 8 bytes. DateTime is represented as UInt32 containing the Unix timestamp as the value. Date is represented as a UInt16 object that contains the number of days since 1970-01-01 as the value. String is represented as a varint length (unsigned LEB128), followed by the bytes of the string. FixedString is represented simply as a sequence of bytes. Array is represented as a varint length (unsigned LEB128), followed by successive elements of the array. For NULL support, an additional byte containing 1 or 0 is added before each Nullable value. If 1, then the value is NULL and this byte is interpreted as a separate value. If 0, the value after the byte is not NULL. "},{"title":"RowBinaryWithNames​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#rowbinarywithnames","content":"Similar to RowBinary, but with added header: LEB128-encoded number of columns (N)N Strings specifying column names "},{"title":"RowBinaryWithNamesAndTypes​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#rowbinarywithnamesandtypes","content":"Similar to RowBinary, but with added header: LEB128-encoded number of columns (N)N Strings specifying column namesN Strings specifying column types "},{"title":"Values​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data-format-values","content":"Prints every row in brackets. Rows are separated by commas. There is no comma after the last row. The values inside the brackets are also comma-separated. Numbers are output in a decimal format without quotes. Arrays are output in square brackets. Strings, dates, and dates with times are output in quotes. Escaping rules and parsing are similar to the TabSeparated format. During formatting, extra spaces aren’t inserted, but during parsing, they are allowed and skipped (except for spaces inside array values, which are not allowed). NULL is represented as NULL. The minimum set of characters that you need to escape when passing data in Values ​​format: single quotes and backslashes. This is the format that is used in INSERT INTO t VALUES ..., but you can also use it for formatting query results. See also: input_format_values_interpret_expressions and input_format_values_deduce_templates_of_expressions settings. "},{"title":"Vertical​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#vertical","content":"Prints each value on a separate line with the column name specified. This format is convenient for printing just one or a few rows if each row consists of a large number of columns. NULL is output as ᴺᵁᴸᴸ. Example: SELECT * FROM t_null FORMAT Vertical  Row 1: ────── x: 1 y: ᴺᵁᴸᴸ  Rows are not escaped in Vertical format: SELECT 'string with \\'quotes\\' and \\t with some special \\n characters' AS test FORMAT Vertical  Row 1: ────── test: string with 'quotes' and with some special characters  This format is only appropriate for outputting a query result, but not for parsing (retrieving data to insert in a table). "},{"title":"XML​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#xml","content":"XML format is suitable only for output, not for parsing. Example: &lt;?xml version='1.0' encoding='UTF-8' ?&gt; &lt;result&gt; &lt;meta&gt; &lt;columns&gt; &lt;column&gt; &lt;name&gt;SearchPhrase&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;/column&gt; &lt;column&gt; &lt;name&gt;count()&lt;/name&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;/column&gt; &lt;/columns&gt; &lt;/meta&gt; &lt;data&gt; &lt;row&gt; &lt;SearchPhrase&gt;&lt;/SearchPhrase&gt; &lt;field&gt;8267016&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;bathroom interior design&lt;/SearchPhrase&gt; &lt;field&gt;2166&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;clickhouse&lt;/SearchPhrase&gt; &lt;field&gt;1655&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;2014 spring fashion&lt;/SearchPhrase&gt; &lt;field&gt;1549&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;freeform photos&lt;/SearchPhrase&gt; &lt;field&gt;1480&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;angelina jolie&lt;/SearchPhrase&gt; &lt;field&gt;1245&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;omsk&lt;/SearchPhrase&gt; &lt;field&gt;1112&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;photos of dog breeds&lt;/SearchPhrase&gt; &lt;field&gt;1091&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;curtain designs&lt;/SearchPhrase&gt; &lt;field&gt;1064&lt;/field&gt; &lt;/row&gt; &lt;row&gt; &lt;SearchPhrase&gt;baku&lt;/SearchPhrase&gt; &lt;field&gt;1000&lt;/field&gt; &lt;/row&gt; &lt;/data&gt; &lt;rows&gt;10&lt;/rows&gt; &lt;rows_before_limit_at_least&gt;141137&lt;/rows_before_limit_at_least&gt; &lt;/result&gt;  If the column name does not have an acceptable format, just ‘field’ is used as the element name. In general, the XML structure follows the JSON structure. Just as for JSON, invalid UTF-8 sequences are changed to the replacement character � so the output text will consist of valid UTF-8 sequences. In string values, the characters &lt; and &amp; are escaped as &lt; and &amp;. Arrays are output as &lt;array&gt;&lt;elem&gt;Hello&lt;/elem&gt;&lt;elem&gt;World&lt;/elem&gt;...&lt;/array&gt;,and tuples as &lt;tuple&gt;&lt;elem&gt;Hello&lt;/elem&gt;&lt;elem&gt;World&lt;/elem&gt;...&lt;/tuple&gt;. "},{"title":"CapnProto​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#capnproto","content":"CapnProto is a binary message format similar to Protocol Buffers and Thrift, but not like JSON or MessagePack. CapnProto messages are strictly typed and not self-describing, meaning they need an external schema description. The schema is applied on the fly and cached for each query. See also Format Schema. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data_types-matching-capnproto","content":"The table below shows supported data types and how they match ClickHouse data types in INSERT and SELECT queries. CapnProto data type (INSERT)\tClickHouse data type\tCapnProto data type (SELECT)UINT8, BOOL\tUInt8\tUINT8 INT8\tInt8\tINT8 UINT16\tUInt16, Date\tUINT16 INT16\tInt16\tINT16 UINT32\tUInt32, DateTime\tUINT32 INT32\tInt32\tINT32 UINT64\tUInt64\tUINT64 INT64\tInt64, DateTime64\tINT64 FLOAT32\tFloat32\tFLOAT32 FLOAT64\tFloat64\tFLOAT64 TEXT, DATA\tString, FixedString\tTEXT, DATA union(T, Void), union(Void, T)\tNullable(T)\tunion(T, Void), union(Void, T) ENUM\tEnum(8|16)\tENUM LIST\tArray\tLIST STRUCT\tTuple\tSTRUCT For working with Enum in CapnProto format use the format_capn_proto_enum_comparising_mode setting. Arrays can be nested and can have a value of the Nullable type as an argument. Tuple type also can be nested. "},{"title":"Inserting and Selecting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#inserting-and-selecting-data-capnproto","content":"You can insert CapnProto data from a file into ClickHouse table by the following command: $ cat capnproto_messages.bin | clickhouse-client --query &quot;INSERT INTO test.hits FORMAT CapnProto SETTINGS format_schema = 'schema:Message'&quot;  Where schema.capnp looks like this: struct Message { SearchPhrase @0 :Text; c @1 :Uint64; }  You can select data from a ClickHouse table and save them into some file in the CapnProto format by the following command: $ clickhouse-client --query = &quot;SELECT * FROM test.hits FORMAT CapnProto SETTINGS format_schema = 'schema:Message'&quot;  "},{"title":"Prometheus​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#prometheus","content":"Expose metrics in Prometheus text-based exposition format. The output table should have a proper structure. Columns name (String) and value (number) are required. Rows may optionally contain help (String) and timestamp (number). Column type (String) is either counter, gauge, histogram, summary, untyped or empty. Each metric value may also have some labels (Map(String, String)). Several consequent rows may refer to the one metric with different lables. The table should be sorted by metric name (e.g., with ORDER BY name). There's special requirements for labels for histogram and summary, see Prometheus doc for the details. Special rules applied to row with labels {'count':''} and {'sum':''}, they'll be convered to &lt;metric_name&gt;_count and &lt;metric_name&gt;_sum respectively. Example: ┌─name────────────────────────────────┬─type──────┬─help──────────────────────────────────────┬─labels─────────────────────────┬────value─┬─────timestamp─┐ │ http_request_duration_seconds │ histogram │ A histogram of the request duration. │ {'le':'0.05'} │ 24054 │ 0 │ │ http_request_duration_seconds │ histogram │ │ {'le':'0.1'} │ 33444 │ 0 │ │ http_request_duration_seconds │ histogram │ │ {'le':'0.2'} │ 100392 │ 0 │ │ http_request_duration_seconds │ histogram │ │ {'le':'0.5'} │ 129389 │ 0 │ │ http_request_duration_seconds │ histogram │ │ {'le':'1'} │ 133988 │ 0 │ │ http_request_duration_seconds │ histogram │ │ {'le':'+Inf'} │ 144320 │ 0 │ │ http_request_duration_seconds │ histogram │ │ {'sum':''} │ 53423 │ 0 │ │ http_requests_total │ counter │ Total number of HTTP requests │ {'method':'post','code':'200'} │ 1027 │ 1395066363000 │ │ http_requests_total │ counter │ │ {'method':'post','code':'400'} │ 3 │ 1395066363000 │ │ metric_without_timestamp_and_labels │ │ │ {} │ 12.47 │ 0 │ │ rpc_duration_seconds │ summary │ A summary of the RPC duration in seconds. │ {'quantile':'0.01'} │ 3102 │ 0 │ │ rpc_duration_seconds │ summary │ │ {'quantile':'0.05'} │ 3272 │ 0 │ │ rpc_duration_seconds │ summary │ │ {'quantile':'0.5'} │ 4773 │ 0 │ │ rpc_duration_seconds │ summary │ │ {'quantile':'0.9'} │ 9001 │ 0 │ │ rpc_duration_seconds │ summary │ │ {'quantile':'0.99'} │ 76656 │ 0 │ │ rpc_duration_seconds │ summary │ │ {'count':''} │ 2693 │ 0 │ │ rpc_duration_seconds │ summary │ │ {'sum':''} │ 17560473 │ 0 │ │ something_weird │ │ │ {'problem':'division by zero'} │ inf │ -3982045 │ └─────────────────────────────────────┴───────────┴───────────────────────────────────────────┴────────────────────────────────┴──────────┴───────────────┘  Will be formatted as: # HELP http_request_duration_seconds A histogram of the request duration. # TYPE http_request_duration_seconds histogram http_request_duration_seconds_bucket{le=&quot;0.05&quot;} 24054 http_request_duration_seconds_bucket{le=&quot;0.1&quot;} 33444 http_request_duration_seconds_bucket{le=&quot;0.5&quot;} 129389 http_request_duration_seconds_bucket{le=&quot;1&quot;} 133988 http_request_duration_seconds_bucket{le=&quot;+Inf&quot;} 144320 http_request_duration_seconds_sum 53423 http_request_duration_seconds_count 144320 # HELP http_requests_total Total number of HTTP requests # TYPE http_requests_total counter http_requests_total{code=&quot;200&quot;,method=&quot;post&quot;} 1027 1395066363000 http_requests_total{code=&quot;400&quot;,method=&quot;post&quot;} 3 1395066363000 metric_without_timestamp_and_labels 12.47 # HELP rpc_duration_seconds A summary of the RPC duration in seconds. # TYPE rpc_duration_seconds summary rpc_duration_seconds{quantile=&quot;0.01&quot;} 3102 rpc_duration_seconds{quantile=&quot;0.05&quot;} 3272 rpc_duration_seconds{quantile=&quot;0.5&quot;} 4773 rpc_duration_seconds{quantile=&quot;0.9&quot;} 9001 rpc_duration_seconds{quantile=&quot;0.99&quot;} 76656 rpc_duration_seconds_sum 17560473 rpc_duration_seconds_count 2693 something_weird{problem=&quot;division by zero&quot;} +Inf -3982045  "},{"title":"Protobuf​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#protobuf","content":"Protobuf - is a Protocol Buffers format. This format requires an external format schema. The schema is cached between queries. ClickHouse supports both proto2 and proto3 syntaxes. Repeated/optional/required fields are supported. Usage examples: SELECT * FROM test.table FORMAT Protobuf SETTINGS format_schema = 'schemafile:MessageType'  cat protobuf_messages.bin | clickhouse-client --query &quot;INSERT INTO test.table FORMAT Protobuf SETTINGS format_schema='schemafile:MessageType'&quot;  where the file schemafile.proto looks like this: syntax = &quot;proto3&quot;; message MessageType { string name = 1; string surname = 2; uint32 birthDate = 3; repeated string phoneNumbers = 4; };  To find the correspondence between table columns and fields of Protocol Buffers’ message type ClickHouse compares their names. This comparison is case-insensitive and the characters _ (underscore) and . (dot) are considered as equal. If types of a column and a field of Protocol Buffers’ message are different the necessary conversion is applied. Nested messages are supported. For example, for the field z in the following message type message MessageType { message XType { message YType { int32 z; }; repeated YType y; }; XType x; };  ClickHouse tries to find a column named x.y.z (or x_y_z or X.y_Z and so on). Nested messages are suitable to input or output a nested data structures. Default values defined in a protobuf schema like this syntax = &quot;proto2&quot;; message MessageType { optional int32 result_per_page = 3 [default = 10]; }  are not applied; the table defaults are used instead of them. ClickHouse inputs and outputs protobuf messages in the length-delimited format. It means before every message should be written its length as a varint. See also how to read/write length-delimited protobuf messages in popular languages. "},{"title":"ProtobufSingle​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#protobufsingle","content":"Same as Protobuf but for storing/parsing single Protobuf message without length delimiters. "},{"title":"Avro​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data-format-avro","content":"Apache Avro is a row-oriented data serialization framework developed within Apache’s Hadoop project. ClickHouse Avro format supports reading and writing Avro data files. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data_types-matching","content":"The table below shows supported data types and how they match ClickHouse data types in INSERT and SELECT queries. Avro data type INSERT\tClickHouse data type\tAvro data type SELECTboolean, int, long, float, double\tInt(8|16|32), UInt(8|16|32)\tint boolean, int, long, float, double\tInt64, UInt64\tlong boolean, int, long, float, double\tFloat32\tfloat boolean, int, long, float, double\tFloat64\tdouble bytes, string, fixed, enum\tString\tbytes or string * bytes, string, fixed\tFixedString(N)\tfixed(N) enum\tEnum(8|16)\tenum array(T)\tArray(T)\tarray(T) union(null, T), union(T, null)\tNullable(T)\tunion(null, T) null\tNullable(Nothing)\tnull int (date) **\tDate\tint (date) ** long (timestamp-millis) **\tDateTime64(3)\tlong (timestamp-millis) * long (timestamp-micros) **\tDateTime64(6)\tlong (timestamp-micros) * * bytes is default, controlled by output_format_avro_string_column_pattern** Avro logical types Unsupported Avro data types: record (non-root), map Unsupported Avro logical data types: time-millis, time-micros, duration "},{"title":"Inserting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#inserting-data-1","content":"To insert data from an Avro file into ClickHouse table: $ cat file.avro | clickhouse-client --query=&quot;INSERT INTO {some_table} FORMAT Avro&quot;  The root schema of input Avro file must be of record type. To find the correspondence between table columns and fields of Avro schema ClickHouse compares their names. This comparison is case-sensitive. Unused fields are skipped. Data types of ClickHouse table columns can differ from the corresponding fields of the Avro data inserted. When inserting data, ClickHouse interprets data types according to the table above and then casts the data to corresponding column type. "},{"title":"Selecting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#selecting-data-1","content":"To select data from ClickHouse table into an Avro file: $ clickhouse-client --query=&quot;SELECT * FROM {some_table} FORMAT Avro&quot; &gt; file.avro  Column names must: start with [A-Za-z_]subsequently contain only [A-Za-z0-9_] Output Avro file compression and sync interval can be configured with output_format_avro_codec and output_format_avro_sync_interval respectively. "},{"title":"AvroConfluent​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data-format-avro-confluent","content":"AvroConfluent supports decoding single-object Avro messages commonly used with Kafka and Confluent Schema Registry. Each Avro message embeds a schema id that can be resolved to the actual schema with help of the Schema Registry. Schemas are cached once resolved. Schema Registry URL is configured with format_avro_schema_registry_url. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data_types-matching-1","content":"Same as Avro. "},{"title":"Usage​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#usage","content":"To quickly verify schema resolution you can use kafkacat with clickhouse-local: $ kafkacat -b kafka-broker -C -t topic1 -o beginning -f '%s' -c 3 | clickhouse-local --input-format AvroConfluent --format_avro_schema_registry_url 'http://schema-registry' -S &quot;field1 Int64, field2 String&quot; -q 'select * from table' 1 a 2 b 3 c  To use AvroConfluent with Kafka: CREATE TABLE topic1_stream ( field1 String, field2 String ) ENGINE = Kafka() SETTINGS kafka_broker_list = 'kafka-broker', kafka_topic_list = 'topic1', kafka_group_name = 'group1', kafka_format = 'AvroConfluent'; SET format_avro_schema_registry_url = 'http://schema-registry'; SELECT * FROM topic1_stream;  warning Setting format_avro_schema_registry_url needs to be configured in users.xml to maintain it’s value after a restart. Also you can use the format_avro_schema_registry_url setting of the Kafka table engine. "},{"title":"Parquet​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data-format-parquet","content":"Apache Parquet is a columnar storage format widespread in the Hadoop ecosystem. ClickHouse supports read and write operations for this format. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data_types-matching-2","content":"The table below shows supported data types and how they match ClickHouse data types in INSERT and SELECT queries. Parquet data type (INSERT)\tClickHouse data type\tParquet data type (SELECT)UINT8, BOOL\tUInt8\tUINT8 INT8\tInt8\tINT8 UINT16\tUInt16\tUINT16 INT16\tInt16\tINT16 UINT32\tUInt32\tUINT32 INT32\tInt32\tINT32 UINT64\tUInt64\tUINT64 INT64\tInt64\tINT64 FLOAT, HALF_FLOAT\tFloat32\tFLOAT DOUBLE\tFloat64\tDOUBLE DATE32\tDate\tUINT16 DATE64, TIMESTAMP\tDateTime\tUINT32 STRING, BINARY\tString\tBINARY —\tFixedString\tBINARY DECIMAL\tDecimal\tDECIMAL LIST\tArray\tLIST STRUCT\tTuple\tSTRUCT MAP\tMap\tMAP Arrays can be nested and can have a value of the Nullable type as an argument. Tuple and Map types also can be nested. ClickHouse supports configurable precision of Decimal type. The INSERT query treats the Parquet DECIMAL type as the ClickHouse Decimal128 type. Unsupported Parquet data types: TIME32, FIXED_SIZE_BINARY, JSON, UUID, ENUM. Data types of ClickHouse table columns can differ from the corresponding fields of the Parquet data inserted. When inserting data, ClickHouse interprets data types according to the table above and then cast the data to that data type which is set for the ClickHouse table column. "},{"title":"Inserting and Selecting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#inserting-and-selecting-data","content":"You can insert Parquet data from a file into ClickHouse table by the following command: $ cat {filename} | clickhouse-client --query=&quot;INSERT INTO {some_table} FORMAT Parquet&quot;  To insert data into Nested columns as an array of structs values you must switch on the input_format_parquet_import_nested setting. You can select data from a ClickHouse table and save them into some file in the Parquet format by the following command: $ clickhouse-client --query=&quot;SELECT * FROM {some_table} FORMAT Parquet&quot; &gt; {some_file.pq}  To exchange data with Hadoop, you can use HDFS table engine. "},{"title":"Arrow​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data-format-arrow","content":"Apache Arrow comes with two built-in columnar storage formats. ClickHouse supports read and write operations for these formats. Arrow is Apache Arrow’s &quot;file mode&quot; format. It is designed for in-memory random access. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data_types-matching-arrow","content":"The table below shows supported data types and how they match ClickHouse data types in INSERT and SELECT queries. Arrow data type (INSERT)\tClickHouse data type\tArrow data type (SELECT)UINT8, BOOL\tUInt8\tUINT8 INT8\tInt8\tINT8 UINT16\tUInt16\tUINT16 INT16\tInt16\tINT16 UINT32\tUInt32\tUINT32 INT32\tInt32\tINT32 UINT64\tUInt64\tUINT64 INT64\tInt64\tINT64 FLOAT, HALF_FLOAT\tFloat32\tFLOAT32 DOUBLE\tFloat64\tFLOAT64 DATE32\tDate\tUINT16 DATE64, TIMESTAMP\tDateTime\tUINT32 STRING, BINARY\tString\tBINARY STRING, BINARY\tFixedString\tBINARY DECIMAL\tDecimal\tDECIMAL DECIMAL256\tDecimal256\tDECIMAL256 LIST\tArray\tLIST STRUCT\tTuple\tSTRUCT MAP\tMap\tMAP Arrays can be nested and can have a value of the Nullable type as an argument. Tuple and Map types also can be nested. The DICTIONARY type is supported for INSERT queries, and for SELECT queries there is an output_format_arrow_low_cardinality_as_dictionary setting that allows to output LowCardinality type as a DICTIONARY type. ClickHouse supports configurable precision of the Decimal type. The INSERT query treats the Arrow DECIMAL type as the ClickHouse Decimal128 type. Unsupported Arrow data types: TIME32, FIXED_SIZE_BINARY, JSON, UUID, ENUM. The data types of ClickHouse table columns do not have to match the corresponding Arrow data fields. When inserting data, ClickHouse interprets data types according to the table above and then casts the data to the data type set for the ClickHouse table column. "},{"title":"Inserting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#inserting-data-arrow","content":"You can insert Arrow data from a file into ClickHouse table by the following command: $ cat filename.arrow | clickhouse-client --query=&quot;INSERT INTO some_table FORMAT Arrow&quot;  To insert data into Nested columns as an array of structs values you must switch on the input_format_arrow_import_nested setting. "},{"title":"Selecting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#selecting-data-arrow","content":"You can select data from a ClickHouse table and save them into some file in the Arrow format by the following command: $ clickhouse-client --query=&quot;SELECT * FROM {some_table} FORMAT Arrow&quot; &gt; {filename.arrow}  "},{"title":"ArrowStream​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data-format-arrow-stream","content":"ArrowStream is Apache Arrow’s “stream mode” format. It is designed for in-memory stream processing. "},{"title":"ORC​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data-format-orc","content":"Apache ORC is a columnar storage format widespread in the Hadoop ecosystem. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data_types-matching-3","content":"The table below shows supported data types and how they match ClickHouse data types in INSERT and SELECT queries. ORC data type (INSERT)\tClickHouse data type\tORC data type (SELECT)UINT8, BOOL\tUInt8\tUINT8 INT8\tInt8\tINT8 UINT16\tUInt16\tUINT16 INT16\tInt16\tINT16 UINT32\tUInt32\tUINT32 INT32\tInt32\tINT32 UINT64\tUInt64\tUINT64 INT64\tInt64\tINT64 FLOAT, HALF_FLOAT\tFloat32\tFLOAT DOUBLE\tFloat64\tDOUBLE DATE32\tDate\tDATE32 DATE64, TIMESTAMP\tDateTime\tTIMESTAMP STRING, BINARY\tString\tBINARY DECIMAL\tDecimal\tDECIMAL LIST\tArray\tLIST STRUCT\tTuple\tSTRUCT MAP\tMap\tMAP Arrays can be nested and can have a value of the Nullable type as an argument. Tuple and Map types also can be nested. ClickHouse supports configurable precision of the Decimal type. The INSERT query treats the ORC DECIMAL type as the ClickHouse Decimal128 type. Unsupported ORC data types: TIME32, FIXED_SIZE_BINARY, JSON, UUID, ENUM. The data types of ClickHouse table columns do not have to match the corresponding ORC data fields. When inserting data, ClickHouse interprets data types according to the table above and then casts the data to the data type set for the ClickHouse table column. "},{"title":"Inserting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#inserting-data-2","content":"You can insert ORC data from a file into ClickHouse table by the following command: $ cat filename.orc | clickhouse-client --query=&quot;INSERT INTO some_table FORMAT ORC&quot;  To insert data into Nested columns as an array of structs values you must switch on the input_format_orc_import_nested setting. "},{"title":"Selecting Data​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#selecting-data-2","content":"You can select data from a ClickHouse table and save them into some file in the ORC format by the following command: $ clickhouse-client --query=&quot;SELECT * FROM {some_table} FORMAT ORC&quot; &gt; {filename.orc}  To exchange data with Hadoop, you can use HDFS table engine. "},{"title":"LineAsString​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#lineasstring","content":"In this format, every line of input data is interpreted as a single string value. This format can only be parsed for table with a single field of type String. The remaining columns must be set to DEFAULT or MATERIALIZED, or omitted. Example Query: DROP TABLE IF EXISTS line_as_string; CREATE TABLE line_as_string (field String) ENGINE = Memory; INSERT INTO line_as_string FORMAT LineAsString &quot;I love apple&quot;, &quot;I love banana&quot;, &quot;I love orange&quot;; SELECT * FROM line_as_string;  Result: ┌─field─────────────────────────────────────────────┐ │ &quot;I love apple&quot;, &quot;I love banana&quot;, &quot;I love orange&quot;; │ └───────────────────────────────────────────────────┘  "},{"title":"Regexp​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data-format-regexp","content":"Each line of imported data is parsed according to the regular expression. When working with the Regexp format, you can use the following settings: format_regexp — String. Contains regular expression in the re2 format. format_regexp_escaping_rule — String. The following escaping rules are supported: CSV (similarly to CSV)JSON (similarly to JSONEachRow)Escaped (similarly to TSV)Quoted (similarly to Values)Raw (extracts subpatterns as a whole, no escaping rules, similarly to TSVRaw) format_regexp_skip_unmatched — UInt8. Defines the need to throw an exeption in case the format_regexp expression does not match the imported data. Can be set to 0 or 1. Usage The regular expression from format_regexp setting is applied to every line of imported data. The number of subpatterns in the regular expression must be equal to the number of columns in imported dataset. Lines of the imported data must be separated by newline character '\\n' or DOS-style newline &quot;\\r\\n&quot;. The content of every matched subpattern is parsed with the method of corresponding data type, according to format_regexp_escaping_rule setting. If the regular expression does not match the line and format_regexp_skip_unmatched is set to 1, the line is silently skipped. If format_regexp_skip_unmatched is set to 0, exception is thrown. Example Consider the file data.tsv: id: 1 array: [1,2,3] string: str1 date: 2020-01-01 id: 2 array: [1,2,3] string: str2 date: 2020-01-02 id: 3 array: [1,2,3] string: str3 date: 2020-01-03  and the table: CREATE TABLE imp_regex_table (id UInt32, array Array(UInt32), string String, date Date) ENGINE = Memory;  Import command: $ cat data.tsv | clickhouse-client --query &quot;INSERT INTO imp_regex_table FORMAT Regexp SETTINGS format_regexp='id: (.+?) array: (.+?) string: (.+?) date: (.+?)', format_regexp_escaping_rule='Escaped', format_regexp_skip_unmatched=0;&quot;  Query: SELECT * FROM imp_regex_table;  Result: ┌─id─┬─array───┬─string─┬───────date─┐ │ 1 │ [1,2,3] │ str1 │ 2020-01-01 │ │ 2 │ [1,2,3] │ str2 │ 2020-01-02 │ │ 3 │ [1,2,3] │ str3 │ 2020-01-03 │ └────┴─────────┴────────┴────────────┘  "},{"title":"Format Schema​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#formatschema","content":"The file name containing the format schema is set by the setting format_schema. It’s required to set this setting when it is used one of the formats Cap'n Proto and Protobuf. The format schema is a combination of a file name and the name of a message type in this file, delimited by a colon, e.g. schemafile.proto:MessageType. If the file has the standard extension for the format (for example, .proto for Protobuf), it can be omitted and in this case, the format schema looks like schemafile:MessageType. If you input or output data via the client in the interactive mode, the file name specified in the format schema can contain an absolute path or a path relative to the current directory on the client. If you use the client in the batch mode, the path to the schema must be relative due to security reasons. If you input or output data via the HTTP interface the file name specified in the format schema should be located in the directory specified in format_schema_pathin the server configuration. "},{"title":"Skipping Errors​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#skippingerrors","content":"Some formats such as CSV, TabSeparated, TSKV, JSONEachRow, Template, CustomSeparated and Protobuf can skip broken row if parsing error occurred and continue parsing from the beginning of next row. See input_format_allow_errors_num andinput_format_allow_errors_ratio settings. Limitations: In case of parsing error JSONEachRow skips all data until the new line (or EOF), so rows must be delimited by \\n to count errors correctly.Template and CustomSeparated use delimiter after the last column and delimiter between rows to find the beginning of next row, so skipping errors works only if at least one of them is not empty. "},{"title":"RawBLOB​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#rawblob","content":"In this format, all input data is read to a single value. It is possible to parse only a table with a single field of type String or similar. The result is output in binary format without delimiters and escaping. If more than one value is output, the format is ambiguous, and it will be impossible to read the data back. Below is a comparison of the formats RawBLOB and TabSeparatedRaw.RawBLOB: data is output in binary format, no escaping;there are no delimiters between values;no newline at the end of each value.[TabSeparatedRaw] (#tabseparatedraw):data is output without escaping;the rows contain values separated by tabs;there is a line feed after the last value in every row. The following is a comparison of the RawBLOB and RowBinary formats.RawBLOB: String fields are output without being prefixed by length.RowBinary:String fields are represented as length in varint format (unsigned [LEB128] (https://en.wikipedia.org/wiki/LEB128)), followed by the bytes of the string. When an empty data is passed to the RawBLOB input, ClickHouse throws an exception: Code: 108. DB::Exception: No data to insert  Example $ clickhouse-client --query &quot;CREATE TABLE {some_table} (a String) ENGINE = Memory;&quot; $ cat {filename} | clickhouse-client --query=&quot;INSERT INTO {some_table} FORMAT RawBLOB&quot; $ clickhouse-client --query &quot;SELECT * FROM {some_table} FORMAT RawBLOB&quot; | md5sum  Result: f9725a22f9191e064120d718e26862a9 -  "},{"title":"MsgPack​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#msgpack","content":"ClickHouse supports reading and writing MessagePack data files. "},{"title":"Data Types Matching​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#data-types-matching-msgpack","content":"MessagePack data type (INSERT)\tClickHouse data type\tMessagePack data type (SELECT)uint N, positive fixint\tUIntN\tuint N int N\tIntN\tint N bool\tUInt8\tuint 8 fixstr, str 8, str 16, str 32, bin 8, bin 16, bin 32\tString\tbin 8, bin 16, bin 32 fixstr, str 8, str 16, str 32, bin 8, bin 16, bin 32\tFixedString\tbin 8, bin 16, bin 32 float 32\tFloat32\tfloat 32 float 64\tFloat64\tfloat 64 uint 16\tDate\tuint 16 uint 32\tDateTime\tuint 32 uint 64\tDateTime64\tuint 64 fixarray, array 16, array 32\tArray\tfixarray, array 16, array 32 fixmap, map 16, map 32\tMap\tfixmap, map 16, map 32 Example: Writing to a file &quot;.msgpk&quot;: $ clickhouse-client --query=&quot;CREATE TABLE msgpack (array Array(UInt8)) ENGINE = Memory;&quot; $ clickhouse-client --query=&quot;INSERT INTO msgpack VALUES ([0, 1, 2, 3, 42, 253, 254, 255]), ([255, 254, 253, 42, 3, 2, 1, 0])&quot;; $ clickhouse-client --query=&quot;SELECT * FROM msgpack FORMAT MsgPack&quot; &gt; tmp_msgpack.msgpk;  "},{"title":"MySQLDump​","type":1,"pageTitle":"Formats for Input and Output Data","url":"en/interfaces/formats#msgpack","content":"ClickHouse supports reading MySQL dumps. It reads all data from INSERT queries belonging to one table in dump. If there are more than one table, by default it reads data from the first one. You can specify the name of the table from which to read data from using input_format_mysql_dump_table_name settings. If setting input_format_mysql_dump_map_columns is set to 1 and dump contains CREATE query for specified table or column names in INSERT query the columns from input data will be mapped to the columns from the table by their names, columns with unknown names will be skipped if setting input_format_skip_unknown_fields is set to 1. This format supports schema inference: if the dump contains CREATE query for the specified table, the structure is extracted from it, otherwise schema is inferred from the data of INSERT queries. Examples: File dump.sql: /*!40101 SET @saved_cs_client = @@character_set_client */; /*!50503 SET character_set_client = utf8mb4 */; CREATE TABLE `test` ( `x` int DEFAULT NULL, `y` int DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; /*!40101 SET character_set_client = @saved_cs_client */; INSERT INTO `test` VALUES (1,NULL),(2,NULL),(3,NULL),(3,NULL),(4,NULL),(5,NULL),(6,7); /*!40101 SET @saved_cs_client = @@character_set_client */; /*!50503 SET character_set_client = utf8mb4 */; CREATE TABLE `test 3` ( `y` int DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; /*!40101 SET character_set_client = @saved_cs_client */; INSERT INTO `test 3` VALUES (1); /*!40101 SET @saved_cs_client = @@character_set_client */; /*!50503 SET character_set_client = utf8mb4 */; CREATE TABLE `test2` ( `x` int DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci; /*!40101 SET character_set_client = @saved_cs_client */; INSERT INTO `test2` VALUES (1),(2),(3);  Queries: :) desc file(dump.sql, MySQLDump) settings input_format_mysql_dump_table_name='test2' DESCRIBE TABLE file(dump.sql, MySQLDump) SETTINGS input_format_mysql_dump_table_name = 'test2' Query id: 25e66c89-e10a-42a8-9b42-1ee8bbbde5ef ┌─name─┬─type────────────┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┬─ttl_expression─┐ │ x │ Nullable(Int32) │ │ │ │ │ │ └──────┴─────────────────┴──────────────┴────────────────────┴─────────┴──────────────────┴────────────────┘ :) select * from file(dump.sql, MySQLDump) settings input_format_mysql_dump_table_name='test2' SELECT * FROM file(dump.sql, MySQLDump) SETTINGS input_format_mysql_dump_table_name = 'test2' Query id: 17d59664-ebce-4053-bb79-d46a516fb590 ┌─x─┐ │ 1 │ │ 2 │ │ 3 │ └───┘  "},{"title":"System Tables","type":0,"sectionRef":"#","url":"en/operations/system-tables/","content":"","keywords":""},{"title":"Introduction​","type":1,"pageTitle":"System Tables","url":"en/operations/system-tables/#system-tables-introduction","content":"System tables provide information about: Server states, processes, and environment.Server’s internal processes. System tables: Located in the system database.Available only for reading data.Can’t be dropped or altered, but can be detached. Most of system tables store their data in RAM. A ClickHouse server creates such system tables at the start. Unlike other system tables, the system log tables metric_log, query_log, query_thread_log, trace_log, part_log, crash_log and text_log are served by MergeTree table engine and store their data in a filesystem by default. If you remove a table from a filesystem, the ClickHouse server creates the empty one again at the time of the next data writing. If system table schema changed in a new release, then ClickHouse renames the current table and creates a new one. System log tables can be customized by creating a config file with the same name as the table under /etc/clickhouse-server/config.d/, or setting corresponding elements in /etc/clickhouse-server/config.xml. Elements can be customized are: database: database the system log table belongs to. This option is deprecated now. All system log tables are under database system.table: table to insert data.partition_by: specify PARTITION BY expression.ttl: specify table TTL expression.flush_interval_milliseconds: interval of flushing data to disk.engine: provide full engine expression (starting with ENGINE = ) with parameters. This option is contradict with partition_by and ttl. If set together, the server would raise an exception and exit. An example: &lt;clickhouse&gt; &lt;query_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_log&lt;/table&gt; &lt;partition_by&gt;toYYYYMM(event_date)&lt;/partition_by&gt; &lt;ttl&gt;event_date + INTERVAL 30 DAY DELETE&lt;/ttl&gt; &lt;!-- &lt;engine&gt;ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024&lt;/engine&gt; --&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_log&gt; &lt;/clickhouse&gt;  By default, table growth is unlimited. To control a size of a table, you can use TTL settings for removing outdated log records. Also you can use the partitioning feature of MergeTree-engine tables. "},{"title":"Sources of System Metrics​","type":1,"pageTitle":"System Tables","url":"en/operations/system-tables/#system-tables-sources-of-system-metrics","content":"For collecting system metrics ClickHouse server uses: CAP_NET_ADMIN capability.procfs (only in Linux). procfs If ClickHouse server does not have CAP_NET_ADMIN capability, it tries to fall back to ProcfsMetricsProvider. ProcfsMetricsProvider allows collecting per-query system metrics (for CPU and I/O). If procfs is supported and enabled on the system, ClickHouse server collects these metrics: OSCPUVirtualTimeMicrosecondsOSCPUWaitMicrosecondsOSIOWaitMicrosecondsOSReadCharsOSWriteCharsOSReadBytesOSWriteBytes Original article "},{"title":"asynchronous_metric_log","type":0,"sectionRef":"#","url":"en/operations/system-tables/asynchronous_metric_log","content":"asynchronous_metric_log Contains the historical values for system.asynchronous_metrics, which are saved once per minute. Enabled by default. Columns: event_date (Date) — Event date.event_time (DateTime) — Event time.event_time_microseconds (DateTime64) — Event time with microseconds resolution.name (String) — Metric name.value (Float64) — Metric value. Example SELECT * FROM system.asynchronous_metric_log LIMIT 10 ┌─event_date─┬──────────event_time─┬────event_time_microseconds─┬─name─────────────────────────────────────┬─────value─┐ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ CPUFrequencyMHz_0 │ 2120.9 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.arenas.all.pmuzzy │ 743 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.arenas.all.pdirty │ 26288 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.background_thread.run_intervals │ 0 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.background_thread.num_runs │ 0 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.retained │ 60694528 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.mapped │ 303161344 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.resident │ 260931584 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.metadata │ 12079488 │ │ 2020-09-05 │ 2020-09-05 15:56:30 │ 2020-09-05 15:56:30.025227 │ jemalloc.allocated │ 133756128 │ └────────────┴─────────────────────┴────────────────────────────┴──────────────────────────────────────────┴───────────┘ See Also system.asynchronous_metrics — Contains metrics, calculated periodically in the background.system.metric_log — Contains history of metrics values from tables system.metrics and system.events, periodically flushed to disk. Original article","keywords":""},{"title":"asynchronous_metrics","type":0,"sectionRef":"#","url":"en/operations/system-tables/asynchronous_metrics","content":"asynchronous_metrics Contains metrics that are calculated periodically in the background. For example, the amount of RAM in use. Columns: metric (String) — Metric name.value (Float64) — Metric value. Example SELECT * FROM system.asynchronous_metrics LIMIT 10 ┌─metric──────────────────────────────────┬──────value─┐ │ jemalloc.background_thread.run_interval │ 0 │ │ jemalloc.background_thread.num_runs │ 0 │ │ jemalloc.background_thread.num_threads │ 0 │ │ jemalloc.retained │ 422551552 │ │ jemalloc.mapped │ 1682989056 │ │ jemalloc.resident │ 1656446976 │ │ jemalloc.metadata_thp │ 0 │ │ jemalloc.metadata │ 10226856 │ │ UncompressedCacheCells │ 0 │ │ MarkCacheFiles │ 0 │ └─────────────────────────────────────────┴────────────┘ See Also Monitoring — Base concepts of ClickHouse monitoring. system.metrics — Contains instantly calculated metrics. system.events — Contains a number of events that have occurred. system.metric_log — Contains a history of metrics values from tables system.metrics and system.events. Original article","keywords":""},{"title":"Server Settings","type":0,"sectionRef":"#","url":"en/operations/server-configuration-parameters/settings","content":"","keywords":""},{"title":"builtin_dictionaries_reload_interval​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#builtin-dictionaries-reload-interval","content":"The interval in seconds before reloading built-in dictionaries. ClickHouse reloads built-in dictionaries every x seconds. This makes it possible to edit dictionaries “on the fly” without restarting the server. Default value: 3600. Example &lt;builtin_dictionaries_reload_interval&gt;3600&lt;/builtin_dictionaries_reload_interval&gt;  "},{"title":"compression​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server-settings-compression","content":"Data compression settings for MergeTree-engine tables. warning Don’t use it if you have just started using ClickHouse. Configuration template: &lt;compression&gt; &lt;case&gt; &lt;min_part_size&gt;...&lt;/min_part_size&gt; &lt;min_part_size_ratio&gt;...&lt;/min_part_size_ratio&gt; &lt;method&gt;...&lt;/method&gt; &lt;level&gt;...&lt;/level&gt; &lt;/case&gt; ... &lt;/compression&gt;  &lt;case&gt; fields: min_part_size – The minimum size of a data part.min_part_size_ratio – The ratio of the data part size to the table size.method – Compression method. Acceptable values: lz4, lz4hc, zstd.level – Compression level. See Codecs. You can configure multiple &lt;case&gt; sections. Actions when conditions are met: If a data part matches a condition set, ClickHouse uses the specified compression method.If a data part matches multiple condition sets, ClickHouse uses the first matched condition set. If no conditions met for a data part, ClickHouse uses the lz4 compression. Example &lt;compression incl=&quot;clickhouse_compression&quot;&gt; &lt;case&gt; &lt;min_part_size&gt;10000000000&lt;/min_part_size&gt; &lt;min_part_size_ratio&gt;0.01&lt;/min_part_size_ratio&gt; &lt;method&gt;zstd&lt;/method&gt; &lt;level&gt;1&lt;/level&gt; &lt;/case&gt; &lt;/compression&gt;  "},{"title":"encryption​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server-settings-encryption","content":"Configures a command to obtain a key to be used by encryption codecs. Key (or keys) should be written in environment variables or set in the configuration file. Keys can be hex or string with a length equal to 16 bytes. Example Loading from config: &lt;encryption_codecs&gt; &lt;aes_128_gcm_siv&gt; &lt;key&gt;1234567812345678&lt;/key&gt; &lt;/aes_128_gcm_siv&gt; &lt;/encryption_codecs&gt;  note Storing keys in the configuration file is not recommended. It isn't secure. You can move the keys into a separate config file on a secure disk and put a symlink to that config file to config.d/ folder. Loading from config, when the key is in hex: &lt;encryption_codecs&gt; &lt;aes_128_gcm_siv&gt; &lt;key_hex&gt;00112233445566778899aabbccddeeff&lt;/key_hex&gt; &lt;/aes_128_gcm_siv&gt; &lt;/encryption_codecs&gt;  Loading key from the environment variable: &lt;encryption_codecs&gt; &lt;aes_128_gcm_siv&gt; &lt;key_hex from_env=&quot;ENVVAR&quot;&gt;&lt;/key_hex&gt; &lt;/aes_128_gcm_siv&gt; &lt;/encryption_codecs&gt;  Here current_key_id sets the current key for encryption, and all specified keys can be used for decryption. Each of these methods can be applied for multiple keys: &lt;encryption_codecs&gt; &lt;aes_128_gcm_siv&gt; &lt;key_hex id=&quot;0&quot;&gt;00112233445566778899aabbccddeeff&lt;/key_hex&gt; &lt;key_hex id=&quot;1&quot; from_env=&quot;ENVVAR&quot;&gt;&lt;/key_hex&gt; &lt;current_key_id&gt;1&lt;/current_key_id&gt; &lt;/aes_128_gcm_siv&gt; &lt;/encryption_codecs&gt;  Here current_key_id shows current key for encryption. Also, users can add nonce that must be 12 bytes long (by default encryption and decryption processes use nonce that consists of zero bytes): &lt;encryption_codecs&gt; &lt;aes_128_gcm_siv&gt; &lt;nonce&gt;012345678910&lt;/nonce&gt; &lt;/aes_128_gcm_siv&gt; &lt;/encryption_codecs&gt;  Or it can be set in hex: &lt;encryption_codecs&gt; &lt;aes_128_gcm_siv&gt; &lt;nonce_hex&gt;abcdefabcdef&lt;/nonce_hex&gt; &lt;/aes_128_gcm_siv&gt; &lt;/encryption_codecs&gt;  Everything mentioned above can be applied for aes_256_gcm_siv (but the key must be 32 bytes long). "},{"title":"custom_settings_prefixes​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#custom_settings_prefixes","content":"List of prefixes for custom settings. The prefixes must be separated with commas. Example &lt;custom_settings_prefixes&gt;custom_&lt;/custom_settings_prefixes&gt;  See Also Custom settings "},{"title":"core_dump​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-core_dump","content":"Configures soft limit for core dump file size. Possible values: Positive integer. Default value: 1073741824 (1 GB). note Hard limit is configured via system tools Example &lt;core_dump&gt; &lt;size_limit&gt;1073741824&lt;/size_limit&gt; &lt;/core_dump&gt;  "},{"title":"database_atomic_delay_before_drop_table_sec​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#database_atomic_delay_before_drop_table_sec","content":"Sets the delay before remove table data in seconds. If the query has SYNC modifier, this setting is ignored. Default value: 480 (8 minute). "},{"title":"default_database​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#default-database","content":"The default database. To get a list of databases, use the SHOW DATABASES query. Example &lt;default_database&gt;default&lt;/default_database&gt;  "},{"title":"default_profile​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#default-profile","content":"Default settings profile. Settings profiles are located in the file specified in the parameter user_config. Example &lt;default_profile&gt;default&lt;/default_profile&gt;  "},{"title":"default_replica_path​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#default_replica_path","content":"The path to the table in ZooKeeper. Example &lt;default_replica_path&gt;/clickhouse/tables/{uuid}/{shard}&lt;/default_replica_path&gt;  "},{"title":"default_replica_name​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#default_replica_name","content":"The replica name in ZooKeeper. Example &lt;default_replica_name&gt;{replica}&lt;/default_replica_name&gt;  "},{"title":"dictionaries_config​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-dictionaries_config","content":"The path to the config file for external dictionaries. Path: Specify the absolute path or the path relative to the server config file.The path can contain wildcards * and ?. See also “External dictionaries”. Example &lt;dictionaries_config&gt;*_dictionary.xml&lt;/dictionaries_config&gt;  "},{"title":"user_defined_executable_functions_config​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-user_defined_executable_functions_config","content":"The path to the config file for executable user defined functions. Path: Specify the absolute path or the path relative to the server config file.The path can contain wildcards * and ?. See also “Executable User Defined Functions.”. Example &lt;user_defined_executable_functions_config&gt;*_function.xml&lt;/user_defined_executable_functions_config&gt;  "},{"title":"dictionaries_lazy_load​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-dictionaries_lazy_load","content":"Lazy loading of dictionaries. If true, then each dictionary is created on first use. If dictionary creation failed, the function that was using the dictionary throws an exception. If false, all dictionaries are created when the server starts, if the dictionary or dictionaries are created too long or are created with errors, then the server boots without of these dictionaries and continues to try to create these dictionaries. The default is true. Example &lt;dictionaries_lazy_load&gt;true&lt;/dictionaries_lazy_load&gt;  "},{"title":"format_schema_path​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-format_schema_path","content":"The path to the directory with the schemes for the input data, such as schemas for the CapnProto format. Example  &lt;!-- Directory containing schema files for various input formats. --&gt; &lt;format_schema_path&gt;format_schemas/&lt;/format_schema_path&gt;  "},{"title":"graphite​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-graphite","content":"Sending data to Graphite. Settings: host – The Graphite server.port – The port on the Graphite server.interval – The interval for sending, in seconds.timeout – The timeout for sending data, in seconds.root_path – Prefix for keys.metrics – Sending data from the system.metrics table.events – Sending deltas data accumulated for the time period from the system.events table.events_cumulative – Sending cumulative data from the system.events table.asynchronous_metrics – Sending data from the system.asynchronous_metrics table. You can configure multiple &lt;graphite&gt; clauses. For instance, you can use this for sending different data at different intervals. Example &lt;graphite&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;42000&lt;/port&gt; &lt;timeout&gt;0.1&lt;/timeout&gt; &lt;interval&gt;60&lt;/interval&gt; &lt;root_path&gt;one_min&lt;/root_path&gt; &lt;metrics&gt;true&lt;/metrics&gt; &lt;events&gt;true&lt;/events&gt; &lt;events_cumulative&gt;false&lt;/events_cumulative&gt; &lt;asynchronous_metrics&gt;true&lt;/asynchronous_metrics&gt; &lt;/graphite&gt;  "},{"title":"graphite_rollup​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-graphite-rollup","content":"Settings for thinning data for Graphite. For more details, see GraphiteMergeTree. Example &lt;graphite_rollup_example&gt; &lt;default&gt; &lt;function&gt;max&lt;/function&gt; &lt;retention&gt; &lt;age&gt;0&lt;/age&gt; &lt;precision&gt;60&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;3600&lt;/age&gt; &lt;precision&gt;300&lt;/precision&gt; &lt;/retention&gt; &lt;retention&gt; &lt;age&gt;86400&lt;/age&gt; &lt;precision&gt;3600&lt;/precision&gt; &lt;/retention&gt; &lt;/default&gt; &lt;/graphite_rollup_example&gt;  "},{"title":"http_port/https_port​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#http-porthttps-port","content":"The port for connecting to the server over HTTP(s). If https_port is specified, openSSL must be configured. If http_port is specified, the OpenSSL configuration is ignored even if it is set. Example &lt;https_port&gt;9999&lt;/https_port&gt;  "},{"title":"http_server_default_response​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-http_server_default_response","content":"The page that is shown by default when you access the ClickHouse HTTP(s) server. The default value is “Ok.” (with a line feed at the end) Example Opens https://tabix.io/ when accessing http://localhost: http_port. &lt;http_server_default_response&gt; &lt;![CDATA[&lt;html ng-app=&quot;SMI2&quot;&gt;&lt;head&gt;&lt;base href=&quot;http://ui.tabix.io/&quot;&gt;&lt;/head&gt;&lt;body&gt;&lt;div ui-view=&quot;&quot; class=&quot;content-ui&quot;&gt;&lt;/div&gt;&lt;script src=&quot;http://loader.tabix.io/master.js&quot;&gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;]]&gt; &lt;/http_server_default_response&gt;  "},{"title":"hsts_max_age​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#hsts-max-age","content":"Expired time for HSTS in seconds. The default value is 0 means clickhouse disabled HSTS. If you set a positive number, the HSTS will be enabled and the max-age is the number you set. Example &lt;hsts_max_age&gt;600000&lt;/hsts_max_age&gt;  "},{"title":"include_from​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-include_from","content":"The path to the file with substitutions. For more information, see the section “Configuration files”. Example &lt;include_from&gt;/etc/metrica.xml&lt;/include_from&gt;  "},{"title":"interserver_http_port​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#interserver-http-port","content":"Port for exchanging data between ClickHouse servers. Example &lt;interserver_http_port&gt;9009&lt;/interserver_http_port&gt;  "},{"title":"interserver_http_host​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#interserver-http-host","content":"The hostname that can be used by other servers to access this server. If omitted, it is defined in the same way as the hostname-f command. Useful for breaking away from a specific network interface. Example &lt;interserver_http_host&gt;example.clickhouse.com&lt;/interserver_http_host&gt;  "},{"title":"interserver_https_port​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#interserver-https-port","content":"Port for exchanging data between ClickHouse servers over HTTPS. Example &lt;interserver_https_port&gt;9010&lt;/interserver_https_port&gt;  "},{"title":"interserver_https_host​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#interserver-https-host","content":"Similar to interserver_http_host, except that this hostname can be used by other servers to access this server over HTTPS. Example &lt;interserver_https_host&gt;example.clickhouse.com&lt;/interserver_https_host&gt;  "},{"title":"interserver_http_credentials​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server-settings-interserver-http-credentials","content":"A username and a password used to connect to other servers during replication. Also the server authenticates other replicas using these credentials. So, interserver_http_credentials must be the same for all replicas in a cluster. By default, if interserver_http_credentials section is omitted, authentication is not used during replication. note interserver_http_credentials settings do not relate to a ClickHouse client credentials configuration. note These credentials are common for replication via HTTP and HTTPS. The section contains the following parameters: user — Username.password — Password.allow_empty — If true, then other replicas are allowed to connect without authentication even if credentials are set. If false, then connections without authentication are refused. Default value: false.old — Contains old user and password used during credential rotation. Several old sections can be specified. Credentials Rotation ClickHouse supports dynamic interserver credentials rotation without stopping all replicas at the same time to update their configuration. Credentials can be changed in several steps. To enable authentication, set interserver_http_credentials.allow_empty to true and add credentials. This allows connections with authentication and without it. &lt;interserver_http_credentials&gt; &lt;user&gt;admin&lt;/user&gt; &lt;password&gt;111&lt;/password&gt; &lt;allow_empty&gt;true&lt;/allow_empty&gt; &lt;/interserver_http_credentials&gt;  After configuring all replicas set allow_empty to false or remove this setting. It makes authentication with new credentials mandatory. To change existing credentials, move the username and the password to interserver_http_credentials.old section and update user and password with new values. At this point the server uses new credentials to connect to other replicas and accepts connections with either new or old credentials. &lt;interserver_http_credentials&gt; &lt;user&gt;admin&lt;/user&gt; &lt;password&gt;222&lt;/password&gt; &lt;old&gt; &lt;user&gt;admin&lt;/user&gt; &lt;password&gt;111&lt;/password&gt; &lt;/old&gt; &lt;old&gt; &lt;user&gt;temp&lt;/user&gt; &lt;password&gt;000&lt;/password&gt; &lt;/old&gt; &lt;/interserver_http_credentials&gt;  When new credentials are applied to all replicas, old credentials may be removed. "},{"title":"keep_alive_timeout​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#keep-alive-timeout","content":"The number of seconds that ClickHouse waits for incoming requests before closing the connection. Defaults to 10 seconds. Example &lt;keep_alive_timeout&gt;10&lt;/keep_alive_timeout&gt;  "},{"title":"listen_host​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-listen_host","content":"Restriction on hosts that requests can come from. If you want the server to answer all of them, specify ::. Examples: &lt;listen_host&gt;::1&lt;/listen_host&gt; &lt;listen_host&gt;127.0.0.1&lt;/listen_host&gt;  "},{"title":"listen_backlog​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-listen_backlog","content":"Backlog (queue size of pending connections) of the listen socket. Default value: 4096 (as in linux 5.4+). Usually this value does not need to be changed, since: default value is large enough,and for accepting client's connections server has separate thread. So even if you have TcpExtListenOverflows (from nstat) non zero and this counter grows for ClickHouse server it does not mean that this value need to be increased, since: usually if 4096 is not enough it shows some internal ClickHouse scaling issue, so it is better to report an issue.and it does not mean that the server can handle more connections later (and even if it could, by that moment clients may be gone or disconnected). Examples: &lt;listen_backlog&gt;4096&lt;/listen_backlog&gt;  "},{"title":"logger​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-logger","content":"Logging settings. Keys: level – Logging level. Acceptable values: trace, debug, information, warning, error.log – The log file. Contains all the entries according to level.errorlog – Error log file.size – Size of the file. Applies to log and errorlog. Once the file reaches size, ClickHouse archives and renames it, and creates a new log file in its place.count – The number of archived log files that ClickHouse stores.console – Send log and errorlog to the console instead of file. To enable, set to 1 or true. Example &lt;logger&gt; &lt;level&gt;trace&lt;/level&gt; &lt;log&gt;/var/log/clickhouse-server/clickhouse-server.log&lt;/log&gt; &lt;errorlog&gt;/var/log/clickhouse-server/clickhouse-server.err.log&lt;/errorlog&gt; &lt;size&gt;1000M&lt;/size&gt; &lt;count&gt;10&lt;/count&gt; &lt;/logger&gt;  Writing to the console can be configured. Config example: &lt;logger&gt; &lt;level&gt;information&lt;/level&gt; &lt;console&gt;1&lt;/console&gt; &lt;/logger&gt;  Writing to the syslog is also supported. Config example: &lt;logger&gt; &lt;use_syslog&gt;1&lt;/use_syslog&gt; &lt;syslog&gt; &lt;address&gt;syslog.remote:10514&lt;/address&gt; &lt;hostname&gt;myhost.local&lt;/hostname&gt; &lt;facility&gt;LOG_LOCAL6&lt;/facility&gt; &lt;format&gt;syslog&lt;/format&gt; &lt;/syslog&gt; &lt;/logger&gt;  Keys for syslog: use_syslog — Required setting if you want to write to the syslog.address — The host[:port] of syslogd. If omitted, the local daemon is used.hostname — Optional. The name of the host that logs are sent from.facility — The syslog facility keyword in uppercase letters with the “LOG_” prefix: (LOG_USER, LOG_DAEMON, LOG_LOCAL3, and so on). Default value: LOG_USER if address is specified, LOG_DAEMON otherwise.format – Message format. Possible values: bsd and syslog. "},{"title":"send_crash_reports​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-send_crash_reports","content":"Settings for opt-in sending crash reports to the ClickHouse core developers team via Sentry. Enabling it, especially in pre-production environments, is highly appreciated. The server will need access to the public Internet via IPv4 (at the time of writing IPv6 is not supported by Sentry) for this feature to be functioning properly. Keys: enabled – Boolean flag to enable the feature, false by default. Set to true to allow sending crash reports.endpoint – You can override the Sentry endpoint URL for sending crash reports. It can be either a separate Sentry account or your self-hosted Sentry instance. Use the Sentry DSN syntax.anonymize - Avoid attaching the server hostname to the crash report.http_proxy - Configure HTTP proxy for sending crash reports.debug - Sets the Sentry client into debug mode.tmp_path - Filesystem path for temporary crash report state. Recommended way to use &lt;send_crash_reports&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/send_crash_reports&gt;  "},{"title":"macros​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#macros","content":"Parameter substitutions for replicated tables. Can be omitted if replicated tables are not used. For more information, see the section Creating replicated tables. Example &lt;macros incl=&quot;macros&quot; optional=&quot;true&quot; /&gt;  "},{"title":"mark_cache_size​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server-mark-cache-size","content":"Approximate size (in bytes) of the cache of marks used by table engines of the MergeTree family. The cache is shared for the server and memory is allocated as needed. Example &lt;mark_cache_size&gt;5368709120&lt;/mark_cache_size&gt;  "},{"title":"max_server_memory_usage​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#max_server_memory_usage","content":"Limits total RAM usage by the ClickHouse server. Possible values: Positive integer.0 — Auto. Default value: 0. Additional Info The default max_server_memory_usage value is calculated as memory_amount * max_server_memory_usage_to_ram_ratio. See also max_memory_usagemax_server_memory_usage_to_ram_ratio "},{"title":"max_server_memory_usage_to_ram_ratio​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#max_server_memory_usage_to_ram_ratio","content":"Defines the fraction of total physical RAM amount, available to the ClickHouse server. If the server tries to utilize more, the memory is cut down to the appropriate amount. Possible values: Positive double.0 — The ClickHouse server can use all available RAM. Default value: 0.9. Usage On hosts with low RAM and swap, you possibly need setting max_server_memory_usage_to_ram_ratio larger than 1. Example &lt;max_server_memory_usage_to_ram_ratio&gt;0.9&lt;/max_server_memory_usage_to_ram_ratio&gt;  See Also max_server_memory_usage "},{"title":"max_concurrent_queries​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#max-concurrent-queries","content":"The maximum number of simultaneously processed queries. Note that other limits also apply: max_concurrent_insert_queries, max_concurrent_select_queries, max_concurrent_queries_for_user, max_concurrent_queries_for_all_users. note These settings can be modified at runtime and will take effect immediately. Queries that are already running will remain unchanged. Possible values: Positive integer.0 — No limit. Default value: 100. Example &lt;max_concurrent_queries&gt;200&lt;/max_concurrent_queries&gt;  "},{"title":"max_concurrent_insert_queries​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#max-concurrent-insert-queries","content":"The maximum number of simultaneously processed INSERT queries. note These settings can be modified at runtime and will take effect immediately. Queries that are already running will remain unchanged. Possible values: Positive integer.0 — No limit. Default value: 0. Example &lt;max_concurrent_insert_queries&gt;100&lt;/max_concurrent_insert_queries&gt;  "},{"title":"max_concurrent_select_queries​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#max-concurrent-select-queries","content":"The maximum number of simultaneously processed SELECT queries. note These settings can be modified at runtime and will take effect immediately. Queries that are already running will remain unchanged. Possible values: Positive integer.0 — No limit. Default value: 0. Example &lt;max_concurrent_select_queries&gt;100&lt;/max_concurrent_select_queries&gt;  "},{"title":"max_concurrent_queries_for_user​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#max-concurrent-queries-for-user","content":"The maximum number of simultaneously processed queries related to MergeTree table per user. Possible values: Positive integer.0 — No limit. Default value: 0. Example &lt;max_concurrent_queries_for_user&gt;5&lt;/max_concurrent_queries_for_user&gt;  "},{"title":"max_concurrent_queries_for_all_users​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#max-concurrent-queries-for-all-users","content":"Throw exception if the value of this setting is less or equal than the current number of simultaneously processed queries. Example: max_concurrent_queries_for_all_users can be set to 99 for all users and database administrator can set it to 100 for itself to run queries for investigation even when the server is overloaded. Modifying the setting for one query or user does not affect other queries. Possible values: Positive integer.0 — No limit. Default value: 0. Example &lt;max_concurrent_queries_for_all_users&gt;99&lt;/max_concurrent_queries_for_all_users&gt;  See Also max_concurrent_queries "},{"title":"max_connections​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#max-connections","content":"The maximum number of inbound connections. Example &lt;max_connections&gt;4096&lt;/max_connections&gt;  "},{"title":"max_open_files​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#max-open-files","content":"The maximum number of open files. By default: maximum. We recommend using this option in Mac OS X since the getrlimit() function returns an incorrect value. Example &lt;max_open_files&gt;262144&lt;/max_open_files&gt;  "},{"title":"max_table_size_to_drop​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#max-table-size-to-drop","content":"Restriction on deleting tables. If the size of a MergeTree table exceeds max_table_size_to_drop (in bytes), you can’t delete it using a DROP query. If you still need to delete the table without restarting the ClickHouse server, create the &lt;clickhouse-path&gt;/flags/force_drop_table file and run the DROP query. Default value: 50 GB. The value 0 means that you can delete all tables without any restrictions. Example &lt;max_table_size_to_drop&gt;0&lt;/max_table_size_to_drop&gt;  "},{"title":"max_thread_pool_size​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#max-thread-pool-size","content":"ClickHouse uses threads from the Global Thread pool to process queries. If there is no idle thread to process a query, then a new thread is created in the pool. max_thread_pool_size limits the maximum number of threads in the pool. Possible values: Positive integer. Default value: 10000. Example &lt;max_thread_pool_size&gt;12000&lt;/max_thread_pool_size&gt;  "},{"title":"max_thread_pool_free_size​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#max-thread-pool-free-size","content":"If the number of idle threads in the Global Thread pool is greater than max_thread_pool_free_size, then ClickHouse releases resources occupied by some threads and the pool size is decreased. Threads can be created again if necessary. Possible values: Positive integer. Default value: 1000. Example &lt;max_thread_pool_free_size&gt;1200&lt;/max_thread_pool_free_size&gt;  "},{"title":"thread_pool_queue_size​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#thread-pool-queue-size","content":"The maximum number of jobs that can be scheduled on the Global Thread pool. Increasing queue size leads to larger memory usage. It is recommended to keep this value equal to max_thread_pool_size. Possible values: Positive integer. Default value: 10000. Example &lt;thread_pool_queue_size&gt;12000&lt;/thread_pool_queue_size&gt;  "},{"title":"background_pool_size​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#background_pool_size","content":"Sets the number of threads performing background merges and mutations for tables with MergeTree engines. This setting is also could be applied at server startup from the default profile configuration for backward compatibility at the ClickHouse server start. You can only increase the number of threads at runtime. To lower the number of threads you have to restart the server. By adjusting this setting, you manage CPU and disk load. Smaller pool size utilizes less CPU and disk resources, but background processes advance slower which might eventually impact query performance. Before changing it, please also take a look at related MergeTree settings, such as number_of_free_entries_in_pool_to_lower_max_size_of_merge and number_of_free_entries_in_pool_to_execute_mutation. Possible values: Any positive integer. Default value: 16. Example &lt;background_pool_size&gt;16&lt;/background_pool_size&gt;  "},{"title":"background_merges_mutations_concurrency_ratio​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#background_merges_mutations_concurrency_ratio","content":"Sets a ratio between the number of threads and the number of background merges and mutations that can be executed concurrently. For example if the ratio equals to 2 andbackground_pool_size is set to 16 then ClickHouse can execute 32 background merges concurrently. This is possible, because background operation could be suspended and postponed. This is needed to give small merges more execution priority. You can only increase this ratio at runtime. To lower it you have to restart the server. The same as for background_pool_size setting background_merges_mutations_concurrency_ratio could be applied from the default profile for backward compatibility. Possible values: Any positive integer. Default value: 2. Example &lt;background_merges_mutations_concurrency_ratio&gt;3&lt;/background_pbackground_merges_mutations_concurrency_ratio&gt;  "},{"title":"background_move_pool_size​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#background_move_pool_size","content":"Sets the number of threads performing background moves for tables with MergeTree engines. Could be increased at runtime and could be applied at server startup from the default profile for backward compatibility. Possible values: Any positive integer. Default value: 8. Example &lt;background_move_pool_size&gt;36&lt;/background_move_pool_size&gt;  "},{"title":"background_fetches_pool_size​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#background_fetches_pool_size","content":"Sets the number of threads performing background fetches for tables with ReplicatedMergeTree engines. Could be increased at runtime and could be applied at server startup from the default profile for backward compatibility. Possible values: Any positive integer. Default value: 8. Example &lt;background_fetches_pool_size&gt;36&lt;/background_fetches_pool_size&gt;  "},{"title":"background_common_pool_size​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#background_common_pool_size","content":"Sets the number of threads performing background non-specialized operations like cleaning the filesystem etc. for tables with MergeTree engines. Could be increased at runtime and could be applied at server startup from the default profile for backward compatibility. Possible values: Any positive integer. Default value: 8. Example &lt;background_common_pool_size&gt;36&lt;/background_common_pool_size&gt;  "},{"title":"merge_tree​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-merge_tree","content":"Fine tuning for tables in the MergeTree. For more information, see the MergeTreeSettings.h header file. Example &lt;merge_tree&gt; &lt;max_suspicious_broken_parts&gt;5&lt;/max_suspicious_broken_parts&gt; &lt;/merge_tree&gt;  "},{"title":"metric_log​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#metric_log","content":"It is enabled by default. If it`s not, you can do this manually. Enabling To manually turn on metrics history collection system.metric_log, create /etc/clickhouse-server/config.d/metric_log.xml with the following content: &lt;clickhouse&gt; &lt;metric_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;metric_log&lt;/table&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;collect_interval_milliseconds&gt;1000&lt;/collect_interval_milliseconds&gt; &lt;/metric_log&gt; &lt;/clickhouse&gt;  Disabling To disable metric_log setting, you should create the following file /etc/clickhouse-server/config.d/disable_metric_log.xml with the following content: &lt;clickhouse&gt; &lt;metric_log remove=&quot;1&quot; /&gt; &lt;/clickhouse&gt;  "},{"title":"replicated_merge_tree​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-replicated_merge_tree","content":"Fine tuning for tables in the ReplicatedMergeTree. This setting has a higher priority. For more information, see the MergeTreeSettings.h header file. Example &lt;replicated_merge_tree&gt; &lt;max_suspicious_broken_parts&gt;5&lt;/max_suspicious_broken_parts&gt; &lt;/replicated_merge_tree&gt;  "},{"title":"openSSL​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-openssl","content":"SSL client/server configuration. Support for SSL is provided by the libpoco library. The available configuration options are explained in SSLManager.h. Default values can be found in SSLManager.cpp. Keys for server/client settings: privateKeyFile – The path to the file with the secret key of the PEM certificate. The file may contain a key and certificate at the same time.certificateFile – The path to the client/server certificate file in PEM format. You can omit it if privateKeyFile contains the certificate.caConfig (default: none) – The path to the file or directory that contains trusted CA certificates. If this points to a file, it must be in PEM format and can contain several CA certificates. If this points to a directory, it must contain one .pem file per CA certificate. The filenames are looked up by the CA subject name hash value. Details can be found in the man page of SSL_CTX_load_verify_locations.verificationMode (default: relaxed) – The method for checking the node’s certificates. Details are in the description of the Context class. Possible values: none, relaxed, strict, once.verificationDepth (default: 9) – The maximum length of the verification chain. Verification will fail if the certificate chain length exceeds the set value.loadDefaultCAFile (default: true) – Wether built-in CA certificates for OpenSSL will be used. ClickHouse assumes that builtin CA certificates are in the file /etc/ssl/cert.pem (resp. the directory /etc/ssl/certs) or in file (resp. directory) specified by the environment variable SSL_CERT_FILE (resp. SSL_CERT_DIR).cipherList (default: ALL:!ADH:!LOW:!EXP:!MD5:@STRENGTH) - Supported OpenSSL encryptions.cacheSessions (default: false) – Enables or disables caching sessions. Must be used in combination with sessionIdContext. Acceptable values: true, false.sessionIdContext (default: ${application.name}) – A unique set of random characters that the server appends to each generated identifier. The length of the string must not exceed SSL_MAX_SSL_SESSION_ID_LENGTH. This parameter is always recommended since it helps avoid problems both if the server caches the session and if the client requested caching. Default value: ${application.name}.sessionCacheSize (default: 1024*20) – The maximum number of sessions that the server caches. A value of 0 means unlimited sessions.sessionTimeout (default: 2h) – Time for caching the session on the server.extendedVerification (default: false) – If enabled, verify that the certificate CN or SAN matches the peer hostname.requireTLSv1 (default: false) – Require a TLSv1 connection. Acceptable values: true, false.requireTLSv1_1 (default: false) – Require a TLSv1.1 connection. Acceptable values: true, false.requireTLSv1_2 (default: false) – Require a TLSv1.2 connection. Acceptable values: true, false.fips (default: false) – Activates OpenSSL FIPS mode. Supported if the library’s OpenSSL version supports FIPS.privateKeyPassphraseHandler (default: KeyConsoleHandler)– Class (PrivateKeyPassphraseHandler subclass) that requests the passphrase for accessing the private key. For example: &lt;privateKeyPassphraseHandler&gt;, &lt;name&gt;KeyFileHandler&lt;/name&gt;, &lt;options&gt;&lt;password&gt;test&lt;/password&gt;&lt;/options&gt;, &lt;/privateKeyPassphraseHandler&gt;.invalidCertificateHandler (default: ConsoleCertificateHandler) – Class (a subclass of CertificateHandler) for verifying invalid certificates. For example: &lt;invalidCertificateHandler&gt; &lt;name&gt;ConsoleCertificateHandler&lt;/name&gt; &lt;/invalidCertificateHandler&gt; .disableProtocols (default: &quot;&quot;) – Protocols that are not allowed to use.preferServerCiphers (default: false) – Preferred server ciphers on the client. Example of settings: &lt;openSSL&gt; &lt;server&gt; &lt;!-- openssl req -subj &quot;/CN=localhost&quot; -new -newkey rsa:2048 -days 365 -nodes -x509 -keyout /etc/clickhouse-server/server.key -out /etc/clickhouse-server/server.crt --&gt; &lt;certificateFile&gt;/etc/clickhouse-server/server.crt&lt;/certificateFile&gt; &lt;privateKeyFile&gt;/etc/clickhouse-server/server.key&lt;/privateKeyFile&gt; &lt;!-- openssl dhparam -out /etc/clickhouse-server/dhparam.pem 4096 --&gt; &lt;dhParamsFile&gt;/etc/clickhouse-server/dhparam.pem&lt;/dhParamsFile&gt; &lt;verificationMode&gt;none&lt;/verificationMode&gt; &lt;loadDefaultCAFile&gt;true&lt;/loadDefaultCAFile&gt; &lt;cacheSessions&gt;true&lt;/cacheSessions&gt; &lt;disableProtocols&gt;sslv2,sslv3&lt;/disableProtocols&gt; &lt;preferServerCiphers&gt;true&lt;/preferServerCiphers&gt; &lt;/server&gt; &lt;client&gt; &lt;loadDefaultCAFile&gt;true&lt;/loadDefaultCAFile&gt; &lt;cacheSessions&gt;true&lt;/cacheSessions&gt; &lt;disableProtocols&gt;sslv2,sslv3&lt;/disableProtocols&gt; &lt;preferServerCiphers&gt;true&lt;/preferServerCiphers&gt; &lt;!-- Use for self-signed: &lt;verificationMode&gt;none&lt;/verificationMode&gt; --&gt; &lt;invalidCertificateHandler&gt; &lt;!-- Use for self-signed: &lt;name&gt;AcceptCertificateHandler&lt;/name&gt; --&gt; &lt;name&gt;RejectCertificateHandler&lt;/name&gt; &lt;/invalidCertificateHandler&gt; &lt;/client&gt; &lt;/openSSL&gt;  "},{"title":"part_log​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-part-log","content":"Logging events that are associated with MergeTree. For instance, adding or merging data. You can use the log to simulate merge algorithms and compare their characteristics. You can visualize the merge process. Queries are logged in the system.part_log table, not in a separate file. You can configure the name of this table in the table parameter (see below). Use the following parameters to configure logging: database – Name of the database.table – Name of the system table.partition_by — Custom partitioning key for a system table. Can't be used if engine defined.engine - MergeTree Engine Definition for a system table. Can't be used if partition_by defined.flush_interval_milliseconds – Interval for flushing data from the buffer in memory to the table. Example &lt;part_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;part_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/part_log&gt;  "},{"title":"path​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-path","content":"The path to the directory containing data. note The trailing slash is mandatory. Example &lt;path&gt;/var/lib/clickhouse/&lt;/path&gt;  "},{"title":"prometheus​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-prometheus","content":"Exposing metrics data for scraping from Prometheus. Settings: endpoint – HTTP endpoint for scraping metrics by prometheus server. Start from ‘/’.port – Port for endpoint.metrics – Flag that sets to expose metrics from the system.metrics table.events – Flag that sets to expose metrics from the system.events table.asynchronous_metrics – Flag that sets to expose current metrics values from the system.asynchronous_metrics table. Example  &lt;prometheus&gt; &lt;endpoint&gt;/metrics&lt;/endpoint&gt; &lt;port&gt;8001&lt;/port&gt; &lt;metrics&gt;true&lt;/metrics&gt; &lt;events&gt;true&lt;/events&gt; &lt;asynchronous_metrics&gt;true&lt;/asynchronous_metrics&gt; &lt;/prometheus&gt;  "},{"title":"query_log​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-query-log","content":"Setting for logging queries received with the log_queries=1 setting. Queries are logged in the system.query_log table, not in a separate file. You can change the name of the table in the table parameter (see below). Use the following parameters to configure logging: database – Name of the database.table – Name of the system table the queries will be logged in.partition_by — Custom partitioning key for a system table. Can't be used if engine defined.engine - MergeTree Engine Definition for a system table. Can't be used if partition_by defined.flush_interval_milliseconds – Interval for flushing data from the buffer in memory to the table. If the table does not exist, ClickHouse will create it. If the structure of the query log changed when the ClickHouse server was updated, the table with the old structure is renamed, and a new table is created automatically. Example &lt;query_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_log&lt;/table&gt; &lt;engine&gt;Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + INTERVAL 30 day&lt;/engine&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_log&gt;  "},{"title":"query_thread_log​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-query_thread_log","content":"Setting for logging threads of queries received with the log_query_threads=1 setting. Queries are logged in the system.query_thread_log table, not in a separate file. You can change the name of the table in the table parameter (see below). Use the following parameters to configure logging: database – Name of the database.table – Name of the system table the queries will be logged in.partition_by — Custom partitioning key for a system table. Can't be used if engine defined.engine - MergeTree Engine Definition for a system table. Can't be used if partition_by defined.flush_interval_milliseconds – Interval for flushing data from the buffer in memory to the table. If the table does not exist, ClickHouse will create it. If the structure of the query thread log changed when the ClickHouse server was updated, the table with the old structure is renamed, and a new table is created automatically. Example &lt;query_thread_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_thread_log&lt;/table&gt; &lt;partition_by&gt;toMonday(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_thread_log&gt;  "},{"title":"query_views_log​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-query_views_log","content":"Setting for logging views (live, materialized etc) dependant of queries received with the log_query_views=1 setting. Queries are logged in the system.query_views_log table, not in a separate file. You can change the name of the table in the table parameter (see below). Use the following parameters to configure logging: database – Name of the database.table – Name of the system table the queries will be logged in.partition_by — Custom partitioning key for a system table. Can't be used if engine defined.engine - MergeTree Engine Definition for a system table. Can't be used if partition_by defined.flush_interval_milliseconds – Interval for flushing data from the buffer in memory to the table. If the table does not exist, ClickHouse will create it. If the structure of the query views log changed when the ClickHouse server was updated, the table with the old structure is renamed, and a new table is created automatically. Example &lt;query_views_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;query_views_log&lt;/table&gt; &lt;partition_by&gt;toYYYYMM(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/query_views_log&gt;  "},{"title":"text_log​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-text_log","content":"Settings for the text_log system table for logging text messages. Parameters: level — Maximum Message Level (by default Trace) which will be stored in a table.database — Database name.table — Table name.partition_by — Custom partitioning key for a system table. Can't be used if engine defined.engine - MergeTree Engine Definition for a system table. Can't be used if partition_by defined.flush_interval_milliseconds — Interval for flushing data from the buffer in memory to the table. Example &lt;clickhouse&gt; &lt;text_log&gt; &lt;level&gt;notice&lt;/level&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;text_log&lt;/table&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;!-- &lt;partition_by&gt;event_date&lt;/partition_by&gt; --&gt; &lt;engine&gt;Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + INTERVAL 30 day&lt;/engine&gt; &lt;/text_log&gt; &lt;/clickhouse&gt;  "},{"title":"trace_log​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-trace_log","content":"Settings for the trace_log system table operation. Parameters: database — Database for storing a table.table — Table name.partition_by — Custom partitioning key for a system table. Can't be used if engine defined.engine - MergeTree Engine Definition for a system table. Can't be used if partition_by defined.flush_interval_milliseconds — Interval for flushing data from the buffer in memory to the table. The default server configuration file config.xml contains the following settings section: &lt;trace_log&gt; &lt;database&gt;system&lt;/database&gt; &lt;table&gt;trace_log&lt;/table&gt; &lt;partition_by&gt;toYYYYMM(event_date)&lt;/partition_by&gt; &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt; &lt;/trace_log&gt;  "},{"title":"query_masking_rules​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#query-masking-rules","content":"Regexp-based rules, which will be applied to queries as well as all log messages before storing them in server logs,system.query_log, system.text_log, system.processes tables, and in logs sent to the client. That allows preventing sensitive data leakage from SQL queries (like names, emails, personal identifiers or credit card numbers) to logs. Example &lt;query_masking_rules&gt; &lt;rule&gt; &lt;name&gt;hide SSN&lt;/name&gt; &lt;regexp&gt;(^|\\D)\\d{3}-\\d{2}-\\d{4}($|\\D)&lt;/regexp&gt; &lt;replace&gt;000-00-0000&lt;/replace&gt; &lt;/rule&gt; &lt;/query_masking_rules&gt;  Config fields: name - name for the rule (optional)regexp - RE2 compatible regular expression (mandatory)replace - substitution string for sensitive data (optional, by default - six asterisks) The masking rules are applied to the whole query (to prevent leaks of sensitive data from malformed / non-parsable queries). system.events table have counter QueryMaskingRulesMatch which have an overall number of query masking rules matches. For distributed queries each server have to be configured separately, otherwise, subqueries passed to other nodes will be stored without masking. "},{"title":"remote_servers​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server-settings-remote-servers","content":"Configuration of clusters used by the Distributed table engine and by the cluster table function. Example &lt;remote_servers incl=&quot;clickhouse_remote_servers&quot; /&gt;  For the value of the incl attribute, see the section “Configuration files”. See Also skip_unavailable_shards "},{"title":"timezone​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-timezone","content":"The server’s time zone. Specified as an IANA identifier for the UTC timezone or geographic location (for example, Africa/Abidjan). The time zone is necessary for conversions between String and DateTime formats when DateTime fields are output to text format (printed on the screen or in a file), and when getting DateTime from a string. Besides, the time zone is used in functions that work with the time and date if they didn’t receive the time zone in the input parameters. Example &lt;timezone&gt;Asia/Istanbul&lt;/timezone&gt;  "},{"title":"tcp_port​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-tcp_port","content":"Port for communicating with clients over the TCP protocol. Example &lt;tcp_port&gt;9000&lt;/tcp_port&gt;  "},{"title":"tcp_port_secure​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-tcp_port_secure","content":"TCP port for secure communication with clients. Use it with OpenSSL settings. Possible values Positive integer. Default value &lt;tcp_port_secure&gt;9440&lt;/tcp_port_secure&gt;  "},{"title":"mysql_port​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-mysql_port","content":"Port for communicating with clients over MySQL protocol. Possible values Positive integer. Example &lt;mysql_port&gt;9004&lt;/mysql_port&gt;  "},{"title":"postgresql_port​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-postgresql_port","content":"Port for communicating with clients over PostgreSQL protocol. Possible values Positive integer. Example &lt;postgresql_port&gt;9005&lt;/postgresql_port&gt;  "},{"title":"tmp_path​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#tmp-path","content":"Path to temporary data for processing large queries. note The trailing slash is mandatory. Example &lt;tmp_path&gt;/var/lib/clickhouse/tmp/&lt;/tmp_path&gt;  "},{"title":"tmp_policy​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#tmp-policy","content":"Policy from storage_configuration to store temporary files. If not set, tmp_path is used, otherwise it is ignored. note move_factor is ignored.keep_free_space_bytes is ignored.max_data_part_size_bytes is ignored.Уou must have exactly one volume in that policy. "},{"title":"uncompressed_cache_size​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server-settings-uncompressed_cache_size","content":"Cache size (in bytes) for uncompressed data used by table engines from the MergeTree. There is one shared cache for the server. Memory is allocated on demand. The cache is used if the option use_uncompressed_cache is enabled. The uncompressed cache is advantageous for very short queries in individual cases. Example &lt;uncompressed_cache_size&gt;8589934592&lt;/uncompressed_cache_size&gt;  "},{"title":"user_files_path​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-user_files_path","content":"The directory with user files. Used in the table function file(). Example &lt;user_files_path&gt;/var/lib/clickhouse/user_files/&lt;/user_files_path&gt;  "},{"title":"user_scripts_path​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-user_scripts_path","content":"The directory with user scripts files. Used for Executable user defined functions Executable User Defined Functions. Example &lt;user_scripts_path&gt;/var/lib/clickhouse/user_scripts/&lt;/user_scripts_path&gt;  "},{"title":"user_defined_path​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server_configuration_parameters-user_defined_path","content":"The directory with user defined files. Used for SQL user defined functions SQL User Defined Functions. Example &lt;user_defined_path&gt;/var/lib/clickhouse/user_defined/&lt;/user_defined_path&gt;  "},{"title":"users_config​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#users-config","content":"Path to the file that contains: User configurations.Access rights.Settings profiles.Quota settings. Example &lt;users_config&gt;users.xml&lt;/users_config&gt;  "},{"title":"zookeeper​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server-settings_zookeeper","content":"Contains settings that allow ClickHouse to interact with a ZooKeeper cluster. ClickHouse uses ZooKeeper for storing metadata of replicas when using replicated tables. If replicated tables are not used, this section of parameters can be omitted. This section contains the following parameters: node — ZooKeeper endpoint. You can set multiple endpoints. For example:  &lt;node index=&quot;1&quot;&gt; &lt;host&gt;example_host&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt;   The `index` attribute specifies the node order when trying to connect to the ZooKeeper cluster.  session_timeout_ms — Maximum timeout for the client session in milliseconds.operation_timeout_ms — Maximum timeout for one operation in milliseconds.root — The znode that is used as the root for znodes used by the ClickHouse server. Optional.identity — User and password, that can be required by ZooKeeper to give access to requested znodes. Optional. Example configuration &lt;zookeeper&gt; &lt;node&gt; &lt;host&gt;example1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;node&gt; &lt;host&gt;example2&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;session_timeout_ms&gt;30000&lt;/session_timeout_ms&gt; &lt;operation_timeout_ms&gt;10000&lt;/operation_timeout_ms&gt; &lt;!-- Optional. Chroot suffix. Should exist. --&gt; &lt;root&gt;/path/to/zookeeper/node&lt;/root&gt; &lt;!-- Optional. Zookeeper digest ACL string. --&gt; &lt;identity&gt;user:password&lt;/identity&gt; &lt;/zookeeper&gt;  See Also ReplicationZooKeeper Programmer’s GuideOptional secured communication between ClickHouse and Zookeeper "},{"title":"use_minimalistic_part_header_in_zookeeper​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server-settings-use_minimalistic_part_header_in_zookeeper","content":"Storage method for data part headers in ZooKeeper. This setting only applies to the MergeTree family. It can be specified: Globally in the merge_tree section of the config.xml file. ClickHouse uses the setting for all the tables on the server. You can change the setting at any time. Existing tables change their behaviour when the setting changes. For each table. When creating a table, specify the corresponding engine setting. The behaviour of an existing table with this setting does not change, even if the global setting changes. Possible values 0 — Functionality is turned off.1 — Functionality is turned on. If use_minimalistic_part_header_in_zookeeper = 1, then replicated tables store the headers of the data parts compactly using a single znode. If the table contains many columns, this storage method significantly reduces the volume of the data stored in Zookeeper. note After applying use_minimalistic_part_header_in_zookeeper = 1, you can’t downgrade the ClickHouse server to a version that does not support this setting. Be careful when upgrading ClickHouse on servers in a cluster. Don’t upgrade all the servers at once. It is safer to test new versions of ClickHouse in a test environment, or on just a few servers of a cluster. Data part headers already stored with this setting can't be restored to their previous (non-compact) representation. Default value: 0. "},{"title":"disable_internal_dns_cache​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server-settings-disable-internal-dns-cache","content":"Disables the internal DNS cache. Recommended for operating ClickHouse in systems with frequently changing infrastructure such as Kubernetes. Default value: 0. "},{"title":"dns_cache_update_period​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server-settings-dns-cache-update-period","content":"The period of updating IP addresses stored in the ClickHouse internal DNS cache (in seconds). The update is performed asynchronously, in a separate system thread. Default value: 15. See also background_schedule_pool_size "},{"title":"distributed_ddl​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#server-settings-distributed_ddl","content":"Manage executing distributed ddl queries (CREATE, DROP, ALTER, RENAME) on cluster. Works only if ZooKeeper is enabled. Example &lt;distributed_ddl&gt; &lt;!-- Path in ZooKeeper to queue with DDL queries --&gt; &lt;path&gt;/clickhouse/task_queue/ddl&lt;/path&gt; &lt;!-- Settings from this profile will be used to execute DDL queries --&gt; &lt;profile&gt;default&lt;/profile&gt; &lt;!-- Controls how much ON CLUSTER queries can be run simultaneously. --&gt; &lt;pool_size&gt;1&lt;/pool_size&gt; &lt;!-- Cleanup settings (active tasks will not be removed) --&gt; &lt;!-- Controls task TTL (default 1 week) --&gt; &lt;task_max_lifetime&gt;604800&lt;/task_max_lifetime&gt; &lt;!-- Controls how often cleanup should be performed (in seconds) --&gt; &lt;cleanup_delay_period&gt;60&lt;/cleanup_delay_period&gt; &lt;!-- Controls how many tasks could be in the queue --&gt; &lt;max_tasks_in_queue&gt;1000&lt;/max_tasks_in_queue&gt; &lt;/distributed_ddl&gt;  "},{"title":"access_control_path​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#access_control_path","content":"Path to a folder where a ClickHouse server stores user and role configurations created by SQL commands. Default value: /var/lib/clickhouse/access/. See also Access Control and Account Management "},{"title":"user_directories​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#user_directories","content":"Section of the configuration file that contains settings: Path to configuration file with predefined users.Path to folder where users created by SQL commands are stored.ZooKeeper node path where users created by SQL commands are stored and replicated (experimental). If this section is specified, the path from users_config and access_control_path won't be used. The user_directories section can contain any number of items, the order of the items means their precedence (the higher the item the higher the precedence). Examples &lt;user_directories&gt; &lt;users_xml&gt; &lt;path&gt;/etc/clickhouse-server/users.xml&lt;/path&gt; &lt;/users_xml&gt; &lt;local_directory&gt; &lt;path&gt;/var/lib/clickhouse/access/&lt;/path&gt; &lt;/local_directory&gt; &lt;/user_directories&gt;  Users, roles, row policies, quotas, and profiles can be also stored in ZooKeeper: &lt;user_directories&gt; &lt;users_xml&gt; &lt;path&gt;/etc/clickhouse-server/users.xml&lt;/path&gt; &lt;/users_xml&gt; &lt;replicated&gt; &lt;zookeeper_path&gt;/clickhouse/access/&lt;/zookeeper_path&gt; &lt;/replicated&gt; &lt;/user_directories&gt;  You can also define sections memory — means storing information only in memory, without writing to disk, and ldap — means storing information on an LDAP server. To add an LDAP server as a remote user directory of users that are not defined locally, define a single ldap section with a following parameters: server — one of LDAP server names defined in ldap_servers config section. This parameter is mandatory and cannot be empty.roles — section with a list of locally defined roles that will be assigned to each user retrieved from the LDAP server. If no roles are specified, user will not be able to perform any actions after authentication. If any of the listed roles is not defined locally at the time of authentication, the authentication attempt will fail as if the provided password was incorrect. Example &lt;ldap&gt; &lt;server&gt;my_ldap_server&lt;/server&gt; &lt;roles&gt; &lt;my_local_role1 /&gt; &lt;my_local_role2 /&gt; &lt;/roles&gt; &lt;/ldap&gt;  "},{"title":"total_memory_profiler_step​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#total-memory-profiler-step","content":"Sets the memory size (in bytes) for a stack trace at every peak allocation step. The data is stored in the system.trace_log system table with query_id equal to an empty string. Possible values: Positive integer. Default value: 4194304. "},{"title":"total_memory_tracker_sample_probability​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#total-memory-tracker-sample-probability","content":"Allows to collect random allocations and deallocations and writes them in the system.trace_log system table with trace_type equal to a MemorySample with the specified probability. The probability is for every allocation or deallocations, regardless of the size of the allocation. Note that sampling happens only when the amount of untracked memory exceeds the untracked memory limit (default value is 4 MiB). It can be lowered if total_memory_profiler_step is lowered. You can set total_memory_profiler_step equal to 1 for extra fine-grained sampling. Possible values: Positive integer.0 — Writing of random allocations and deallocations in the system.trace_log system table is disabled. Default value: 0. "},{"title":"mmap_cache_size​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#mmap-cache-size","content":"Sets the cache size (in bytes) for mapped files. This setting allows to avoid frequent open/mmap/munmap/close calls (which are very expensive due to consequent page faults) and to reuse mappings from several threads and queries. The setting value is the number of mapped regions (usually equal to the number of mapped files). The amount of data in mapped files can be monitored in system.metrics, system.metric_log system tables by the MMappedFiles and MMappedFileBytes metrics, in system.asynchronous_metrics, system.asynchronous_metrics_log by the MMapCacheCells metric, and also in system.events, system.processes, system.query_log, system.query_thread_log, system.query_views_log by the CreatedReadBufferMMap, CreatedReadBufferMMapFailed, MMappedFileCacheHits, MMappedFileCacheMisses events. Note that the amount of data in mapped files does not consume memory directly and is not accounted in query or server memory usage — because this memory can be discarded similar to OS page cache. The cache is dropped (the files are closed) automatically on the removal of old parts in tables of the MergeTree family, also it can be dropped manually by the SYSTEM DROP MMAP CACHE query. Possible values: Positive integer. Default value: 1000. "},{"title":"compiled_expression_cache_size​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#compiled-expression-cache-size","content":"Sets the cache size (in bytes) for compiled expressions. Possible values: Positive integer. Default value: 134217728. "},{"title":"compiled_expression_cache_elements_size​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#compiled_expression_cache_elements_size","content":"Sets the cache size (in elements) for compiled expressions. Possible values: Positive integer. Default value: 10000. "},{"title":"global_memory_usage_overcommit_max_wait_microseconds​","type":1,"pageTitle":"Server Settings","url":"en/operations/server-configuration-parameters/settings#global_memory_usage_overcommit_max_wait_microseconds","content":"Sets maximum waiting time for global overcommit tracker. Possible values: Positive integer. Default value: 200. "},{"title":"clusters","type":0,"sectionRef":"#","url":"en/operations/system-tables/clusters","content":"clusters Contains information about clusters available in the config file and the servers in them. Columns: cluster (String) — The cluster name.shard_num (UInt32) — The shard number in the cluster, starting from 1.shard_weight (UInt32) — The relative weight of the shard when writing data.replica_num (UInt32) — The replica number in the shard, starting from 1.host_name (String) — The host name, as specified in the config.host_address (String) — The host IP address obtained from DNS.port (UInt16) — The port to use for connecting to the server.is_local (UInt8) — Flag that indicates whether the host is local.user (String) — The name of the user for connecting to the server.default_database (String) — The default database name.errors_count (UInt32) — The number of times this host failed to reach replica.slowdowns_count (UInt32) — The number of slowdowns that led to changing replica when establishing a connection with hedged requests.estimated_recovery_time (UInt32) — Seconds remaining until the replica error count is zeroed and it is considered to be back to normal. Example Query: SELECT * FROM system.clusters LIMIT 2 FORMAT Vertical; Result: Row 1: ────── cluster: test_cluster_two_shards shard_num: 1 shard_weight: 1 replica_num: 1 host_name: 127.0.0.1 host_address: 127.0.0.1 port: 9000 is_local: 1 user: default default_database: errors_count: 0 slowdowns_count: 0 estimated_recovery_time: 0 Row 2: ────── cluster: test_cluster_two_shards shard_num: 2 shard_weight: 1 replica_num: 1 host_name: 127.0.0.2 host_address: 127.0.0.2 port: 9000 is_local: 0 user: default default_database: errors_count: 0 slowdowns_count: 0 estimated_recovery_time: 0 See Also Table engine Distributeddistributed_replica_error_cap settingdistributed_replica_error_half_life setting Original article","keywords":""},{"title":"columns","type":0,"sectionRef":"#","url":"en/operations/system-tables/columns","content":"columns Contains information about columns in all the tables. You can use this table to get information similar to the DESCRIBE TABLE query, but for multiple tables at once. Columns from temporary tables are visible in the system.columns only in those session where they have been created. They are shown with the empty database field. The system.columns table contains the following columns (the column type is shown in brackets): database (String) — Database name.table (String) — Table name.name (String) — Column name.type (String) — Column type.position (UInt64) — Ordinal position of a column in a table starting with 1.default_kind (String) — Expression type (DEFAULT, MATERIALIZED, ALIAS) for the default value, or an empty string if it is not defined.default_expression (String) — Expression for the default value, or an empty string if it is not defined.data_compressed_bytes (UInt64) — The size of compressed data, in bytes.data_uncompressed_bytes (UInt64) — The size of decompressed data, in bytes.marks_bytes (UInt64) — The size of marks, in bytes.comment (String) — Comment on the column, or an empty string if it is not defined.is_in_partition_key (UInt8) — Flag that indicates whether the column is in the partition expression.is_in_sorting_key (UInt8) — Flag that indicates whether the column is in the sorting key expression.is_in_primary_key (UInt8) — Flag that indicates whether the column is in the primary key expression.is_in_sampling_key (UInt8) — Flag that indicates whether the column is in the sampling key expression.compression_codec (String) — Compression codec name.character_octet_length (Nullable(UInt64)) — Maximum length in bytes for binary data, character data, or text data and images. In ClickHouse makes sense only for FixedString data type. Otherwise, the NULL value is returned.numeric_precision (Nullable(UInt64)) — Accuracy of approximate numeric data, exact numeric data, integer data, or monetary data. In ClickHouse it is bitness for integer types and decimal precision for Decimal types. Otherwise, the NULL value is returned.numeric_precision_radix (Nullable(UInt64)) — The base of the number system is the accuracy of approximate numeric data, exact numeric data, integer data or monetary data. In ClickHouse it's 2 for integer types and 10 for Decimal types. Otherwise, the NULL value is returned.numeric_scale (Nullable(UInt64)) — The scale of approximate numeric data, exact numeric data, integer data, or monetary data. In ClickHouse makes sense only for Decimal types. Otherwise, the NULL value is returned.datetime_precision (Nullable(UInt64)) — Decimal precision of DateTime64 data type. For other data types, the NULL value is returned. Example SELECT * FROM system.columns LIMIT 2 FORMAT Vertical; Row 1: ────── database: INFORMATION_SCHEMA table: COLUMNS name: table_catalog type: String position: 1 default_kind: default_expression: data_compressed_bytes: 0 data_uncompressed_bytes: 0 marks_bytes: 0 comment: is_in_partition_key: 0 is_in_sorting_key: 0 is_in_primary_key: 0 is_in_sampling_key: 0 compression_codec: character_octet_length: ᴺᵁᴸᴸ numeric_precision: ᴺᵁᴸᴸ numeric_precision_radix: ᴺᵁᴸᴸ numeric_scale: ᴺᵁᴸᴸ datetime_precision: ᴺᵁᴸᴸ Row 2: ────── database: INFORMATION_SCHEMA table: COLUMNS name: table_schema type: String position: 2 default_kind: default_expression: data_compressed_bytes: 0 data_uncompressed_bytes: 0 marks_bytes: 0 comment: is_in_partition_key: 0 is_in_sorting_key: 0 is_in_primary_key: 0 is_in_sampling_key: 0 compression_codec: character_octet_length: ᴺᵁᴸᴸ numeric_precision: ᴺᵁᴸᴸ numeric_precision_radix: ᴺᵁᴸᴸ numeric_scale: ᴺᵁᴸᴸ datetime_precision: ᴺᵁᴸᴸ Original article","keywords":""},{"title":"contributors","type":0,"sectionRef":"#","url":"en/operations/system-tables/contributors","content":"contributors Contains information about contributors. The order is random at query execution time. Columns: name (String) — Contributor (author) name from git log. Example SELECT * FROM system.contributors LIMIT 10 ┌─name─────────────┐ │ Olga Khvostikova │ │ Max Vetrov │ │ LiuYangkuan │ │ svladykin │ │ zamulla │ │ Šimon Podlipský │ │ BayoNet │ │ Ilya Khomutov │ │ Amy Krishnevsky │ │ Loud_Scream │ └──────────────────┘ To find out yourself in the table, use a query: SELECT * FROM system.contributors WHERE name = 'Olga Khvostikova' ┌─name─────────────┐ │ Olga Khvostikova │ └──────────────────┘ Original article","keywords":""},{"title":"crash_log","type":0,"sectionRef":"#","url":"en/operations/system-tables/crash-log","content":"crash_log Contains information about stack traces for fatal errors. The table does not exist in the database by default, it is created only when fatal errors occur. Columns: event_date (Datetime) — Date of the event.event_time (Datetime) — Time of the event.timestamp_ns (UInt64) — Timestamp of the event with nanoseconds.signal (Int32) — Signal number.thread_id (UInt64) — Thread ID.query_id (String) — Query ID.trace (Array(UInt64)) — Stack trace at the moment of crash. Each element is a virtual memory address inside ClickHouse server process.trace_full (Array(String)) — Stack trace at the moment of crash. Each element contains a called method inside ClickHouse server process.version (String) — ClickHouse server version.revision (UInt32) — ClickHouse server revision.build_id (String) — BuildID that is generated by compiler. Example Query: SELECT * FROM system.crash_log ORDER BY event_time DESC LIMIT 1; Result (not full): Row 1: ────── event_date: 2020-10-14 event_time: 2020-10-14 15:47:40 timestamp_ns: 1602679660271312710 signal: 11 thread_id: 23624 query_id: 428aab7c-8f5c-44e9-9607-d16b44467e69 trace: [188531193,...] trace_full: ['3. DB::(anonymous namespace)::FunctionFormatReadableTimeDelta::executeImpl(std::__1::vector&lt;DB::ColumnWithTypeAndName, std::__1::allocator&lt;DB::ColumnWithTypeAndName&gt; &gt;&amp;, std::__1::vector&lt;unsigned long, std::__1::allocator&lt;unsigned long&gt; &gt; const&amp;, unsigned long, unsigned long) const @ 0xb3cc1f9 in /home/username/work/ClickHouse/build/programs/clickhouse',...] version: ClickHouse 20.11.1.1 revision: 54442 build_id: See also trace_log system table Original article","keywords":""},{"title":"current_roles","type":0,"sectionRef":"#","url":"en/operations/system-tables/current-roles","content":"current_roles Contains active roles of a current user. SET ROLE changes the contents of this table. Columns: role_name (String)) — Role name. with_admin_option (UInt8) — Flag that shows whether current_role is a role with ADMIN OPTION privilege. is_default (UInt8) — Flag that shows whether current_role is a default role. Original article","keywords":""},{"title":"data_skipping_indices","type":0,"sectionRef":"#","url":"en/operations/system-tables/data_skipping_indices","content":"data_skipping_indices Contains information about existing data skipping indices in all the tables. Columns: database (String) — Database name.table (String) — Table name.name (String) — Index name.type (String) — Index type.expr (String) — Expression for the index calculation.granularity (UInt64) — The number of granules in the block.data_compressed_bytes (UInt64) — The size of compressed data, in bytes.data_uncompressed_bytes (UInt64) — The size of decompressed data, in bytes.marks_bytes (UInt64) — The size of marks, in bytes. Example SELECT * FROM system.data_skipping_indices LIMIT 2 FORMAT Vertical; Row 1: ────── database: default table: user_actions name: clicks_idx type: minmax expr: clicks granularity: 1 data_compressed_bytes: 58 data_uncompressed_bytes: 6 marks: 48 Row 2: ────── database: default table: users name: contacts_null_idx type: minmax expr: assumeNotNull(contacts_null) granularity: 1 data_compressed_bytes: 58 data_uncompressed_bytes: 6 marks: 48 ","keywords":""},{"title":"data_type_families","type":0,"sectionRef":"#","url":"en/operations/system-tables/data_type_families","content":"data_type_families Contains information about supported data types. Columns: name (String) — Data type name.case_insensitive (UInt8) — Property that shows whether you can use a data type name in a query in case insensitive manner or not. For example, Date and date are both valid.alias_to (String) — Data type name for which name is an alias. Example SELECT * FROM system.data_type_families WHERE alias_to = 'String' ┌─name───────┬─case_insensitive─┬─alias_to─┐ │ LONGBLOB │ 1 │ String │ │ LONGTEXT │ 1 │ String │ │ TINYTEXT │ 1 │ String │ │ TEXT │ 1 │ String │ │ VARCHAR │ 1 │ String │ │ MEDIUMBLOB │ 1 │ String │ │ BLOB │ 1 │ String │ │ TINYBLOB │ 1 │ String │ │ CHAR │ 1 │ String │ │ MEDIUMTEXT │ 1 │ String │ └────────────┴──────────────────┴──────────┘ See Also Syntax — Information about supported syntax. Original article","keywords":""},{"title":"databases","type":0,"sectionRef":"#","url":"en/operations/system-tables/databases","content":"databases Contains information about the databases that are available to the current user. Columns: name (String) — Database name.engine (String) — Database engine.data_path (String) — Data path.metadata_path (String) — Metadata path.uuid (UUID) — Database UUID.comment (String) — Database comment. The name column from this system table is used for implementing the SHOW DATABASES query. Example Create a database. CREATE DATABASE test; Check all of the available databases to the user. SELECT * FROM system.databases; ┌─name───────────────┬─engine─┬─data_path──────────────────┬─metadata_path───────────────────────────────────────────────────────┬─uuid─────────────────────────────────┬─comment─┐ │ INFORMATION_SCHEMA │ Memory │ /var/lib/clickhouse/ │ │ 00000000-0000-0000-0000-000000000000 │ │ │ default │ Atomic │ /var/lib/clickhouse/store/ │ /var/lib/clickhouse/store/d31/d317b4bd-3595-4386-81ee-c2334694128a/ │ 24363899-31d7-42a0-a436-389931d752a0 │ │ │ information_schema │ Memory │ /var/lib/clickhouse/ │ │ 00000000-0000-0000-0000-000000000000 │ │ │ system │ Atomic │ /var/lib/clickhouse/store/ │ /var/lib/clickhouse/store/1d1/1d1c869d-e465-4b1b-a51f-be033436ebf9/ │ 03e9f3d1-cc88-4a49-83e9-f3d1cc881a49 │ │ └────────────────────┴────────┴────────────────────────────┴─────────────────────────────────────────────────────────────────────┴──────────────────────────────────────┴─────────┘ ","keywords":""},{"title":"detached_parts","type":0,"sectionRef":"#","url":"en/operations/system-tables/detached_parts","content":"detached_parts Contains information about detached parts of MergeTree tables. The reason column specifies why the part was detached. For user-detached parts, the reason is empty. Such parts can be attached with ALTER TABLE ATTACH PARTITION|PART command. For the description of other columns, see system.parts. If part name is invalid, values of some columns may be NULL. Such parts can be deleted with ALTER TABLE DROP DETACHED PART. Original article","keywords":""},{"title":"dictionaries","type":0,"sectionRef":"#","url":"en/operations/system-tables/dictionaries","content":"dictionaries Contains information about external dictionaries. Columns: database (String) — Name of the database containing the dictionary created by DDL query. Empty string for other dictionaries.name (String) — Dictionary name.uuid (UUID) — Dictionary UUID.status (Enum8) — Dictionary status. Possible values: NOT_LOADED — Dictionary was not loaded because it was not used.LOADED — Dictionary loaded successfully.FAILED — Unable to load the dictionary as a result of an error.LOADING — Dictionary is loading now.LOADED_AND_RELOADING — Dictionary is loaded successfully, and is being reloaded right now (frequent reasons: SYSTEM RELOAD DICTIONARY query, timeout, dictionary config has changed).FAILED_AND_RELOADING — Could not load the dictionary as a result of an error and is loading now. origin (String) — Path to the configuration file that describes the dictionary.type (String) — Type of a dictionary allocation. Storing Dictionaries in Memory.key.names (Array(String)) — Array of key names provided by the dictionary.key.types (Array(String)) — Corresponding array of key types provided by the dictionary.attribute.names (Array(String)) — Array of attribute names provided by the dictionary.attribute.types (Array(String)) — Corresponding array of attribute types provided by the dictionary.bytes_allocated (UInt64) — Amount of RAM allocated for the dictionary.query_count (UInt64) — Number of queries since the dictionary was loaded or since the last successful reboot.hit_rate (Float64) — For cache dictionaries, the percentage of uses for which the value was in the cache.found_rate (Float64) — The percentage of uses for which the value was found.element_count (UInt64) — Number of items stored in the dictionary.load_factor (Float64) — Percentage filled in the dictionary (for a hashed dictionary, the percentage filled in the hash table).source (String) — Text describing the data source for the dictionary.lifetime_min (UInt64) — Minimum lifetime of the dictionary in memory, after which ClickHouse tries to reload the dictionary (if invalidate_query is set, then only if it has changed). Set in seconds.lifetime_max (UInt64) — Maximum lifetime of the dictionary in memory, after which ClickHouse tries to reload the dictionary (if invalidate_query is set, then only if it has changed). Set in seconds.loading_start_time (DateTime) — Start time for loading the dictionary.last_successful_update_time (DateTime) — End time for loading or updating the dictionary. Helps to monitor some troubles with external sources and investigate causes.loading_duration (Float32) — Duration of a dictionary loading.last_exception (String) — Text of the error that occurs when creating or reloading the dictionary if the dictionary couldn’t be created.comment (String) — Text of the comment to dictionary. Example Configure the dictionary: CREATE DICTIONARY dictionary_with_comment ( id UInt64, value String ) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() TABLE 'source_table')) LAYOUT(FLAT()) LIFETIME(MIN 0 MAX 1000) COMMENT 'The temporary dictionary'; Make sure that the dictionary is loaded. SELECT * FROM system.dictionaries LIMIT 1 FORMAT Vertical; Row 1: ────── database: default name: dictionary_with_comment uuid: 4654d460-0d03-433a-8654-d4600d03d33a status: NOT_LOADED origin: 4654d460-0d03-433a-8654-d4600d03d33a type: key.names: ['id'] key.types: ['UInt64'] attribute.names: ['value'] attribute.types: ['String'] bytes_allocated: 0 query_count: 0 hit_rate: 0 found_rate: 0 element_count: 0 load_factor: 0 source: lifetime_min: 0 lifetime_max: 0 loading_start_time: 1970-01-01 00:00:00 last_successful_update_time: 1970-01-01 00:00:00 loading_duration: 0 last_exception: comment: The temporary dictionary ","keywords":""},{"title":"disks","type":0,"sectionRef":"#","url":"en/operations/system-tables/disks","content":"disks Contains information about disks defined in the server configuration. Columns: name (String) — Name of a disk in the server configuration.path (String) — Path to the mount point in the file system.free_space (UInt64) — Free space on disk in bytes.total_space (UInt64) — Disk volume in bytes.keep_free_space (UInt64) — Amount of disk space that should stay free on disk in bytes. Defined in the keep_free_space_bytes parameter of disk configuration. Example :) SELECT * FROM system.disks; ┌─name────┬─path─────────────────┬───free_space─┬──total_space─┬─keep_free_space─┐ │ default │ /var/lib/clickhouse/ │ 276392587264 │ 490652508160 │ 0 │ └─────────┴──────────────────────┴──────────────┴──────────────┴─────────────────┘ 1 rows in set. Elapsed: 0.001 sec. Original article","keywords":""},{"title":"distributed_ddl_queue","type":0,"sectionRef":"#","url":"en/operations/system-tables/distributed_ddl_queue","content":"distributed_ddl_queue Contains information about distributed ddl queries (ON CLUSTER clause) that were executed on a cluster. Columns: entry (String) — Query id.host_name (String) — Hostname.host_address (String) — IP address that the Hostname resolves to.port (UInt16) — Host Port.status (Enum8) — Status of the query.cluster (String) — Cluster name.query (String) — Query executed.initiator (String) — Node that executed the query.query_start_time (DateTime) — Query start time.query_finish_time (DateTime) — Query finish time.query_duration_ms (UInt64) — Duration of query execution (in milliseconds).exception_code (Enum8) — Exception code from ZooKeeper. Example SELECT * FROM system.distributed_ddl_queue WHERE cluster = 'test_cluster' LIMIT 2 FORMAT Vertical Query id: f544e72a-6641-43f1-836b-24baa1c9632a Row 1: ────── entry: query-0000000000 host_name: clickhouse01 host_address: 172.23.0.11 port: 9000 status: Finished cluster: test_cluster query: CREATE DATABASE test_db UUID '4a82697e-c85e-4e5b-a01e-a36f2a758456' ON CLUSTER test_cluster initiator: clickhouse01:9000 query_start_time: 2020-12-30 13:07:51 query_finish_time: 2020-12-30 13:07:51 query_duration_ms: 6 exception_code: ZOK Row 2: ────── entry: query-0000000000 host_name: clickhouse02 host_address: 172.23.0.12 port: 9000 status: Finished cluster: test_cluster query: CREATE DATABASE test_db UUID '4a82697e-c85e-4e5b-a01e-a36f2a758456' ON CLUSTER test_cluster initiator: clickhouse01:9000 query_start_time: 2020-12-30 13:07:51 query_finish_time: 2020-12-30 13:07:51 query_duration_ms: 6 exception_code: ZOK 2 rows in set. Elapsed: 0.025 sec. Original article","keywords":""},{"title":"distribution_queue","type":0,"sectionRef":"#","url":"en/operations/system-tables/distribution_queue","content":"distribution_queue Contains information about local files that are in the queue to be sent to the shards. These local files contain new parts that are created by inserting new data into the Distributed table in asynchronous mode. Columns: database (String) — Name of the database. table (String) — Name of the table. data_path (String) — Path to the folder with local files. is_blocked (UInt8) — Flag indicates whether sending local files to the server is blocked. error_count (UInt64) — Number of errors. data_files (UInt64) — Number of local files in a folder. data_compressed_bytes (UInt64) — Size of compressed data in local files, in bytes. broken_data_files (UInt64) — Number of files that has been marked as broken (due to an error). broken_data_compressed_bytes (UInt64) — Size of compressed data in broken files, in bytes. last_exception (String) — Text message about the last error that occurred (if any). Example SELECT * FROM system.distribution_queue LIMIT 1 FORMAT Vertical; Row 1: ────── database: default table: dist data_path: ./store/268/268bc070-3aad-4b1a-9cf2-4987580161af/default@127%2E0%2E0%2E2:9000/ is_blocked: 1 error_count: 0 data_files: 1 data_compressed_bytes: 499 last_exception: See Also Distributed table engine Original article","keywords":""},{"title":"enabled_roles","type":0,"sectionRef":"#","url":"en/operations/system-tables/enabled-roles","content":"enabled_roles Contains all active roles at the moment, including current role of the current user and granted roles for current role. Columns: role_name (String)) — Role name.with_admin_option (UInt8) — Flag that shows whether enabled_role is a role with ADMIN OPTION privilege.is_current (UInt8) — Flag that shows whether enabled_role is a current role of a current user.is_default (UInt8) — Flag that shows whether enabled_role is a default role. Original article","keywords":""},{"title":"errors","type":0,"sectionRef":"#","url":"en/operations/system-tables/errors","content":"errors Contains error codes with the number of times they have been triggered. Columns: name (String) — name of the error (errorCodeToName).code (Int32) — code number of the error.value (UInt64) — the number of times this error has been happened.last_error_time (DateTime) — time when the last error happened.last_error_message (String) — message for the last error.last_error_trace (Array(UInt64)) — A stack trace which represents a list of physical addresses where the called methods are stored.remote (UInt8) — remote exception (i.e. received during one of the distributed query). Example SELECT name, code, value FROM system.errors WHERE value &gt; 0 ORDER BY code ASC LIMIT 1 ┌─name─────────────┬─code─┬─value─┐ │ CANNOT_OPEN_FILE │ 76 │ 1 │ └──────────────────┴──────┴───────┘ WITH arrayMap(x -&gt; demangle(addressToSymbol(x)), last_error_trace) AS all SELECT name, arrayStringConcat(all, '\\n') AS res FROM system.errors LIMIT 1 SETTINGS allow_introspection_functions=1\\G ","keywords":""},{"title":"grants","type":0,"sectionRef":"#","url":"en/operations/system-tables/grants","content":"grants Privileges granted to ClickHouse user accounts. Columns: user_name (Nullable(String)) — User name. role_name (Nullable(String)) — Role assigned to user account. access_type (Enum8) — Access parameters for ClickHouse user account. database (Nullable(String)) — Name of a database. table (Nullable(String)) — Name of a table. column (Nullable(String)) — Name of a column to which access is granted. is_partial_revoke (UInt8) — Logical value. It shows whether some privileges have been revoked. Possible values: 0 — The row describes a partial revoke. 1 — The row describes a grant. grant_option (UInt8) — Permission is granted WITH GRANT OPTION, see GRANT. Original article","keywords":""},{"title":"functions","type":0,"sectionRef":"#","url":"en/operations/system-tables/functions","content":"functions Contains information about normal and aggregate functions. Columns: name(String) – The name of the function.is_aggregate(UInt8) — Whether the function is aggregate. Example SELECT * FROM system.functions LIMIT 10; ┌─name──────────────────┬─is_aggregate─┬─case_insensitive─┬─alias_to─┬─create_query─┬─origin─┐ │ logTrace │ 0 │ 0 │ │ │ System │ │ aes_decrypt_mysql │ 0 │ 0 │ │ │ System │ │ aes_encrypt_mysql │ 0 │ 0 │ │ │ System │ │ decrypt │ 0 │ 0 │ │ │ System │ │ encrypt │ 0 │ 0 │ │ │ System │ │ toBool │ 0 │ 0 │ │ │ System │ │ windowID │ 0 │ 0 │ │ │ System │ │ hopStart │ 0 │ 0 │ │ │ System │ │ hop │ 0 │ 0 │ │ │ System │ │ snowflakeToDateTime64 │ 0 │ 0 │ │ │ System │ └───────────────────────┴──────────────┴──────────────────┴──────────┴──────────────┴────────┘ 10 rows in set. Elapsed: 0.002 sec. Original article","keywords":""},{"title":"graphite_retentions","type":0,"sectionRef":"#","url":"en/operations/system-tables/graphite_retentions","content":"graphite_retentions Contains information about parameters graphite_rollup which are used in tables with *GraphiteMergeTree engines. Columns: config_name (String) - graphite_rollup parameter name.regexp (String) - A pattern for the metric name.function (String) - The name of the aggregating function.age (UInt64) - The minimum age of the data in seconds.precision (UInt64) - How precisely to define the age of the data in seconds.priority (UInt16) - Pattern priority.is_default (UInt8) - Whether the pattern is the default.Tables.database (Array(String)) - Array of names of database tables that use the config_name parameter.Tables.table (Array(String)) - Array of table names that use the config_name parameter. Original article","keywords":""},{"title":"licenses","type":0,"sectionRef":"#","url":"en/operations/system-tables/licenses","content":"licenses Сontains licenses of third-party libraries that are located in the contrib directory of ClickHouse sources. Columns: library_name (String) — Name of the library, which is license connected with.license_type (String) — License type — e.g. Apache, MIT.license_path (String) — Path to the file with the license text.license_text (String) — License text. Example SELECT library_name, license_type, license_path FROM system.licenses LIMIT 15 ┌─library_name───────┬─license_type─┬─license_path────────────────────────┐ │ FastMemcpy │ MIT │ /contrib/FastMemcpy/LICENSE │ │ arrow │ Apache │ /contrib/arrow/LICENSE.txt │ │ avro │ Apache │ /contrib/avro/LICENSE.txt │ │ aws-c-common │ Apache │ /contrib/aws-c-common/LICENSE │ │ aws-c-event-stream │ Apache │ /contrib/aws-c-event-stream/LICENSE │ │ aws-checksums │ Apache │ /contrib/aws-checksums/LICENSE │ │ aws │ Apache │ /contrib/aws/LICENSE.txt │ │ base64 │ BSD 2-clause │ /contrib/base64/LICENSE │ │ boost │ Boost │ /contrib/boost/LICENSE_1_0.txt │ │ brotli │ MIT │ /contrib/brotli/LICENSE │ │ capnproto │ MIT │ /contrib/capnproto/LICENSE │ │ cassandra │ Apache │ /contrib/cassandra/LICENSE.txt │ │ cctz │ Apache │ /contrib/cctz/LICENSE.txt │ │ cityhash102 │ MIT │ /contrib/cityhash102/COPYING │ │ cppkafka │ BSD 2-clause │ /contrib/cppkafka/LICENSE │ └────────────────────┴──────────────┴─────────────────────────────────────┘ Original article","keywords":""},{"title":"merge_tree_settings","type":0,"sectionRef":"#","url":"en/operations/system-tables/merge_tree_settings","content":"merge_tree_settings Contains information about settings for MergeTree tables. Columns: name (String) — Setting name.value (String) — Setting value.description (String) — Setting description.type (String) — Setting type (implementation specific string value).changed (UInt8) — Whether the setting was explicitly defined in the config or explicitly changed. Example :) SELECT * FROM system.merge_tree_settings LIMIT 4 FORMAT Vertical; Row 1: ────── name: index_granularity value: 8192 changed: 0 description: How many rows correspond to one primary key value. type: SettingUInt64 Row 2: ────── name: min_bytes_for_wide_part value: 0 changed: 0 description: Minimal uncompressed size in bytes to create part in wide format instead of compact type: SettingUInt64 Row 3: ────── name: min_rows_for_wide_part value: 0 changed: 0 description: Minimal number of rows to create part in wide format instead of compact type: SettingUInt64 Row 4: ────── name: merge_max_block_size value: 8192 changed: 0 description: How many rows in blocks should be formed for merge operations. type: SettingUInt64 4 rows in set. Elapsed: 0.001 sec. Original article","keywords":""},{"title":"mutations","type":0,"sectionRef":"#","url":"en/operations/system-tables/mutations","content":"mutations The table contains information about mutations of MergeTree tables and their progress. Each mutation command is represented by a single row. Columns: database (String) — The name of the database to which the mutation was applied. table (String) — The name of the table to which the mutation was applied. mutation_id (String) — The ID of the mutation. For replicated tables these IDs correspond to znode names in the &lt;table_path_in_zookeeper&gt;/mutations/ directory in ZooKeeper. For non-replicated tables the IDs correspond to file names in the data directory of the table. command (String) — The mutation command string (the part of the query after ALTER TABLE [db.]table). create_time (Datetime) — Date and time when the mutation command was submitted for execution. block_numbers.partition_id (Array(String)) — For mutations of replicated tables, the array contains the partitions' IDs (one record for each partition). For mutations of non-replicated tables the array is empty. block_numbers.number (Array(Int64)) — For mutations of replicated tables, the array contains one record for each partition, with the block number that was acquired by the mutation. Only parts that contain blocks with numbers less than this number will be mutated in the partition. In non-replicated tables, block numbers in all partitions form a single sequence. This means that for mutations of non-replicated tables, the column will contain one record with a single block number acquired by the mutation. parts_to_do_names (Array(String)) — An array of names of data parts that need to be mutated for the mutation to complete. parts_to_do (Int64) — The number of data parts that need to be mutated for the mutation to complete. is_done (UInt8) — The flag whether the mutation is done or not. Possible values: 1 if the mutation is completed,0 if the mutation is still in process. note Even if parts_to_do = 0 it is possible that a mutation of a replicated table is not completed yet because of a long-running INSERT query, that will create a new data part needed to be mutated. If there were problems with mutating some data parts, the following columns contain additional information: latest_failed_part (String) — The name of the most recent part that could not be mutated. latest_fail_time (Datetime) — The date and time of the most recent part mutation failure. latest_fail_reason (String) — The exception message that caused the most recent part mutation failure. See Also MutationsMergeTree table engineReplicatedMergeTree family Original article","keywords":""},{"title":"numbers","type":0,"sectionRef":"#","url":"en/operations/system-tables/numbers","content":"numbers This table contains a single UInt64 column named number that contains almost all the natural numbers starting from zero. You can use this table for tests, or if you need to do a brute force search. Reads from this table are not parallelized. Example :) SELECT * FROM system.numbers LIMIT 10; ┌─number─┐ │ 0 │ │ 1 │ │ 2 │ │ 3 │ │ 4 │ │ 5 │ │ 6 │ │ 7 │ │ 8 │ │ 9 │ └────────┘ 10 rows in set. Elapsed: 0.001 sec. Original article","keywords":""},{"title":"one","type":0,"sectionRef":"#","url":"en/operations/system-tables/one","content":"one This table contains a single row with a single dummy UInt8 column containing the value 0. This table is used if a SELECT query does not specify the FROM clause. This is similar to the DUAL table found in other DBMSs. Example :) SELECT * FROM system.one LIMIT 10; ┌─dummy─┐ │ 0 │ └───────┘ 1 rows in set. Elapsed: 0.001 sec. Original article","keywords":""},{"title":"opentelemetry_span_log","type":0,"sectionRef":"#","url":"en/operations/system-tables/opentelemetry_span_log","content":"opentelemetry_span_log Contains information about trace spans for executed queries. Columns: trace_id (UUID) — ID of the trace for executed query. span_id (UInt64) — ID of the trace span. parent_span_id (UInt64) — ID of the parent trace span. operation_name (String) — The name of the operation. start_time_us (UInt64) — The start time of the trace span (in microseconds). finish_time_us (UInt64) — The finish time of the trace span (in microseconds). finish_date (Date) — The finish date of the trace span. attribute.names (Array(String)) — Attribute names depending on the trace span. They are filled in according to the recommendations in the OpenTelemetry standard. attribute.values (Array(String)) — Attribute values depending on the trace span. They are filled in according to the recommendations in the OpenTelemetry standard. Example Query: SELECT * FROM system.opentelemetry_span_log LIMIT 1 FORMAT Vertical; Result: Row 1: ────── trace_id: cdab0847-0d62-61d5-4d38-dd65b19a1914 span_id: 701487461015578150 parent_span_id: 2991972114672045096 operation_name: DB::Block DB::InterpreterSelectQuery::getSampleBlockImpl() start_time_us: 1612374594529090 finish_time_us: 1612374594529108 finish_date: 2021-02-03 attribute.names: [] attribute.values: [] See Also OpenTelemetry Original article","keywords":""},{"title":"numbers_mt","type":0,"sectionRef":"#","url":"en/operations/system-tables/numbers_mt","content":"numbers_mt The same as system.numbers but reads are parallelized. The numbers can be returned in any order. Used for tests. Example :) SELECT * FROM system.numbers_mt LIMIT 10; ┌─number─┐ │ 0 │ │ 1 │ │ 2 │ │ 3 │ │ 4 │ │ 5 │ │ 6 │ │ 7 │ │ 8 │ │ 9 │ └────────┘ 10 rows in set. Elapsed: 0.001 sec. Original article","keywords":""},{"title":"merges","type":0,"sectionRef":"#","url":"en/operations/system-tables/merges","content":"merges Contains information about merges and part mutations currently in process for tables in the MergeTree family. Columns: database (String) — The name of the database the table is in.table (String) — Table name.elapsed (Float64) — The time elapsed (in seconds) since the merge started.progress (Float64) — The percentage of completed work from 0 to 1.num_parts (UInt64) — The number of pieces to be merged.result_part_name (String) — The name of the part that will be formed as the result of merging.is_mutation (UInt8) — 1 if this process is a part mutation.total_size_bytes_compressed (UInt64) — The total size of the compressed data in the merged chunks.total_size_marks (UInt64) — The total number of marks in the merged parts.bytes_read_uncompressed (UInt64) — Number of bytes read, uncompressed.rows_read (UInt64) — Number of rows read.bytes_written_uncompressed (UInt64) — Number of bytes written, uncompressed.rows_written (UInt64) — Number of rows written.memory_usage (UInt64) — Memory consumption of the merge process.thread_id (UInt64) — Thread ID of the merge process.merge_type — The type of current merge. Empty if it's an mutation.merge_algorithm — The algorithm used in current merge. Empty if it's an mutation. Original article","keywords":""},{"title":"processes","type":0,"sectionRef":"#","url":"en/operations/system-tables/processes","content":"processes This system table is used for implementing the SHOW PROCESSLIST query. Columns: user (String) – The user who made the query. Keep in mind that for distributed processing, queries are sent to remote servers under the default user. The field contains the username for a specific query, not for a query that this query initiated.address (String) – The IP address the request was made from. The same for distributed processing. To track where a distributed query was originally made from, look at system.processes on the query requestor server.elapsed (Float64) – The time in seconds since request execution started.rows_read (UInt64) – The number of rows read from the table. For distributed processing, on the requestor server, this is the total for all remote servers.bytes_read (UInt64) – The number of uncompressed bytes read from the table. For distributed processing, on the requestor server, this is the total for all remote servers.total_rows_approx (UInt64) – The approximation of the total number of rows that should be read. For distributed processing, on the requestor server, this is the total for all remote servers. It can be updated during request processing, when new sources to process become known.memory_usage (UInt64) – Amount of RAM the request uses. It might not include some types of dedicated memory. See the max_memory_usage setting.query (String) – The query text. For INSERT, it does not include the data to insert.query_id (String) – Query ID, if defined.is_cancelled (Int8) – Was query cancelled.is_all_data_sent (Int8) – Was all data sent to the client (in other words query had been finished on the server). :) SELECT * FROM system.processes LIMIT 10 FORMAT Vertical; Row 1: ────── is_initial_query: 1 user: default query_id: 35a360fa-3743-441d-8e1f-228c938268da address: ::ffff:172.23.0.1 port: 47588 initial_user: default initial_query_id: 35a360fa-3743-441d-8e1f-228c938268da initial_address: ::ffff:172.23.0.1 initial_port: 47588 interface: 1 os_user: bharatnc client_hostname: tower client_name: ClickHouse client_revision: 54437 client_version_major: 20 client_version_minor: 7 client_version_patch: 2 http_method: 0 http_user_agent: quota_key: elapsed: 0.000582537 is_cancelled: 0 is_all_data_sent: 0 read_rows: 0 read_bytes: 0 total_rows_approx: 0 written_rows: 0 written_bytes: 0 memory_usage: 0 peak_memory_usage: 0 query: SELECT * from system.processes LIMIT 10 FORMAT Vertical; thread_ids: [67] ProfileEvents: {'Query':1,'SelectQuery':1,'ReadCompressedBytes':36,'CompressedReadBufferBlocks':1,'CompressedReadBufferBytes':10,'IOBufferAllocs':1,'IOBufferAllocBytes':89,'ContextLock':15,'RWLockAcquiredReadLocks':1} Settings: {'background_pool_size':'32','load_balancing':'random','allow_suspicious_low_cardinality_types':'1','distributed_aggregation_memory_efficient':'1','skip_unavailable_shards':'1','log_queries':'1','max_bytes_before_external_group_by':'20000000000','max_bytes_before_external_sort':'20000000000','allow_introspection_functions':'1'} 1 rows in set. Elapsed: 0.002 sec. Original article","keywords":""},{"title":"quota_limits","type":0,"sectionRef":"#","url":"en/operations/system-tables/quota_limits","content":"quota_limits Contains information about maximums for all intervals of all quotas. Any number of rows or zero can correspond to one quota. Columns: quota_name (String) — Quota name.duration (UInt32) — Length of the time interval for calculating resource consumption, in seconds.is_randomized_interval (UInt8) — Logical value. It shows whether the interval is randomized. Interval always starts at the same time if it is not randomized. For example, an interval of 1 minute always starts at an integer number of minutes (i.e. it can start at 11:20:00, but it never starts at 11:20:01), an interval of one day always starts at midnight UTC. If interval is randomized, the very first interval starts at random time, and subsequent intervals starts one by one. Values:0 — Interval is not randomized.1 — Interval is randomized.max_queries (Nullable(UInt64)) — Maximum number of queries.max_query_selects (Nullable(UInt64)) — Maximum number of select queries.max_query_inserts (Nullable(UInt64)) — Maximum number of insert queries.max_errors (Nullable(UInt64)) — Maximum number of errors.max_result_rows (Nullable(UInt64)) — Maximum number of result rows.max_result_bytes (Nullable(UInt64)) — Maximum number of RAM volume in bytes used to store a queries result.max_read_rows (Nullable(UInt64)) — Maximum number of rows read from all tables and table functions participated in queries.max_read_bytes (Nullable(UInt64)) — Maximum number of bytes read from all tables and table functions participated in queries.max_execution_time (Nullable(Float64)) — Maximum of the query execution time, in seconds. Original article","keywords":""},{"title":"parts","type":0,"sectionRef":"#","url":"en/operations/system-tables/parts","content":"parts Contains information about parts of MergeTree tables. Each row describes one data part. Columns: partition (String) – The partition name. To learn what a partition is, see the description of the ALTER query. Formats: YYYYMM for automatic partitioning by month.any_string when partitioning manually. name (String) – Name of the data part. part_type (String) — The data part storing format. Possible Values: Wide — Each column is stored in a separate file in a filesystem.Compact — All columns are stored in one file in a filesystem. Data storing format is controlled by the min_bytes_for_wide_part and min_rows_for_wide_part settings of the MergeTree table. active (UInt8) – Flag that indicates whether the data part is active. If a data part is active, it’s used in a table. Otherwise, it’s deleted. Inactive data parts remain after merging. marks (UInt64) – The number of marks. To get the approximate number of rows in a data part, multiply marks by the index granularity (usually 8192) (this hint does not work for adaptive granularity). rows (UInt64) – The number of rows. bytes_on_disk (UInt64) – Total size of all the data part files in bytes. data_compressed_bytes (UInt64) – Total size of compressed data in the data part. All the auxiliary files (for example, files with marks) are not included. data_uncompressed_bytes (UInt64) – Total size of uncompressed data in the data part. All the auxiliary files (for example, files with marks) are not included. marks_bytes (UInt64) – The size of the file with marks. secondary_indices_compressed_bytes (UInt64) – Total size of compressed data for secondary indices in the data part. All the auxiliary files (for example, files with marks) are not included. secondary_indices_uncompressed_bytes (UInt64) – Total size of uncompressed data for secondary indices in the data part. All the auxiliary files (for example, files with marks) are not included. secondary_indices_marks_bytes (UInt64) – The size of the file with marks for secondary indices. modification_time (DateTime) – The time the directory with the data part was modified. This usually corresponds to the time of data part creation. remove_time (DateTime) – The time when the data part became inactive. refcount (UInt32) – The number of places where the data part is used. A value greater than 2 indicates that the data part is used in queries or merges. min_date (Date) – The minimum value of the date key in the data part. max_date (Date) – The maximum value of the date key in the data part. min_time (DateTime) – The minimum value of the date and time key in the data part. max_time(DateTime) – The maximum value of the date and time key in the data part. partition_id (String) – ID of the partition. min_block_number (UInt64) – The minimum number of data parts that make up the current part after merging. max_block_number (UInt64) – The maximum number of data parts that make up the current part after merging. level (UInt32) – Depth of the merge tree. Zero means that the current part was created by insert rather than by merging other parts. data_version (UInt64) – Number that is used to determine which mutations should be applied to the data part (mutations with a version higher than data_version). primary_key_bytes_in_memory (UInt64) – The amount of memory (in bytes) used by primary key values. primary_key_bytes_in_memory_allocated (UInt64) – The amount of memory (in bytes) reserved for primary key values. is_frozen (UInt8) – Flag that shows that a partition data backup exists. 1, the backup exists. 0, the backup does not exist. For more details, see FREEZE PARTITION database (String) – Name of the database. table (String) – Name of the table. engine (String) – Name of the table engine without parameters. path (String) – Absolute path to the folder with data part files. disk_name (String) – Name of a disk that stores the data part. hash_of_all_files (String) – sipHash128 of compressed files. hash_of_uncompressed_files (String) – sipHash128 of uncompressed files (files with marks, index file etc.). uncompressed_hash_of_compressed_files (String) – sipHash128 of data in the compressed files as if they were uncompressed. delete_ttl_info_min (DateTime) — The minimum value of the date and time key for TTL DELETE rule. delete_ttl_info_max (DateTime) — The maximum value of the date and time key for TTL DELETE rule. move_ttl_info.expression (Array(String)) — Array of expressions. Each expression defines a TTL MOVE rule. warning The move_ttl_info.expression array is kept mostly for backward compatibility, now the simpliest way to check TTL MOVE rule is to use the move_ttl_info.min and move_ttl_info.max fields. move_ttl_info.min (Array(DateTime)) — Array of date and time values. Each element describes the minimum key value for a TTL MOVE rule. move_ttl_info.max (Array(DateTime)) — Array of date and time values. Each element describes the maximum key value for a TTL MOVE rule. bytes (UInt64) – Alias for bytes_on_disk. marks_size (UInt64) – Alias for marks_bytes. Example SELECT * FROM system.parts LIMIT 1 FORMAT Vertical; Row 1: ────── partition: tuple() name: all_1_4_1_6 part_type: Wide active: 1 marks: 2 rows: 6 bytes_on_disk: 310 data_compressed_bytes: 157 data_uncompressed_bytes: 91 secondary_indices_compressed_bytes: 58 secondary_indices_uncompressed_bytes: 6 secondary_indices_marks_bytes: 48 marks_bytes: 144 modification_time: 2020-06-18 13:01:49 remove_time: 1970-01-01 00:00:00 refcount: 1 min_date: 1970-01-01 max_date: 1970-01-01 min_time: 1970-01-01 00:00:00 max_time: 1970-01-01 00:00:00 partition_id: all min_block_number: 1 max_block_number: 4 level: 1 data_version: 6 primary_key_bytes_in_memory: 8 primary_key_bytes_in_memory_allocated: 64 is_frozen: 0 database: default table: months engine: MergeTree disk_name: default path: /var/lib/clickhouse/data/default/months/all_1_4_1_6/ hash_of_all_files: 2d0657a16d9430824d35e327fcbd87bf hash_of_uncompressed_files: 84950cc30ba867c77a408ae21332ba29 uncompressed_hash_of_compressed_files: 1ad78f1c6843bbfb99a2c931abe7df7d delete_ttl_info_min: 1970-01-01 00:00:00 delete_ttl_info_max: 1970-01-01 00:00:00 move_ttl_info.expression: [] move_ttl_info.min: [] move_ttl_info.max: [] See Also MergeTree familyTTL for Columns and Tables Original article","keywords":""},{"title":"parts_columns","type":0,"sectionRef":"#","url":"en/operations/system-tables/parts_columns","content":"parts_columns Contains information about parts and columns of MergeTree tables. Each row describes one data part. Columns: partition (String) — The partition name. To learn what a partition is, see the description of the ALTER query. Formats: YYYYMM for automatic partitioning by month.any_string when partitioning manually. name (String) — Name of the data part. part_type (String) — The data part storing format. Possible values: Wide — Each column is stored in a separate file in a filesystem.Compact — All columns are stored in one file in a filesystem. Data storing format is controlled by the min_bytes_for_wide_part and min_rows_for_wide_part settings of the MergeTree table. active (UInt8) — Flag that indicates whether the data part is active. If a data part is active, it’s used in a table. Otherwise, it’s deleted. Inactive data parts remain after merging. marks (UInt64) — The number of marks. To get the approximate number of rows in a data part, multiply marks by the index granularity (usually 8192) (this hint does not work for adaptive granularity). rows (UInt64) — The number of rows. bytes_on_disk (UInt64) — Total size of all the data part files in bytes. data_compressed_bytes (UInt64) — Total size of compressed data in the data part. All the auxiliary files (for example, files with marks) are not included. data_uncompressed_bytes (UInt64) — Total size of uncompressed data in the data part. All the auxiliary files (for example, files with marks) are not included. marks_bytes (UInt64) — The size of the file with marks. modification_time (DateTime) — The time the directory with the data part was modified. This usually corresponds to the time of data part creation. remove_time (DateTime) — The time when the data part became inactive. refcount (UInt32) — The number of places where the data part is used. A value greater than 2 indicates that the data part is used in queries or merges. min_date (Date) — The minimum value of the date key in the data part. max_date (Date) — The maximum value of the date key in the data part. partition_id (String) — ID of the partition. min_block_number (UInt64) — The minimum number of data parts that make up the current part after merging. max_block_number (UInt64) — The maximum number of data parts that make up the current part after merging. level (UInt32) — Depth of the merge tree. Zero means that the current part was created by insert rather than by merging other parts. data_version (UInt64) — Number that is used to determine which mutations should be applied to the data part (mutations with a version higher than data_version). primary_key_bytes_in_memory (UInt64) — The amount of memory (in bytes) used by primary key values. primary_key_bytes_in_memory_allocated (UInt64) — The amount of memory (in bytes) reserved for primary key values. database (String) — Name of the database. table (String) — Name of the table. engine (String) — Name of the table engine without parameters. disk_name (String) — Name of a disk that stores the data part. path (String) — Absolute path to the folder with data part files. column (String) — Name of the column. type (String) — Column type. column_position (UInt64) — Ordinal position of a column in a table starting with 1. default_kind (String) — Expression type (DEFAULT, MATERIALIZED, ALIAS) for the default value, or an empty string if it is not defined. default_expression (String) — Expression for the default value, or an empty string if it is not defined. column_bytes_on_disk (UInt64) — Total size of the column in bytes. column_data_compressed_bytes (UInt64) — Total size of compressed data in the column, in bytes. column_data_uncompressed_bytes (UInt64) — Total size of the decompressed data in the column, in bytes. column_marks_bytes (UInt64) — The size of the column with marks, in bytes. bytes (UInt64) — Alias for bytes_on_disk. marks_size (UInt64) — Alias for marks_bytes. Example SELECT * FROM system.parts_columns LIMIT 1 FORMAT Vertical; Row 1: ────── partition: tuple() name: all_1_2_1 part_type: Wide active: 1 marks: 2 rows: 2 bytes_on_disk: 155 data_compressed_bytes: 56 data_uncompressed_bytes: 4 marks_bytes: 96 modification_time: 2020-09-23 10:13:36 remove_time: 2106-02-07 06:28:15 refcount: 1 min_date: 1970-01-01 max_date: 1970-01-01 partition_id: all min_block_number: 1 max_block_number: 2 level: 1 data_version: 1 primary_key_bytes_in_memory: 2 primary_key_bytes_in_memory_allocated: 64 database: default table: 53r93yleapyears engine: MergeTree disk_name: default path: /var/lib/clickhouse/data/default/53r93yleapyears/all_1_2_1/ column: id type: Int8 column_position: 1 default_kind: default_expression: column_bytes_on_disk: 76 column_data_compressed_bytes: 28 column_data_uncompressed_bytes: 2 column_marks_bytes: 48 See Also MergeTree family Original article","keywords":""},{"title":"quotas_usage","type":0,"sectionRef":"#","url":"en/operations/system-tables/quotas_usage","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"quotas_usage","url":"en/operations/system-tables/quotas_usage#see-also","content":"SHOW QUOTA Original article "},{"title":"metric_log","type":0,"sectionRef":"#","url":"en/operations/system-tables/metric_log","content":"metric_log Contains history of metrics values from tables system.metrics and system.events, periodically flushed to disk. Columns: event_date (Date) — Event date.event_time (DateTime) — Event time.event_time_microseconds (DateTime64) — Event time with microseconds resolution. Example SELECT * FROM system.metric_log LIMIT 1 FORMAT Vertical; Row 1: ────── event_date: 2020-09-05 event_time: 2020-09-05 16:22:33 event_time_microseconds: 2020-09-05 16:22:33.196807 milliseconds: 196 ProfileEvent_Query: 0 ProfileEvent_SelectQuery: 0 ProfileEvent_InsertQuery: 0 ProfileEvent_FailedQuery: 0 ProfileEvent_FailedSelectQuery: 0 ... ... CurrentMetric_Revision: 54439 CurrentMetric_VersionInteger: 20009001 CurrentMetric_RWLockWaitingReaders: 0 CurrentMetric_RWLockWaitingWriters: 0 CurrentMetric_RWLockActiveReaders: 0 CurrentMetric_RWLockActiveWriters: 0 CurrentMetric_GlobalThread: 74 CurrentMetric_GlobalThreadActive: 26 CurrentMetric_LocalThread: 0 CurrentMetric_LocalThreadActive: 0 CurrentMetric_DistributedFilesToInsert: 0 See also metric_log setting — Enabling and disabling the setting.system.asynchronous_metrics — Contains periodically calculated metrics.system.events — Contains a number of events that occurred.system.metrics — Contains instantly calculated metrics.Monitoring — Base concepts of ClickHouse monitoring. Original article","keywords":""},{"title":"metrics","type":0,"sectionRef":"#","url":"en/operations/system-tables/metrics","content":"metrics Contains metrics which can be calculated instantly, or have a current value. For example, the number of simultaneously processed queries or the current replica delay. This table is always up to date. Columns: metric (String) — Metric name.value (Int64) — Metric value.description (String) — Metric description. The list of supported metrics you can find in the src/Common/CurrentMetrics.cpp source file of ClickHouse. Example SELECT * FROM system.metrics LIMIT 10 ┌─metric───────────────────────────────┬─value─┬─description────────────────────────────────────────────────────────────┐ │ Query │ 1 │ Number of executing queries │ │ Merge │ 0 │ Number of executing background merges │ │ PartMutation │ 0 │ Number of mutations (ALTER DELETE/UPDATE) │ │ ReplicatedFetch │ 0 │ Number of data parts being fetched from replicas │ │ ReplicatedSend │ 0 │ Number of data parts being sent to replicas │ │ ReplicatedChecks │ 0 │ Number of data parts checking for consistency │ │ BackgroundMergesAndMutationsPoolTask │ 0 │ Number of active merges and mutations in an associated background pool │ │ BackgroundFetchesPoolTask │ 0 │ Number of active fetches in an associated background pool │ │ BackgroundCommonPoolTask │ 0 │ Number of active tasks in an associated background pool │ │ BackgroundMovePoolTask │ 0 │ Number of active tasks in BackgroundProcessingPool for moves │ └──────────────────────────────────────┴───────┴────────────────────────────────────────────────────────────────────────┘ See Also system.asynchronous_metrics — Contains periodically calculated metrics.system.events — Contains a number of events that occurred.system.metric_log — Contains a history of metrics values from tables system.metrics and system.events.Monitoring — Base concepts of ClickHouse monitoring. Original article","keywords":""},{"title":"query_thread_log","type":0,"sectionRef":"#","url":"en/operations/system-tables/query_thread_log","content":"query_thread_log Contains information about threads that execute queries, for example, thread name, thread start time, duration of query processing. To start logging: Configure parameters in the query_thread_log section.Set log_query_threads to 1. The flushing period of data is set in flush_interval_milliseconds parameter of the query_thread_log server settings section. To force flushing, use the SYSTEM FLUSH LOGS query. ClickHouse does not delete data from the table automatically. See Introduction for more details. You can use the log_queries_probability setting to reduce the number of queries, registered in the query_thread_log table. Columns: event_date (Date) — The date when the thread has finished execution of the query.event_time (DateTime) — The date and time when the thread has finished execution of the query.event_time_microsecinds (DateTime) — The date and time when the thread has finished execution of the query with microseconds precision.query_start_time (DateTime) — Start time of query execution.query_start_time_microseconds (DateTime64) — Start time of query execution with microsecond precision.query_duration_ms (UInt64) — Duration of query execution.read_rows (UInt64) — Number of read rows.read_bytes (UInt64) — Number of read bytes.written_rows (UInt64) — For INSERT queries, the number of written rows. For other queries, the column value is 0.written_bytes (UInt64) — For INSERT queries, the number of written bytes. For other queries, the column value is 0.memory_usage (Int64) — The difference between the amount of allocated and freed memory in context of this thread.peak_memory_usage (Int64) — The maximum difference between the amount of allocated and freed memory in context of this thread.thread_name (String) — Name of the thread.thread_number (UInt32) — Internal thread ID.thread_id (Int32) — thread ID.master_thread_id (UInt64) — OS initial ID of initial thread.query (String) — Query string.is_initial_query (UInt8) — Query type. Possible values: 1 — Query was initiated by the client.0 — Query was initiated by another query for distributed query execution. user (String) — Name of the user who initiated the current query.query_id (String) — ID of the query.address (IPv6) — IP address that was used to make the query.port (UInt16) — The client port that was used to make the query.initial_user (String) — Name of the user who ran the initial query (for distributed query execution).initial_query_id (String) — ID of the initial query (for distributed query execution).initial_address (IPv6) — IP address that the parent query was launched from.initial_port (UInt16) — The client port that was used to make the parent query.interface (UInt8) — Interface that the query was initiated from. Possible values: 1 — TCP.2 — HTTP. os_user (String) — OS’s username who runs clickhouse-client.client_hostname (String) — Hostname of the client machine where the clickhouse-client or another TCP client is run.client_name (String) — The clickhouse-client or another TCP client name.client_revision (UInt32) — Revision of the clickhouse-client or another TCP client.client_version_major (UInt32) — Major version of the clickhouse-client or another TCP client.client_version_minor (UInt32) — Minor version of the clickhouse-client or another TCP client.client_version_patch (UInt32) — Patch component of the clickhouse-client or another TCP client version.http_method (UInt8) — HTTP method that initiated the query. Possible values: 0 — The query was launched from the TCP interface.1 — GET method was used.2 — POST method was used. http_user_agent (String) — The UserAgent header passed in the HTTP request.quota_key (String) — The “quota key” specified in the quotas setting (see keyed).revision (UInt32) — ClickHouse revision.ProfileEvents (Map(String, UInt64)) — ProfileEvents that measure different metrics for this thread. The description of them could be found in the table system.events. Example SELECT * FROM system.query_thread_log LIMIT 1 \\G Row 1: ────── event_date: 2020-09-11 event_time: 2020-09-11 10:08:17 event_time_microseconds: 2020-09-11 10:08:17.134042 query_start_time: 2020-09-11 10:08:17 query_start_time_microseconds: 2020-09-11 10:08:17.063150 query_duration_ms: 70 read_rows: 0 read_bytes: 0 written_rows: 1 written_bytes: 12 memory_usage: 4300844 peak_memory_usage: 4300844 thread_name: TCPHandler thread_id: 638133 master_thread_id: 638133 query: INSERT INTO test1 VALUES is_initial_query: 1 user: default query_id: 50a320fd-85a8-49b8-8761-98a86bcbacef address: ::ffff:127.0.0.1 port: 33452 initial_user: default initial_query_id: 50a320fd-85a8-49b8-8761-98a86bcbacef initial_address: ::ffff:127.0.0.1 initial_port: 33452 interface: 1 os_user: bharatnc client_hostname: tower client_name: ClickHouse client_revision: 54437 client_version_major: 20 client_version_minor: 7 client_version_patch: 2 http_method: 0 http_user_agent: quota_key: revision: 54440 ProfileEvents: {'Query':1,'SelectQuery':1,'ReadCompressedBytes':36,'CompressedReadBufferBlocks':1,'CompressedReadBufferBytes':10,'IOBufferAllocs':1,'IOBufferAllocBytes':89,'ContextLock':15,'RWLockAcquiredReadLocks':1} See Also system.query_log — Description of the query_log system table which contains common information about queries execution.system.query_views_log — This table contains information about each view executed during a query. Original article","keywords":""},{"title":"query_views_log","type":0,"sectionRef":"#","url":"en/operations/system-tables/query_views_log","content":"query_views_log Contains information about the dependent views executed when running a query, for example, the view type or the execution time. To start logging: Configure parameters in the query_views_log section.Set log_query_views to 1. The flushing period of data is set in flush_interval_milliseconds parameter of the query_views_log server settings section. To force flushing, use the SYSTEM FLUSH LOGS query. ClickHouse does not delete data from the table automatically. See Introduction for more details. You can use the log_queries_probability setting to reduce the number of queries, registered in the query_views_log table. Columns: event_date (Date) — The date when the last event of the view happened.event_time (DateTime) — The date and time when the view finished execution.event_time_microseconds (DateTime) — The date and time when the view finished execution with microseconds precision.view_duration_ms (UInt64) — Duration of view execution (sum of its stages) in milliseconds.initial_query_id (String) — ID of the initial query (for distributed query execution).view_name (String) — Name of the view.view_uuid (UUID) — UUID of the view.view_type (Enum8) — Type of the view. Values: 'Default' = 1 — Default views. Should not appear in this log.'Materialized' = 2 — Materialized views.'Live' = 3 — Live views. view_query (String) — The query executed by the view.view_target (String) — The name of the view target table.read_rows (UInt64) — Number of read rows.read_bytes (UInt64) — Number of read bytes.written_rows (UInt64) — Number of written rows.written_bytes (UInt64) — Number of written bytes.peak_memory_usage (Int64) — The maximum difference between the amount of allocated and freed memory in context of this view.ProfileEvents (Map(String, UInt64)) — ProfileEvents that measure different metrics. The description of them could be found in the table system.events.status (Enum8) — Status of the view. Values: 'QueryStart' = 1 — Successful start the view execution. Should not appear.'QueryFinish' = 2 — Successful end of the view execution.'ExceptionBeforeStart' = 3 — Exception before the start of the view execution.'ExceptionWhileProcessing' = 4 — Exception during the view execution. exception_code (Int32) — Code of an exception.exception (String) — Exception message.stack_trace (String) — Stack trace. An empty string, if the query was completed successfully. Example Query: SELECT * FROM system.query_views_log LIMIT 1 \\G; Result: Row 1: ────── event_date: 2021-06-22 event_time: 2021-06-22 13:23:07 event_time_microseconds: 2021-06-22 13:23:07.738221 view_duration_ms: 0 initial_query_id: c3a1ac02-9cad-479b-af54-9e9c0a7afd70 view_name: default.matview_inner view_uuid: 00000000-0000-0000-0000-000000000000 view_type: Materialized view_query: SELECT * FROM default.table_b view_target: default.`.inner.matview_inner` read_rows: 4 read_bytes: 64 written_rows: 2 written_bytes: 32 peak_memory_usage: 4196188 ProfileEvents: {'FileOpen':2,'WriteBufferFromFileDescriptorWrite':2,'WriteBufferFromFileDescriptorWriteBytes':187,'IOBufferAllocs':3,'IOBufferAllocBytes':3145773,'FunctionExecute':3,'DiskWriteElapsedMicroseconds':13,'InsertedRows':2,'InsertedBytes':16,'SelectedRows':4,'SelectedBytes':48,'ContextLock':16,'RWLockAcquiredReadLocks':1,'RealTimeMicroseconds':698,'SoftPageFaults':4,'OSReadChars':463} status: QueryFinish exception_code: 0 exception: stack_trace: See Also system.query_log — Description of the query_log system table which contains common information about queries execution.system.query_thread_log — This table contains information about each query execution thread. Original article","keywords":""},{"title":"quotas","type":0,"sectionRef":"#","url":"en/operations/system-tables/quotas","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"quotas","url":"en/operations/system-tables/quotas#see-also","content":"SHOW QUOTAS Original article "},{"title":"replicated_fetches","type":0,"sectionRef":"#","url":"en/operations/system-tables/replicated_fetches","content":"replicated_fetches Contains information about currently running background fetches. Columns: database (String) — Name of the database. table (String) — Name of the table. elapsed (Float64) — The time elapsed (in seconds) since showing currently running background fetches started. progress (Float64) — The percentage of completed work from 0 to 1. result_part_name (String) — The name of the part that will be formed as the result of showing currently running background fetches. result_part_path (String) — Absolute path to the part that will be formed as the result of showing currently running background fetches. partition_id (String) — ID of the partition. total_size_bytes_compressed (UInt64) — The total size (in bytes) of the compressed data in the result part. bytes_read_compressed (UInt64) — The number of compressed bytes read from the result part. source_replica_path (String) — Absolute path to the source replica. source_replica_hostname (String) — Hostname of the source replica. source_replica_port (UInt16) — Port number of the source replica. interserver_scheme (String) — Name of the interserver scheme. URI (String) — Uniform resource identifier. to_detached (UInt8) — The flag indicates whether the currently running background fetch is being performed using the TO DETACHED expression. thread_id (UInt64) — Thread identifier. Example SELECT * FROM system.replicated_fetches LIMIT 1 FORMAT Vertical; Row 1: ────── database: default table: t elapsed: 7.243039876 progress: 0.41832135995612835 result_part_name: all_0_0_0 result_part_path: /var/lib/clickhouse/store/700/70080a04-b2de-4adf-9fa5-9ea210e81766/all_0_0_0/ partition_id: all total_size_bytes_compressed: 1052783726 bytes_read_compressed: 440401920 source_replica_path: /clickhouse/test/t/replicas/1 source_replica_hostname: node1 source_replica_port: 9009 interserver_scheme: http URI: http://node1:9009/?endpoint=DataPartsExchange%3A%2Fclickhouse%2Ftest%2Ft%2Freplicas%2F1&amp;part=all_0_0_0&amp;client_protocol_version=4&amp;compress=false to_detached: 0 thread_id: 54 See Also Managing ReplicatedMergeTree Tables Original article","keywords":""},{"title":"quota_usage","type":0,"sectionRef":"#","url":"en/operations/system-tables/quota_usage","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"quota_usage","url":"en/operations/system-tables/quota_usage#see-also","content":"SHOW QUOTA Original article "},{"title":"events","type":0,"sectionRef":"#","url":"en/operations/system-tables/events","content":"events Contains information about the number of events that have occurred in the system. For example, in the table, you can find how many SELECT queries were processed since the ClickHouse server started. Columns: event (String) — Event name.value (UInt64) — Number of events occurred.description (String) — Event description. Example SELECT * FROM system.events LIMIT 5 ┌─event─────────────────────────────────┬─value─┬─description────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ Query │ 12 │ Number of queries to be interpreted and potentially executed. Does not include queries that failed to parse or were rejected due to AST size limits, quota limits or limits on the number of simultaneously running queries. May include internal queries initiated by ClickHouse itself. Does not count subqueries. │ │ SelectQuery │ 8 │ Same as Query, but only for SELECT queries. │ │ FileOpen │ 73 │ Number of files opened. │ │ ReadBufferFromFileDescriptorRead │ 155 │ Number of reads (read/pread) from a file descriptor. Does not include sockets. │ │ ReadBufferFromFileDescriptorReadBytes │ 9931 │ Number of bytes read from file descriptors. If the file is compressed, this will show the compressed data size. │ └───────────────────────────────────────┴───────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ See Also system.asynchronous_metrics — Contains periodically calculated metrics.system.metrics — Contains instantly calculated metrics.system.metric_log — Contains a history of metrics values from tables system.metrics и system.events.Monitoring — Base concepts of ClickHouse monitoring. Original article","keywords":""},{"title":"replication_queue","type":0,"sectionRef":"#","url":"en/operations/system-tables/replication_queue","content":"replication_queue Contains information about tasks from replication queues stored in ZooKeeper for tables in the ReplicatedMergeTree family. Columns: database (String) — Name of the database. table (String) — Name of the table. replica_name (String) — Replica name in ZooKeeper. Different replicas of the same table have different names. position (UInt32) — Position of the task in the queue. node_name (String) — Node name in ZooKeeper. type (String) — Type of the task in the queue, one of: GET_PART — Get the part from another replica.ATTACH_PART — Attach the part, possibly from our own replica (if found in the detached folder). You may think of it as a GET_PART with some optimizations as they're nearly identical.MERGE_PARTS — Merge the parts.DROP_RANGE — Delete the parts in the specified partition in the specified number range.CLEAR_COLUMN — NOTE: Deprecated. Drop specific column from specified partition.CLEAR_INDEX — NOTE: Deprecated. Drop specific index from specified partition.REPLACE_RANGE — Drop a certain range of parts and replace them with new ones.MUTATE_PART — Apply one or several mutations to the part.ALTER_METADATA — Apply alter modification according to global /metadata and /columns paths. create_time (Datetime) — Date and time when the task was submitted for execution. required_quorum (UInt32) — The number of replicas waiting for the task to complete with confirmation of completion. This column is only relevant for the GET_PARTS task. source_replica (String) — Name of the source replica. new_part_name (String) — Name of the new part. parts_to_merge (Array (String)) — Names of parts to merge or update. is_detach (UInt8) — The flag indicates whether the DETACH_PARTS task is in the queue. is_currently_executing (UInt8) — The flag indicates whether a specific task is being performed right now. num_tries (UInt32) — The number of failed attempts to complete the task. last_exception (String) — Text message about the last error that occurred (if any). last_attempt_time (Datetime) — Date and time when the task was last attempted. num_postponed (UInt32) — The number of postponed tasks. postpone_reason (String) — The reason why the task was postponed. last_postpone_time (Datetime) — Date and time when the task was last postponed. merge_type (String) — Type of the current merge. Empty if it's a mutation. Example SELECT * FROM system.replication_queue LIMIT 1 FORMAT Vertical; Row 1: ────── database: merge table: visits_v2 replica_name: mtgiga001-1t position: 15 node_name: queue-0009325559 type: MERGE_PARTS create_time: 2020-12-07 14:04:21 required_quorum: 0 source_replica: mtgiga001-1t new_part_name: 20201130_121373_121384_2 parts_to_merge: ['20201130_121373_121378_1','20201130_121379_121379_0','20201130_121380_121380_0','20201130_121381_121381_0','20201130_121382_121382_0','20201130_121383_121383_0','20201130_121384_121384_0'] is_detach: 0 is_currently_executing: 0 num_tries: 36 last_exception: Code: 226, e.displayText() = DB::Exception: Marks file '/opt/clickhouse/data/merge/visits_v2/tmp_fetch_20201130_121373_121384_2/CounterID.mrk' does not exist (version 20.8.7.15 (official build)) last_attempt_time: 2020-12-08 17:35:54 num_postponed: 0 postpone_reason: last_postpone_time: 1970-01-01 03:00:00 See Also Managing ReplicatedMergeTree Tables Original article","keywords":""},{"title":"part_log","type":0,"sectionRef":"#","url":"en/operations/system-tables/part_log","content":"part_log The system.part_log table is created only if the part_log server setting is specified. This table contains information about events that occurred with data parts in the MergeTree family tables, such as adding or merging data. The system.part_log table contains the following columns: query_id (String) — Identifier of the INSERT query that created this data part.event_type (Enum8) — Type of the event that occurred with the data part. Can have one of the following values: NEW_PART — Inserting of a new data part.MERGE_PARTS — Merging of data parts.DOWNLOAD_PART — Downloading a data part.REMOVE_PART — Removing or detaching a data part using DETACH PARTITION.MUTATE_PART — Mutating of a data part.MOVE_PART — Moving the data part from the one disk to another one. event_date (Date) — Event date.event_time (DateTime) — Event time.event_time_microseconds (DateTime64) — Event time with microseconds precision.duration_ms (UInt64) — Duration.database (String) — Name of the database the data part is in.table (String) — Name of the table the data part is in.part_name (String) — Name of the data part.partition_id (String) — ID of the partition that the data part was inserted to. The column takes the all value if the partitioning is by tuple().path_on_disk (String) — Absolute path to the folder with data part files.rows (UInt64) — The number of rows in the data part.size_in_bytes (UInt64) — Size of the data part in bytes.merged_from (Array(String)) — An array of names of the parts which the current part was made up from (after the merge).bytes_uncompressed (UInt64) — Size of uncompressed bytes.read_rows (UInt64) — The number of rows was read during the merge.read_bytes (UInt64) — The number of bytes was read during the merge.peak_memory_usage (Int64) — The maximum difference between the amount of allocated and freed memory in context of this thread.error (UInt16) — The code number of the occurred error.exception (String) — Text message of the occurred error. The system.part_log table is created after the first inserting data to the MergeTree table. Example SELECT * FROM system.part_log LIMIT 1 FORMAT Vertical; Row 1: ────── query_id: 983ad9c7-28d5-4ae1-844e-603116b7de31 event_type: NewPart event_date: 2021-02-02 event_time: 2021-02-02 11:14:28 event_time_microseconds: 2021-02-02 11:14:28.861919 duration_ms: 35 database: default table: log_mt_2 part_name: all_1_1_0 partition_id: all path_on_disk: db/data/default/log_mt_2/all_1_1_0/ rows: 115418 size_in_bytes: 1074311 merged_from: [] bytes_uncompressed: 0 read_rows: 0 read_bytes: 0 peak_memory_usage: 0 error: 0 exception: Original article","keywords":""},{"title":"role_grants","type":0,"sectionRef":"#","url":"en/operations/system-tables/role-grants","content":"role_grants Contains the role grants for users and roles. To add entries to this table, use GRANT role TO user. Columns: user_name (Nullable(String)) — User name. role_name (Nullable(String)) — Role name. granted_role_name (String) — Name of role granted to the role_name role. To grant one role to another one use GRANT role1 TO role2. granted_role_is_default (UInt8) — Flag that shows whether granted_role is a default role. Possible values: 1 — granted_role is a default role.0 — granted_role is not a default role. with_admin_option (UInt8) — Flag that shows whether granted_role is a role with ADMIN OPTION privilege. Possible values: 1 — The role has ADMIN OPTION privilege.0 — The role without ADMIN OPTION privilege. Original article","keywords":""},{"title":"INFORMATION_SCHEMA","type":0,"sectionRef":"#","url":"en/operations/system-tables/information_schema","content":"","keywords":""},{"title":"COLUMNS​","type":1,"pageTitle":"INFORMATION_SCHEMA","url":"en/operations/system-tables/information_schema#columns","content":"Contains columns read from the system.columns system table and columns that are not supported in ClickHouse or do not make sense (always NULL), but must be by the standard. Columns: table_catalog (String) — The name of the database in which the table is located.table_schema (String) — The name of the database in which the table is located.table_name (String) — Table name.column_name (String) — Column name.ordinal_position (UInt64) — Ordinal position of a column in a table starting with 1.column_default (String) — Expression for the default value, or an empty string if it is not defined.is_nullable (UInt8) — Flag that indicates whether the column type is Nullable.data_type (String) — Column type.character_maximum_length (Nullable(UInt64)) — Maximum length in bytes for binary data, character data, or text data and images. In ClickHouse makes sense only for FixedString data type. Otherwise, the NULL value is returned.character_octet_length (Nullable(UInt64)) — Maximum length in bytes for binary data, character data, or text data and images. In ClickHouse makes sense only for FixedString data type. Otherwise, the NULL value is returned.numeric_precision (Nullable(UInt64)) — Accuracy of approximate numeric data, exact numeric data, integer data, or monetary data. In ClickHouse it is bitness for integer types and decimal precision for Decimal types. Otherwise, the NULL value is returned.numeric_precision_radix (Nullable(UInt64)) — The base of the number system is the accuracy of approximate numeric data, exact numeric data, integer data or monetary data. In ClickHouse it's 2 for integer types and 10 for Decimal types. Otherwise, the NULL value is returned.numeric_scale (Nullable(UInt64)) — The scale of approximate numeric data, exact numeric data, integer data, or monetary data. In ClickHouse makes sense only for Decimal types. Otherwise, the NULL value is returned.datetime_precision (Nullable(UInt64)) — Decimal precision of DateTime64 data type. For other data types, the NULL value is returned.character_set_catalog (Nullable(String)) — NULL, not supported.character_set_schema (Nullable(String)) — NULL, not supported.character_set_name (Nullable(String)) — NULL, not supported.collation_catalog (Nullable(String)) — NULL, not supported.collation_schema (Nullable(String)) — NULL, not supported.collation_name (Nullable(String)) — NULL, not supported.domain_catalog (Nullable(String)) — NULL, not supported.domain_schema (Nullable(String)) — NULL, not supported.domain_name (Nullable(String)) — NULL, not supported. Example Query: SELECT * FROM INFORMATION_SCHEMA.COLUMNS WHERE (table_schema=currentDatabase() OR table_schema='') AND table_name NOT LIKE '%inner%' LIMIT 1 FORMAT Vertical;  Result: Row 1: ────── table_catalog: default table_schema: default table_name: describe_example column_name: id ordinal_position: 1 column_default: is_nullable: 0 data_type: UInt64 character_maximum_length: ᴺᵁᴸᴸ character_octet_length: ᴺᵁᴸᴸ numeric_precision: 64 numeric_precision_radix: 2 numeric_scale: 0 datetime_precision: ᴺᵁᴸᴸ character_set_catalog: ᴺᵁᴸᴸ character_set_schema: ᴺᵁᴸᴸ character_set_name: ᴺᵁᴸᴸ collation_catalog: ᴺᵁᴸᴸ collation_schema: ᴺᵁᴸᴸ collation_name: ᴺᵁᴸᴸ domain_catalog: ᴺᵁᴸᴸ domain_schema: ᴺᵁᴸᴸ domain_name: ᴺᵁᴸᴸ  "},{"title":"SCHEMATA​","type":1,"pageTitle":"INFORMATION_SCHEMA","url":"en/operations/system-tables/information_schema#schemata","content":"Contains columns read from the system.databases system table and columns that are not supported in ClickHouse or do not make sense (always NULL), but must be by the standard. Columns: catalog_name (String) — The name of the database.schema_name (String) — The name of the database.schema_owner (String) — Schema owner name, always 'default'.default_character_set_catalog (Nullable(String)) — NULL, not supported.default_character_set_schema (Nullable(String)) — NULL, not supported.default_character_set_name (Nullable(String)) — NULL, not supported.sql_path (Nullable(String)) — NULL, not supported. Example Query: SELECT * FROM information_schema.schemata WHERE schema_name ILIKE 'information_schema' LIMIT 1 FORMAT Vertical;  Result: Row 1: ────── catalog_name: INFORMATION_SCHEMA schema_name: INFORMATION_SCHEMA schema_owner: default default_character_set_catalog: ᴺᵁᴸᴸ default_character_set_schema: ᴺᵁᴸᴸ default_character_set_name: ᴺᵁᴸᴸ sql_path: ᴺᵁᴸᴸ  "},{"title":"TABLES​","type":1,"pageTitle":"INFORMATION_SCHEMA","url":"en/operations/system-tables/information_schema#tables","content":"Contains columns read from the system.tables system table. Columns: table_catalog (String) — The name of the database in which the table is located.table_schema (String) — The name of the database in which the table is located.table_name (String) — Table name.table_type (Enum8) — Table type. Possible values: BASE TABLEVIEWFOREIGN TABLELOCAL TEMPORARYSYSTEM VIEW Example Query: SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE (table_schema = currentDatabase() OR table_schema = '') AND table_name NOT LIKE '%inner%' LIMIT 1 FORMAT Vertical;  Result: Row 1: ────── table_catalog: default table_schema: default table_name: describe_example table_type: BASE TABLE  "},{"title":"VIEWS​","type":1,"pageTitle":"INFORMATION_SCHEMA","url":"en/operations/system-tables/information_schema#views","content":"Contains columns read from the system.tables system table, when the table engine View is used. Columns: table_catalog (String) — The name of the database in which the table is located.table_schema (String) — The name of the database in which the table is located.table_name (String) — Table name.view_definition (String) — SELECT query for view.check_option (String) — NONE, no checking.is_updatable (Enum8) — NO, the view is not updated.is_insertable_into (Enum8) — Shows whether the created view is materialized. Possible values: NO — The created view is not materialized.YES — The created view is materialized. is_trigger_updatable (Enum8) — NO, the trigger is not updated.is_trigger_deletable (Enum8) — NO, the trigger is not deleted.is_trigger_insertable_into (Enum8) — NO, no data is inserted into the trigger. Example Query: CREATE VIEW v (n Nullable(Int32), f Float64) AS SELECT n, f FROM t; CREATE MATERIALIZED VIEW mv ENGINE = Null AS SELECT * FROM system.one; SELECT * FROM information_schema.views WHERE table_schema = currentDatabase() LIMIT 1 FORMAT Vertical;  Result: Row 1: ────── table_catalog: default table_schema: default table_name: mv view_definition: SELECT * FROM system.one check_option: NONE is_updatable: NO is_insertable_into: YES is_trigger_updatable: NO is_trigger_deletable: NO is_trigger_insertable_into: NO  "},{"title":"replicas","type":0,"sectionRef":"#","url":"en/operations/system-tables/replicas","content":"replicas Contains information and status for replicated tables residing on the local server. This table can be used for monitoring. The table contains a row for every Replicated* table. Example: SELECT * FROM system.replicas WHERE table = 'test_table' FORMAT Vertical Query id: dc6dcbcb-dc28-4df9-ae27-4354f5b3b13e Row 1: ─────── database: db table: test_table engine: ReplicatedMergeTree is_leader: 1 can_become_leader: 1 is_readonly: 0 is_session_expired: 0 future_parts: 0 parts_to_check: 0 zookeeper_path: /test/test_table replica_name: r1 replica_path: /test/test_table/replicas/r1 columns_version: -1 queue_size: 27 inserts_in_queue: 27 merges_in_queue: 0 part_mutations_in_queue: 0 queue_oldest_time: 2021-10-12 14:48:48 inserts_oldest_time: 2021-10-12 14:48:48 merges_oldest_time: 1970-01-01 03:00:00 part_mutations_oldest_time: 1970-01-01 03:00:00 oldest_part_to_get: 1_17_17_0 oldest_part_to_merge_to: oldest_part_to_mutate_to: log_max_index: 206 log_pointer: 207 last_queue_update: 2021-10-12 14:50:08 absolute_delay: 99 total_replicas: 5 active_replicas: 5 last_queue_update_exception: zookeeper_exception: replica_is_active: {'r1':1,'r2':1} Columns: database (String) - Database nametable (String) - Table nameengine (String) - Table engine nameis_leader (UInt8) - Whether the replica is the leader. Multiple replicas can be leaders at the same time. A replica can be prevented from becoming a leader using the merge_tree setting replicated_can_become_leader. The leaders are responsible for scheduling background merges. Note that writes can be performed to any replica that is available and has a session in ZK, regardless of whether it is a leader.can_become_leader (UInt8) - Whether the replica can be a leader.is_readonly (UInt8) - Whether the replica is in read-only mode. This mode is turned on if the config does not have sections with ZooKeeper, if an unknown error occurred when reinitializing sessions in ZooKeeper, and during session reinitialization in ZooKeeper.is_session_expired (UInt8) - the session with ZooKeeper has expired. Basically the same as is_readonly.future_parts (UInt32) - The number of data parts that will appear as the result of INSERTs or merges that haven’t been done yet.parts_to_check (UInt32) - The number of data parts in the queue for verification. A part is put in the verification queue if there is suspicion that it might be damaged.zookeeper_path (String) - Path to table data in ZooKeeper.replica_name (String) - Replica name in ZooKeeper. Different replicas of the same table have different names.replica_path (String) - Path to replica data in ZooKeeper. The same as concatenating ‘zookeeper_path/replicas/replica_path’.columns_version (Int32) - Version number of the table structure. Indicates how many times ALTER was performed. If replicas have different versions, it means some replicas haven’t made all of the ALTERs yet.queue_size (UInt32) - Size of the queue for operations waiting to be performed. Operations include inserting blocks of data, merges, and certain other actions. It usually coincides with future_parts.inserts_in_queue (UInt32) - Number of inserts of blocks of data that need to be made. Insertions are usually replicated fairly quickly. If this number is large, it means something is wrong.merges_in_queue (UInt32) - The number of merges waiting to be made. Sometimes merges are lengthy, so this value may be greater than zero for a long time.part_mutations_in_queue (UInt32) - The number of mutations waiting to be made.queue_oldest_time (DateTime) - If queue_size greater than 0, shows when the oldest operation was added to the queue.inserts_oldest_time (DateTime) - See queue_oldest_timemerges_oldest_time (DateTime) - See queue_oldest_timepart_mutations_oldest_time (DateTime) - See queue_oldest_time The next 4 columns have a non-zero value only where there is an active session with ZK. log_max_index (UInt64) - Maximum entry number in the log of general activity.log_pointer (UInt64) - Maximum entry number in the log of general activity that the replica copied to its execution queue, plus one. If log_pointer is much smaller than log_max_index, something is wrong.last_queue_update (DateTime) - When the queue was updated last time.absolute_delay (UInt64) - How big lag in seconds the current replica has.total_replicas (UInt8) - The total number of known replicas of this table.active_replicas (UInt8) - The number of replicas of this table that have a session in ZooKeeper (i.e., the number of functioning replicas).last_queue_update_exception (String) - When the queue contains broken entries. Especially important when ClickHouse breaks backward compatibility between versions and log entries written by newer versions aren't parseable by old versions.zookeeper_exception (String) - The last exception message, got if the error happened when fetching the info from ZooKeeper. replica_is_active (Map(String, UInt8)) — Map between replica name and is replica active. If you request all the columns, the table may work a bit slowly, since several reads from ZooKeeper are made for each row. If you do not request the last 4 columns (log_max_index, log_pointer, total_replicas, active_replicas), the table works quickly. For example, you can check that everything is working correctly like this: SELECT database, table, is_leader, is_readonly, is_session_expired, future_parts, parts_to_check, columns_version, queue_size, inserts_in_queue, merges_in_queue, log_max_index, log_pointer, total_replicas, active_replicas FROM system.replicas WHERE is_readonly OR is_session_expired OR future_parts &gt; 20 OR parts_to_check &gt; 10 OR queue_size &gt; 20 OR inserts_in_queue &gt; 10 OR log_max_index - log_pointer &gt; 10 OR total_replicas &lt; 2 OR active_replicas &lt; total_replicas If this query does not return anything, it means that everything is fine. Original article","keywords":""},{"title":"query_log","type":0,"sectionRef":"#","url":"en/operations/system-tables/query_log","content":"query_log Contains information about executed queries, for example, start time, duration of processing, error messages. note This table does not contain the ingested data for INSERT queries. You can change settings of queries logging in the query_log section of the server configuration. You can disable queries logging by setting log_queries = 0. We do not recommend to turn off logging because information in this table is important for solving issues. The flushing period of data is set in flush_interval_milliseconds parameter of the query_log server settings section. To force flushing, use the SYSTEM FLUSH LOGS query. ClickHouse does not delete data from the table automatically. See Introduction for more details. The system.query_log table registers two kinds of queries: Initial queries that were run directly by the client.Child queries that were initiated by other queries (for distributed query execution). For these types of queries, information about the parent queries is shown in the initial_* columns. Each query creates one or two rows in the query_log table, depending on the status (see the type column) of the query: If the query execution was successful, two rows with the QueryStart and QueryFinish types are created.If an error occurred during query processing, two events with the QueryStart and ExceptionWhileProcessing types are created.If an error occurred before launching the query, a single event with the ExceptionBeforeStart type is created. You can use the log_queries_probability setting to reduce the number of queries, registered in the query_log table. You can use the log_formatted_queries setting to log formatted queries to the formatted_query column. Columns: type (Enum8) — Type of an event that occurred when executing the query. Values: 'QueryStart' = 1 — Successful start of query execution.'QueryFinish' = 2 — Successful end of query execution.'ExceptionBeforeStart' = 3 — Exception before the start of query execution.'ExceptionWhileProcessing' = 4 — Exception during the query execution. event_date (Date) — Query starting date.event_time (DateTime) — Query starting time.event_time_microseconds (DateTime) — Query starting time with microseconds precision.query_start_time (DateTime) — Start time of query execution.query_start_time_microseconds (DateTime64) — Start time of query execution with microsecond precision.query_duration_ms (UInt64) — Duration of query execution in milliseconds.read_rows (UInt64) — Total number of rows read from all tables and table functions participated in query. It includes usual subqueries, subqueries for IN and JOIN. For distributed queries read_rows includes the total number of rows read at all replicas. Each replica sends it’s read_rows value, and the server-initiator of the query summarizes all received and local values. The cache volumes do not affect this value.read_bytes (UInt64) — Total number of bytes read from all tables and table functions participated in query. It includes usual subqueries, subqueries for IN and JOIN. For distributed queries read_bytes includes the total number of rows read at all replicas. Each replica sends it’s read_bytes value, and the server-initiator of the query summarizes all received and local values. The cache volumes do not affect this value.written_rows (UInt64) — For INSERT queries, the number of written rows. For other queries, the column value is 0.written_bytes (UInt64) — For INSERT queries, the number of written bytes. For other queries, the column value is 0.result_rows (UInt64) — Number of rows in a result of the SELECT query, or a number of rows in the INSERT query.result_bytes (UInt64) — RAM volume in bytes used to store a query result.memory_usage (UInt64) — Memory consumption by the query.current_database (String) — Name of the current database.query (String) — Query string.formatted_query (String) — Formatted query string.normalized_query_hash (UInt64) — Identical hash value without the values of literals for similar queries.query_kind (LowCardinality(String)) — Type of the query.databases (Array(LowCardinality(String))) — Names of the databases present in the query.tables (Array(LowCardinality(String))) — Names of the tables present in the query.views (Array(LowCardinality(String))) — Names of the (materialized or live) views present in the query.columns (Array(LowCardinality(String))) — Names of the columns present in the query.projections (String) — Names of the projections used during the query execution.exception_code (Int32) — Code of an exception.exception (String) — Exception message.stack_trace (String) — Stack trace. An empty string, if the query was completed successfully.is_initial_query (UInt8) — Query type. Possible values: 1 — Query was initiated by the client.0 — Query was initiated by another query as part of distributed query execution. user (String) — Name of the user who initiated the current query.query_id (String) — ID of the query.address (IPv6) — IP address that was used to make the query.port (UInt16) — The client port that was used to make the query.initial_user (String) — Name of the user who ran the initial query (for distributed query execution).initial_query_id (String) — ID of the initial query (for distributed query execution).initial_address (IPv6) — IP address that the parent query was launched from.initial_port (UInt16) — The client port that was used to make the parent query.initial_query_start_time (DateTime) — Initial query starting time (for distributed query execution).initial_query_start_time_microseconds (DateTime64) — Initial query starting time with microseconds precision (for distributed query execution).interface (UInt8) — Interface that the query was initiated from. Possible values: 1 — TCP.2 — HTTP. os_user (String) — Operating system username who runs clickhouse-client.client_hostname (String) — Hostname of the client machine where the clickhouse-client or another TCP client is run.client_name (String) — The clickhouse-client or another TCP client name.client_revision (UInt32) — Revision of the clickhouse-client or another TCP client.client_version_major (UInt32) — Major version of the clickhouse-client or another TCP client.client_version_minor (UInt32) — Minor version of the clickhouse-client or another TCP client.client_version_patch (UInt32) — Patch component of the clickhouse-client or another TCP client version.http_method (UInt8) — HTTP method that initiated the query. Possible values: 0 — The query was launched from the TCP interface.1 — GET method was used.2 — POST method was used. http_user_agent (String) — HTTP header UserAgent passed in the HTTP query.http_referer (String) — HTTP header Referer passed in the HTTP query (contains an absolute or partial address of the page making the query).forwarded_for (String) — HTTP header X-Forwarded-For passed in the HTTP query.quota_key (String) — The quota key specified in the quotas setting (see keyed).revision (UInt32) — ClickHouse revision.ProfileEvents (Map(String, UInt64)) — ProfileEvents that measure different metrics. The description of them could be found in the table system.eventsSettings (Map(String, String)) — Settings that were changed when the client ran the query. To enable logging changes to settings, set the log_query_settings parameter to 1.log_comment (String) — Log comment. It can be set to arbitrary string no longer than max_query_size. An empty string if it is not defined.thread_ids (Array(UInt64)) — Thread ids that are participating in query execution.used_aggregate_functions (Array(String)) — Canonical names of aggregate functions, which were used during query execution.used_aggregate_function_combinators (Array(String)) — Canonical names of aggregate functions combinators, which were used during query execution.used_database_engines (Array(String)) — Canonical names of database engines, which were used during query execution.used_data_type_families (Array(String)) — Canonical names of data type families, which were used during query execution.used_dictionaries (Array(String)) — Canonical names of dictionaries, which were used during query execution.used_formats (Array(String)) — Canonical names of formats, which were used during query execution.used_functions (Array(String)) — Canonical names of functions, which were used during query execution.used_storages (Array(String)) — Canonical names of storages, which were used during query execution.used_table_functions (Array(String)) — Canonical names of table functions, which were used during query execution. Example SELECT * FROM system.query_log WHERE type = 'QueryFinish' ORDER BY query_start_time DESC LIMIT 1 FORMAT Vertical; Row 1: ────── type: QueryFinish event_date: 2021-11-03 event_time: 2021-11-03 16:13:54 event_time_microseconds: 2021-11-03 16:13:54.953024 query_start_time: 2021-11-03 16:13:54 query_start_time_microseconds: 2021-11-03 16:13:54.952325 query_duration_ms: 0 read_rows: 69 read_bytes: 6187 written_rows: 0 written_bytes: 0 result_rows: 69 result_bytes: 48256 memory_usage: 0 current_database: default query: DESCRIBE TABLE system.query_log formatted_query: normalized_query_hash: 8274064835331539124 query_kind: databases: [] tables: [] columns: [] projections: [] views: [] exception_code: 0 exception: stack_trace: is_initial_query: 1 user: default query_id: 7c28bbbb-753b-4eba-98b1-efcbe2b9bdf6 address: ::ffff:127.0.0.1 port: 40452 initial_user: default initial_query_id: 7c28bbbb-753b-4eba-98b1-efcbe2b9bdf6 initial_address: ::ffff:127.0.0.1 initial_port: 40452 initial_query_start_time: 2021-11-03 16:13:54 initial_query_start_time_microseconds: 2021-11-03 16:13:54.952325 interface: 1 os_user: sevirov client_hostname: clickhouse.ru-central1.internal client_name: ClickHouse client_revision: 54449 client_version_major: 21 client_version_minor: 10 client_version_patch: 1 http_method: 0 http_user_agent: http_referer: forwarded_for: quota_key: revision: 54456 log_comment: thread_ids: [30776,31174] ProfileEvents: {'Query':1,'NetworkSendElapsedMicroseconds':59,'NetworkSendBytes':2643,'SelectedRows':69,'SelectedBytes':6187,'ContextLock':9,'RWLockAcquiredReadLocks':1,'RealTimeMicroseconds':817,'UserTimeMicroseconds':427,'SystemTimeMicroseconds':212,'OSCPUVirtualTimeMicroseconds':639,'OSReadChars':894,'OSWriteChars':319} Settings: {'load_balancing':'random','max_memory_usage':'10000000000'} used_aggregate_functions: [] used_aggregate_function_combinators: [] used_database_engines: [] used_data_type_families: [] used_dictionaries: [] used_formats: [] used_functions: [] used_storages: [] used_table_functions: [] See Also system.query_thread_log — This table contains information about each query execution thread.","keywords":""},{"title":"roles","type":0,"sectionRef":"#","url":"en/operations/system-tables/roles","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"roles","url":"en/operations/system-tables/roles#see-also","content":"SHOW ROLES Original article "},{"title":"row_policies","type":0,"sectionRef":"#","url":"en/operations/system-tables/row_policies","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"row_policies","url":"en/operations/system-tables/row_policies#see-also","content":"SHOW POLICIES Original article "},{"title":"session_log","type":0,"sectionRef":"#","url":"en/operations/system-tables/session_log","content":"session_log Contains information about all successful and failed login and logout events. Columns: type (Enum8) — Login/logout result. Possible values: LoginFailure — Login error.LoginSuccess — Successful login.Logout — Logout from the system. auth_id (UUID) — Authentication ID, which is a UUID that is automatically generated each time user logins.session_id (String) — Session ID that is passed by client via HTTP interface.event_date (Date) — Login/logout date.event_time (DateTime) — Login/logout time.event_time_microseconds (DateTime64) — Login/logout starting time with microseconds precision.user (String) — User name.auth_type (Enum8) — The authentication type. Possible values: NO_PASSWORDPLAINTEXT_PASSWORDSHA256_PASSWORDDOUBLE_SHA1_PASSWORDLDAPKERBEROS profiles (Array(LowCardinality(String))) — The list of profiles set for all roles and/or users.roles (Array(LowCardinality(String))) — The list of roles to which the profile is applied.settings (Array(Tuple(LowCardinality(String), String))) — Settings that were changed when the client logged in/out.client_address (IPv6) — The IP address that was used to log in/out.client_port (UInt16) — The client port that was used to log in/out.interface (Enum8) — The interface from which the login was initiated. Possible values: TCPHTTPgRPCMySQLPostgreSQL client_hostname (String) — The hostname of the client machine where the clickhouse-client or another TCP client is run.client_name (String) — The clickhouse-client or another TCP client name.client_revision (UInt32) — Revision of the clickhouse-client or another TCP client.client_version_major (UInt32) — The major version of the clickhouse-client or another TCP client.client_version_minor (UInt32) — The minor version of the clickhouse-client or another TCP client.client_version_patch (UInt32) — Patch component of the clickhouse-client or another TCP client version.failure_reason (String) — The exception message containing the reason for the login/logout failure. Example Query: SELECT * FROM system.session_log LIMIT 1 FORMAT Vertical; Result: Row 1: ────── type: LoginSuccess auth_id: 45e6bd83-b4aa-4a23-85e6-bd83b4aa1a23 session_id: event_date: 2021-10-14 event_time: 2021-10-14 20:33:52 event_time_microseconds: 2021-10-14 20:33:52.104247 user: default auth_type: PLAINTEXT_PASSWORD profiles: ['default'] roles: [] settings: [('load_balancing','random'),('max_memory_usage','10000000000')] client_address: ::ffff:127.0.0.1 client_port: 38490 interface: TCP client_hostname: client_name: ClickHouse client client_revision: 54449 client_version_major: 21 client_version_minor: 10 client_version_patch: 0 failure_reason: ","keywords":""},{"title":"settings","type":0,"sectionRef":"#","url":"en/operations/system-tables/settings","content":"settings Contains information about session settings for current user. Columns: name (String) — Setting name.value (String) — Setting value.changed (UInt8) — Shows whether a setting is changed from its default value.description (String) — Short setting description.min (Nullable(String)) — Minimum value of the setting, if any is set via constraints. If the setting has no minimum value, contains NULL.max (Nullable(String)) — Maximum value of the setting, if any is set via constraints. If the setting has no maximum value, contains NULL.readonly (UInt8) — Shows whether the current user can change the setting: 0 — Current user can change the setting.1 — Current user can’t change the setting. Example The following example shows how to get information about settings which name contains min_i. SELECT * FROM system.settings WHERE name LIKE '%min_i%' ┌─name────────────────────────────────────────┬─value─────┬─changed─┬─description───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─min──┬─max──┬─readonly─┐ │ min_insert_block_size_rows │ 1048576 │ 0 │ Squash blocks passed to INSERT query to specified size in rows, if blocks are not big enough. │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ 0 │ │ min_insert_block_size_bytes │ 268435456 │ 0 │ Squash blocks passed to INSERT query to specified size in bytes, if blocks are not big enough. │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ 0 │ │ read_backoff_min_interval_between_events_ms │ 1000 │ 0 │ Settings to reduce the number of threads in case of slow reads. Do not pay attention to the event, if the previous one has passed less than a certain amount of time. │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ 0 │ └─────────────────────────────────────────────┴───────────┴─────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴──────┴──────┴──────────┘ Using of WHERE changed can be useful, for example, when you want to check: Whether settings in configuration files are loaded correctly and are in use.Settings that changed in the current session. SELECT * FROM system.settings WHERE changed AND name='load_balancing' See also SettingsPermissions for QueriesConstraints on SettingsSHOW SETTINGS statement Original article","keywords":""},{"title":"settings_profile_elements","type":0,"sectionRef":"#","url":"en/operations/system-tables/settings_profile_elements","content":"settings_profile_elements Describes the content of the settings profile: Сonstraints.Roles and users that the setting applies to.Parent settings profiles. Columns: profile_name (Nullable(String)) — Setting profile name. user_name (Nullable(String)) — User name. role_name (Nullable(String)) — Role name. index (UInt64) — Sequential number of the settings profile element. setting_name (Nullable(String)) — Setting name. value (Nullable(String)) — Setting value. min (Nullable(String)) — The minimum value of the setting. NULL if not set. max (Nullable(String)) — The maximum value of the setting. NULL if not set. readonly (Nullable(UInt8)) — Profile that allows only read queries. inherit_profile (Nullable(String)) — A parent profile for this setting profile. NULL if not set. Setting profile will inherit all the settings' values and constraints (min, max, readonly) from its parent profiles. Original article","keywords":""},{"title":"settings_profiles","type":0,"sectionRef":"#","url":"en/operations/system-tables/settings_profiles","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"settings_profiles","url":"en/operations/system-tables/settings_profiles#see-also","content":"SHOW PROFILES Original article "},{"title":"stack_trace","type":0,"sectionRef":"#","url":"en/operations/system-tables/stack_trace","content":"stack_trace Contains stack traces of all server threads. Allows developers to introspect the server state. To analyze stack frames, use the addressToLine, addressToLineWithInlines, addressToSymbol and demangle introspection functions. Columns: thread_name (String) — Thread name.thread_id (UInt64) — Thread identifier.query_id (String) — Query identifier that can be used to get details about a query that was running from the query_log system table.trace (Array(UInt64)) — A stack trace which represents a list of physical addresses where the called methods are stored. Example Enabling introspection functions: SET allow_introspection_functions = 1; Getting symbols from ClickHouse object files: WITH arrayMap(x -&gt; demangle(addressToSymbol(x)), trace) AS all SELECT thread_name, thread_id, query_id, arrayStringConcat(all, '\\n') AS res FROM system.stack_trace LIMIT 1 \\G; Row 1: ────── thread_name: clickhouse-serv thread_id: 686 query_id: 1a11f70b-626d-47c1-b948-f9c7b206395d res: sigqueue DB::StorageSystemStackTrace::fillData(std::__1::vector&lt;COW&lt;DB::IColumn&gt;::mutable_ptr&lt;DB::IColumn&gt;, std::__1::allocator&lt;COW&lt;DB::IColumn&gt;::mutable_ptr&lt;DB::IColumn&gt; &gt; &gt;&amp;, DB::Context const&amp;, DB::SelectQueryInfo const&amp;) const DB::IStorageSystemOneBlock&lt;DB::StorageSystemStackTrace&gt;::read(std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;, DB::SelectQueryInfo const&amp;, DB::Context const&amp;, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPipeline&amp;, std::__1::shared_ptr&lt;DB::PrewhereInfo&gt; const&amp;, std::__1::vector&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt;, std::__1::allocator&lt;std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; &gt; &gt; const&amp;) DB::InterpreterSelectQuery::executeImpl(DB::QueryPipeline&amp;, std::__1::shared_ptr&lt;DB::IBlockInputStream&gt; const&amp;, std::__1::optional&lt;DB::Pipe&gt;) DB::InterpreterSelectQuery::execute() DB::InterpreterSelectWithUnionQuery::execute() DB::executeQueryImpl(char const*, char const*, DB::Context&amp;, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer*) DB::executeQuery(std::__1::basic_string&lt;char, std::__1::char_traits&lt;char&gt;, std::__1::allocator&lt;char&gt; &gt; const&amp;, DB::Context&amp;, bool, DB::QueryProcessingStage::Enum, bool) DB::TCPHandler::runImpl() DB::TCPHandler::run() Poco::Net::TCPServerConnection::start() Poco::Net::TCPServerDispatcher::run() Poco::PooledThread::run() Poco::ThreadImpl::runnableEntry(void*) start_thread __clone Getting filenames and line numbers in ClickHouse source code: WITH arrayMap(x -&gt; addressToLine(x), trace) AS all, arrayFilter(x -&gt; x LIKE '%/dbms/%', all) AS dbms SELECT thread_name, thread_id, query_id, arrayStringConcat(notEmpty(dbms) ? dbms : all, '\\n') AS res FROM system.stack_trace LIMIT 1 \\G; Row 1: ────── thread_name: clickhouse-serv thread_id: 686 query_id: cad353e7-1c29-4b2e-949f-93e597ab7a54 res: /lib/x86_64-linux-gnu/libc-2.27.so /build/obj-x86_64-linux-gnu/../src/Storages/System/StorageSystemStackTrace.cpp:182 /build/obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:656 /build/obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:1338 /build/obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:751 /build/obj-x86_64-linux-gnu/../contrib/libcxx/include/optional:224 /build/obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:192 /build/obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:384 /build/obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:643 /build/obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:251 /build/obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1197 /build/obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:57 /build/obj-x86_64-linux-gnu/../contrib/libcxx/include/atomic:856 /build/obj-x86_64-linux-gnu/../contrib/poco/Foundation/include/Poco/Mutex_POSIX.h:59 /build/obj-x86_64-linux-gnu/../contrib/poco/Foundation/include/Poco/AutoPtr.h:223 /lib/x86_64-linux-gnu/libpthread-2.27.so /lib/x86_64-linux-gnu/libc-2.27.so See Also Introspection Functions — Which introspection functions are available and how to use them.system.trace_log — Contains stack traces collected by the sampling query profiler.arrayMap — Description and usage example of the arrayMap function.arrayFilter — Description and usage example of the arrayFilter function.","keywords":""},{"title":"storage_policies","type":0,"sectionRef":"#","url":"en/operations/system-tables/storage_policies","content":"storage_policies Contains information about storage policies and volumes defined in the server configuration. Columns: policy_name (String) — Name of the storage policy.volume_name (String) — Volume name defined in the storage policy.volume_priority (UInt64) — Volume order number in the configuration, the data fills the volumes according this priority, i.e. data during inserts and merges is written to volumes with a lower priority (taking into account other rules: TTL, max_data_part_size, move_factor).disks (Array(String)) — Disk names, defined in the storage policy.max_data_part_size (UInt64) — Maximum size of a data part that can be stored on volume disks (0 — no limit).move_factor (Float64) — Ratio of free disk space. When the ratio exceeds the value of configuration parameter, ClickHouse start to move data to the next volume in order.prefer_not_to_merge (UInt8) — Value of the prefer_not_to_merge setting. When this setting is enabled, merging data on this volume is not allowed. This allows controlling how ClickHouse works with slow disks. If the storage policy contains more then one volume, then information for each volume is stored in the individual row of the table. Original article","keywords":""},{"title":"table_engines","type":0,"sectionRef":"#","url":"en/operations/system-tables/table_engines","content":"table_engines Contains description of table engines supported by server and their feature support information. This table contains the following columns (the column type is shown in brackets): name (String) — The name of table engine.supports_settings (UInt8) — Flag that indicates if table engine supports SETTINGS clause.supports_skipping_indices (UInt8) — Flag that indicates if table engine supports skipping indices.supports_ttl (UInt8) — Flag that indicates if table engine supports TTL.supports_sort_order (UInt8) — Flag that indicates if table engine supports clauses PARTITION_BY, PRIMARY_KEY, ORDER_BY and SAMPLE_BY.supports_replication (UInt8) — Flag that indicates if table engine supports data replication.supports_duduplication (UInt8) — Flag that indicates if table engine supports data deduplication.supports_parallel_insert (UInt8) — Flag that indicates if table engine supports parallel insert (see max_insert_threads setting). Example: SELECT * FROM system.table_engines WHERE name in ('Kafka', 'MergeTree', 'ReplicatedCollapsingMergeTree') ┌─name──────────────────────────┬─supports_settings─┬─supports_skipping_indices─┬─supports_sort_order─┬─supports_ttl─┬─supports_replication─┬─supports_deduplication─┬─supports_parallel_insert─┐ │ MergeTree │ 1 │ 1 │ 1 │ 1 │ 0 │ 0 │ 1 │ │ Kafka │ 1 │ 0 │ 0 │ 0 │ 0 │ 0 │ 0 │ │ ReplicatedCollapsingMergeTree │ 1 │ 1 │ 1 │ 1 │ 1 │ 1 │ 1 │ └───────────────────────────────┴───────────────────┴───────────────────────────┴─────────────────────┴──────────────┴──────────────────────┴────────────────────────┴──────────────────────────┘ See also MergeTree family query clausesKafka settingsJoin settings Original article","keywords":""},{"title":"tables","type":0,"sectionRef":"#","url":"en/operations/system-tables/tables","content":"tables Contains metadata of each table that the server knows about. Detached tables are not shown in system.tables. Temporary tables are visible in the system.tables only in those session where they have been created. They are shown with the empty database field and with the is_temporary flag switched on. Columns: database (String) — The name of the database the table is in. name (String) — Table name. engine (String) — Table engine name (without parameters). is_temporary (UInt8) - Flag that indicates whether the table is temporary. data_path (String) - Path to the table data in the file system. metadata_path (String) - Path to the table metadata in the file system. metadata_modification_time (DateTime) - Time of latest modification of the table metadata. dependencies_database (Array(String)) - Database dependencies. dependencies_table (Array(String)) - Table dependencies (MaterializedView tables based on the current table). create_table_query (String) - The query that was used to create the table. engine_full (String) - Parameters of the table engine. as_select (String) - SELECT query for view. partition_key (String) - The partition key expression specified in the table. sorting_key (String) - The sorting key expression specified in the table. primary_key (String) - The primary key expression specified in the table. sampling_key (String) - The sampling key expression specified in the table. storage_policy (String) - The storage policy: MergeTreeDistributed total_rows (Nullable(UInt64)) - Total number of rows, if it is possible to quickly determine exact number of rows in the table, otherwise NULL (including underying Buffer table). total_bytes (Nullable(UInt64)) - Total number of bytes, if it is possible to quickly determine exact number of bytes for the table on storage, otherwise NULL (does not includes any underlying storage). If the table stores data on disk, returns used space on disk (i.e. compressed).If the table stores data in memory, returns approximated number of used bytes in memory. lifetime_rows (Nullable(UInt64)) - Total number of rows INSERTed since server start (only for Buffer tables). lifetime_bytes (Nullable(UInt64)) - Total number of bytes INSERTed since server start (only for Buffer tables). comment (String) - The comment for the table. has_own_data (UInt8) — Flag that indicates whether the table itself stores some data on disk or only accesses some other source. The system.tables table is used in SHOW TABLES query implementation. Example SELECT * FROM system.tables LIMIT 2 FORMAT Vertical; Row 1: ────── database: base name: t1 uuid: 81b1c20a-b7c6-4116-a2ce-7583fb6b6736 engine: MergeTree is_temporary: 0 data_paths: ['/var/lib/clickhouse/store/81b/81b1c20a-b7c6-4116-a2ce-7583fb6b6736/'] metadata_path: /var/lib/clickhouse/store/461/461cf698-fd0b-406d-8c01-5d8fd5748a91/t1.sql metadata_modification_time: 2021-01-25 19:14:32 dependencies_database: [] dependencies_table: [] create_table_query: CREATE TABLE base.t1 (`n` UInt64) ENGINE = MergeTree ORDER BY n SETTINGS index_granularity = 8192 engine_full: MergeTree ORDER BY n SETTINGS index_granularity = 8192 as_select: SELECT database AS table_catalog partition_key: sorting_key: n primary_key: n sampling_key: storage_policy: default total_rows: 1 total_bytes: 99 lifetime_rows: ᴺᵁᴸᴸ lifetime_bytes: ᴺᵁᴸᴸ comment: has_own_data: 0 Row 2: ────── database: default name: 53r93yleapyears uuid: 00000000-0000-0000-0000-000000000000 engine: MergeTree is_temporary: 0 data_paths: ['/var/lib/clickhouse/data/default/53r93yleapyears/'] metadata_path: /var/lib/clickhouse/metadata/default/53r93yleapyears.sql metadata_modification_time: 2020-09-23 09:05:36 dependencies_database: [] dependencies_table: [] create_table_query: CREATE TABLE default.`53r93yleapyears` (`id` Int8, `febdays` Int8) ENGINE = MergeTree ORDER BY id SETTINGS index_granularity = 8192 engine_full: MergeTree ORDER BY id SETTINGS index_granularity = 8192 as_select: SELECT name AS catalog_name partition_key: sorting_key: id primary_key: id sampling_key: storage_policy: default total_rows: 2 total_bytes: 155 lifetime_rows: ᴺᵁᴸᴸ lifetime_bytes: ᴺᵁᴸᴸ comment: has_own_data: 0 ","keywords":""},{"title":"text_log","type":0,"sectionRef":"#","url":"en/operations/system-tables/text_log","content":"text_log Contains logging entries. The logging level which goes to this table can be limited to the text_log.level server setting. Columns: event_date (Date) — Date of the entry.event_time (DateTime) — Time of the entry.event_time_microseconds (DateTime) — Time of the entry with microseconds precision.microseconds (UInt32) — Microseconds of the entry.thread_name (String) — Name of the thread from which the logging was done.thread_id (UInt64) — OS thread ID.level (Enum8) — Entry level. Possible values: 1 or 'Fatal'.2 or 'Critical'.3 or 'Error'.4 or 'Warning'.5 or 'Notice'.6 or 'Information'.7 or 'Debug'.8 or 'Trace'. query_id (String) — ID of the query.logger_name (LowCardinality(String)) — Name of the logger (i.e. DDLWorker).message (String) — The message itself.revision (UInt32) — ClickHouse revision.source_file (LowCardinality(String)) — Source file from which the logging was done.source_line (UInt64) — Source line from which the logging was done. Example SELECT * FROM system.text_log LIMIT 1 \\G Row 1: ────── event_date: 2020-09-10 event_time: 2020-09-10 11:23:07 event_time_microseconds: 2020-09-10 11:23:07.871397 microseconds: 871397 thread_name: clickhouse-serv thread_id: 564917 level: Information query_id: logger_name: DNSCacheUpdater message: Update period 15 seconds revision: 54440 source_file: /ClickHouse/src/Interpreters/DNSCacheUpdater.cpp; void DB::DNSCacheUpdater::start() source_line: 45 Original article","keywords":""},{"title":"time_zones","type":0,"sectionRef":"#","url":"en/operations/system-tables/time_zones","content":"time_zones Contains a list of time zones that are supported by the ClickHouse server. This list of timezones might vary depending on the version of ClickHouse. Columns: time_zone (String) — List of supported time zones. Example SELECT * FROM system.time_zones LIMIT 10 ┌─time_zone──────────┐ │ Africa/Abidjan │ │ Africa/Accra │ │ Africa/Addis_Ababa │ │ Africa/Algiers │ │ Africa/Asmara │ │ Africa/Asmera │ │ Africa/Bamako │ │ Africa/Bangui │ │ Africa/Banjul │ │ Africa/Bissau │ └────────────────────┘ Original article","keywords":""},{"title":"trace_log","type":0,"sectionRef":"#","url":"en/operations/system-tables/trace_log","content":"trace_log Contains stack traces collected by the sampling query profiler. ClickHouse creates this table when the trace_log server configuration section is set. Also the query_profiler_real_time_period_ns and query_profiler_cpu_time_period_ns settings should be set. To analyze logs, use the addressToLine, addressToLineWithInlines, addressToSymbol and demangle introspection functions. Columns: event_date (Date) — Date of sampling moment. event_time (DateTime) — Timestamp of the sampling moment. event_time_microseconds (DateTime64) — Timestamp of the sampling moment with microseconds precision. timestamp_ns (UInt64) — Timestamp of the sampling moment in nanoseconds. revision (UInt32) — ClickHouse server build revision. When connecting to the server by clickhouse-client, you see the string similar to Connected to ClickHouse server version 19.18.1 revision 54429.. This field contains the revision, but not the version of a server. trace_type (Enum8) — Trace type: Real represents collecting stack traces by wall-clock time.CPU represents collecting stack traces by CPU time.Memory represents collecting allocations and deallocations when memory allocation exceeds the subsequent watermark.MemorySample represents collecting random allocations and deallocations. thread_number (UInt32) — Thread identifier. query_id (String) — Query identifier that can be used to get details about a query that was running from the query_log system table. trace (Array(UInt64)) — Stack trace at the moment of sampling. Each element is a virtual memory address inside ClickHouse server process. Example SELECT * FROM system.trace_log LIMIT 1 \\G Row 1: ────── event_date: 2020-09-10 event_time: 2020-09-10 11:23:09 event_time_microseconds: 2020-09-10 11:23:09.872924 timestamp_ns: 1599762189872924510 revision: 54440 trace_type: Memory thread_id: 564963 query_id: trace: [371912858,371912789,371798468,371799717,371801313,371790250,624462773,566365041,566440261,566445834,566460071,566459914,566459842,566459580,566459469,566459389,566459341,566455774,371993941,371988245,372158848,372187428,372187309,372187093,372185478,140222123165193,140222122205443] size: 5244400 Original article","keywords":""},{"title":"users","type":0,"sectionRef":"#","url":"en/operations/system-tables/users","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"users","url":"en/operations/system-tables/users#see-also","content":"SHOW USERS Original article "},{"title":"Settings","type":0,"sectionRef":"#","url":"en/operations/settings/settings","content":"","keywords":""},{"title":"allow_nondeterministic_mutations​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#allow_nondeterministic_mutations","content":"User-level setting that allows mutations on replicated tables to make use of non-deterministic functions such as dictGet. Given that, for example, dictionaries, can be out of sync across nodes, mutations that pull values from them are disallowed on replicated tables by default. Enabling this setting allows this behavior, making it the user's responsibility to ensure that the data used is in sync across all nodes. Default value: 0. Example &lt;profiles&gt; &lt;default&gt; &lt;allow_nondeterministic_mutations&gt;1&lt;/allow_nondeterministic_mutations&gt; &lt;!-- ... --&gt; &lt;/default&gt; &lt;!-- ... --&gt; &lt;/profiles&gt;  "},{"title":"distributed_product_mode​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#distributed-product-mode","content":"Changes the behaviour of distributed subqueries. ClickHouse applies this setting when the query contains the product of distributed tables, i.e. when the query for a distributed table contains a non-GLOBAL subquery for the distributed table. Restrictions: Only applied for IN and JOIN subqueries.Only if the FROM section uses a distributed table containing more than one shard.If the subquery concerns a distributed table containing more than one shard.Not used for a table-valued remote function. Possible values: deny — Default value. Prohibits using these types of subqueries (returns the “Double-distributed in/JOIN subqueries is denied” exception).local — Replaces the database and table in the subquery with local ones for the destination server (shard), leaving the normal IN/JOIN.global — Replaces the IN/JOIN query with GLOBAL IN/GLOBAL JOIN.allow — Allows the use of these types of subqueries. "},{"title":"prefer_global_in_and_join​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#prefer-global-in-and-join","content":"Enables the replacement of IN/JOIN operators with GLOBAL IN/GLOBAL JOIN. Possible values: 0 — Disabled. IN/JOIN operators are not replaced with GLOBAL IN/GLOBAL JOIN.1 — Enabled. IN/JOIN operators are replaced with GLOBAL IN/GLOBAL JOIN. Default value: 0. Usage Although SET distributed_product_mode=global can change the queries behavior for the distributed tables, it's not suitable for local tables or tables from external resources. Here is when the prefer_global_in_and_join setting comes into play. For example, we have query serving nodes that contain local tables, which are not suitable for distribution. We need to scatter their data on the fly during distributed processing with the GLOBAL keyword — GLOBAL IN/GLOBAL JOIN. Another use case of prefer_global_in_and_join is accessing tables created by external engines. This setting helps to reduce the number of calls to external sources while joining such tables: only one call per query. See also: Distributed subqueries for more information on how to use GLOBAL IN/GLOBAL JOIN "},{"title":"enable_optimize_predicate_expression​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#enable-optimize-predicate-expression","content":"Turns on predicate pushdown in SELECT queries. Predicate pushdown may significantly reduce network traffic for distributed queries. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. Usage Consider the following queries: SELECT count() FROM test_table WHERE date = '2018-10-10'SELECT count() FROM (SELECT * FROM test_table) WHERE date = '2018-10-10' If enable_optimize_predicate_expression = 1, then the execution time of these queries is equal because ClickHouse applies WHERE to the subquery when processing it. If enable_optimize_predicate_expression = 0, then the execution time of the second query is much longer because the WHERE clause applies to all the data after the subquery finishes. "},{"title":"fallback_to_stale_replicas_for_distributed_queries​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-fallback_to_stale_replicas_for_distributed_queries","content":"Forces a query to an out-of-date replica if updated data is not available. See Replication. ClickHouse selects the most relevant from the outdated replicas of the table. Used when performing SELECT from a distributed table that points to replicated tables. By default, 1 (enabled). "},{"title":"force_index_by_date​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-force_index_by_date","content":"Disables query execution if the index can’t be used by date. Works with tables in the MergeTree family. If force_index_by_date=1, ClickHouse checks whether the query has a date key condition that can be used for restricting data ranges. If there is no suitable condition, it throws an exception. However, it does not check whether the condition reduces the amount of data to read. For example, the condition Date != ' 2000-01-01 ' is acceptable even when it matches all the data in the table (i.e., running the query requires a full scan). For more information about ranges of data in MergeTree tables, see MergeTree. "},{"title":"force_primary_key​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#force-primary-key","content":"Disables query execution if indexing by the primary key is not possible. Works with tables in the MergeTree family. If force_primary_key=1, ClickHouse checks to see if the query has a primary key condition that can be used for restricting data ranges. If there is no suitable condition, it throws an exception. However, it does not check whether the condition reduces the amount of data to read. For more information about data ranges in MergeTree tables, see MergeTree. "},{"title":"use_skip_indexes​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-use_skip_indexes","content":"Use data skipping indexes during query execution. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. "},{"title":"force_data_skipping_indices​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-force_data_skipping_indices","content":"Disables query execution if passed data skipping indices wasn't used. Consider the following example: CREATE TABLE data ( key Int, d1 Int, d1_null Nullable(Int), INDEX d1_idx d1 TYPE minmax GRANULARITY 1, INDEX d1_null_idx assumeNotNull(d1_null) TYPE minmax GRANULARITY 1 ) Engine=MergeTree() ORDER BY key; SELECT * FROM data_01515; SELECT * FROM data_01515 SETTINGS force_data_skipping_indices=''; -- query will produce CANNOT_PARSE_TEXT error. SELECT * FROM data_01515 SETTINGS force_data_skipping_indices='d1_idx'; -- query will produce INDEX_NOT_USED error. SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='d1_idx'; -- Ok. SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`'; -- Ok (example of full featured parser). SELECT * FROM data_01515 WHERE d1 = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- query will produce INDEX_NOT_USED error, since d1_null_idx is not used. SELECT * FROM data_01515 WHERE d1 = 0 AND assumeNotNull(d1_null) = 0 SETTINGS force_data_skipping_indices='`d1_idx`, d1_null_idx'; -- Ok.  Works with tables in the MergeTree family. "},{"title":"format_schema​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#format-schema","content":"This parameter is useful when you are using formats that require a schema definition, such as Cap’n Proto or Protobuf. The value depends on the format. "},{"title":"fsync_metadata​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#fsync-metadata","content":"Enables or disables fsync when writing .sql files. Enabled by default. It makes sense to disable it if the server has millions of tiny tables that are constantly being created and destroyed. "},{"title":"function_range_max_elements_in_block​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-function_range_max_elements_in_block","content":"Sets the safety threshold for data volume generated by function range. Defines the maximum number of values generated by function per block of data (sum of array sizes for every row in a block). Possible values: Positive integer. Default value: 500,000,000. See Also max_block_sizemin_insert_block_size_rows "},{"title":"enable_http_compression​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-enable_http_compression","content":"Enables or disables data compression in the response to an HTTP request. For more information, read the HTTP interface description. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"http_zlib_compression_level​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-http_zlib_compression_level","content":"Sets the level of data compression in the response to an HTTP request if enable_http_compression = 1. Possible values: Numbers from 1 to 9. Default value: 3. "},{"title":"http_native_compression_disable_checksumming_on_decompress​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-http_native_compression_disable_checksumming_on_decompress","content":"Enables or disables checksum verification when decompressing the HTTP POST data from the client. Used only for ClickHouse native compression format (not used with gzip or deflate). For more information, read the HTTP interface description. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"http_max_uri_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#http-max-uri-size","content":"Sets the maximum URI length of an HTTP request. Possible values: Positive integer. Default value: 1048576. "},{"title":"table_function_remote_max_addresses​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#table_function_remote_max_addresses","content":"Sets the maximum number of addresses generated from patterns for the remote function. Possible values: Positive integer. Default value: 1000. "},{"title":"glob_expansion_max_elements {#glob_expansion_max_elements }​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#glob_expansion_max_elements--glob_expansion_max_elements-","content":"Sets the maximum number of addresses generated from patterns for external storages and table functions (like url) except the remote function. Possible values: Positive integer. Default value: 1000. "},{"title":"send_progress_in_http_headers​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-send_progress_in_http_headers","content":"Enables or disables X-ClickHouse-Progress HTTP response headers in clickhouse-server responses. For more information, read the HTTP interface description. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"max_http_get_redirects​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#setting-max_http_get_redirects","content":"Limits the maximum number of HTTP GET redirect hops for URL-engine tables. The setting applies to both types of tables: those created by the CREATE TABLE query and by the url table function. Possible values: Any positive integer number of hops.0 — No hops allowed. Default value: 0. "},{"title":"input_format_allow_errors_num​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input_format_allow_errors_num","content":"Sets the maximum number of acceptable errors when reading from text formats (CSV, TSV, etc.). The default value is 0. Always pair it with input_format_allow_errors_ratio. If an error occurred while reading rows but the error counter is still less than input_format_allow_errors_num, ClickHouse ignores the row and moves on to the next one. If both input_format_allow_errors_num and input_format_allow_errors_ratio are exceeded, ClickHouse throws an exception. "},{"title":"input_format_allow_errors_ratio​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input_format_allow_errors_ratio","content":"Sets the maximum percentage of errors allowed when reading from text formats (CSV, TSV, etc.). The percentage of errors is set as a floating-point number between 0 and 1. The default value is 0. Always pair it with input_format_allow_errors_num. If an error occurred while reading rows but the error counter is still less than input_format_allow_errors_ratio, ClickHouse ignores the row and moves on to the next one. If both input_format_allow_errors_num and input_format_allow_errors_ratio are exceeded, ClickHouse throws an exception. "},{"title":"input_format_parquet_import_nested​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#input_format_parquet_import_nested","content":"Enables or disables the ability to insert the data into Nested columns as an array of structs in Parquet input format. Possible values: 0 — Data can not be inserted into Nested columns as an array of structs.1 — Data can be inserted into Nested columns as an array of structs. Default value: 0. "},{"title":"input_format_arrow_import_nested​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#input_format_arrow_import_nested","content":"Enables or disables the ability to insert the data into Nested columns as an array of structs in Arrow input format. Possible values: 0 — Data can not be inserted into Nested columns as an array of structs.1 — Data can be inserted into Nested columns as an array of structs. Default value: 0. "},{"title":"input_format_orc_import_nested​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#input_format_orc_import_nested","content":"Enables or disables the ability to insert the data into Nested columns as an array of structs in ORC input format. Possible values: 0 — Data can not be inserted into Nested columns as an array of structs.1 — Data can be inserted into Nested columns as an array of structs. Default value: 0. "},{"title":"input_format_values_interpret_expressions​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input_format_values_interpret_expressions","content":"Enables or disables the full SQL parser if the fast stream parser can’t parse the data. This setting is used only for the Values format at the data insertion. For more information about syntax parsing, see the Syntax section. Possible values: 0 — Disabled. In this case, you must provide formatted data. See the Formats section. 1 — Enabled. In this case, you can use an SQL expression as a value, but data insertion is much slower this way. If you insert only formatted data, then ClickHouse behaves as if the setting value is 0. Default value: 1. Example of Use Insert the DateTime type value with the different settings. SET input_format_values_interpret_expressions = 0; INSERT INTO datetime_t VALUES (now())  Exception on client: Code: 27. DB::Exception: Cannot parse input: expected ) before: now()): (at row 1)  SET input_format_values_interpret_expressions = 1; INSERT INTO datetime_t VALUES (now())  Ok.  The last query is equivalent to the following: SET input_format_values_interpret_expressions = 0; INSERT INTO datetime_t SELECT now()  Ok.  "},{"title":"input_format_values_deduce_templates_of_expressions​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input_format_values_deduce_templates_of_expressions","content":"Enables or disables template deduction for SQL expressions in Values format. It allows parsing and interpreting expressions in Values much faster if expressions in consecutive rows have the same structure. ClickHouse tries to deduce the template of an expression, parse the following rows using this template and evaluate the expression on a batch of successfully parsed rows. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. For the following query: INSERT INTO test VALUES (lower('Hello')), (lower('world')), (lower('INSERT')), (upper('Values')), ...  If input_format_values_interpret_expressions=1 and format_values_deduce_templates_of_expressions=0, expressions are interpreted separately for each row (this is very slow for large number of rows).If input_format_values_interpret_expressions=0 and format_values_deduce_templates_of_expressions=1, expressions in the first, second and third rows are parsed using template lower(String) and interpreted together, expression in the forth row is parsed with another template (upper(String)).If input_format_values_interpret_expressions=1 and format_values_deduce_templates_of_expressions=1, the same as in previous case, but also allows fallback to interpreting expressions separately if it’s not possible to deduce template. "},{"title":"input_format_values_accurate_types_of_literals​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input-format-values-accurate-types-of-literals","content":"This setting is used only when input_format_values_deduce_templates_of_expressions = 1. Expressions for some column may have the same structure, but contain numeric literals of different types, e.g. (..., abs(0), ...), -- UInt64 literal (..., abs(3.141592654), ...), -- Float64 literal (..., abs(-1), ...), -- Int64 literal  Possible values: 0 — Disabled. In this case, ClickHouse may use a more general type for some literals (e.g., Float64 or Int64 instead of UInt64 for 42), but it may cause overflow and precision issues. 1 — Enabled. In this case, ClickHouse checks the actual type of literal and uses an expression template of the corresponding type. In some cases, it may significantly slow down expression evaluation in Values. Default value: 1. "},{"title":"input_format_defaults_for_omitted_fields​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#session_settings-input_format_defaults_for_omitted_fields","content":"When performing INSERT queries, replace omitted input column values with default values of the respective columns. This option only applies to JSONEachRow, CSV, TabSeparated formats and formats with WithNames/WithNamesAndTypes suffixes. note When this option is enabled, extended table metadata are sent from server to client. It consumes additional computing resources on the server and can reduce performance. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. "},{"title":"input_format_tsv_empty_as_default​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input-format-tsv-empty-as-default","content":"When enabled, replace empty input fields in TSV with default values. For complex default expressions input_format_defaults_for_omitted_fields must be enabled too. Disabled by default. "},{"title":"input_format_csv_empty_as_default​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input-format-csv-empty-as-default","content":"When enabled, replace empty input fields in CSV with default values. For complex default expressions input_format_defaults_for_omitted_fields must be enabled too. Enabled by default. "},{"title":"input_format_tsv_enum_as_number​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input_format_tsv_enum_as_number","content":"When enabled, always treat enum values as enum ids for TSV input format. It's recommended to enable this setting if data contains only enum ids to optimize enum parsing. Possible values: 0 — Enum values are parsed as values or as enum IDs.1 — Enum values are parsed only as enum IDs. Default value: 0. Example Consider the table: CREATE TABLE table_with_enum_column_for_tsv_insert (Id Int32,Value Enum('first' = 1, 'second' = 2)) ENGINE=Memory();  When the input_format_tsv_enum_as_number setting is enabled: Query: SET input_format_tsv_enum_as_number = 1; INSERT INTO table_with_enum_column_for_tsv_insert FORMAT TSV 102 2; SELECT * FROM table_with_enum_column_for_tsv_insert;  Result: ┌──Id─┬─Value──┐ │ 102 │ second │ └─────┴────────┘  Query: SET input_format_tsv_enum_as_number = 1; INSERT INTO table_with_enum_column_for_tsv_insert FORMAT TSV 103 'first';  throws an exception. When the input_format_tsv_enum_as_number setting is disabled: Query: SET input_format_tsv_enum_as_number = 0; INSERT INTO table_with_enum_column_for_tsv_insert FORMAT TSV 102 2; INSERT INTO table_with_enum_column_for_tsv_insert FORMAT TSV 103 'first'; SELECT * FROM table_with_enum_column_for_tsv_insert;  Result: ┌──Id─┬─Value──┐ │ 102 │ second │ └─────┴────────┘ ┌──Id─┬─Value──┐ │ 103 │ first │ └─────┴────────┘  "},{"title":"input_format_null_as_default​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input-format-null-as-default","content":"Enables or disables the initialization of NULL fields with default values, if data type of these fields is not nullable. If column type is not nullable and this setting is disabled, then inserting NULL causes an exception. If column type is nullable, then NULL values are inserted as is, regardless of this setting. This setting is applicable to INSERT ... VALUES queries for text input formats. Possible values: 0 — Inserting NULL into a not nullable column causes an exception.1 — NULL fields are initialized with default column values. Default value: 1. "},{"title":"insert_null_as_default​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#insert_null_as_default","content":"Enables or disables the insertion of default values instead of NULL into columns with not nullable data type. If column type is not nullable and this setting is disabled, then inserting NULL causes an exception. If column type is nullable, then NULL values are inserted as is, regardless of this setting. This setting is applicable to INSERT ... SELECT queries. Note that SELECT subqueries may be concatenated with UNION ALL clause. Possible values: 0 — Inserting NULL into a not nullable column causes an exception.1 — Default column value is inserted instead of NULL. Default value: 1. "},{"title":"input_format_skip_unknown_fields​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input-format-skip-unknown-fields","content":"Enables or disables skipping insertion of extra data. When writing data, ClickHouse throws an exception if input data contain columns that do not exist in the target table. If skipping is enabled, ClickHouse does not insert extra data and does not throw an exception. Supported formats: JSONEachRowCSVWithNamesTabSeparatedWithNamesTSKV Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"input_format_import_nested_json​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input_format_import_nested_json","content":"Enables or disables the insertion of JSON data with nested objects. Supported formats: JSONEachRow Possible values: 0 — Disabled.1 — Enabled. Default value: 0. See also: Usage of Nested Structures with the JSONEachRow format. "},{"title":"input_format_with_names_use_header​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input-format-with-names-use-header","content":"Enables or disables checking the column order when inserting data. To improve insert performance, we recommend disabling this check if you are sure that the column order of the input data is the same as in the target table. Supported formats: CSVWithNamesCSVWithNamesTabSeparatedWithNamesTabSeparatedWithNamesAndTypesJSONCompactEachRowWithNamesJSONCompactEachRowWithNamesAndTypesJSONCompactStringsEachRowWithNamesJSONCompactStringsEachRowWithNamesAndTypesRowBinaryWithNamesRowBinaryWithNamesAndTypes Possible values: 0 — Disabled.1 — Enabled. Default value: 1. "},{"title":"input_format_with_types_use_header​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input-format-with-types-use-header","content":"Controls whether format parser should check if data types from the input data match data types from the target table. Supported formats: CSVWithNamesCSVWithNamesTabSeparatedWithNamesTabSeparatedWithNamesAndTypesJSONCompactEachRowWithNamesJSONCompactEachRowWithNamesAndTypesJSONCompactStringsEachRowWithNamesJSONCompactStringsEachRowWithNamesAndTypesRowBinaryWithNamesRowBinaryWithNamesAndTypes Possible values: 0 — Disabled.1 — Enabled. Default value: 1. "},{"title":"date_time_input_format​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-date_time_input_format","content":"Allows choosing a parser of the text representation of date and time. The setting does not apply to date and time functions. Possible values: 'best_effort' — Enables extended parsing. ClickHouse can parse the basic YYYY-MM-DD HH:MM:SS format and all ISO 8601 date and time formats. For example, '2018-06-08T01:02:03.000Z'. 'basic' — Use basic parser. ClickHouse can parse only the basic YYYY-MM-DD HH:MM:SS or YYYY-MM-DD format. For example, 2019-08-20 10:18:56 or 2019-08-20. Default value: 'basic'. See also: DateTime data type.Functions for working with dates and times. "},{"title":"date_time_output_format​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-date_time_output_format","content":"Allows choosing different output formats of the text representation of date and time. Possible values: simple - Simple output format. ClickHouse output date and time YYYY-MM-DD hh:mm:ss format. For example, 2019-08-20 10:18:56. The calculation is performed according to the data type's time zone (if present) or server time zone. iso - ISO output format. ClickHouse output date and time in ISO 8601 YYYY-MM-DDThh:mm:ssZ format. For example, 2019-08-20T10:18:56Z. Note that output is in UTC (Z means UTC). unix_timestamp - Unix timestamp output format. ClickHouse output date and time in Unix timestamp format. For example 1566285536. Default value: simple. See also: DateTime data type.Functions for working with dates and times. "},{"title":"join_default_strictness​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-join_default_strictness","content":"Sets default strictness for JOIN clauses. Possible values: ALL — If the right table has several matching rows, ClickHouse creates a Cartesian product from matching rows. This is the normal JOIN behaviour from standard SQL.ANY — If the right table has several matching rows, only the first one found is joined. If the right table has only one matching row, the results of ANY and ALL are the same.ASOF — For joining sequences with an uncertain match.Empty string — If ALL or ANY is not specified in the query, ClickHouse throws an exception. Default value: ALL. "},{"title":"join_algorithm​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-join_algorithm","content":"Specifies JOIN algorithm. Possible values: hash — Hash join algorithm is used.partial_merge — Sort-merge algorithm is used.prefer_partial_merge — ClickHouse always tries to use merge join if possible.auto — ClickHouse tries to change hash join to merge join on the fly to avoid out of memory. Default value: hash. When using hash algorithm the right part of JOIN is uploaded into RAM. When using partial_merge algorithm ClickHouse sorts the data and dumps it to the disk. The merge algorithm in ClickHouse differs a bit from the classic realization. First ClickHouse sorts the right table by join key in blocks and creates min-max index for sorted blocks. Then it sorts parts of left table by join key and joins them over right table. The min-max index is also used to skip unneeded right table blocks. "},{"title":"join_any_take_last_row​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-join_any_take_last_row","content":"Changes behaviour of join operations with ANY strictness. warning This setting applies only for JOIN operations with Join engine tables. Possible values: 0 — If the right table has more than one matching row, only the first one found is joined.1 — If the right table has more than one matching row, only the last one found is joined. Default value: 0. See also: JOIN clauseJoin table enginejoin_default_strictness "},{"title":"join_use_nulls​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#join_use_nulls","content":"Sets the type of JOIN behaviour. When merging tables, empty cells may appear. ClickHouse fills them differently based on this setting. Possible values: 0 — The empty cells are filled with the default value of the corresponding field type.1 — JOIN behaves the same way as in standard SQL. The type of the corresponding field is converted to Nullable, and empty cells are filled with NULL. Default value: 0. "},{"title":"partial_merge_join_optimizations​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#partial_merge_join_optimizations","content":"Disables optimizations in partial merge join algorithm for JOIN queries. By default, this setting enables improvements that could lead to wrong results. If you see suspicious results in your queries, disable optimizations by this setting. Optimizations can be different in different versions of the ClickHouse server. Possible values: 0 — Optimizations disabled.1 — Optimizations enabled. Default value: 1. "},{"title":"partial_merge_join_rows_in_right_blocks​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#partial_merge_join_rows_in_right_blocks","content":"Limits sizes of right-hand join data blocks in partial merge join algorithm for JOIN queries. ClickHouse server: Splits right-hand join data into blocks with up to the specified number of rows.Indexes each block with its minimum and maximum values.Unloads prepared blocks to disk if it is possible. Possible values: Any positive integer. Recommended range of values: [1000, 100000]. Default value: 65536. "},{"title":"join_on_disk_max_files_to_merge​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#join_on_disk_max_files_to_merge","content":"Limits the number of files allowed for parallel sorting in MergeJoin operations when they are executed on disk. The bigger the value of the setting, the more RAM used and the less disk I/O needed. Possible values: Any positive integer, starting from 2. Default value: 64. "},{"title":"any_join_distinct_right_table_keys​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#any_join_distinct_right_table_keys","content":"Enables legacy ClickHouse server behaviour in ANY INNER|LEFT JOIN operations. warning Use this setting only for backward compatibility if your use cases depend on legacy JOIN behaviour. When the legacy behaviour enabled: Results of t1 ANY LEFT JOIN t2 and t2 ANY RIGHT JOIN t1 operations are not equal because ClickHouse uses the logic with many-to-one left-to-right table keys mapping.Results of ANY INNER JOIN operations contain all rows from the left table like the SEMI LEFT JOIN operations do. When the legacy behaviour disabled: Results of t1 ANY LEFT JOIN t2 and t2 ANY RIGHT JOIN t1 operations are equal because ClickHouse uses the logic which provides one-to-many keys mapping in ANY RIGHT JOIN operations.Results of ANY INNER JOIN operations contain one row per key from both the left and right tables. Possible values: 0 — Legacy behaviour is disabled.1 — Legacy behaviour is enabled. Default value: 0. See also: JOIN strictness "},{"title":"temporary_files_codec​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#temporary_files_codec","content":"Sets compression codec for temporary files used in sorting and joining operations on disk. Possible values: LZ4 — LZ4 compression is applied.NONE — No compression is applied. Default value: LZ4. "},{"title":"max_block_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#setting-max_block_size","content":"In ClickHouse, data is processed by blocks (sets of column parts). The internal processing cycles for a single block are efficient enough, but there are noticeable expenditures on each block. The max_block_size setting is a recommendation for what size of the block (in a count of rows) to load from tables. The block size shouldn’t be too small, so that the expenditures on each block are still noticeable, but not too large so that the query with LIMIT that is completed after the first block is processed quickly. The goal is to avoid consuming too much memory when extracting a large number of columns in multiple threads and to preserve at least some cache locality. Default value: 65,536. Blocks the size of max_block_size are not always loaded from the table. If it is obvious that less data needs to be retrieved, a smaller block is processed. "},{"title":"preferred_block_size_bytes​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#preferred-block-size-bytes","content":"Used for the same purpose as max_block_size, but it sets the recommended block size in bytes by adapting it to the number of rows in the block. However, the block size cannot be more than max_block_size rows. By default: 1,000,000. It only works when reading from MergeTree engines. "},{"title":"merge_tree_min_rows_for_concurrent_read​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#setting-merge-tree-min-rows-for-concurrent-read","content":"If the number of rows to be read from a file of a MergeTree table exceeds merge_tree_min_rows_for_concurrent_read then ClickHouse tries to perform a concurrent reading from this file on several threads. Possible values: Positive integer. Default value: 163840. "},{"title":"merge_tree_min_rows_for_concurrent_read_for_remote_filesystem​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#merge-tree-min-rows-for-concurrent-read-for-remote-filesystem","content":"The minimum number of lines to read from one file before MergeTree engine can parallelize reading, when reading from remote filesystem. Possible values: Positive integer. Default value: 163840. "},{"title":"merge_tree_min_bytes_for_concurrent_read​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#setting-merge-tree-min-bytes-for-concurrent-read","content":"If the number of bytes to read from one file of a MergeTree-engine table exceeds merge_tree_min_bytes_for_concurrent_read, then ClickHouse tries to concurrently read from this file in several threads. Possible value: Positive integer. Default value: 251658240. "},{"title":"merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#merge-tree-min-bytes-for-concurrent-read-for-remote-filesystem","content":"The minimum number of bytes to read from one file before MergeTree engine can parallelize reading, when reading from remote filesystem. Possible values: Positive integer. Default value: 251658240. "},{"title":"merge_tree_min_rows_for_seek​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#setting-merge-tree-min-rows-for-seek","content":"If the distance between two data blocks to be read in one file is less than merge_tree_min_rows_for_seek rows, then ClickHouse does not seek through the file but reads the data sequentially. Possible values: Any positive integer. Default value: 0. "},{"title":"merge_tree_min_bytes_for_seek​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#setting-merge-tree-min-bytes-for-seek","content":"If the distance between two data blocks to be read in one file is less than merge_tree_min_bytes_for_seek bytes, then ClickHouse sequentially reads a range of file that contains both blocks, thus avoiding extra seek. Possible values: Any positive integer. Default value: 0. "},{"title":"merge_tree_coarse_index_granularity​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#setting-merge-tree-coarse-index-granularity","content":"When searching for data, ClickHouse checks the data marks in the index file. If ClickHouse finds that required keys are in some range, it divides this range into merge_tree_coarse_index_granularity subranges and searches the required keys there recursively. Possible values: Any positive even integer. Default value: 8. "},{"title":"merge_tree_max_rows_to_use_cache​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#setting-merge-tree-max-rows-to-use-cache","content":"If ClickHouse should read more than merge_tree_max_rows_to_use_cache rows in one query, it does not use the cache of uncompressed blocks. The cache of uncompressed blocks stores data extracted for queries. ClickHouse uses this cache to speed up responses to repeated small queries. This setting protects the cache from trashing by queries that read a large amount of data. The uncompressed_cache_size server setting defines the size of the cache of uncompressed blocks. Possible values: Any positive integer. Default value: 128 ✕ 8192. "},{"title":"merge_tree_max_bytes_to_use_cache​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#setting-merge-tree-max-bytes-to-use-cache","content":"If ClickHouse should read more than merge_tree_max_bytes_to_use_cache bytes in one query, it does not use the cache of uncompressed blocks. The cache of uncompressed blocks stores data extracted for queries. ClickHouse uses this cache to speed up responses to repeated small queries. This setting protects the cache from trashing by queries that read a large amount of data. The uncompressed_cache_size server setting defines the size of the cache of uncompressed blocks. Possible values: Any positive integer. Default value: 2013265920. "},{"title":"min_bytes_to_use_direct_io​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-min-bytes-to-use-direct-io","content":"The minimum data volume required for using direct I/O access to the storage disk. ClickHouse uses this setting when reading data from tables. If the total storage volume of all the data to be read exceeds min_bytes_to_use_direct_io bytes, then ClickHouse reads the data from the storage disk with the O_DIRECT option. Possible values: 0 — Direct I/O is disabled.Positive integer. Default value: 0. "},{"title":"network_compression_method​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#network_compression_method","content":"Sets the method of data compression that is used for communication between servers and between server and clickhouse-client. Possible values: LZ4 — sets LZ4 compression method.ZSTD — sets ZSTD compression method. Default value: LZ4. See Also network_zstd_compression_level "},{"title":"network_zstd_compression_level​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#network_zstd_compression_level","content":"Adjusts the level of ZSTD compression. Used only when network_compression_method is set to ZSTD. Possible values: Positive integer from 1 to 15. Default value: 1. "},{"title":"log_queries​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-log-queries","content":"Setting up query logging. Queries sent to ClickHouse with this setup are logged according to the rules in the query_log server configuration parameter. Example: log_queries=1  "},{"title":"log_queries_min_query_duration_ms​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-log-queries-min-query-duration-ms","content":"If enabled (non-zero), queries faster then the value of this setting will not be logged (you can think about this as a long_query_time for MySQL Slow Query Log), and this basically means that you will not find them in the following tables: system.query_logsystem.query_thread_log Only the queries with the following type will get to the log: QUERY_FINISH EXCEPTION_WHILE_PROCESSING Type: milliseconds Default value: 0 (any query) "},{"title":"log_queries_min_type​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-log-queries-min-type","content":"query_log minimal type to log. Possible values: QUERY_START (=1)QUERY_FINISH (=2)EXCEPTION_BEFORE_START (=3)EXCEPTION_WHILE_PROCESSING (=4) Default value: QUERY_START. Can be used to limit which entities will go to query_log, say you are interested only in errors, then you can use EXCEPTION_WHILE_PROCESSING: log_queries_min_type='EXCEPTION_WHILE_PROCESSING'  "},{"title":"log_query_threads​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-log-query-threads","content":"Setting up query threads logging. Query threads log into system.query_thread_log table. This setting have effect only when log_queries is true. Queries’ threads run by ClickHouse with this setup are logged according to the rules in the query_thread_log server configuration parameter. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. Example log_query_threads=1  "},{"title":"log_query_views​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-log-query-views","content":"Setting up query views logging. When a query run by ClickHouse with this setup on has associated views (materialized or live views), they are logged in the query_views_log server configuration parameter. Example: log_query_views=1  "},{"title":"log_formatted_queries​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-log-formatted-queries","content":"Allows to log formatted queries to the system.query_log system table. Possible values: 0 — Formatted queries are not logged in the system table.1 — Formatted queries are logged in the system table. Default value: 0. "},{"title":"log_comment​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-log-comment","content":"Specifies the value for the log_comment field of the system.query_log table and comment text for the server log. It can be used to improve the readability of server logs. Additionally, it helps to select queries related to the test from the system.query_log after running clickhouse-test. Possible values: Any string no longer than max_query_size. If length is exceeded, the server throws an exception. Default value: empty string. Example Query: SET log_comment = 'log_comment test', log_queries = 1; SELECT 1; SYSTEM FLUSH LOGS; SELECT type, query FROM system.query_log WHERE log_comment = 'log_comment test' AND event_date &gt;= yesterday() ORDER BY event_time DESC LIMIT 2;  Result: ┌─type────────┬─query─────┐ │ QueryStart │ SELECT 1; │ │ QueryFinish │ SELECT 1; │ └─────────────┴───────────┘  "},{"title":"max_insert_block_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-max_insert_block_size","content":"The size of blocks (in a count of rows) to form for insertion into a table. This setting only applies in cases when the server forms the blocks. For example, for an INSERT via the HTTP interface, the server parses the data format and forms blocks of the specified size. But when using clickhouse-client, the client parses the data itself, and the ‘max_insert_block_size’ setting on the server does not affect the size of the inserted blocks. The setting also does not have a purpose when using INSERT SELECT, since data is inserted using the same blocks that are formed after SELECT. Default value: 1,048,576. The default is slightly more than max_block_size. The reason for this is because certain table engines (*MergeTree) form a data part on the disk for each inserted block, which is a fairly large entity. Similarly, *MergeTree tables sort data during insertion, and a large enough block size allow sorting more data in RAM. "},{"title":"min_insert_block_size_rows​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#min-insert-block-size-rows","content":"Sets the minimum number of rows in the block which can be inserted into a table by an INSERT query. Smaller-sized blocks are squashed into bigger ones. Possible values: Positive integer.0 — Squashing disabled. Default value: 1048576. "},{"title":"min_insert_block_size_bytes​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#min-insert-block-size-bytes","content":"Sets the minimum number of bytes in the block which can be inserted into a table by an INSERT query. Smaller-sized blocks are squashed into bigger ones. Possible values: Positive integer.0 — Squashing disabled. Default value: 268435456. "},{"title":"max_replica_delay_for_distributed_queries​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-max_replica_delay_for_distributed_queries","content":"Disables lagging replicas for distributed queries. See Replication. Sets the time in seconds. If a replica lags more than the set value, this replica is not used. Default value: 300. Used when performing SELECT from a distributed table that points to replicated tables. "},{"title":"max_threads​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-max_threads","content":"The maximum number of query processing threads, excluding threads for retrieving data from remote servers (see the ‘max_distributed_connections’ parameter). This parameter applies to threads that perform the same stages of the query processing pipeline in parallel. For example, when reading from a table, if it is possible to evaluate expressions with functions, filter with WHERE and pre-aggregate for GROUP BY in parallel using at least ‘max_threads’ number of threads, then ‘max_threads’ are used. Default value: the number of physical CPU cores. For queries that are completed quickly because of a LIMIT, you can set a lower ‘max_threads’. For example, if the necessary number of entries are located in every block and max_threads = 8, then 8 blocks are retrieved, although it would have been enough to read just one. The smaller the max_threads value, the less memory is consumed. "},{"title":"max_insert_threads​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-max-insert-threads","content":"The maximum number of threads to execute the INSERT SELECT query. Possible values: 0 (or 1) — INSERT SELECT no parallel execution.Positive integer. Bigger than 1. Default value: 0. Parallel INSERT SELECT has effect only if the SELECT part is executed in parallel, see max_threads setting. Higher values will lead to higher memory usage. "},{"title":"max_compress_block_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#max-compress-block-size","content":"The maximum size of blocks of uncompressed data before compressing for writing to a table. By default, 1,048,576 (1 MiB). Specifying smaller block size generally leads to slightly reduced compression ratio, the compression and decompression speed increases slightly due to cache locality, and memory consumption is reduced. warning This is an expert-level setting, and you shouldn't change it if you're just getting started with ClickHouse. Don’t confuse blocks for compression (a chunk of memory consisting of bytes) with blocks for query processing (a set of rows from a table). "},{"title":"min_compress_block_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#min-compress-block-size","content":"For MergeTree tables. In order to reduce latency when processing queries, a block is compressed when writing the next mark if its size is at least min_compress_block_size. By default, 65,536. The actual size of the block, if the uncompressed data is less than max_compress_block_size, is no less than this value and no less than the volume of data for one mark. Let’s look at an example. Assume that index_granularity was set to 8192 during table creation. We are writing a UInt32-type column (4 bytes per value). When writing 8192 rows, the total will be 32 KB of data. Since min_compress_block_size = 65,536, a compressed block will be formed for every two marks. We are writing a URL column with the String type (average size of 60 bytes per value). When writing 8192 rows, the average will be slightly less than 500 KB of data. Since this is more than 65,536, a compressed block will be formed for each mark. In this case, when reading data from the disk in the range of a single mark, extra data won’t be decompressed. warning This is an expert-level setting, and you shouldn't change it if you're just getting started with ClickHouse. "},{"title":"max_query_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-max_query_size","content":"The maximum part of a query that can be taken to RAM for parsing with the SQL parser. The INSERT query also contains data for INSERT that is processed by a separate stream parser (that consumes O(1) RAM), which is not included in this restriction. Default value: 256 KiB. "},{"title":"max_parser_depth​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#max_parser_depth","content":"Limits maximum recursion depth in the recursive descent parser. Allows controlling the stack size. Possible values: Positive integer.0 — Recursion depth is unlimited. Default value: 1000. "},{"title":"interactive_delay​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#interactive-delay","content":"The interval in microseconds for checking whether request execution has been cancelled and sending the progress. Default value: 100,000 (checks for cancelling and sends the progress ten times per second). "},{"title":"connect_timeout, receive_timeout, send_timeout​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#connect-timeout-receive-timeout-send-timeout","content":"Timeouts in seconds on the socket used for communicating with the client. Default value: 10, 300, 300. "},{"title":"cancel_http_readonly_queries_on_client_close​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#cancel-http-readonly-queries-on-client-close","content":"Cancels HTTP read-only queries (e.g. SELECT) when a client closes the connection without waiting for the response. Default value: 0 "},{"title":"poll_interval​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#poll-interval","content":"Lock in a wait loop for the specified number of seconds. Default value: 10. "},{"title":"max_distributed_connections​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#max-distributed-connections","content":"The maximum number of simultaneous connections with remote servers for distributed processing of a single query to a single Distributed table. We recommend setting a value no less than the number of servers in the cluster. Default value: 1024. The following parameters are only used when creating Distributed tables (and when launching a server), so there is no reason to change them at runtime. "},{"title":"distributed_connections_pool_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#distributed-connections-pool-size","content":"The maximum number of simultaneous connections with remote servers for distributed processing of all queries to a single Distributed table. We recommend setting a value no less than the number of servers in the cluster. Default value: 1024. "},{"title":"max_distributed_depth​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#max-distributed-depth","content":"Limits the maximum depth of recursive queries for Distributed tables. If the value is exceeded, the server throws an exception. Possible values: Positive integer.0 — Unlimited depth. Default value: 5. "},{"title":"max_replicated_fetches_network_bandwidth_for_server​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#max_replicated_fetches_network_bandwidth_for_server","content":"Limits the maximum speed of data exchange over the network in bytes per second for replicated fetches for the server. Only has meaning at server startup. You can also limit the speed for a particular table with max_replicated_fetches_network_bandwidth setting. The setting isn't followed perfectly accurately. Possible values: Positive integer.0 — Unlimited. Default value: 0. Usage Could be used for throttling speed when replicating the data to add or replace new nodes. note 60000000 bytes/s approximatly corresponds to 457 Mbps (60000000 / 1024 / 1024 * 8). "},{"title":"max_replicated_sends_network_bandwidth_for_server​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#max_replicated_sends_network_bandwidth_for_server","content":"Limits the maximum speed of data exchange over the network in bytes per second for replicated sends for the server. Only has meaning at server startup. You can also limit the speed for a particular table with max_replicated_sends_network_bandwidth setting. The setting isn't followed perfectly accurately. Possible values: Positive integer.0 — Unlimited. Default value: 0. Usage Could be used for throttling speed when replicating the data to add or replace new nodes. note 60000000 bytes/s approximatly corresponds to 457 Mbps (60000000 / 1024 / 1024 * 8). "},{"title":"connect_timeout_with_failover_ms​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#connect-timeout-with-failover-ms","content":"The timeout in milliseconds for connecting to a remote server for a Distributed table engine, if the ‘shard’ and ‘replica’ sections are used in the cluster definition. If unsuccessful, several attempts are made to connect to various replicas. Default value: 50. "},{"title":"connection_pool_max_wait_ms​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#connection-pool-max-wait-ms","content":"The wait time in milliseconds for a connection when the connection pool is full. Possible values: Positive integer.0 — Infinite timeout. Default value: 0. "},{"title":"connections_with_failover_max_tries​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#connections-with-failover-max-tries","content":"The maximum number of connection attempts with each replica for the Distributed table engine. Default value: 3. "},{"title":"extremes​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#extremes","content":"Whether to count extreme values (the minimums and maximums in columns of a query result). Accepts 0 or 1. By default, 0 (disabled). For more information, see the section “Extreme values”. "},{"title":"kafka_max_wait_ms​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#kafka-max-wait-ms","content":"The wait time in milliseconds for reading messages from Kafka before retry. Possible values: Positive integer.0 — Infinite timeout. Default value: 5000. See also: Apache Kafka "},{"title":"use_uncompressed_cache​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#setting-use_uncompressed_cache","content":"Whether to use a cache of uncompressed blocks. Accepts 0 or 1. By default, 0 (disabled). Using the uncompressed cache (only for tables in the MergeTree family) can significantly reduce latency and increase throughput when working with a large number of short queries. Enable this setting for users who send frequent short requests. Also pay attention to the uncompressed_cache_size configuration parameter (only set in the config file) – the size of uncompressed cache blocks. By default, it is 8 GiB. The uncompressed cache is filled in as needed and the least-used data is automatically deleted. For queries that read at least a somewhat large volume of data (one million rows or more), the uncompressed cache is disabled automatically to save space for truly small queries. This means that you can keep the ‘use_uncompressed_cache’ setting always set to 1. "},{"title":"replace_running_query​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#replace-running-query","content":"When using the HTTP interface, the ‘query_id’ parameter can be passed. This is any string that serves as the query identifier. If a query from the same user with the same ‘query_id’ already exists at this time, the behaviour depends on the ‘replace_running_query’ parameter. 0 (default) – Throw an exception (do not allow the query to run if a query with the same ‘query_id’ is already running). 1 – Cancel the old query and start running the new one. Set this parameter to 1 for implementing suggestions for segmentation conditions. After entering the next character, if the old query hasn’t finished yet, it should be cancelled. "},{"title":"replace_running_query_max_wait_ms​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#replace-running-query-max-wait-ms","content":"The wait time for running the query with the same query_id to finish, when the replace_running_query setting is active. Possible values: Positive integer.0 — Throwing an exception that does not allow to run a new query if the server already executes a query with the same query_id. Default value: 5000. "},{"title":"stream_flush_interval_ms​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#stream-flush-interval-ms","content":"Works for tables with streaming in the case of a timeout, or when a thread generates max_insert_block_size rows. The default value is 7500. The smaller the value, the more often data is flushed into the table. Setting the value too low leads to poor performance. "},{"title":"load_balancing​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-load_balancing","content":"Specifies the algorithm of replicas selection that is used for distributed query processing. ClickHouse supports the following algorithms of choosing replicas: Random (by default)Nearest hostnameIn orderFirst or randomRound robin See also: distributed_replica_max_ignored_errors "},{"title":"Random (by Default)​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#load_balancing-random","content":"load_balancing = random  The number of errors is counted for each replica. The query is sent to the replica with the fewest errors, and if there are several of these, to anyone of them. Disadvantages: Server proximity is not accounted for; if the replicas have different data, you will also get different data. "},{"title":"Nearest Hostname​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#load_balancing-nearest_hostname","content":"load_balancing = nearest_hostname  The number of errors is counted for each replica. Every 5 minutes, the number of errors is integrally divided by 2. Thus, the number of errors is calculated for a recent time with exponential smoothing. If there is one replica with a minimal number of errors (i.e. errors occurred recently on the other replicas), the query is sent to it. If there are multiple replicas with the same minimal number of errors, the query is sent to the replica with a hostname that is most similar to the server’s hostname in the config file (for the number of different characters in identical positions, up to the minimum length of both hostnames). For instance, example01-01-1 and example01-01-2 are different in one position, while example01-01-1 and example01-02-2 differ in two places. This method might seem primitive, but it does not require external data about network topology, and it does not compare IP addresses, which would be complicated for our IPv6 addresses. Thus, if there are equivalent replicas, the closest one by name is preferred. We can also assume that when sending a query to the same server, in the absence of failures, a distributed query will also go to the same servers. So even if different data is placed on the replicas, the query will return mostly the same results. "},{"title":"In Order​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#load_balancing-in_order","content":"load_balancing = in_order  Replicas with the same number of errors are accessed in the same order as they are specified in the configuration. This method is appropriate when you know exactly which replica is preferable. "},{"title":"First or Random​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#load_balancing-first_or_random","content":"load_balancing = first_or_random  This algorithm chooses the first replica in the set or a random replica if the first is unavailable. It’s effective in cross-replication topology setups, but useless in other configurations. The first_or_random algorithm solves the problem of the in_order algorithm. With in_order, if one replica goes down, the next one gets a double load while the remaining replicas handle the usual amount of traffic. When using the first_or_random algorithm, the load is evenly distributed among replicas that are still available. It's possible to explicitly define what the first replica is by using the setting load_balancing_first_offset. This gives more control to rebalance query workloads among replicas. "},{"title":"Round Robin​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#load_balancing-round_robin","content":"load_balancing = round_robin  This algorithm uses a round-robin policy across replicas with the same number of errors (only the queries with round_robin policy is accounted). "},{"title":"prefer_localhost_replica​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-prefer-localhost-replica","content":"Enables/disables preferable using the localhost replica when processing distributed queries. Possible values: 1 — ClickHouse always sends a query to the localhost replica if it exists.0 — ClickHouse uses the balancing strategy specified by the load_balancing setting. Default value: 1. warning Disable this setting if you use max_parallel_replicas. "},{"title":"totals_mode​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#totals-mode","content":"How to calculate TOTALS when HAVING is present, as well as when max_rows_to_group_by and group_by_overflow_mode = ‘any’ are present. See the section “WITH TOTALS modifier”. "},{"title":"totals_auto_threshold​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#totals-auto-threshold","content":"The threshold for totals_mode = 'auto'. See the section “WITH TOTALS modifier”. "},{"title":"max_parallel_replicas​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-max_parallel_replicas","content":"The maximum number of replicas for each shard when executing a query. Possible values: Positive integer. Default value: 1. Additional Info This setting is useful for replicated tables with a sampling key. A query may be processed faster if it is executed on several servers in parallel. But the query performance may degrade in the following cases: The position of the sampling key in the partitioning key does not allow efficient range scans.Adding a sampling key to the table makes filtering by other columns less efficient.The sampling key is an expression that is expensive to calculate.The cluster latency distribution has a long tail, so that querying more servers increases the query overall latency. warning This setting will produce incorrect results when joins or subqueries are involved, and all tables don't meet certain requirements. See Distributed Subqueries and max_parallel_replicas for more details. "},{"title":"compile_expressions​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#compile-expressions","content":"Enables or disables compilation of frequently used simple functions and operators to native code with LLVM at runtime. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. "},{"title":"min_count_to_compile_expression​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#min-count-to-compile-expression","content":"Minimum count of executing same expression before it is get compiled. Default value: 3. "},{"title":"compile_aggregate_expressions​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#compile_aggregate_expressions","content":"Enables or disables JIT-compilation of aggregate functions to native code. Enabling this setting can improve the performance. Possible values: 0 — Aggregation is done without JIT compilation.1 — Aggregation is done using JIT compilation. Default value: 1. See Also min_count_to_compile_aggregate_expression "},{"title":"min_count_to_compile_aggregate_expression​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#min_count_to_compile_aggregate_expression","content":"The minimum number of identical aggregate expressions to start JIT-compilation. Works only if the compile_aggregate_expressions setting is enabled. Possible values: Positive integer.0 — Identical aggregate expressions are always JIT-compiled. Default value: 3. "},{"title":"output_format_json_quote_64bit_integers​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#session_settings-output_format_json_quote_64bit_integers","content":"Controls quoting of 64-bit or bigger integers (like UInt64 or Int128) when they are output in a JSON format. Such integers are enclosed in quotes by default. This behavior is compatible with most JavaScript implementations. Possible values: 0 — Integers are output without quotes.1 — Integers are enclosed in quotes. Default value: 1. "},{"title":"output_format_json_quote_denormals​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-output_format_json_quote_denormals","content":"Enables +nan, -nan, +inf, -inf outputs in JSON output format. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. Example Consider the following table account_orders: ┌─id─┬─name───┬─duration─┬─period─┬─area─┐ │ 1 │ Andrew │ 20 │ 0 │ 400 │ │ 2 │ John │ 40 │ 0 │ 0 │ │ 3 │ Bob │ 15 │ 0 │ -100 │ └────┴────────┴──────────┴────────┴──────┘  When output_format_json_quote_denormals = 0, the query returns null values in output: SELECT area/period FROM account_orders FORMAT JSON;  { &quot;meta&quot;: [ { &quot;name&quot;: &quot;divide(area, period)&quot;, &quot;type&quot;: &quot;Float64&quot; } ], &quot;data&quot;: [ { &quot;divide(area, period)&quot;: null }, { &quot;divide(area, period)&quot;: null }, { &quot;divide(area, period)&quot;: null } ], &quot;rows&quot;: 3, &quot;statistics&quot;: { &quot;elapsed&quot;: 0.003648093, &quot;rows_read&quot;: 3, &quot;bytes_read&quot;: 24 } }  When output_format_json_quote_denormals = 1, the query returns: { &quot;meta&quot;: [ { &quot;name&quot;: &quot;divide(area, period)&quot;, &quot;type&quot;: &quot;Float64&quot; } ], &quot;data&quot;: [ { &quot;divide(area, period)&quot;: &quot;inf&quot; }, { &quot;divide(area, period)&quot;: &quot;-nan&quot; }, { &quot;divide(area, period)&quot;: &quot;-inf&quot; } ], &quot;rows&quot;: 3, &quot;statistics&quot;: { &quot;elapsed&quot;: 0.000070241, &quot;rows_read&quot;: 3, &quot;bytes_read&quot;: 24 } }  "},{"title":"format_csv_delimiter​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-format_csv_delimiter","content":"The character is interpreted as a delimiter in the CSV data. By default, the delimiter is ,. "},{"title":"input_format_csv_enum_as_number​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-input_format_csv_enum_as_number","content":"When enabled, always treat enum values as enum ids for CSV input format. It's recommended to enable this setting if data contains only enum ids to optimize enum parsing. Possible values: 0 — Enum values are parsed as values or as enum IDs.1 — Enum values are parsed only as enum IDs. Default value: 0. Examples Consider the table: CREATE TABLE table_with_enum_column_for_csv_insert (Id Int32,Value Enum('first' = 1, 'second' = 2)) ENGINE=Memory();  When the input_format_csv_enum_as_number setting is enabled: Query: SET input_format_csv_enum_as_number = 1; INSERT INTO table_with_enum_column_for_csv_insert FORMAT CSV 102,2  Result: ┌──Id─┬─Value──┐ │ 102 │ second │ └─────┴────────┘  Query: SET input_format_csv_enum_as_number = 1; INSERT INTO table_with_enum_column_for_csv_insert FORMAT CSV 103,'first'  throws an exception. When the input_format_csv_enum_as_number setting is disabled: Query: SET input_format_csv_enum_as_number = 0; INSERT INTO table_with_enum_column_for_csv_insert FORMAT CSV 102,2 INSERT INTO table_with_enum_column_for_csv_insert FORMAT CSV 103,'first' SELECT * FROM table_with_enum_column_for_csv_insert;  Result: ┌──Id─┬─Value──┐ │ 102 │ second │ └─────┴────────┘ ┌──Id─┬─Value─┐ │ 103 │ first │ └─────┴───────┘  "},{"title":"output_format_csv_crlf_end_of_line​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-output-format-csv-crlf-end-of-line","content":"Use DOS/Windows-style line separator (CRLF) in CSV instead of Unix style (LF). "},{"title":"output_format_tsv_crlf_end_of_line​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-output-format-tsv-crlf-end-of-line","content":"Use DOC/Windows-style line separator (CRLF) in TSV instead of Unix style (LF). "},{"title":"insert_quorum​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-insert_quorum","content":"Enables the quorum writes. If insert_quorum &lt; 2, the quorum writes are disabled.If insert_quorum &gt;= 2, the quorum writes are enabled. Default value: 0. Quorum writes INSERT succeeds only when ClickHouse manages to correctly write data to the insert_quorum of replicas during the insert_quorum_timeout. If for any reason the number of replicas with successful writes does not reach the insert_quorum, the write is considered failed and ClickHouse will delete the inserted block from all the replicas where data has already been written. When insert_quorum_parallel is disabled, all replicas in the quorum are consistent, i.e. they contain data from all previous INSERT queries (the INSERT sequence is linearized). When reading data written using insert_quorum and insert_quorum_parallel is disabled, you can turn on sequential consistency for SELECT queries using select_sequential_consistency. ClickHouse generates an exception: If the number of available replicas at the time of the query is less than the insert_quorum.When insert_quorum_parallel is disabled and an attempt to write data is made when the previous block has not yet been inserted in insert_quorum of replicas. This situation may occur if the user tries to perform another INSERT query to the same table before the previous one with insert_quorum is completed. See also: insert_quorum_timeoutinsert_quorum_parallelselect_sequential_consistency "},{"title":"insert_quorum_timeout​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-insert_quorum_timeout","content":"Write to a quorum timeout in milliseconds. If the timeout has passed and no write has taken place yet, ClickHouse will generate an exception and the client must repeat the query to write the same block to the same or any other replica. Default value: 600 000 milliseconds (ten minutes). See also: insert_quoruminsert_quorum_parallelselect_sequential_consistency "},{"title":"insert_quorum_parallel​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-insert_quorum_parallel","content":"Enables or disables parallelism for quorum INSERT queries. If enabled, additional INSERT queries can be sent while previous queries have not yet finished. If disabled, additional writes to the same table will be rejected. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. See also: insert_quoruminsert_quorum_timeoutselect_sequential_consistency "},{"title":"select_sequential_consistency​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-select_sequential_consistency","content":"Enables or disables sequential consistency for SELECT queries. Requires insert_quorum_parallel to be disabled (enabled by default). Possible values: 0 — Disabled.1 — Enabled. Default value: 0. Usage When sequential consistency is enabled, ClickHouse allows the client to execute the SELECT query only for those replicas that contain data from all previous INSERT queries executed with insert_quorum. If the client refers to a partial replica, ClickHouse will generate an exception. The SELECT query will not include data that has not yet been written to the quorum of replicas. When insert_quorum_parallel is enabled (the default), then select_sequential_consistency does not work. This is because parallel INSERT queries can be written to different sets of quorum replicas so there is no guarantee a single replica will have received all writes. See also: insert_quoruminsert_quorum_timeoutinsert_quorum_parallel "},{"title":"insert_deduplicate​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-insert-deduplicate","content":"Enables or disables block deduplication of INSERT (for Replicated* tables). Possible values: 0 — Disabled.1 — Enabled. Default value: 1. By default, blocks inserted into replicated tables by the INSERT statement are deduplicated (see Data Replication). "},{"title":"deduplicate_blocks_in_dependent_materialized_views​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-deduplicate-blocks-in-dependent-materialized-views","content":"Enables or disables the deduplication check for materialized views that receive data from Replicated* tables. Possible values:  0 — Disabled. 1 — Enabled.  Default value: 0. Usage By default, deduplication is not performed for materialized views but is done upstream, in the source table. If an INSERTed block is skipped due to deduplication in the source table, there will be no insertion into attached materialized views. This behaviour exists to enable the insertion of highly aggregated data into materialized views, for cases where inserted blocks are the same after materialized view aggregation but derived from different INSERTs into the source table. At the same time, this behaviour “breaks” INSERT idempotency. If an INSERT into the main table was successful and INSERT into a materialized view failed (e.g. because of communication failure with Zookeeper) a client will get an error and can retry the operation. However, the materialized view won’t receive the second insert because it will be discarded by deduplication in the main (source) table. The setting deduplicate_blocks_in_dependent_materialized_views allows for changing this behaviour. On retry, a materialized view will receive the repeat insert and will perform a deduplication check by itself, ignoring check result for the source table, and will insert rows lost because of the first failure. "},{"title":"insert_deduplication_token​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#insert_deduplication_token","content":"The setting allows a user to provide own deduplication semantic in MergeTree/ReplicatedMergeTree For example, by providing a unique value for the setting in each INSERT statement, user can avoid the same inserted data being deduplicated. Possilbe values: Any string Default value: empty string (disabled) insert_deduplication_token is used for deduplication only when not empty. Example: CREATE TABLE test_table ( A Int64 ) ENGINE = MergeTree ORDER BY A SETTINGS non_replicated_deduplication_window = 100; INSERT INTO test_table Values SETTINGS insert_deduplication_token = 'test' (1); -- the next insert won't be deduplicated because insert_deduplication_token is different INSERT INTO test_table Values SETTINGS insert_deduplication_token = 'test1' (1); -- the next insert will be deduplicated because insert_deduplication_token -- is the same as one of the previous INSERT INTO test_table Values SETTINGS insert_deduplication_token = 'test' (2); SELECT * FROM test_table ┌─A─┐ │ 1 │ └───┘ ┌─A─┐ │ 1 │ └───┘  "},{"title":"max_network_bytes​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-max-network-bytes","content":"Limits the data volume (in bytes) that is received or transmitted over the network when executing a query. This setting applies to every individual query. Possible values: Positive integer.0 — Data volume control is disabled. Default value: 0. "},{"title":"max_network_bandwidth​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-max-network-bandwidth","content":"Limits the speed of the data exchange over the network in bytes per second. This setting applies to every query. Possible values: Positive integer.0 — Bandwidth control is disabled. Default value: 0. "},{"title":"max_network_bandwidth_for_user​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-max-network-bandwidth-for-user","content":"Limits the speed of the data exchange over the network in bytes per second. This setting applies to all concurrently running queries performed by a single user. Possible values: Positive integer.0 — Control of the data speed is disabled. Default value: 0. "},{"title":"max_network_bandwidth_for_all_users​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-max-network-bandwidth-for-all-users","content":"Limits the speed that data is exchanged at over the network in bytes per second. This setting applies to all concurrently running queries on the server. Possible values: Positive integer.0 — Control of the data speed is disabled. Default value: 0. "},{"title":"count_distinct_implementation​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-count_distinct_implementation","content":"Specifies which of the uniq* functions should be used to perform the COUNT(DISTINCT …) construction. Possible values: uniquniqCombineduniqCombined64uniqHLL12uniqExact Default value: uniqExact. "},{"title":"skip_unavailable_shards​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-skip_unavailable_shards","content":"Enables or disables silently skipping of unavailable shards. Shard is considered unavailable if all its replicas are unavailable. A replica is unavailable in the following cases: ClickHouse can’t connect to replica for any reason. When connecting to a replica, ClickHouse performs several attempts. If all these attempts fail, the replica is considered unavailable. Replica can’t be resolved through DNS. If replica’s hostname can’t be resolved through DNS, it can indicate the following situations: Replica’s host has no DNS record. It can occur in systems with dynamic DNS, for example, Kubernetes, where nodes can be unresolvable during downtime, and this is not an error. Configuration error. ClickHouse configuration file contains a wrong hostname. Possible values: 1 — skipping enabled. If a shard is unavailable, ClickHouse returns a result based on partial data and does not report node availability issues. 0 — skipping disabled. If a shard is unavailable, ClickHouse throws an exception. Default value: 0. "},{"title":"distributed_group_by_no_merge​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#distributed-group-by-no-merge","content":"Do not merge aggregation states from different servers for distributed query processing, you can use this in case it is for certain that there are different keys on different shards Possible values: 0 — Disabled (final query processing is done on the initiator node).1 - Do not merge aggregation states from different servers for distributed query processing (query completelly processed on the shard, initiator only proxy the data), can be used in case it is for certain that there are different keys on different shards.2 - Same as 1 but applies ORDER BY and LIMIT (it is not possible when the query processed completelly on the remote node, like for distributed_group_by_no_merge=1) on the initiator (can be used for queries with ORDER BY and/or LIMIT). Default value: 0 Example SELECT * FROM remote('127.0.0.{2,3}', system.one) GROUP BY dummy LIMIT 1 SETTINGS distributed_group_by_no_merge = 1 FORMAT PrettyCompactMonoBlock ┌─dummy─┐ │ 0 │ │ 0 │ └───────┘  SELECT * FROM remote('127.0.0.{2,3}', system.one) GROUP BY dummy LIMIT 1 SETTINGS distributed_group_by_no_merge = 2 FORMAT PrettyCompactMonoBlock ┌─dummy─┐ │ 0 │ └───────┘  "},{"title":"distributed_push_down_limit​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#distributed-push-down-limit","content":"Enables or disables LIMIT applying on each shard separatelly. This will allow to avoid: Sending extra rows over network;Processing rows behind the limit on the initiator. Starting from 21.9 version you cannot get inaccurate results anymore, since distributed_push_down_limit changes query execution only if at least one of the conditions met: distributed_group_by_no_merge &gt; 0.Query does not have GROUP BY/DISTINCT/LIMIT BY, but it has ORDER BY/LIMIT.Query has GROUP BY/DISTINCT/LIMIT BY with ORDER BY/LIMIT and: optimize_skip_unused_shards is enabled.optimize_distributed_group_by_sharding_key is enabled. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. See also: distributed_group_by_no_mergeoptimize_skip_unused_shardsoptimize_distributed_group_by_sharding_key "},{"title":"optimize_skip_unused_shards_limit​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#optimize-skip-unused-shards-limit","content":"Limit for number of sharding key values, turns off optimize_skip_unused_shards if the limit is reached. Too many values may require significant amount for processing, while the benefit is doubtful, since if you have huge number of values in IN (...), then most likely the query will be sent to all shards anyway. Default value: 1000 "},{"title":"optimize_skip_unused_shards​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#optimize-skip-unused-shards","content":"Enables or disables skipping of unused shards for SELECT queries that have sharding key condition in WHERE/PREWHERE (assuming that the data is distributed by sharding key, otherwise a query yields incorrect result). Possible values: 0 — Disabled.1 — Enabled. Default value: 0 "},{"title":"optimize_skip_unused_shards_rewrite_in​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#optimize-skip-unused-shards-rewrite-in","content":"Rewrite IN in query for remote shards to exclude values that does not belong to the shard (requires optimize_skip_unused_shards). Possible values: 0 — Disabled.1 — Enabled. Default value: 1 (since it requires optimize_skip_unused_shards anyway, which 0 by default) "},{"title":"allow_nondeterministic_optimize_skip_unused_shards​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#allow-nondeterministic-optimize-skip-unused-shards","content":"Allow nondeterministic (like rand or dictGet, since later has some caveats with updates) functions in sharding key. Possible values: 0 — Disallowed.1 — Allowed. Default value: 0 "},{"title":"optimize_skip_unused_shards_nesting​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#optimize-skip-unused-shards-nesting","content":"Controls optimize_skip_unused_shards (hence still requires optimize_skip_unused_shards) depends on the nesting level of the distributed query (case when you have Distributed table that look into another Distributed table). Possible values: 0 — Disabled, optimize_skip_unused_shards works always.1 — Enables optimize_skip_unused_shards only for the first level.2 — Enables optimize_skip_unused_shards up to the second level. Default value: 0 "},{"title":"force_optimize_skip_unused_shards​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#force-optimize-skip-unused-shards","content":"Enables or disables query execution if optimize_skip_unused_shards is enabled and skipping of unused shards is not possible. If the skipping is not possible and the setting is enabled, an exception will be thrown. Possible values: 0 — Disabled. ClickHouse does not throw an exception.1 — Enabled. Query execution is disabled only if the table has a sharding key.2 — Enabled. Query execution is disabled regardless of whether a sharding key is defined for the table. Default value: 0 "},{"title":"force_optimize_skip_unused_shards_nesting​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-force_optimize_skip_unused_shards_nesting","content":"Controls force_optimize_skip_unused_shards (hence still requires force_optimize_skip_unused_shards) depends on the nesting level of the distributed query (case when you have Distributed table that look into another Distributed table). Possible values: 0 - Disabled, force_optimize_skip_unused_shards works always.1 — Enables force_optimize_skip_unused_shards only for the first level.2 — Enables force_optimize_skip_unused_shards up to the second level. Default value: 0 "},{"title":"optimize_distributed_group_by_sharding_key​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#optimize-distributed-group-by-sharding-key","content":"Optimize GROUP BY sharding_key queries, by avoiding costly aggregation on the initiator server (which will reduce memory usage for the query on the initiator server). The following types of queries are supported (and all combinations of them): SELECT DISTINCT [..., ]sharding_key[, ...] FROM distSELECT ... FROM dist GROUP BY sharding_key[, ...]SELECT ... FROM dist GROUP BY sharding_key[, ...] ORDER BY xSELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1SELECT ... FROM dist GROUP BY sharding_key[, ...] LIMIT 1 BY x The following types of queries are not supported (support for some of them may be added later): SELECT ... GROUP BY sharding_key[, ...] WITH TOTALSSELECT ... GROUP BY sharding_key[, ...] WITH ROLLUPSELECT ... GROUP BY sharding_key[, ...] WITH CUBESELECT ... GROUP BY sharding_key[, ...] SETTINGS extremes=1 Possible values: 0 — Disabled.1 — Enabled. Default value: 0 See also: distributed_group_by_no_mergedistributed_push_down_limitoptimize_skip_unused_shards note Right now it requires optimize_skip_unused_shards (the reason behind this is that one day it may be enabled by default, and it will work correctly only if data was inserted via Distributed table, i.e. data is distributed according to sharding_key). "},{"title":"optimize_throw_if_noop​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#setting-optimize_throw_if_noop","content":"Enables or disables throwing an exception if an OPTIMIZE query didn’t perform a merge. By default, OPTIMIZE returns successfully even if it didn’t do anything. This setting lets you differentiate these situations and get the reason in an exception message. Possible values: 1 — Throwing an exception is enabled.0 — Throwing an exception is disabled. Default value: 0. "},{"title":"optimize_functions_to_subcolumns​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#optimize-functions-to-subcolumns","content":"Enables or disables optimization by transforming some functions to reading subcolumns. This reduces the amount of data to read. These functions can be transformed: length to read the size0 subcolumn.empty to read the size0 subcolumn.notEmpty to read the size0 subcolumn.isNull to read the null subcolumn.isNotNull to read the null subcolumn.count to read the null subcolumn.mapKeys to read the keys subcolumn.mapValues to read the values subcolumn. Possible values: 0 — Optimization disabled.1 — Optimization enabled. Default value: 0. "},{"title":"optimize_trivial_count_query​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#optimize-trivial-count-query","content":"Enables or disables the optimization to trivial query SELECT count() FROM table using metadata from MergeTree. If you need to use row-level security, disable this setting. Possible values: 0 — Optimization disabled.1 — Optimization enabled. Default value: 1. See also: optimize_functions_to_subcolumns "},{"title":"distributed_replica_error_half_life​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-distributed_replica_error_half_life","content":"Type: secondsDefault value: 60 seconds Controls how fast errors in distributed tables are zeroed. If a replica is unavailable for some time, accumulates 5 errors, and distributed_replica_error_half_life is set to 1 second, then the replica is considered normal 3 seconds after the last error. See also: load_balancingTable engine Distributeddistributed_replica_error_capdistributed_replica_max_ignored_errors "},{"title":"distributed_replica_error_cap​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-distributed_replica_error_cap","content":"Type: unsigned intDefault value: 1000 The error count of each replica is capped at this value, preventing a single replica from accumulating too many errors. See also: load_balancingTable engine Distributeddistributed_replica_error_half_lifedistributed_replica_max_ignored_errors "},{"title":"distributed_replica_max_ignored_errors​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-distributed_replica_max_ignored_errors","content":"Type: unsigned intDefault value: 0 The number of errors that will be ignored while choosing replicas (according to load_balancing algorithm). See also: load_balancingTable engine Distributeddistributed_replica_error_capdistributed_replica_error_half_life "},{"title":"distributed_directory_monitor_sleep_time_ms​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#distributed_directory_monitor_sleep_time_ms","content":"Base interval for the Distributed table engine to send data. The actual interval grows exponentially in the event of errors. Possible values: A positive integer number of milliseconds. Default value: 100 milliseconds. "},{"title":"distributed_directory_monitor_max_sleep_time_ms​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#distributed_directory_monitor_max_sleep_time_ms","content":"Maximum interval for the Distributed table engine to send data. Limits exponential growth of the interval set in the distributed_directory_monitor_sleep_time_ms setting. Possible values: A positive integer number of milliseconds. Default value: 30000 milliseconds (30 seconds). "},{"title":"distributed_directory_monitor_batch_inserts​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#distributed_directory_monitor_batch_inserts","content":"Enables/disables inserted data sending in batches. When batch sending is enabled, the Distributed table engine tries to send multiple files of inserted data in one operation instead of sending them separately. Batch sending improves cluster performance by better-utilizing server and network resources. Possible values: 1 — Enabled.0 — Disabled. Default value: 0. "},{"title":"distributed_directory_monitor_split_batch_on_failure​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#distributed_directory_monitor_split_batch_on_failure","content":"Enables/disables splitting batches on failures. Sometimes sending particular batch to the remote shard may fail, because of some complex pipeline after (i.e. MATERIALIZED VIEW with GROUP BY) due to Memory limit exceeded or similar errors. In this case, retrying will not help (and this will stuck distributed sends for the table) but sending files from that batch one by one may succeed INSERT. So installing this setting to 1 will disable batching for such batches (i.e. temporary disables distributed_directory_monitor_batch_inserts for failed batches). Possible values: 1 — Enabled.0 — Disabled. Default value: 0. note This setting also affects broken batches (that may appears because of abnormal server (machine) termination and no fsync_after_insert/fsync_directories for Distributed table engine). warning You should not rely on automatic batch splitting, since this may hurt performance. "},{"title":"os_thread_priority​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#setting-os-thread-priority","content":"Sets the priority (nice) for threads that execute queries. The OS scheduler considers this priority when choosing the next thread to run on each available CPU core. warning To use this setting, you need to set the CAP_SYS_NICE capability. The clickhouse-server package sets it up during installation. Some virtual environments do not allow you to set the CAP_SYS_NICE capability. In this case, clickhouse-server shows a message about it at the start. Possible values: You can set values in the range [-20, 19]. Lower values mean higher priority. Threads with low nice priority values are executed more frequently than threads with high values. High values are preferable for long-running non-interactive queries because it allows them to quickly give up resources in favour of short interactive queries when they arrive. Default value: 0. "},{"title":"query_profiler_real_time_period_ns​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#query_profiler_real_time_period_ns","content":"Sets the period for a real clock timer of the query profiler. Real clock timer counts wall-clock time. Possible values: Positive integer number, in nanoseconds. Recommended values: - 10000000 (100 times a second) nanoseconds and less for single queries. - 1000000000 (once a second) for cluster-wide profiling. 0 for turning off the timer. Type: UInt64. Default value: 1000000000 nanoseconds (once a second). See also: System table trace_log "},{"title":"query_profiler_cpu_time_period_ns​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#query_profiler_cpu_time_period_ns","content":"Sets the period for a CPU clock timer of the query profiler. This timer counts only CPU time. Possible values: A positive integer number of nanoseconds. Recommended values: - 10000000 (100 times a second) nanoseconds and more for single queries. - 1000000000 (once a second) for cluster-wide profiling. 0 for turning off the timer. Type: UInt64. Default value: 1000000000 nanoseconds. See also: System table trace_log "},{"title":"allow_introspection_functions​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-allow_introspection_functions","content":"Enables or disables introspections functions for query profiling. Possible values: 1 — Introspection functions enabled.0 — Introspection functions disabled. Default value: 0. See Also Sampling Query ProfilerSystem table trace_log "},{"title":"input_format_parallel_parsing​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#input-format-parallel-parsing","content":"Enables or disables order-preserving parallel parsing of data formats. Supported only for TSV, TKSV, CSV and JSONEachRow formats. Possible values: 1 — Enabled.0 — Disabled. Default value: 1. "},{"title":"output_format_parallel_formatting​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#output-format-parallel-formatting","content":"Enables or disables parallel formatting of data formats. Supported only for TSV, TKSV, CSV and JSONEachRow formats. Possible values: 1 — Enabled.0 — Disabled. Default value: 1. "},{"title":"min_chunk_bytes_for_parallel_parsing​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#min-chunk-bytes-for-parallel-parsing","content":"Type: unsigned intDefault value: 1 MiB The minimum chunk size in bytes, which each thread will parse in parallel. "},{"title":"output_format_avro_codec​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-output_format_avro_codec","content":"Sets the compression codec used for output Avro file. Type: string Possible values: null — No compressiondeflate — Compress with Deflate (zlib)snappy — Compress with Snappy Default value: snappy (if available) or deflate. "},{"title":"output_format_avro_sync_interval​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#settings-output_format_avro_sync_interval","content":"Sets minimum data size (in bytes) between synchronization markers for output Avro file. Type: unsigned int Possible values: 32 (32 bytes) - 1073741824 (1 GiB) Default value: 32768 (32 KiB) "},{"title":"output_format_avro_string_column_pattern​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#output_format_avro_string_column_pattern","content":"Regexp of column names of type String to output as Avro string (default is bytes). RE2 syntax is supported. Type: string "},{"title":"format_avro_schema_registry_url​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#format_avro_schema_registry_url","content":"Sets Confluent Schema Registry URL to use with AvroConfluent format. Default value: Empty. "},{"title":"input_format_avro_allow_missing_fields​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#input_format_avro_allow_missing_fields","content":"Enables using fields that are not specified in Avro or AvroConfluent format schema. When a field is not found in the schema, ClickHouse uses the default value instead of throwing an exception. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"merge_selecting_sleep_ms​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#merge_selecting_sleep_ms","content":"Sleep time for merge selecting when no part is selected. A lower setting triggers selecting tasks in background_schedule_pool frequently, which results in a large number of requests to Zookeeper in large-scale clusters. Possible values: Any positive integer. Default value: 5000. "},{"title":"parallel_distributed_insert_select​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#parallel_distributed_insert_select","content":"Enables parallel distributed INSERT ... SELECT query. If we execute INSERT INTO distributed_table_a SELECT ... FROM distributed_table_b queries and both tables use the same cluster, and both tables are either replicated or non-replicated, then this query is processed locally on every shard. Possible values: 0 — Disabled.1 — SELECT will be executed on each shard from the underlying table of the distributed engine.2 — SELECT and INSERT will be executed on each shard from/to the underlying table of the distributed engine. Default value: 0. "},{"title":"insert_distributed_sync​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#insert_distributed_sync","content":"Enables or disables synchronous data insertion into a Distributed table. By default, when inserting data into a Distributed table, the ClickHouse server sends data to cluster nodes in asynchronous mode. When insert_distributed_sync=1, the data is processed synchronously, and the INSERT operation succeeds only after all the data is saved on all shards (at least one replica for each shard if internal_replication is true). Possible values: 0 — Data is inserted in asynchronous mode.1 — Data is inserted in synchronous mode. Default value: 0. See Also Distributed Table EngineManaging Distributed Tables "},{"title":"insert_distributed_one_random_shard​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#insert_distributed_one_random_shard","content":"Enables or disables random shard insertion into a Distributed table when there is no distributed key. By default, when inserting data into a Distributed table with more than one shard, the ClickHouse server will reject any insertion request if there is no distributed key. When insert_distributed_one_random_shard = 1, insertions are allowed and data is forwarded randomly among all shards. Possible values: 0 — Insertion is rejected if there are multiple shards and no distributed key is given.1 — Insertion is done randomly among all available shards when no distributed key is given. Default value: 0. "},{"title":"insert_shard_id​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#insert_shard_id","content":"If not 0, specifies the shard of Distributed table into which the data will be inserted synchronously. If insert_shard_id value is incorrect, the server will throw an exception. To get the number of shards on requested_cluster, you can check server config or use this query: SELECT uniq(shard_num) FROM system.clusters WHERE cluster = 'requested_cluster';  Possible values: 0 — Disabled.Any number from 1 to shards_num of corresponding Distributed table. Default value: 0. Example Query: CREATE TABLE x AS system.numbers ENGINE = MergeTree ORDER BY number; CREATE TABLE x_dist AS x ENGINE = Distributed('test_cluster_two_shards_localhost', currentDatabase(), x); INSERT INTO x_dist SELECT * FROM numbers(5) SETTINGS insert_shard_id = 1; SELECT * FROM x_dist ORDER BY number ASC;  Result: ┌─number─┐ │ 0 │ │ 0 │ │ 1 │ │ 1 │ │ 2 │ │ 2 │ │ 3 │ │ 3 │ │ 4 │ │ 4 │ └────────┘  "},{"title":"use_compact_format_in_distributed_parts_names​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#use_compact_format_in_distributed_parts_names","content":"Uses compact format for storing blocks for async (insert_distributed_sync) INSERT into tables with Distributed engine. Possible values: 0 — Uses user[:password]@host:port#default_database directory format.1 — Uses [shard{shard_index}[_replica{replica_index}]] directory format. Default value: 1. note with use_compact_format_in_distributed_parts_names=0 changes from cluster definition will not be applied for async INSERT.with use_compact_format_in_distributed_parts_names=1 changing the order of the nodes in the cluster definition, will change the shard_index/replica_index so be aware. "},{"title":"background_buffer_flush_schedule_pool_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#background_buffer_flush_schedule_pool_size","content":"Sets the number of threads performing background flush in Buffer-engine tables. This setting is applied at the ClickHouse server start and can’t be changed in a user session. Possible values: Any positive integer. Default value: 16. "},{"title":"background_move_pool_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#background_move_pool_size","content":"Sets the number of threads performing background moves of data parts for MergeTree-engine tables. This setting is applied at the ClickHouse server start and can’t be changed in a user session. Possible values: Any positive integer. Default value: 8. "},{"title":"background_schedule_pool_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#background_schedule_pool_size","content":"Sets the number of threads performing background tasks for replicated tables, Kafka streaming, DNS cache updates. This setting is applied at ClickHouse server start and can’t be changed in a user session. Possible values: Any positive integer. Default value: 128. "},{"title":"background_fetches_pool_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#background_fetches_pool_size","content":"Sets the number of threads performing background fetches for replicated tables. This setting is applied at the ClickHouse server start and can’t be changed in a user session. For production usage with frequent small insertions or slow ZooKeeper cluster is recommended to use default value. Possible values: Any positive integer. Default value: 8. "},{"title":"always_fetch_merged_part​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#always_fetch_merged_part","content":"Prohibits data parts merging in Replicated*MergeTree-engine tables. When merging is prohibited, the replica never merges parts and always downloads merged parts from other replicas. If there is no required data yet, the replica waits for it. CPU and disk load on the replica server decreases, but the network load on the cluster increases. This setting can be useful on servers with relatively weak CPUs or slow disks, such as servers for backups storage. Possible values: 0 — Replicated*MergeTree-engine tables merge data parts at the replica.1 — Replicated*MergeTree-engine tables do not merge data parts at the replica. The tables download merged data parts from other replicas. Default value: 0. See Also Data Replication "},{"title":"background_distributed_schedule_pool_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#background_distributed_schedule_pool_size","content":"Sets the number of threads performing background tasks for distributed sends. This setting is applied at the ClickHouse server start and can’t be changed in a user session. Possible values: Any positive integer. Default value: 16. "},{"title":"background_message_broker_schedule_pool_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#background_message_broker_schedule_pool_size","content":"Sets the number of threads performing background tasks for message streaming. This setting is applied at the ClickHouse server start and can’t be changed in a user session. Possible values: Any positive integer. Default value: 16. See Also Kafka engine.RabbitMQ engine. "},{"title":"validate_polygons​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#validate_polygons","content":"Enables or disables throwing an exception in the pointInPolygon function, if the polygon is self-intersecting or self-tangent. Possible values: 0 — Throwing an exception is disabled. pointInPolygon accepts invalid polygons and returns possibly incorrect results for them.1 — Throwing an exception is enabled. Default value: 1. "},{"title":"transform_null_in​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#transform_null_in","content":"Enables equality of NULL values for IN operator. By default, NULL values can’t be compared because NULL means undefined value. Thus, comparison expr = NULL must always return false. With this setting NULL = NULL returns true for IN operator. Possible values: 0 — Comparison of NULL values in IN operator returns false.1 — Comparison of NULL values in IN operator returns true. Default value: 0. Example Consider the null_in table: ┌──idx─┬─────i─┐ │ 1 │ 1 │ │ 2 │ NULL │ │ 3 │ 3 │ └──────┴───────┘  Query: SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 0;  Result: ┌──idx─┬────i─┐ │ 1 │ 1 │ └──────┴──────┘  Query: SELECT idx, i FROM null_in WHERE i IN (1, NULL) SETTINGS transform_null_in = 1;  Result: ┌──idx─┬─────i─┐ │ 1 │ 1 │ │ 2 │ NULL │ └──────┴───────┘  See Also NULL Processing in IN Operators "},{"title":"low_cardinality_max_dictionary_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#low_cardinality_max_dictionary_size","content":"Sets a maximum size in rows of a shared global dictionary for the LowCardinality data type that can be written to a storage file system. This setting prevents issues with RAM in case of unlimited dictionary growth. All the data that can’t be encoded due to maximum dictionary size limitation ClickHouse writes in an ordinary method. Possible values: Any positive integer. Default value: 8192. "},{"title":"low_cardinality_use_single_dictionary_for_part​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#low_cardinality_use_single_dictionary_for_part","content":"Turns on or turns off using of single dictionary for the data part. By default, the ClickHouse server monitors the size of dictionaries and if a dictionary overflows then the server starts to write the next one. To prohibit creating several dictionaries set low_cardinality_use_single_dictionary_for_part = 1. Possible values: 1 — Creating several dictionaries for the data part is prohibited.0 — Creating several dictionaries for the data part is not prohibited. Default value: 0. "},{"title":"low_cardinality_allow_in_native_format​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#low_cardinality_allow_in_native_format","content":"Allows or restricts using the LowCardinality data type with the Native format. If usage of LowCardinality is restricted, ClickHouse server converts LowCardinality-columns to ordinary ones for SELECT queries, and convert ordinary columns to LowCardinality-columns for INSERT queries. This setting is required mainly for third-party clients which do not support LowCardinality data type. Possible values: 1 — Usage of LowCardinality is not restricted.0 — Usage of LowCardinality is restricted. Default value: 1. "},{"title":"allow_suspicious_low_cardinality_types​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#allow_suspicious_low_cardinality_types","content":"Allows or restricts using LowCardinality with data types with fixed size of 8 bytes or less: numeric data types and FixedString(8_bytes_or_less). For small fixed values using of LowCardinality is usually inefficient, because ClickHouse stores a numeric index for each row. As a result: Disk space usage can rise.RAM consumption can be higher, depending on a dictionary size.Some functions can work slower due to extra coding/encoding operations. Merge times in MergeTree-engine tables can grow due to all the reasons described above. Possible values: 1 — Usage of LowCardinality is not restricted.0 — Usage of LowCardinality is restricted. Default value: 0. "},{"title":"min_insert_block_size_rows_for_materialized_views​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#min-insert-block-size-rows-for-materialized-views","content":"Sets the minimum number of rows in the block which can be inserted into a table by an INSERT query. Smaller-sized blocks are squashed into bigger ones. This setting is applied only for blocks inserted into materialized view. By adjusting this setting, you control blocks squashing while pushing to materialized view and avoid excessive memory usage. Possible values: Any positive integer.0 — Squashing disabled. Default value: 1048576. See Also min_insert_block_size_rows "},{"title":"min_insert_block_size_bytes_for_materialized_views​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#min-insert-block-size-bytes-for-materialized-views","content":"Sets the minimum number of bytes in the block which can be inserted into a table by an INSERT query. Smaller-sized blocks are squashed into bigger ones. This setting is applied only for blocks inserted into materialized view. By adjusting this setting, you control blocks squashing while pushing to materialized view and avoid excessive memory usage. Possible values: Any positive integer.0 — Squashing disabled. Default value: 268435456. See also min_insert_block_size_bytes "},{"title":"output_format_pretty_grid_charset​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#output-format-pretty-grid-charset","content":"Allows changing a charset which is used for printing grids borders. Available charsets are UTF-8, ASCII. Example SET output_format_pretty_grid_charset = 'UTF-8'; SELECT * FROM a; ┌─a─┐ │ 1 │ └───┘ SET output_format_pretty_grid_charset = 'ASCII'; SELECT * FROM a; +-a-+ | 1 | +---+  "},{"title":"optimize_read_in_order​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#optimize_read_in_order","content":"Enables ORDER BY optimization in SELECT queries for reading data from MergeTree tables. Possible values: 0 — ORDER BY optimization is disabled.1 — ORDER BY optimization is enabled. Default value: 1. See Also ORDER BY Clause "},{"title":"optimize_aggregation_in_order​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#optimize_aggregation_in_order","content":"Enables GROUP BY optimization in SELECT queries for aggregating data in corresponding order in MergeTree tables. Possible values: 0 — GROUP BY optimization is disabled.1 — GROUP BY optimization is enabled. Default value: 0. See Also GROUP BY optimization "},{"title":"mutations_sync​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#mutations_sync","content":"Allows to execute ALTER TABLE ... UPDATE|DELETE queries (mutations) synchronously. Possible values: 0 - Mutations execute asynchronously.1 - The query waits for all mutations to complete on the current server.2 - The query waits for all mutations to complete on all replicas (if they exist). Default value: 0. See Also Synchronicity of ALTER QueriesMutations "},{"title":"ttl_only_drop_parts​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#ttl_only_drop_parts","content":"Enables or disables complete dropping of data parts where all rows are expired in MergeTree tables. When ttl_only_drop_parts is disabled (by default), the ClickHouse server only deletes expired rows according to their TTL. When ttl_only_drop_parts is enabled, the ClickHouse server drops a whole part when all rows in it are expired. Dropping whole parts instead of partial cleaning TTL-d rows allows having shorter merge_with_ttl_timeout times and lower impact on system performance. Possible values: 0 — The complete dropping of data parts is disabled.1 — The complete dropping of data parts is enabled. Default value: 0. See Also CREATE TABLE query clauses and settings (merge_with_ttl_timeout setting)Table TTL "},{"title":"lock_acquire_timeout​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#lock_acquire_timeout","content":"Defines how many seconds a locking request waits before failing. Locking timeout is used to protect from deadlocks while executing read/write operations with tables. When the timeout expires and the locking request fails, the ClickHouse server throws an exception &quot;Locking attempt timed out! Possible deadlock avoided. Client should retry.&quot; with error code DEADLOCK_AVOIDED. Possible values: Positive integer (in seconds).0 — No locking timeout. Default value: 120 seconds. "},{"title":"cast_keep_nullable​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#cast_keep_nullable","content":"Enables or disables keeping of the Nullable data type in CAST operations. When the setting is enabled and the argument of CAST function is Nullable, the result is also transformed to Nullable type. When the setting is disabled, the result always has the destination type exactly. Possible values: 0 — The CAST result has exactly the destination type specified.1 — If the argument type is Nullable, the CAST result is transformed to Nullable(DestinationDataType). Default value: 0. Examples The following query results in the destination data type exactly: SET cast_keep_nullable = 0; SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);  Result: ┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐ │ 0 │ Int32 │ └───┴───────────────────────────────────────────────────┘  The following query results in the Nullable modification on the destination data type: SET cast_keep_nullable = 1; SELECT CAST(toNullable(toInt32(0)) AS Int32) as x, toTypeName(x);  Result: ┌─x─┬─toTypeName(CAST(toNullable(toInt32(0)), 'Int32'))─┐ │ 0 │ Nullable(Int32) │ └───┴───────────────────────────────────────────────────┘  See Also CAST function "},{"title":"output_format_pretty_max_value_width​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#output_format_pretty_max_value_width","content":"Limits the width of value displayed in Pretty formats. If the value width exceeds the limit, the value is cut. Possible values: Positive integer.0 — The value is cut completely. Default value: 10000 symbols. Examples Query: SET output_format_pretty_max_value_width = 10; SELECT range(number) FROM system.numbers LIMIT 10 FORMAT PrettyCompactNoEscapes;  Result: ┌─range(number)─┐ │ [] │ │ [0] │ │ [0,1] │ │ [0,1,2] │ │ [0,1,2,3] │ │ [0,1,2,3,4⋯ │ │ [0,1,2,3,4⋯ │ │ [0,1,2,3,4⋯ │ │ [0,1,2,3,4⋯ │ │ [0,1,2,3,4⋯ │ └───────────────┘  Query with zero width: SET output_format_pretty_max_value_width = 0; SELECT range(number) FROM system.numbers LIMIT 5 FORMAT PrettyCompactNoEscapes;  Result: ┌─range(number)─┐ │ ⋯ │ │ ⋯ │ │ ⋯ │ │ ⋯ │ │ ⋯ │ └───────────────┘  "},{"title":"output_format_pretty_row_numbers​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#output_format_pretty_row_numbers","content":"Adds row numbers to output in the Pretty format. Possible values: 0 — Output without row numbers.1 — Output with row numbers. Default value: 0. Example Query: SET output_format_pretty_row_numbers = 1; SELECT TOP 3 name, value FROM system.settings;  Result:  ┌─name────────────────────┬─value───┐ 1. │ min_compress_block_size │ 65536 │ 2. │ max_compress_block_size │ 1048576 │ 3. │ max_block_size │ 65505 │ └─────────────────────────┴─────────┘  "},{"title":"system_events_show_zero_values​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#system_events_show_zero_values","content":"Allows to select zero-valued events from system.events. Some monitoring systems require passing all the metrics values to them for each checkpoint, even if the metric value is zero. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. Examples Query SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';  Result Ok.  Query SET system_events_show_zero_values = 1; SELECT * FROM system.events WHERE event='QueryMemoryLimitExceeded';  Result ┌─event────────────────────┬─value─┬─description───────────────────────────────────────────┐ │ QueryMemoryLimitExceeded │ 0 │ Number of times when memory limit exceeded for query. │ └──────────────────────────┴───────┴───────────────────────────────────────────────────────┘  "},{"title":"persistent​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#persistent","content":"Disables persistency for the Set and Join table engines. Reduces the I/O overhead. Suitable for scenarios that pursue performance and do not require persistence. Possible values: 1 — Enabled.0 — Disabled. Default value: 1. "},{"title":"format_csv_null_representation​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#format_csv_null_representation","content":"Defines the representation of NULL for CSV output and input formats. User can set any string as a value, for example, My NULL. Default value: \\N. Examples Query SELECT * from csv_custom_null FORMAT CSV;  Result 788 \\N \\N  Query SET format_csv_null_representation = 'My NULL'; SELECT * FROM csv_custom_null FORMAT CSV;  Result 788 My NULL My NULL  "},{"title":"format_tsv_null_representation​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#format_tsv_null_representation","content":"Defines the representation of NULL for TSV output and input formats. User can set any string as a value, for example, My NULL. Default value: \\N. Examples Query SELECT * FROM tsv_custom_null FORMAT TSV;  Result 788 \\N \\N  Query SET format_tsv_null_representation = 'My NULL'; SELECT * FROM tsv_custom_null FORMAT TSV;  Result 788 My NULL My NULL  "},{"title":"output_format_json_array_of_rows​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#output-format-json-array-of-rows","content":"Enables the ability to output all rows as a JSON array in the JSONEachRow format. Possible values: 1 — ClickHouse outputs all rows as an array, each row in the JSONEachRow format.0 — ClickHouse outputs each row separately in the JSONEachRow format. Default value: 0. Example of a query with the enabled setting Query: SET output_format_json_array_of_rows = 1; SELECT number FROM numbers(3) FORMAT JSONEachRow;  Result: [ {&quot;number&quot;:&quot;0&quot;}, {&quot;number&quot;:&quot;1&quot;}, {&quot;number&quot;:&quot;2&quot;} ]  Example of a query with the disabled setting Query: SET output_format_json_array_of_rows = 0; SELECT number FROM numbers(3) FORMAT JSONEachRow;  Result: {&quot;number&quot;:&quot;0&quot;} {&quot;number&quot;:&quot;1&quot;} {&quot;number&quot;:&quot;2&quot;}  "},{"title":"allow_nullable_key​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#allow-nullable-key","content":"Allows using of the Nullable-typed values in a sorting and a primary key for MergeTree tables. Possible values: 1 — Nullable-type expressions are allowed in keys.0 — Nullable-type expressions are not allowed in keys. Default value: 0. warning Nullable primary key usually indicates bad design. It is forbidden in almost all main stream DBMS. The feature is mainly for AggregatingMergeTree and is not heavily tested. Use with care. warning Do not enable this feature in version &lt;= 21.8. It's not properly implemented and may lead to server crash. "},{"title":"aggregate_functions_null_for_empty​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#aggregate_functions_null_for_empty","content":"Enables or disables rewriting all aggregate functions in a query, adding -OrNull suffix to them. Enable it for SQL standard compatibility. It is implemented via query rewrite (similar to count_distinct_implementation setting) to get consistent results for distributed queries. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. Example Consider the following query with aggregate functions: SELECT SUM(-1), MAX(0) FROM system.one WHERE 0;  With aggregate_functions_null_for_empty = 0 it would produce: ┌─SUM(-1)─┬─MAX(0)─┐ │ 0 │ 0 │ └─────────┴────────┘  With aggregate_functions_null_for_empty = 1 the result would be: ┌─SUMOrNull(-1)─┬─MAXOrNull(0)─┐ │ NULL │ NULL │ └───────────────┴──────────────┘  "},{"title":"union_default_mode​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#union-default-mode","content":"Sets a mode for combining SELECT query results. The setting is only used when shared with UNION without explicitly specifying the UNION ALL or UNION DISTINCT. Possible values: 'DISTINCT' — ClickHouse outputs rows as a result of combining queries removing duplicate rows.'ALL' — ClickHouse outputs all rows as a result of combining queries including duplicate rows.'' — ClickHouse generates an exception when used with UNION. Default value: ''. See examples in UNION. "},{"title":"data_type_default_nullable​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#data_type_default_nullable","content":"Allows data types without explicit modifiers NULL or NOT NULL in column definition will be Nullable. Possible values: 1 — The data types in column definitions are set to Nullable by default.0 — The data types in column definitions are set to not Nullable by default. Default value: 0. "},{"title":"execute_merges_on_single_replica_time_threshold​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#execute-merges-on-single-replica-time-threshold","content":"Enables special logic to perform merges on replicas. Possible values: Positive integer (in seconds).0 — Special merges logic is not used. Merges happen in the usual way on all the replicas. Default value: 0. Usage Selects one replica to perform the merge on. Sets the time threshold from the start of the merge. Other replicas wait for the merge to finish, then download the result. If the time threshold passes and the selected replica does not perform the merge, then the merge is performed on other replicas as usual. High values for that threshold may lead to replication delays. It can be useful when merges are CPU bounded not IO bounded (performing heavy data compression, calculating aggregate functions or default expressions that require a large amount of calculations, or just very high number of tiny merges). "},{"title":"max_final_threads​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#max-final-threads","content":"Sets the maximum number of parallel threads for the SELECT query data read phase with the FINAL modifier. Possible values: Positive integer.0 or 1 — Disabled. SELECT queries are executed in a single thread. Default value: 16. "},{"title":"opentelemetry_start_trace_probability​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#opentelemetry-start-trace-probability","content":"Sets the probability that the ClickHouse can start a trace for executed queries (if no parent trace context is supplied). Possible values: 0 — The trace for all executed queries is disabled (if no parent trace context is supplied).Positive floating-point number in the range [0..1]. For example, if the setting value is 0,5, ClickHouse can start a trace on average for half of the queries.1 — The trace for all executed queries is enabled. Default value: 0. "},{"title":"optimize_on_insert​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#optimize-on-insert","content":"Enables or disables data transformation before the insertion, as if merge was done on this block (according to table engine). Possible values: 0 — Disabled.1 — Enabled. Default value: 1. Example The difference between enabled and disabled: Query: SET optimize_on_insert = 1; CREATE TABLE test1 (`FirstTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY FirstTable; INSERT INTO test1 SELECT number % 2 FROM numbers(5); SELECT * FROM test1; SET optimize_on_insert = 0; CREATE TABLE test2 (`SecondTable` UInt32) ENGINE = ReplacingMergeTree ORDER BY SecondTable; INSERT INTO test2 SELECT number % 2 FROM numbers(5); SELECT * FROM test2;  Result: ┌─FirstTable─┐ │ 0 │ │ 1 │ └────────────┘ ┌─SecondTable─┐ │ 0 │ │ 0 │ │ 0 │ │ 1 │ │ 1 │ └─────────────┘  Note that this setting influences Materialized view and MaterializedMySQL behaviour. "},{"title":"engine_file_empty_if_not_exists​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#engine-file-empty_if-not-exists","content":"Allows to select data from a file engine table without file. Possible values: 0 — SELECT throws exception.1 — SELECT returns empty result. Default value: 0. "},{"title":"engine_file_truncate_on_insert​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#engine-file-truncate-on-insert","content":"Enables or disables truncate before insert in File engine tables. Possible values: 0 — INSERT query appends new data to the end of the file.1 — INSERT replaces existing content of the file with the new data. Default value: 0. "},{"title":"allow_experimental_geo_types​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#allow-experimental-geo-types","content":"Allows working with experimental geo data types. Possible values: 0 — Working with geo data types is disabled.1 — Working with geo data types is enabled. Default value: 0. "},{"title":"database_atomic_wait_for_drop_and_detach_synchronously​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#database_atomic_wait_for_drop_and_detach_synchronously","content":"Adds a modifier SYNC to all DROP and DETACH queries. Possible values: 0 — Queries will be executed with delay.1 — Queries will be executed without delay. Default value: 0. "},{"title":"show_table_uuid_in_table_create_query_if_not_nil​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#show_table_uuid_in_table_create_query_if_not_nil","content":"Sets the SHOW TABLE query display. Possible values: 0 — The query will be displayed without table UUID.1 — The query will be displayed with table UUID. Default value: 0. "},{"title":"allow_experimental_live_view​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#allow-experimental-live-view","content":"Allows creation of experimental live views. Possible values: 0 — Working with live views is disabled.1 — Working with live views is enabled. Default value: 0. "},{"title":"live_view_heartbeat_interval​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#live-view-heartbeat-interval","content":"Sets the heartbeat interval in seconds to indicate live view is alive . Default value: 15. "},{"title":"max_live_view_insert_blocks_before_refresh​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#max-live-view-insert-blocks-before-refresh","content":"Sets the maximum number of inserted blocks after which mergeable blocks are dropped and query for live view is re-executed. Default value: 64. "},{"title":"temporary_live_view_timeout​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#temporary-live-view-timeout","content":"Sets the interval in seconds after which live view with timeout is deleted. Default value: 5. "},{"title":"periodic_live_view_refresh​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#periodic-live-view-refresh","content":"Sets the interval in seconds after which periodically refreshed live view is forced to refresh. Default value: 60. "},{"title":"http_connection_timeout​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#http_connection_timeout","content":"HTTP connection timeout (in seconds). Possible values: Any positive integer.0 - Disabled (infinite timeout). Default value: 1. "},{"title":"http_send_timeout​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#http_send_timeout","content":"HTTP send timeout (in seconds). Possible values: Any positive integer.0 - Disabled (infinite timeout). Default value: 1800. "},{"title":"http_receive_timeout​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#http_receive_timeout","content":"HTTP receive timeout (in seconds). Possible values: Any positive integer.0 - Disabled (infinite timeout). Default value: 1800. "},{"title":"check_query_single_value_result​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#check_query_single_value_result","content":"Defines the level of detail for the CHECK TABLE query result for MergeTree family engines . Possible values: 0 — the query shows a check status for every individual data part of a table.1 — the query shows the general table check status. Default value: 0. "},{"title":"prefer_column_name_to_alias​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#prefer-column-name-to-alias","content":"Enables or disables using the original column names instead of aliases in query expressions and clauses. It especially matters when alias is the same as the column name, see Expression Aliases. Enable this setting to make aliases syntax rules in ClickHouse more compatible with most other database engines. Possible values: 0 — The column name is substituted with the alias.1 — The column name is not substituted with the alias. Default value: 0. Example The difference between enabled and disabled: Query: SET prefer_column_name_to_alias = 0; SELECT avg(number) AS number, max(number) FROM numbers(10);  Result: Received exception from server (version 21.5.1): Code: 184. DB::Exception: Received from localhost:9000. DB::Exception: Aggregate function avg(number) is found inside another aggregate function in query: While processing avg(number) AS number.  Query: SET prefer_column_name_to_alias = 1; SELECT avg(number) AS number, max(number) FROM numbers(10);  Result: ┌─number─┬─max(number)─┐ │ 4.5 │ 9 │ └────────┴─────────────┘  "},{"title":"limit​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#limit","content":"Sets the maximum number of rows to get from the query result. It adjusts the value set by the LIMIT clause, so that the limit, specified in the query, cannot exceed the limit, set by this setting. Possible values: 0 — The number of rows is not limited.Positive integer. Default value: 0. "},{"title":"offset​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#offset","content":"Sets the number of rows to skip before starting to return rows from the query. It adjusts the offset set by the OFFSET clause, so that these two values are summarized. Possible values: 0 — No rows are skipped .Positive integer. Default value: 0. Example Input table: CREATE TABLE test (i UInt64) ENGINE = MergeTree() ORDER BY i; INSERT INTO test SELECT number FROM numbers(500);  Query: SET limit = 5; SET offset = 7; SELECT * FROM test LIMIT 10 OFFSET 100;  Result: ┌───i─┐ │ 107 │ │ 108 │ │ 109 │ └─────┘  "},{"title":"optimize_syntax_fuse_functions​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#optimize_syntax_fuse_functions","content":"Enables to fuse aggregate functions with identical argument. It rewrites query contains at least two aggregate functions from sum, count or avg with identical argument to sumCount. Possible values: 0 — Functions with identical argument are not fused.1 — Functions with identical argument are fused. Default value: 0. Example Query: CREATE TABLE fuse_tbl(a Int8, b Int8) Engine = Log; SET optimize_syntax_fuse_functions = 1; EXPLAIN SYNTAX SELECT sum(a), sum(b), count(b), avg(b) from fuse_tbl FORMAT TSV;  Result: SELECT sum(a), sumCount(b).1, sumCount(b).2, (sumCount(b).1) / (sumCount(b).2) FROM fuse_tbl  "},{"title":"allow_experimental_database_replicated​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#allow_experimental_database_replicated","content":"Enables to create databases with Replicated engine. Possible values: 0 — Disabled.1 — Enabled. Default value: 0. "},{"title":"database_replicated_initial_query_timeout_sec​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#database_replicated_initial_query_timeout_sec","content":"Sets how long initial DDL query should wait for Replicated database to precess previous DDL queue entries in seconds. Possible values: Positive integer.0 — Unlimited. Default value: 300. "},{"title":"distributed_ddl_task_timeout​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#distributed_ddl_task_timeout","content":"Sets timeout for DDL query responses from all hosts in cluster. If a DDL request has not been performed on all hosts, a response will contain a timeout error and a request will be executed in an async mode. Negative value means infinite. Possible values: Positive integer.0 — Async mode.Negative integer — infinite timeout. Default value: 180. "},{"title":"distributed_ddl_output_mode​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#distributed_ddl_output_mode","content":"Sets format of distributed DDL query result. Possible values: throw — Returns result set with query execution status for all hosts where query is finished. If query has failed on some hosts, then it will rethrow the first exception. If query is not finished yet on some hosts and distributed_ddl_task_timeout exceeded, then it throws TIMEOUT_EXCEEDED exception.none — Is similar to throw, but distributed DDL query returns no result set.null_status_on_timeout — Returns NULL as execution status in some rows of result set instead of throwing TIMEOUT_EXCEEDED if query is not finished on the corresponding hosts.never_throw — Do not throw TIMEOUT_EXCEEDED and do not rethrow exceptions if query has failed on some hosts. Default value: throw. "},{"title":"flatten_nested​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#flatten-nested","content":"Sets the data format of a nested columns. Possible values: 1 — Nested column is flattened to separate arrays.0 — Nested column stays a single array of tuples. Default value: 1. Usage If the setting is set to 0, it is possible to use an arbitrary level of nesting. Examples Query: SET flatten_nested = 1; CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple(); SHOW CREATE TABLE t_nest;  Result: ┌─statement───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE default.t_nest ( `n.a` Array(UInt32), `n.b` Array(UInt32) ) ENGINE = MergeTree ORDER BY tuple() SETTINGS index_granularity = 8192 │ └─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  Query: SET flatten_nested = 0; CREATE TABLE t_nest (`n` Nested(a UInt32, b UInt32)) ENGINE = MergeTree ORDER BY tuple(); SHOW CREATE TABLE t_nest;  Result: ┌─statement──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE TABLE default.t_nest ( `n` Nested(a UInt32, b UInt32) ) ENGINE = MergeTree ORDER BY tuple() SETTINGS index_granularity = 8192 │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"external_table_functions_use_nulls​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#external-table-functions-use-nulls","content":"Defines how mysql, postgresql and odbc] table functions use Nullable columns. Possible values: 0 — The table function explicitly uses Nullable columns.1 — The table function implicitly uses Nullable columns. Default value: 1. Usage If the setting is set to 0, the table function does not make Nullable columns and inserts default values instead of NULL. This is also applicable for NULL values inside arrays. "},{"title":"output_format_arrow_low_cardinality_as_dictionary​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#output-format-arrow-low-cardinality-as-dictionary","content":"Allows to convert the LowCardinality type to the DICTIONARY type of the Arrow format for SELECT queries. Possible values: 0 — The LowCardinality type is not converted to the DICTIONARY type.1 — The LowCardinality type is converted to the DICTIONARY type. Default value: 0. "},{"title":"allow_experimental_projection_optimization​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#allow-experimental-projection-optimization","content":"Enables or disables projection optimization when processing SELECT queries. Possible values: 0 — Projection optimization disabled.1 — Projection optimization enabled. Default value: 0. "},{"title":"force_optimize_projection​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#force-optimize-projection","content":"Enables or disables the obligatory use of projections in SELECT queries, when projection optimization is enabled (see allow_experimental_projection_optimization setting). Possible values: 0 — Projection optimization is not obligatory.1 — Projection optimization is obligatory. Default value: 0. "},{"title":"replication_alter_partitions_sync​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#replication-alter-partitions-sync","content":"Allows to set up waiting for actions to be executed on replicas by ALTER, OPTIMIZE or TRUNCATE queries. Possible values: 0 — Do not wait.1 — Wait for own execution.2 — Wait for everyone. Default value: 1. "},{"title":"replication_wait_for_inactive_replica_timeout​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#replication-wait-for-inactive-replica-timeout","content":"Specifies how long (in seconds) to wait for inactive replicas to execute ALTER, OPTIMIZE or TRUNCATE queries. Possible values: 0 — Do not wait.Negative integer — Wait for unlimited time.Positive integer — The number of seconds to wait. Default value: 120 seconds. "},{"title":"regexp_max_matches_per_row​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#regexp-max-matches-per-row","content":"Sets the maximum number of matches for a single regular expression per row. Use it to protect against memory overload when using greedy regular expression in the extractAllGroupsHorizontal function. Possible values: Positive integer. Default value: 1000. "},{"title":"http_max_single_read_retries​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#http-max-single-read-retries","content":"Sets the maximum number of retries during a single HTTP read. Possible values: Positive integer. Default value: 1024. "},{"title":"log_queries_probability​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#log-queries-probability","content":"Allows a user to write to query_log, query_thread_log, and query_views_log system tables only a sample of queries selected randomly with the specified probability. It helps to reduce the load with a large volume of queries in a second. Possible values: 0 — Queries are not logged in the system tables.Positive floating-point number in the range [0..1]. For example, if the setting value is 0.5, about half of the queries are logged in the system tables.1 — All queries are logged in the system tables. Default value: 1. "},{"title":"short_circuit_function_evaluation​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#short-circuit-function-evaluation","content":"Allows calculating the if, multiIf, and, and or functions according to a short scheme. This helps optimize the execution of complex expressions in these functions and prevent possible exceptions (such as division by zero when it is not expected). Possible values: enable — Enables short-circuit function evaluation for functions that are suitable for it (can throw an exception or computationally heavy).force_enable — Enables short-circuit function evaluation for all functions.disable — Disables short-circuit function evaluation. Default value: enable. "},{"title":"max_hyperscan_regexp_length​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#max-hyperscan-regexp-length","content":"Defines the maximum length for each regular expression in the hyperscan multi-match functions. Possible values: Positive integer.0 - The length is not limited. Default value: 0. Example Query: SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 3;  Result: ┌─multiMatchAny('abcd', ['ab', 'bcd', 'c', 'd'])─┐ │ 1 │ └────────────────────────────────────────────────┘  Query: SELECT multiMatchAny('abcd', ['ab','bcd','c','d']) SETTINGS max_hyperscan_regexp_length = 2;  Result: Exception: Regexp length too large.  See Also max_hyperscan_regexp_total_length "},{"title":"max_hyperscan_regexp_total_length​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#max-hyperscan-regexp-total-length","content":"Sets the maximum length total of all regular expressions in each hyperscan multi-match function. Possible values: Positive integer.0 - The length is not limited. Default value: 0. Example Query: SELECT multiMatchAny('abcd', ['a','b','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;  Result: ┌─multiMatchAny('abcd', ['a', 'b', 'c', 'd'])─┐ │ 1 │ └─────────────────────────────────────────────┘  Query: SELECT multiMatchAny('abcd', ['ab','bc','c','d']) SETTINGS max_hyperscan_regexp_total_length = 5;  Result: Exception: Total regexp lengths too large.  See Also max_hyperscan_regexp_length "},{"title":"enable_positional_arguments​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#enable-positional-arguments","content":"Enables or disables supporting positional arguments for GROUP BY, LIMIT BY, ORDER BY statements. When you want to use column numbers instead of column names in these clauses, set enable_positional_arguments = 1. Possible values: 0 — Positional arguments aren't supported.1 — Positional arguments are supported: column numbers can use instead of column names. Default value: 0. Example Query: CREATE TABLE positional_arguments(one Int, two Int, three Int) ENGINE=Memory(); INSERT INTO positional_arguments VALUES (10, 20, 30), (20, 20, 10), (30, 10, 20); SET enable_positional_arguments = 1; SELECT * FROM positional_arguments ORDER BY 2,3;  Result: ┌─one─┬─two─┬─three─┐ │ 30 │ 10 │ 20 │ │ 20 │ 20 │ 10 │ │ 10 │ 20 │ 30 │ └─────┴─────┴───────┘  "},{"title":"optimize_move_to_prewhere​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#optimize_move_to_prewhere","content":"Enables or disables automatic PREWHERE optimization in SELECT queries. Works only for *MergeTree tables. Possible values: 0 — Automatic PREWHERE optimization is disabled.1 — Automatic PREWHERE optimization is enabled. Default value: 1. "},{"title":"optimize_move_to_prewhere_if_final​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#optimize_move_to_prewhere_if_final","content":"Enables or disables automatic PREWHERE optimization in SELECT queries with FINAL modifier. Works only for *MergeTree tables. Possible values: 0 — Automatic PREWHERE optimization in SELECT queries with FINAL modifier is disabled.1 — Automatic PREWHERE optimization in SELECT queries with FINAL modifier is enabled. Default value: 0. See Also optimize_move_to_prewhere setting "},{"title":"describe_include_subcolumns​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#describe_include_subcolumns","content":"Enables describing subcolumns for a DESCRIBE query. For example, members of a Tuple or subcolumns of a Map, Nullable or an Array data type. Possible values: 0 — Subcolumns are not included in DESCRIBE queries.1 — Subcolumns are included in DESCRIBE queries. Default value: 0. Example See an example for the DESCRIBE statement. "},{"title":"async_insert​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#async-insert","content":"Enables or disables asynchronous inserts. This makes sense only for insertion over HTTP protocol. Note that deduplication isn't working for such inserts. If enabled, the data is combined into batches before the insertion into tables, so it is possible to do small and frequent insertions into ClickHouse (up to 15000 queries per second) without buffer tables. The data is inserted either after the async_insert_max_data_size is exceeded or after async_insert_busy_timeout_ms milliseconds since the first INSERT query. If the async_insert_stale_timeout_ms is set to a non-zero value, the data is inserted after async_insert_stale_timeout_ms milliseconds since the last query. If wait_for_async_insert is enabled, every client will wait for the data to be processed and flushed to the table. Otherwise, the query would be processed almost instantly, even if the data is not inserted. Possible values: 0 — Insertions are made synchronously, one after another.1 — Multiple asynchronous insertions enabled. Default value: 0. "},{"title":"async_insert_threads​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#async-insert-threads","content":"The maximum number of threads for background data parsing and insertion. Possible values: Positive integer.0 — Asynchronous insertions are disabled. Default value: 16. "},{"title":"wait_for_async_insert​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#wait-for-async-insert","content":"Enables or disables waiting for processing of asynchronous insertion. If enabled, server will return OK only after the data is inserted. Otherwise, it will return OK even if the data wasn't inserted. Possible values: 0 — Server returns OK even if the data is not yet inserted.1 — Server returns OK only after the data is inserted. Default value: 1. "},{"title":"wait_for_async_insert_timeout​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#wait-for-async-insert-timeout","content":"The timeout in seconds for waiting for processing of asynchronous insertion. Possible values: Positive integer.0 — Disabled. Default value: lock_acquire_timeout. "},{"title":"async_insert_max_data_size​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#async-insert-max-data-size","content":"The maximum size of the unparsed data in bytes collected per query before being inserted. Possible values: Positive integer.0 — Asynchronous insertions are disabled. Default value: 1000000. "},{"title":"async_insert_busy_timeout_ms​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#async-insert-busy-timeout-ms","content":"The maximum timeout in milliseconds since the first INSERT query before inserting collected data. Possible values: Positive integer.0 — Timeout disabled. Default value: 200. "},{"title":"async_insert_stale_timeout_ms​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#async-insert-stale-timeout-ms","content":"The maximum timeout in milliseconds since the last INSERT query before dumping collected data. If enabled, the settings prolongs the async_insert_busy_timeout_ms with every INSERT query as long as async_insert_max_data_size is not exceeded. Possible values: Positive integer.0 — Timeout disabled. Default value: 0. "},{"title":"alter_partition_verbose_result​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#alter-partition-verbose-result","content":"Enables or disables the display of information about the parts to which the manipulation operations with partitions and parts have been successfully applied. Applicable to ATTACH PARTITION|PART and to FREEZE PARTITION. Possible values: 0 — disable verbosity.1 — enable verbosity. Default value: 0. Example CREATE TABLE test(a Int64, d Date, s String) ENGINE = MergeTree PARTITION BY toYYYYMM(d) ORDER BY a; INSERT INTO test VALUES(1, '2021-01-01', ''); INSERT INTO test VALUES(1, '2021-01-01', ''); ALTER TABLE test DETACH PARTITION ID '202101'; ALTER TABLE test ATTACH PARTITION ID '202101' SETTINGS alter_partition_verbose_result = 1; ┌─command_type─────┬─partition_id─┬─part_name────┬─old_part_name─┐ │ ATTACH PARTITION │ 202101 │ 202101_7_7_0 │ 202101_5_5_0 │ │ ATTACH PARTITION │ 202101 │ 202101_8_8_0 │ 202101_6_6_0 │ └──────────────────┴──────────────┴──────────────┴───────────────┘ ALTER TABLE test FREEZE SETTINGS alter_partition_verbose_result = 1; ┌─command_type─┬─partition_id─┬─part_name────┬─backup_name─┬─backup_path───────────────────┬─part_backup_path────────────────────────────────────────────┐ │ FREEZE ALL │ 202101 │ 202101_7_7_0 │ 8 │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_7_7_0 │ │ FREEZE ALL │ 202101 │ 202101_8_8_0 │ 8 │ /var/lib/clickhouse/shadow/8/ │ /var/lib/clickhouse/shadow/8/data/default/test/202101_8_8_0 │ └──────────────┴──────────────┴──────────────┴─────────────┴───────────────────────────────┴─────────────────────────────────────────────────────────────┘  "},{"title":"format_capn_proto_enum_comparising_mode​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#format-capn-proto-enum-comparising-mode","content":"Determines how to map ClickHouse Enum data type and CapnProto Enum data type from schema. Possible values: 'by_values' — Values in enums should be the same, names can be different.'by_names' — Names in enums should be the same, values can be different.'by_name_case_insensitive' — Names in enums should be the same case-insensitive, values can be different. Default value: 'by_values'. "},{"title":"min_bytes_to_use_mmap_io​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#min-bytes-to-use-mmap-io","content":"This is an experimental setting. Sets the minimum amount of memory for reading large files without copying data from the kernel to userspace. Recommended threshold is about 64 MB, because mmap/munmap is slow. It makes sense only for large files and helps only if data reside in the page cache. Possible values: Positive integer.0 — Big files read with only copying data from kernel to userspace. Default value: 0. "},{"title":"format_custom_escaping_rule​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#format-custom-escaping-rule","content":"Sets the field escaping rule for CustomSeparated data format. Possible values: 'Escaped' — Similarly to TSV.'Quoted' — Similarly to Values.'CSV' — Similarly to CSV.'JSON' — Similarly to JSONEachRow.'XML' — Similarly to XML.'Raw' — Extracts subpatterns as a whole, no escaping rules, similarly to TSVRaw. Default value: 'Escaped'. "},{"title":"format_custom_field_delimiter​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#format-custom-field-delimiter","content":"Sets the character that is interpreted as a delimiter between the fields for CustomSeparated data format. Default value: '\\t'. "},{"title":"format_custom_row_before_delimiter​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#format-custom-row-before-delimiter","content":"Sets the character that is interpreted as a delimiter before the field of the first column for CustomSeparated data format. Default value: ''. "},{"title":"format_custom_row_after_delimiter​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#format-custom-row-after-delimiter","content":"Sets the character that is interpreted as a delimiter after the field of the last column for CustomSeparated data format. Default value: '\\n'. "},{"title":"format_custom_row_between_delimiter​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#format-custom-row-between-delimiter","content":"Sets the character that is interpreted as a delimiter between the rows for CustomSeparated data format. Default value: ''. "},{"title":"format_custom_result_before_delimiter​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#format-custom-result-before-delimiter","content":"Sets the character that is interpreted as a prefix before the result set for CustomSeparated data format. Default value: ''. "},{"title":"format_custom_result_after_delimiter​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#format-custom-result-after-delimiter","content":"Sets the character that is interpreted as a suffix after the result set for CustomSeparated data format. Default value: ''. "},{"title":"shutdown_wait_unfinished_queries​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#shutdown_wait_unfinished_queries","content":"Enables or disables waiting unfinished queries when shutdown server. Possible values: 0 — Disabled.1 — Enabled. The wait time equal shutdown_wait_unfinished config. Default value: 0. "},{"title":"shutdown_wait_unfinished​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#shutdown_wait_unfinished","content":"The waiting time in seconds for currently handled connections when shutdown server. Default Value: 5. "},{"title":"input_format_mysql_dump_table_name (#input-format-mysql-dump-table-name)​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#input_format_mysql_dump_table_name-input-format-mysql-dump-table-name","content":"The name of the table from which to read data from in MySQLDump input format. "},{"title":"input_format_mysql_dump_map_columns (#input-format-mysql-dump-map-columns)​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#input_format_mysql_dump_map_columns-input-format-mysql-dump-map-columns","content":"Enables matching columns from table in MySQL dump and columns from ClickHouse table by names in MySQLDump input format. Possible values: 0 — Disabled.1 — Enabled. Default value: 1. "},{"title":"memory_overcommit_ratio_denominator​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#memory_overcommit_ratio_denominator","content":"It represents soft memory limit in case when hard limit is reached on user level. This value is used to compute overcommit ratio for the query. Zero means skip the query. Read more about memory overcommit. Default value: 1GiB. "},{"title":"memory_usage_overcommit_max_wait_microseconds​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#memory_usage_overcommit_max_wait_microseconds","content":"Maximum time thread will wait for memory to be freed in the case of memory overcommit on a user level. If the timeout is reached and memory is not freed, an exception is thrown. Read more about memory overcommit. Default value: 200. "},{"title":"memory_overcommit_ratio_denominator_for_user​","type":1,"pageTitle":"Settings","url":"en/operations/settings/settings#memory_overcommit_ratio_denominator_for_user","content":"It represents soft memory limit in case when hard limit is reached on global level. This value is used to compute overcommit ratio for the query. Zero means skip the query. Read more about memory overcommit. Default value: 1GiB. "},{"title":"zookeeper","type":0,"sectionRef":"#","url":"en/operations/system-tables/zookeeper","content":"zookeeper The table does not exist if ZooKeeper is not configured. Allows reading data from the ZooKeeper cluster defined in the config. The query must either have a ‘path =’ condition or a path IN condition set with the WHERE clause as shown below. This corresponds to the path of the children in ZooKeeper that you want to get data for. The query SELECT * FROM system.zookeeper WHERE path = '/clickhouse' outputs data for all children on the /clickhouse node. To output data for all root nodes, write path = ‘/’. If the path specified in ‘path’ does not exist, an exception will be thrown. The query SELECT * FROM system.zookeeper WHERE path IN ('/', '/clickhouse') outputs data for all children on the / and /clickhouse node. If in the specified ‘path’ collection has does not exist path, an exception will be thrown. It can be used to do a batch of ZooKeeper path queries. Columns: name (String) — The name of the node.path (String) — The path to the node.value (String) — Node value.dataLength (Int32) — Size of the value.numChildren (Int32) — Number of descendants.czxid (Int64) — ID of the transaction that created the node.mzxid (Int64) — ID of the transaction that last changed the node.pzxid (Int64) — ID of the transaction that last deleted or added descendants.ctime (DateTime) — Time of node creation.mtime (DateTime) — Time of the last modification of the node.version (Int32) — Node version: the number of times the node was changed.cversion (Int32) — Number of added or removed descendants.aversion (Int32) — Number of changes to the ACL.ephemeralOwner (Int64) — For ephemeral nodes, the ID of the session that owns this node. Example: SELECT * FROM system.zookeeper WHERE path = '/clickhouse/tables/01-08/visits/replicas' FORMAT Vertical Row 1: ────── name: example01-08-1 value: czxid: 932998691229 mzxid: 932998691229 ctime: 2015-03-27 16:49:51 mtime: 2015-03-27 16:49:51 version: 0 cversion: 47 aversion: 0 ephemeralOwner: 0 dataLength: 0 numChildren: 7 pzxid: 987021031383 path: /clickhouse/tables/01-08/visits/replicas Row 2: ────── name: example01-08-2 value: czxid: 933002738135 mzxid: 933002738135 ctime: 2015-03-27 16:57:01 mtime: 2015-03-27 16:57:01 version: 0 cversion: 37 aversion: 0 ephemeralOwner: 0 dataLength: 0 numChildren: 7 pzxid: 987021252247 path: /clickhouse/tables/01-08/visits/replicas Original article","keywords":""},{"title":"zookeeper_log","type":0,"sectionRef":"#","url":"en/operations/system-tables/zookeeper_log","content":"zookeeper_log This table contains information about the parameters of the request to the ZooKeeper server and the response from it. For requests, only columns with request parameters are filled in, and the remaining columns are filled with default values (0 or NULL). When the response arrives, the data from the response is added to the other columns. Columns with request parameters: type (Enum) — Event type in the ZooKeeper client. Can have one of the following values: Request — The request has been sent.Response — The response was received.Finalize — The connection is lost, no response was received. event_date (Date) — The date when the event happened.event_time (DateTime64) — The date and time when the event happened.address (IPv6) — IP address of ZooKeeper server that was used to make the request.port (UInt16) — The port of ZooKeeper server that was used to make the request.session_id (Int64) — The session ID that the ZooKeeper server sets for each connection.xid (Int32) — The ID of the request within the session. This is usually a sequential request number. It is the same for the request row and the paired response/finalize row.has_watch (UInt8) — The request whether the watch has been set.op_num (Enum) — The type of request or response.path (String) — The path to the ZooKeeper node specified in the request, or an empty string if the request not requires specifying a path.data (String) — The data written to the ZooKeeper node (for the SET and CREATE requests — what the request wanted to write, for the response to the GET request — what was read) or an empty string.is_ephemeral (UInt8) — Is the ZooKeeper node being created as an ephemeral.is_sequential (UInt8) — Is the ZooKeeper node being created as an sequential.version (Nullable(Int32)) — The version of the ZooKeeper node that the request expects when executing. This is supported for CHECK, SET, REMOVE requests (is relevant -1 if the request does not check the version or NULL for other requests that do not support version checking).requests_size (UInt32) — The number of requests included in the multi request (this is a special request that consists of several consecutive ordinary requests and executes them atomically). All requests included in multi request will have the same xid.request_idx (UInt32) — The number of the request included in multi request (for multi request — 0, then in order from 1). Columns with request response parameters: zxid (Int64) — ZooKeeper transaction ID. The serial number issued by the ZooKeeper server in response to a successfully executed request (0 if the request was not executed/returned an error/the client does not know whether the request was executed).error (Nullable(Enum)) — Error code. Can have many values, here are just some of them: ZOK — The request was executed seccessfully.ZCONNECTIONLOSS — The connection was lost.ZOPERATIONTIMEOUT — The request execution timeout has expired.ZSESSIONEXPIRED — The session has expired.NULL — The request is completed. watch_type (Nullable(Enum)) — The type of the watch event (for responses with op_num = Watch), for the remaining responses: NULL.watch_state (Nullable(Enum)) — The status of the watch event (for responses with op_num = Watch), for the remaining responses: NULL.path_created (String) — The path to the created ZooKeeper node (for responses to the CREATE request), may differ from the path if the node is created as a sequential.stat_czxid (Int64) — The zxid of the change that caused this ZooKeeper node to be created.stat_mzxid (Int64) — The zxid of the change that last modified this ZooKeeper node.stat_pzxid (Int64) — The transaction ID of the change that last modified childern of this ZooKeeper node.stat_version (Int32) — The number of changes to the data of this ZooKeeper node.stat_cversion (Int32) — The number of changes to the children of this ZooKeeper node.stat_dataLength (Int32) — The length of the data field of this ZooKeeper node.stat_numChildren (Int32) — The number of children of this ZooKeeper node.children (Array(String)) — The list of child ZooKeeper nodes (for responses to LIST request). Example Query: SELECT * FROM system.zookeeper_log WHERE (session_id = '106662742089334927') AND (xid = '10858') FORMAT Vertical; Result: Row 1: ────── type: Request event_date: 2021-08-09 event_time: 2021-08-09 21:38:30.291792 address: :: port: 2181 session_id: 106662742089334927 xid: 10858 has_watch: 1 op_num: List path: /clickhouse/task_queue/ddl data: is_ephemeral: 0 is_sequential: 0 version: ᴺᵁᴸᴸ requests_size: 0 request_idx: 0 zxid: 0 error: ᴺᵁᴸᴸ watch_type: ᴺᵁᴸᴸ watch_state: ᴺᵁᴸᴸ path_created: stat_czxid: 0 stat_mzxid: 0 stat_pzxid: 0 stat_version: 0 stat_cversion: 0 stat_dataLength: 0 stat_numChildren: 0 children: [] Row 2: ────── type: Response event_date: 2021-08-09 event_time: 2021-08-09 21:38:30.292086 address: :: port: 2181 session_id: 106662742089334927 xid: 10858 has_watch: 1 op_num: List path: /clickhouse/task_queue/ddl data: is_ephemeral: 0 is_sequential: 0 version: ᴺᵁᴸᴸ requests_size: 0 request_idx: 0 zxid: 16926267 error: ZOK watch_type: ᴺᵁᴸᴸ watch_state: ᴺᵁᴸᴸ path_created: stat_czxid: 16925469 stat_mzxid: 16925469 stat_pzxid: 16926179 stat_version: 0 stat_cversion: 7 stat_dataLength: 0 stat_numChildren: 7 children: ['query-0000000006','query-0000000005','query-0000000004','query-0000000003','query-0000000002','query-0000000001','query-0000000000'] See Also ZooKeeperZooKeeper guide","keywords":""},{"title":"Usage Recommendations","type":0,"sectionRef":"#","url":"en/operations/tips","content":"","keywords":""},{"title":"CPU Scaling Governor​","type":1,"pageTitle":"Usage Recommendations","url":"en/operations/tips#cpu-scaling-governor","content":"Always use the performance scaling governor. The on-demand scaling governor works much worse with constantly high demand. $ echo 'performance' | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor  "},{"title":"CPU Limitations​","type":1,"pageTitle":"Usage Recommendations","url":"en/operations/tips#cpu-limitations","content":"Processors can overheat. Use dmesg to see if the CPU’s clock rate was limited due to overheating. The restriction can also be set externally at the datacenter level. You can use turbostat to monitor it under a load. "},{"title":"RAM​","type":1,"pageTitle":"Usage Recommendations","url":"en/operations/tips#ram","content":"For small amounts of data (up to ~200 GB compressed), it is best to use as much memory as the volume of data. For large amounts of data and when processing interactive (online) queries, you should use a reasonable amount of RAM (128 GB or more) so the hot data subset will fit in the cache of pages. Even for data volumes of ~50 TB per server, using 128 GB of RAM significantly improves query performance compared to 64 GB. Do not disable overcommit. The value cat /proc/sys/vm/overcommit_memory should be 0 or 1. Run $ echo 0 | sudo tee /proc/sys/vm/overcommit_memory  Use perf top to watch the time spent in the kernel for memory management. Permanent huge pages also do not need to be allocated. warning If your system has less than 16 GB of RAM, you may experience various memory exceptions because default settings do not match this amount of memory. The recommended amount of RAM is 32 GB or more. You can use ClickHouse in a system with a small amount of RAM, even with 2 GB of RAM, but it requires additional tuning and can ingest at a low rate. "},{"title":"Storage Subsystem​","type":1,"pageTitle":"Usage Recommendations","url":"en/operations/tips#storage-subsystem","content":"If your budget allows you to use SSD, use SSD. If not, use HDD. SATA HDDs 7200 RPM will do. Give preference to a lot of servers with local hard drives over a smaller number of servers with attached disk shelves. But for storing archives with rare queries, shelves will work. "},{"title":"RAID​","type":1,"pageTitle":"Usage Recommendations","url":"en/operations/tips#raid","content":"When using HDD, you can combine their RAID-10, RAID-5, RAID-6 or RAID-50. For Linux, software RAID is better (with mdadm). We do not recommend using LVM. When creating RAID-10, select the far layout. If your budget allows, choose RAID-10. If you have more than 4 disks, use RAID-6 (preferred) or RAID-50, instead of RAID-5. When using RAID-5, RAID-6 or RAID-50, always increase stripe_cache_size, since the default value is usually not the best choice. $ echo 4096 | sudo tee /sys/block/md2/md/stripe_cache_size  Calculate the exact number from the number of devices and the block size, using the formula: 2 * num_devices * chunk_size_in_bytes / 4096. A block size of 64 KB is sufficient for most RAID configurations. The average clickhouse-server write size is approximately 1 MB (1024 KB), and thus the recommended stripe size is also 1 MB. The block size can be optimized if needed when set to 1 MB divided by the number of non-parity disks in the RAID array, such that each write is parallelized across all available non-parity disks. Never set the block size too small or too large. You can use RAID-0 on SSD. Regardless of RAID use, always use replication for data security. Enable NCQ with a long queue. For HDD, choose the CFQ scheduler, and for SSD, choose noop. Don’t reduce the ‘readahead’ setting. For HDD, enable the write cache. Make sure that fstrim is enabled for NVME and SSD disks in your OS (usually it's implemented using a cronjob or systemd service). "},{"title":"File System​","type":1,"pageTitle":"Usage Recommendations","url":"en/operations/tips#file-system","content":"Ext4 is the most reliable option. Set the mount options noatime. XFS should be avoided. It works mostly fine but there are some reports about lower performance. Most other file systems should also work fine. Do not use compressed filesystems, because ClickHouse does compression on its own and better. It's not recommended to use encrypted filesystems, because you can use builtin encryption in ClickHouse, which is better. "},{"title":"Linux Kernel​","type":1,"pageTitle":"Usage Recommendations","url":"en/operations/tips#linux-kernel","content":"Don’t use an outdated Linux kernel. "},{"title":"Network​","type":1,"pageTitle":"Usage Recommendations","url":"en/operations/tips#network","content":"If you are using IPv6, increase the size of the route cache. The Linux kernel prior to 3.2 had a multitude of problems with IPv6 implementation. Use at least a 10 GB network, if possible. 1 Gb will also work, but it will be much worse for patching replicas with tens of terabytes of data, or for processing distributed queries with a large amount of intermediate data. "},{"title":"Huge Pages​","type":1,"pageTitle":"Usage Recommendations","url":"en/operations/tips#huge-pages","content":"If you are using old Linux kernel, disable transparent huge pages. It interferes with memory allocators, which leads to significant performance degradation. On newer Linux kernels transparent huge pages are alright. $ echo 'madvise' | sudo tee /sys/kernel/mm/transparent_hugepage/enabled  "},{"title":"Hypervisor configuration​","type":1,"pageTitle":"Usage Recommendations","url":"en/operations/tips#hypervisor-configuration","content":"If you are using OpenStack, set cpu_mode=host-passthrough  in nova.conf. If you are using libvirt, set &lt;cpu mode='host-passthrough'/&gt;  in XML configuration. This is important for ClickHouse to be able to get correct information with cpuid instruction. Otherwise you may get Illegal instruction crashes when hypervisor is run on old CPU models. "},{"title":"ZooKeeper​","type":1,"pageTitle":"Usage Recommendations","url":"en/operations/tips#zookeeper","content":"You are probably already using ZooKeeper for other purposes. You can use the same installation of ZooKeeper, if it isn’t already overloaded. It’s best to use a fresh version of ZooKeeper – 3.4.9 or later. The version in stable Linux distributions may be outdated. You should never use manually written scripts to transfer data between different ZooKeeper clusters, because the result will be incorrect for sequential nodes. Never use the “zkcopy” utility for the same reason: https://github.com/ksprojects/zkcopy/issues/15 If you want to divide an existing ZooKeeper cluster into two, the correct way is to increase the number of its replicas and then reconfigure it as two independent clusters. Do not run ZooKeeper on the same servers as ClickHouse. Because ZooKeeper is very sensitive for latency and ClickHouse may utilize all available system resources. You can have ZooKeeper observers in an ensemble but ClickHouse servers should not interact with observers. Do not change minSessionTimeout setting, large values may affect ClickHouse restart stability. With the default settings, ZooKeeper is a time bomb: The ZooKeeper server won’t delete files from old snapshots and logs when using the default configuration (see autopurge), and this is the responsibility of the operator. This bomb must be defused. The ZooKeeper (3.5.1) configuration below is used in a large production environment: zoo.cfg: # http://hadoop.apache.org/zookeeper/docs/current/zookeeperAdmin.html # The number of milliseconds of each tick tickTime=2000 # The number of ticks that the initial # synchronization phase can take # This value is not quite motivated initLimit=300 # The number of ticks that can pass between # sending a request and getting an acknowledgement syncLimit=10 maxClientCnxns=2000 # It is the maximum value that client may request and the server will accept. # It is Ok to have high maxSessionTimeout on server to allow clients to work with high session timeout if they want. # But we request session timeout of 30 seconds by default (you can change it with session_timeout_ms in ClickHouse config). maxSessionTimeout=60000000 # the directory where the snapshot is stored. dataDir=/opt/zookeeper/{{ '{{' }} cluster['name'] {{ '}}' }}/data # Place the dataLogDir to a separate physical disc for better performance dataLogDir=/opt/zookeeper/{{ '{{' }} cluster['name'] {{ '}}' }}/logs autopurge.snapRetainCount=10 autopurge.purgeInterval=1 # To avoid seeks ZooKeeper allocates space in the transaction log file in # blocks of preAllocSize kilobytes. The default block size is 64M. One reason # for changing the size of the blocks is to reduce the block size if snapshots # are taken more often. (Also, see snapCount). preAllocSize=131072 # Clients can submit requests faster than ZooKeeper can process them, # especially if there are a lot of clients. To prevent ZooKeeper from running # out of memory due to queued requests, ZooKeeper will throttle clients so that # there is no more than globalOutstandingLimit outstanding requests in the # system. The default limit is 1,000.ZooKeeper logs transactions to a # transaction log. After snapCount transactions are written to a log file a # snapshot is started and a new transaction log file is started. The default # snapCount is 10,000. snapCount=3000000 # If this option is defined, requests will be will logged to a trace file named # traceFile.year.month.day. #traceFile= # Leader accepts client connections. Default value is &quot;yes&quot;. The leader machine # coordinates updates. For higher update throughput at thes slight expense of # read throughput the leader can be configured to not accept clients and focus # on coordination. leaderServes=yes standaloneEnabled=false dynamicConfigFile=/etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/zoo.cfg.dynamic  Java version: openjdk 11.0.5-shenandoah 2019-10-15 OpenJDK Runtime Environment (build 11.0.5-shenandoah+10-adhoc.heretic.src) OpenJDK 64-Bit Server VM (build 11.0.5-shenandoah+10-adhoc.heretic.src, mixed mode)  JVM parameters: NAME=zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }} ZOOCFGDIR=/etc/$NAME/conf # TODO this is really ugly # How to find out, which jars are needed? # seems, that log4j requires the log4j.properties file to be in the classpath CLASSPATH=&quot;$ZOOCFGDIR:/usr/build/classes:/usr/build/lib/*.jar:/usr/share/zookeeper-3.6.2/lib/audience-annotations-0.5.0.jar:/usr/share/zookeeper-3.6.2/lib/commons-cli-1.2.jar:/usr/share/zookeeper-3.6.2/lib/commons-lang-2.6.jar:/usr/share/zookeeper-3.6.2/lib/jackson-annotations-2.10.3.jar:/usr/share/zookeeper-3.6.2/lib/jackson-core-2.10.3.jar:/usr/share/zookeeper-3.6.2/lib/jackson-databind-2.10.3.jar:/usr/share/zookeeper-3.6.2/lib/javax.servlet-api-3.1.0.jar:/usr/share/zookeeper-3.6.2/lib/jetty-http-9.4.24.v20191120.jar:/usr/share/zookeeper-3.6.2/lib/jetty-io-9.4.24.v20191120.jar:/usr/share/zookeeper-3.6.2/lib/jetty-security-9.4.24.v20191120.jar:/usr/share/zookeeper-3.6.2/lib/jetty-server-9.4.24.v20191120.jar:/usr/share/zookeeper-3.6.2/lib/jetty-servlet-9.4.24.v20191120.jar:/usr/share/zookeeper-3.6.2/lib/jetty-util-9.4.24.v20191120.jar:/usr/share/zookeeper-3.6.2/lib/jline-2.14.6.jar:/usr/share/zookeeper-3.6.2/lib/json-simple-1.1.1.jar:/usr/share/zookeeper-3.6.2/lib/log4j-1.2.17.jar:/usr/share/zookeeper-3.6.2/lib/metrics-core-3.2.5.jar:/usr/share/zookeeper-3.6.2/lib/netty-buffer-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-codec-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-common-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-handler-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-resolver-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-transport-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-transport-native-epoll-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/netty-transport-native-unix-common-4.1.50.Final.jar:/usr/share/zookeeper-3.6.2/lib/simpleclient-0.6.0.jar:/usr/share/zookeeper-3.6.2/lib/simpleclient_common-0.6.0.jar:/usr/share/zookeeper-3.6.2/lib/simpleclient_hotspot-0.6.0.jar:/usr/share/zookeeper-3.6.2/lib/simpleclient_servlet-0.6.0.jar:/usr/share/zookeeper-3.6.2/lib/slf4j-api-1.7.25.jar:/usr/share/zookeeper-3.6.2/lib/slf4j-log4j12-1.7.25.jar:/usr/share/zookeeper-3.6.2/lib/snappy-java-1.1.7.jar:/usr/share/zookeeper-3.6.2/lib/zookeeper-3.6.2.jar:/usr/share/zookeeper-3.6.2/lib/zookeeper-jute-3.6.2.jar:/usr/share/zookeeper-3.6.2/lib/zookeeper-prometheus-metrics-3.6.2.jar:/usr/share/zookeeper-3.6.2/etc&quot; ZOOCFG=&quot;$ZOOCFGDIR/zoo.cfg&quot; ZOO_LOG_DIR=/var/log/$NAME USER=zookeeper GROUP=zookeeper PIDDIR=/var/run/$NAME PIDFILE=$PIDDIR/$NAME.pid SCRIPTNAME=/etc/init.d/$NAME JAVA=/usr/local/jdk-11/bin/java ZOOMAIN=&quot;org.apache.zookeeper.server.quorum.QuorumPeerMain&quot; ZOO_LOG4J_PROP=&quot;INFO,ROLLINGFILE&quot; JMXLOCALONLY=false JAVA_OPTS=&quot;-Xms{{ '{{' }} cluster.get('xms','128M') {{ '}}' }} \\ -Xmx{{ '{{' }} cluster.get('xmx','1G') {{ '}}' }} \\ -Xlog:safepoint,gc*=info,age*=debug:file=/var/log/$NAME/zookeeper-gc.log:time,level,tags:filecount=16,filesize=16M -verbose:gc \\ -XX:+UseG1GC \\ -Djute.maxbuffer=8388608 \\ -XX:MaxGCPauseMillis=50&quot;  Salt init: description &quot;zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }} centralized coordination service&quot; start on runlevel [2345] stop on runlevel [!2345] respawn limit nofile 8192 8192 pre-start script [ -r &quot;/etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/environment&quot; ] || exit 0 . /etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/environment [ -d $ZOO_LOG_DIR ] || mkdir -p $ZOO_LOG_DIR chown $USER:$GROUP $ZOO_LOG_DIR end script script . /etc/zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }}/conf/environment [ -r /etc/default/zookeeper ] &amp;&amp; . /etc/default/zookeeper if [ -z &quot;$JMXDISABLE&quot; ]; then JAVA_OPTS=&quot;$JAVA_OPTS -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.local.only=$JMXLOCALONLY&quot; fi exec start-stop-daemon --start -c $USER --exec $JAVA --name zookeeper-{{ '{{' }} cluster['name'] {{ '}}' }} \\ -- -cp $CLASSPATH $JAVA_OPTS -Dzookeeper.log.dir=${ZOO_LOG_DIR} \\ -Dzookeeper.root.logger=${ZOO_LOG4J_PROP} $ZOOMAIN $ZOOCFG end script  "},{"title":"Antivirus software​","type":1,"pageTitle":"Usage Recommendations","url":"en/operations/tips#antivirus-software","content":"If you use antivirus software configure it to skip folders with Clickhouse datafiles (/var/lib/clickhouse) otherwise performance may be reduced and you may experience unexpected errors during data ingestion and background merges. Original article "},{"title":"Troubleshooting","type":0,"sectionRef":"#","url":"en/operations/troubleshooting","content":"","keywords":""},{"title":"Installation​","type":1,"pageTitle":"Troubleshooting","url":"en/operations/troubleshooting#troubleshooting-installation-errors","content":""},{"title":"You Cannot Get Deb Packages from ClickHouse Repository with Apt-get​","type":1,"pageTitle":"Troubleshooting","url":"en/operations/troubleshooting#you-cannot-get-deb-packages-from-clickhouse-repository-with-apt-get","content":"Check firewall settings.If you cannot access the repository for any reason, download packages as described in the install guide article and install them manually using the sudo dpkg -i &lt;packages&gt; command. You will also need the tzdata package. "},{"title":"Connecting to the Server​","type":1,"pageTitle":"Troubleshooting","url":"en/operations/troubleshooting#troubleshooting-accepts-no-connections","content":"Possible issues: The server is not running.Unexpected or wrong configuration parameters. "},{"title":"Server Is Not Running​","type":1,"pageTitle":"Troubleshooting","url":"en/operations/troubleshooting#server-is-not-running","content":"Check if server is runnnig Command: $ sudo service clickhouse-server status  If the server is not running, start it with the command: $ sudo service clickhouse-server start  Check logs The main log of clickhouse-server is in /var/log/clickhouse-server/clickhouse-server.log by default. If the server started successfully, you should see the strings: &lt;Information&gt; Application: starting up. — Server started.&lt;Information&gt; Application: Ready for connections. — Server is running and ready for connections. If clickhouse-server start failed with a configuration error, you should see the &lt;Error&gt; string with an error description. For example: 2019.01.11 15:23:25.549505 [ 45 ] {} &lt;Error&gt; ExternalDictionaries: Failed reloading 'event2id' external dictionary: Poco::Exception. Code: 1000, e.code() = 111, e.displayText() = Connection refused, e.what() = Connection refused  If you do not see an error at the end of the file, look through the entire file starting from the string: &lt;Information&gt; Application: starting up.  If you try to start a second instance of clickhouse-server on the server, you see the following log: 2019.01.11 15:25:11.151730 [ 1 ] {} &lt;Information&gt; : Starting ClickHouse 19.1.0 with revision 54413 2019.01.11 15:25:11.154578 [ 1 ] {} &lt;Information&gt; Application: starting up 2019.01.11 15:25:11.156361 [ 1 ] {} &lt;Information&gt; StatusFile: Status file ./status already exists - unclean restart. Contents: PID: 8510 Started at: 2019-01-11 15:24:23 Revision: 54413 2019.01.11 15:25:11.156673 [ 1 ] {} &lt;Error&gt; Application: DB::Exception: Cannot lock file ./status. Another server instance in same directory is already running. 2019.01.11 15:25:11.156682 [ 1 ] {} &lt;Information&gt; Application: shutting down 2019.01.11 15:25:11.156686 [ 1 ] {} &lt;Debug&gt; Application: Uninitializing subsystem: Logging Subsystem 2019.01.11 15:25:11.156716 [ 2 ] {} &lt;Information&gt; BaseDaemon: Stop SignalListener thread  See system.d logs If you do not find any useful information in clickhouse-server logs or there aren’t any logs, you can view system.d logs using the command: $ sudo journalctl -u clickhouse-server  Start clickhouse-server in interactive mode $ sudo -u clickhouse /usr/bin/clickhouse-server --config-file /etc/clickhouse-server/config.xml  This command starts the server as an interactive app with standard parameters of the autostart script. In this mode clickhouse-server prints all the event messages in the console. "},{"title":"Configuration Parameters​","type":1,"pageTitle":"Troubleshooting","url":"en/operations/troubleshooting#configuration-parameters","content":"Check: Docker settings. If you run ClickHouse in Docker in an IPv6 network, make sure that network=host is set. Endpoint settings. Check listen_host and tcp_port settings. ClickHouse server accepts localhost connections only by default. HTTP protocol settings. Check protocol settings for the HTTP API. Secure connection settings. Check: The tcp_port_secure setting.Settings for SSL certificates. Use proper parameters while connecting. For example, use the port_secure parameter with clickhouse_client. User settings. You might be using the wrong user name or password. "},{"title":"Query Processing​","type":1,"pageTitle":"Troubleshooting","url":"en/operations/troubleshooting#troubleshooting-does-not-process-queries","content":"If ClickHouse is not able to process the query, it sends an error description to the client. In the clickhouse-client you get a description of the error in the console. If you are using the HTTP interface, ClickHouse sends the error description in the response body. For example: $ curl 'http://localhost:8123/' --data-binary &quot;SELECT a&quot; Code: 47, e.displayText() = DB::Exception: Unknown identifier: a. Note that there are no tables (FROM clause) in your query, context: required_names: 'a' source_tables: table_aliases: private_aliases: column_aliases: public_columns: 'a' masked_columns: array_join_columns: source_columns: , e.what() = DB::Exception  If you start clickhouse-client with the stack-trace parameter, ClickHouse returns the server stack trace with the description of an error. You might see a message about a broken connection. In this case, you can repeat the query. If the connection breaks every time you perform the query, check the server logs for errors. "},{"title":"Efficiency of Query Processing​","type":1,"pageTitle":"Troubleshooting","url":"en/operations/troubleshooting#troubleshooting-too-slow","content":"If you see that ClickHouse is working too slowly, you need to profile the load on the server resources and network for your queries. You can use the clickhouse-benchmark utility to profile queries. It shows the number of queries processed per second, the number of rows processed per second, and percentiles of query processing times. "},{"title":"ClickHouse Upgrade","type":0,"sectionRef":"#","url":"en/operations/update","content":"ClickHouse Upgrade If ClickHouse was installed from deb packages, execute the following commands on the server: $ sudo apt-get update $ sudo apt-get install clickhouse-client clickhouse-server $ sudo service clickhouse-server restart If you installed ClickHouse using something other than the recommended deb packages, use the appropriate update method. note You can update multiple servers at once as soon as there is no moment when all replicas of one shard are offline. The upgrade of older version of ClickHouse to specific version: As an example: xx.yy.a.b is a current stable version. The latest stable version could be found here $ sudo apt-get update $ sudo apt-get install clickhouse-server=xx.yy.a.b clickhouse-client=xx.yy.a.b clickhouse-common-static=xx.yy.a.b $ sudo service clickhouse-server restart ","keywords":""},{"title":"ClickHouse Utility","type":0,"sectionRef":"#","url":"en/operations/utilities/","content":"ClickHouse Utility clickhouse-local — Allows running SQL queries on data without starting the ClickHouse server, similar to how awk does this.clickhouse-copier — Copies (and reshards) data from one cluster to another cluster.clickhouse-benchmark — Loads server with the custom queries and settings.clickhouse-format — Enables formatting input queries.ClickHouse obfuscator — Obfuscates data.ClickHouse compressor — Compresses and decompresses data.clickhouse-odbc-bridge — A proxy server for ODBC driver.","keywords":""},{"title":"clickhouse-benchmark","type":0,"sectionRef":"#","url":"en/operations/utilities/clickhouse-benchmark","content":"","keywords":""},{"title":"Keys​","type":1,"pageTitle":"clickhouse-benchmark","url":"en/operations/utilities/clickhouse-benchmark#clickhouse-benchmark-keys","content":"--query=QUERY — Query to execute. If this parameter is not passed, clickhouse-benchmark will read queries from standard input.-c N, --concurrency=N — Number of queries that clickhouse-benchmark sends simultaneously. Default value: 1.-d N, --delay=N — Interval in seconds between intermediate reports (to disable reports set 0). Default value: 1.-h HOST, --host=HOST — Server host. Default value: localhost. For the comparison mode you can use multiple -h keys.-p N, --port=N — Server port. Default value: 9000. For the comparison mode you can use multiple -p keys.-i N, --iterations=N — Total number of queries. Default value: 0 (repeat forever).-r, --randomize — Random order of queries execution if there is more than one input query.-s, --secure — Using TLS connection.-t N, --timelimit=N — Time limit in seconds. clickhouse-benchmark stops sending queries when the specified time limit is reached. Default value: 0 (time limit disabled).--confidence=N — Level of confidence for T-test. Possible values: 0 (80%), 1 (90%), 2 (95%), 3 (98%), 4 (99%), 5 (99.5%). Default value: 5. In the comparison mode clickhouse-benchmark performs the Independent two-sample Student’s t-test to determine whether the two distributions aren’t different with the selected level of confidence.--cumulative — Printing cumulative data instead of data per interval.--database=DATABASE_NAME — ClickHouse database name. Default value: default.--json=FILEPATH — JSON output. When the key is set, clickhouse-benchmark outputs a report to the specified JSON-file.--user=USERNAME — ClickHouse user name. Default value: default.--password=PSWD — ClickHouse user password. Default value: empty string.--stacktrace — Stack traces output. When the key is set, clickhouse-bencmark outputs stack traces of exceptions.--stage=WORD — Query processing stage at server. ClickHouse stops query processing and returns an answer to clickhouse-benchmark at the specified stage. Possible values: complete, fetch_columns, with_mergeable_state. Default value: complete.--help — Shows the help message. If you want to apply some settings for queries, pass them as a key --&lt;session setting name&gt;= SETTING_VALUE. For example, --max_memory_usage=1048576. "},{"title":"Output​","type":1,"pageTitle":"clickhouse-benchmark","url":"en/operations/utilities/clickhouse-benchmark#clickhouse-benchmark-output","content":"By default, clickhouse-benchmark reports for each --delay interval. Example of the report: Queries executed: 10. localhost:9000, queries 10, QPS: 6.772, RPS: 67904487.440, MiB/s: 518.070, result RPS: 67721584.984, result MiB/s: 516.675. 0.000% 0.145 sec. 10.000% 0.146 sec. 20.000% 0.146 sec. 30.000% 0.146 sec. 40.000% 0.147 sec. 50.000% 0.148 sec. 60.000% 0.148 sec. 70.000% 0.148 sec. 80.000% 0.149 sec. 90.000% 0.150 sec. 95.000% 0.150 sec. 99.000% 0.150 sec. 99.900% 0.150 sec. 99.990% 0.150 sec.  In the report you can find: Number of queries in the Queries executed: field. Status string containing (in order): Endpoint of ClickHouse server.Number of processed queries.QPS: How many queries the server performed per second during a period specified in the --delay argument.RPS: How many rows the server reads per second during a period specified in the --delay argument.MiB/s: How many mebibytes the server reads per second during a period specified in the --delay argument.result RPS: How many rows placed by the server to the result of a query per second during a period specified in the --delay argument.result MiB/s. How many mebibytes placed by the server to the result of a query per second during a period specified in the --delay argument. Percentiles of queries execution time. "},{"title":"Comparison Mode​","type":1,"pageTitle":"clickhouse-benchmark","url":"en/operations/utilities/clickhouse-benchmark#clickhouse-benchmark-comparison-mode","content":"clickhouse-benchmark can compare performances for two running ClickHouse servers. To use the comparison mode, specify endpoints of both servers by two pairs of --host, --port keys. Keys matched together by position in arguments list, the first --host is matched with the first --port and so on. clickhouse-benchmark establishes connections to both servers, then sends queries. Each query addressed to a randomly selected server. The results are shown for each server separately. "},{"title":"Example​","type":1,"pageTitle":"clickhouse-benchmark","url":"en/operations/utilities/clickhouse-benchmark#clickhouse-benchmark-example","content":"$ echo &quot;SELECT * FROM system.numbers LIMIT 10000000 OFFSET 10000000&quot; | clickhouse-benchmark -i 10  Loaded 1 queries. Queries executed: 6. localhost:9000, queries 6, QPS: 6.153, RPS: 123398340.957, MiB/s: 941.455, result RPS: 61532982.200, result MiB/s: 469.459. 0.000% 0.159 sec. 10.000% 0.159 sec. 20.000% 0.159 sec. 30.000% 0.160 sec. 40.000% 0.160 sec. 50.000% 0.162 sec. 60.000% 0.164 sec. 70.000% 0.165 sec. 80.000% 0.166 sec. 90.000% 0.166 sec. 95.000% 0.167 sec. 99.000% 0.167 sec. 99.900% 0.167 sec. 99.990% 0.167 sec. Queries executed: 10. localhost:9000, queries 10, QPS: 6.082, RPS: 121959604.568, MiB/s: 930.478, result RPS: 60815551.642, result MiB/s: 463.986. 0.000% 0.159 sec. 10.000% 0.159 sec. 20.000% 0.160 sec. 30.000% 0.163 sec. 40.000% 0.164 sec. 50.000% 0.165 sec. 60.000% 0.166 sec. 70.000% 0.166 sec. 80.000% 0.167 sec. 90.000% 0.167 sec. 95.000% 0.170 sec. 99.000% 0.172 sec. 99.900% 0.172 sec. 99.990% 0.172 sec.  Original article "},{"title":"clickhouse-compressor","type":0,"sectionRef":"#","url":"en/operations/utilities/clickhouse-compressor","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"clickhouse-compressor","url":"en/operations/utilities/clickhouse-compressor#examples","content":"Compress data with LZ4: $ ./clickhouse-compressor &lt; input_file &gt; output_file  Decompress data from LZ4 format: $ ./clickhouse-compressor --decompress &lt; input_file &gt; output_file  Compress data with ZSTD at level 5: $ ./clickhouse-compressor --codec 'ZSTD(5)' &lt; input_file &gt; output_file  Compress data with Delta of four bytes and ZSTD level 10. $ ./clickhouse-compressor --codec 'Delta(4)' --codec 'ZSTD(10)' &lt; input_file &gt; output_file  "},{"title":"clickhouse-copier","type":0,"sectionRef":"#","url":"en/operations/utilities/clickhouse-copier","content":"","keywords":""},{"title":"Running Clickhouse-copier​","type":1,"pageTitle":"clickhouse-copier","url":"en/operations/utilities/clickhouse-copier#running-clickhouse-copier","content":"The utility should be run manually: $ clickhouse-copier --daemon --config zookeeper.xml --task-path /task/path --base-dir /path/to/dir  Parameters: daemon — Starts clickhouse-copier in daemon mode.config — The path to the zookeeper.xml file with the parameters for the connection to ZooKeeper.task-path — The path to the ZooKeeper node. This node is used for syncing clickhouse-copier processes and storing tasks. Tasks are stored in $task-path/description.task-file — Optional path to file with task configuration for initial upload to ZooKeeper.task-upload-force — Force upload task-file even if node already exists.base-dir — The path to logs and auxiliary files. When it starts, clickhouse-copier creates clickhouse-copier_YYYYMMHHSS_&lt;PID&gt; subdirectories in $base-dir. If this parameter is omitted, the directories are created in the directory where clickhouse-copier was launched. "},{"title":"Format of Zookeeper.xml​","type":1,"pageTitle":"clickhouse-copier","url":"en/operations/utilities/clickhouse-copier#format-of-zookeeper-xml","content":"&lt;clickhouse&gt; &lt;logger&gt; &lt;level&gt;trace&lt;/level&gt; &lt;size&gt;100M&lt;/size&gt; &lt;count&gt;3&lt;/count&gt; &lt;/logger&gt; &lt;zookeeper&gt; &lt;node index=&quot;1&quot;&gt; &lt;host&gt;127.0.0.1&lt;/host&gt; &lt;port&gt;2181&lt;/port&gt; &lt;/node&gt; &lt;/zookeeper&gt; &lt;/clickhouse&gt;  "},{"title":"Configuration of Copying Tasks​","type":1,"pageTitle":"clickhouse-copier","url":"en/operations/utilities/clickhouse-copier#configuration-of-copying-tasks","content":"&lt;clickhouse&gt; &lt;!-- Configuration of clusters as in an ordinary server config --&gt; &lt;remote_servers&gt; &lt;source_cluster&gt; &lt;!-- source cluster &amp; destination clusters accept exactly the same parameters as parameters for the usual Distributed table see https://clickhouse.com/docs/en/engines/table-engines/special/distributed/ --&gt; &lt;shard&gt; &lt;internal_replication&gt;false&lt;/internal_replication&gt; &lt;replica&gt; &lt;host&gt;127.0.0.1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;!-- &lt;user&gt;default&lt;/user&gt; &lt;password&gt;default&lt;/password&gt; &lt;secure&gt;1&lt;/secure&gt; --&gt; &lt;/replica&gt; &lt;/shard&gt; ... &lt;/source_cluster&gt; &lt;destination_cluster&gt; ... &lt;/destination_cluster&gt; &lt;/remote_servers&gt; &lt;!-- How many simultaneously active workers are possible. If you run more workers superfluous workers will sleep. --&gt; &lt;max_workers&gt;2&lt;/max_workers&gt; &lt;!-- Setting used to fetch (pull) data from source cluster tables --&gt; &lt;settings_pull&gt; &lt;readonly&gt;1&lt;/readonly&gt; &lt;/settings_pull&gt; &lt;!-- Setting used to insert (push) data to destination cluster tables --&gt; &lt;settings_push&gt; &lt;readonly&gt;0&lt;/readonly&gt; &lt;/settings_push&gt; &lt;!-- Common setting for fetch (pull) and insert (push) operations. Also, copier process context uses it. They are overlaid by &lt;settings_pull/&gt; and &lt;settings_push/&gt; respectively. --&gt; &lt;settings&gt; &lt;connect_timeout&gt;3&lt;/connect_timeout&gt; &lt;!-- Sync insert is set forcibly, leave it here just in case. --&gt; &lt;insert_distributed_sync&gt;1&lt;/insert_distributed_sync&gt; &lt;/settings&gt; &lt;!-- Copying tasks description. You could specify several table task in the same task description (in the same ZooKeeper node), they will be performed sequentially. --&gt; &lt;tables&gt; &lt;!-- A table task, copies one table. --&gt; &lt;table_hits&gt; &lt;!-- Source cluster name (from &lt;remote_servers/&gt; section) and tables in it that should be copied --&gt; &lt;cluster_pull&gt;source_cluster&lt;/cluster_pull&gt; &lt;database_pull&gt;test&lt;/database_pull&gt; &lt;table_pull&gt;hits&lt;/table_pull&gt; &lt;!-- Destination cluster name and tables in which the data should be inserted --&gt; &lt;cluster_push&gt;destination_cluster&lt;/cluster_push&gt; &lt;database_push&gt;test&lt;/database_push&gt; &lt;table_push&gt;hits2&lt;/table_push&gt; &lt;!-- Engine of destination tables. If destination tables have not be created, workers create them using columns definition from source tables and engine definition from here. NOTE: If the first worker starts insert data and detects that destination partition is not empty then the partition will be dropped and refilled, take it into account if you already have some data in destination tables. You could directly specify partitions that should be copied in &lt;enabled_partitions/&gt;, they should be in quoted format like partition column of system.parts table. --&gt; &lt;engine&gt; ENGINE=ReplicatedMergeTree('/clickhouse/tables/{cluster}/{shard}/hits2', '{replica}') PARTITION BY toMonday(date) ORDER BY (CounterID, EventDate) &lt;/engine&gt; &lt;!-- Sharding key used to insert data to destination cluster --&gt; &lt;sharding_key&gt;jumpConsistentHash(intHash64(UserID), 2)&lt;/sharding_key&gt; &lt;!-- Optional expression that filter data while pull them from source servers --&gt; &lt;where_condition&gt;CounterID != 0&lt;/where_condition&gt; &lt;!-- This section specifies partitions that should be copied, other partition will be ignored. Partition names should have the same format as partition column of system.parts table (i.e. a quoted text). Since partition key of source and destination cluster could be different, these partition names specify destination partitions. NOTE: In spite of this section is optional (if it is not specified, all partitions will be copied), it is strictly recommended to specify them explicitly. If you already have some ready partitions on destination cluster they will be removed at the start of the copying since they will be interpeted as unfinished data from the previous copying!!! --&gt; &lt;enabled_partitions&gt; &lt;partition&gt;'2018-02-26'&lt;/partition&gt; &lt;partition&gt;'2018-03-05'&lt;/partition&gt; ... &lt;/enabled_partitions&gt; &lt;/table_hits&gt; &lt;!-- Next table to copy. It is not copied until previous table is copying. --&gt; &lt;table_visits&gt; ... &lt;/table_visits&gt; ... &lt;/tables&gt; &lt;/clickhouse&gt;  clickhouse-copier tracks the changes in /task/path/description and applies them on the fly. For instance, if you change the value of max_workers, the number of processes running tasks will also change. Original article "},{"title":"clickhouse-format","type":0,"sectionRef":"#","url":"en/operations/utilities/clickhouse-format","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"clickhouse-format","url":"en/operations/utilities/clickhouse-format#examples","content":"Formatting a query: $ clickhouse-format --query &quot;select number from numbers(10) where number%2 order by number desc;&quot;  Result: SELECT number FROM numbers(10) WHERE number % 2 ORDER BY number DESC  Highlighting and single line: $ clickhouse-format --oneline --hilite &lt;&lt;&lt; &quot;SELECT sum(number) FROM numbers(5);&quot;  Result: SELECT sum(number) FROM numbers(5)  Multiqueries: $ clickhouse-format -n &lt;&lt;&lt; &quot;SELECT * FROM (SELECT 1 AS x UNION ALL SELECT 1 UNION DISTINCT SELECT 3);&quot;  Result: SELECT * FROM ( SELECT 1 AS x UNION ALL SELECT 1 UNION DISTINCT SELECT 3 ) ;  Obfuscating: $ clickhouse-format --seed Hello --obfuscate &lt;&lt;&lt; &quot;SELECT cost_first_screen BETWEEN a AND b, CASE WHEN x &gt;= 123 THEN y ELSE NULL END;&quot;  Result: SELECT treasury_mammoth_hazelnut BETWEEN nutmeg AND span, CASE WHEN chive &gt;= 116 THEN switching ELSE ANYTHING END;  Same query and another seed string: $ clickhouse-format --seed World --obfuscate &lt;&lt;&lt; &quot;SELECT cost_first_screen BETWEEN a AND b, CASE WHEN x &gt;= 123 THEN y ELSE NULL END;&quot;  Result: SELECT horse_tape_summer BETWEEN folklore AND moccasins, CASE WHEN intestine &gt;= 116 THEN nonconformist ELSE FORESTRY END;  Adding backslash: $ clickhouse-format --backslash &lt;&lt;&lt; &quot;SELECT * FROM (SELECT 1 AS x UNION ALL SELECT 1 UNION DISTINCT SELECT 3);&quot;  Result: SELECT * \\ FROM \\ ( \\ SELECT 1 AS x \\ UNION ALL \\ SELECT 1 \\ UNION DISTINCT \\ SELECT 3 \\ )  "},{"title":"clickhouse-local","type":0,"sectionRef":"#","url":"en/operations/utilities/clickhouse-local","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"clickhouse-local","url":"en/operations/utilities/clickhouse-local#usage","content":"Basic usage: $ clickhouse-local --structure &quot;table_structure&quot; --input-format &quot;format_of_incoming_data&quot; \\ --query &quot;query&quot;  Arguments: -S, --structure — table structure for input data.-if, --input-format — input format, TSV by default.-f, --file — path to data, stdin by default.-q, --query — queries to execute with ; as delimeter. You must specify either query or queries-file option.-qf, --queries-file - file path with queries to execute. You must specify either query or queries-file option.-N, --table — table name where to put output data, table by default.-of, --format, --output-format — output format, TSV by default.-d, --database — default database, _local by default.--stacktrace — whether to dump debug output in case of exception.--echo — print query before execution.--verbose — more details on query execution.--logger.console — Log to console.--logger.log — Log file name.--logger.level — Log level.--ignore-error — do not stop processing if a query failed.-c, --config-file — path to configuration file in same format as for ClickHouse server, by default the configuration empty.--no-system-tables — do not attach system tables.--help — arguments references for clickhouse-local.-V, --version — print version information and exit. Also there are arguments for each ClickHouse configuration variable which are more commonly used instead of --config-file. "},{"title":"Examples​","type":1,"pageTitle":"clickhouse-local","url":"en/operations/utilities/clickhouse-local#examples","content":"$ echo -e &quot;1,2\\n3,4&quot; | clickhouse-local --structure &quot;a Int64, b Int64&quot; \\ --input-format &quot;CSV&quot; --query &quot;SELECT * FROM table&quot; Read 2 rows, 32.00 B in 0.000 sec., 5182 rows/sec., 80.97 KiB/sec. 1 2 3 4  Previous example is the same as: $ echo -e &quot;1,2\\n3,4&quot; | clickhouse-local --query &quot; CREATE TABLE table (a Int64, b Int64) ENGINE = File(CSV, stdin); SELECT a, b FROM table; DROP TABLE table&quot; Read 2 rows, 32.00 B in 0.000 sec., 4987 rows/sec., 77.93 KiB/sec. 1 2 3 4  You don't have to use stdin or --file argument, and can open any number of files using the file table function: $ echo 1 | tee 1.tsv 1 $ echo 2 | tee 2.tsv 2 $ clickhouse-local --query &quot; select * from file('1.tsv', TSV, 'a int') t1 cross join file('2.tsv', TSV, 'b int') t2&quot; 1 2  Now let’s output memory user for each Unix user: Query: $ ps aux | tail -n +2 | awk '{ printf(&quot;%s\\t%s\\n&quot;, $1, $4) }' \\ | clickhouse-local --structure &quot;user String, mem Float64&quot; \\ --query &quot;SELECT user, round(sum(mem), 2) as memTotal FROM table GROUP BY user ORDER BY memTotal DESC FORMAT Pretty&quot;  Result: Read 186 rows, 4.15 KiB in 0.035 sec., 5302 rows/sec., 118.34 KiB/sec. ┏━━━━━━━━━━┳━━━━━━━━━━┓ ┃ user ┃ memTotal ┃ ┡━━━━━━━━━━╇━━━━━━━━━━┩ │ bayonet │ 113.5 │ ├──────────┼──────────┤ │ root │ 8.8 │ ├──────────┼──────────┤ ...  Original article "},{"title":"clickhouse-obfuscator","type":0,"sectionRef":"#","url":"en/operations/utilities/clickhouse-obfuscator","content":"clickhouse-obfuscator A simple tool for table data obfuscation. It reads an input table and produces an output table, that retains some properties of input, but contains different data. It allows publishing almost real production data for usage in benchmarks. It is designed to retain the following properties of data: cardinalities of values (number of distinct values) for every column and every tuple of columns; conditional cardinalities: number of distinct values of one column under the condition on the value of another column; probability distributions of the absolute value of integers; the sign of signed integers; exponent and sign for floats; probability distributions of the length of strings; probability of zero values of numbers; empty strings and arrays, NULLs; data compression ratio when compressed with LZ77 and entropy family of codecs; continuity (magnitude of difference) of time values across the table; continuity of floating-point values; date component of DateTime values; UTF-8 validity of string values; string values look natural. Most of the properties above are viable for performance testing: reading data, filtering, aggregatio, and sorting will work at almost the same speed as on original data due to saved cardinalities, magnitudes, compression ratios, etc. It works in a deterministic fashion: you define a seed value and the transformation is determined by input data and by seed. Some transformations are one to one and could be reversed, so you need to have a large seed and keep it in secret. It uses some cryptographic primitives to transform data but from the cryptographic point of view, it does not do it properly, that is why you should not consider the result as secure unless you have another reason. The result may retain some data you don't want to publish. It always leaves 0, 1, -1 numbers, dates, lengths of arrays, and null flags exactly as in source data. For example, you have a column IsMobile in your table with values 0 and 1. In transformed data, it will have the same value. So, the user will be able to count the exact ratio of mobile traffic. Let's give another example. When you have some private data in your table, like user email and you don't want to publish any single email address. If your table is large enough and contains multiple different emails and no email has a very high frequency than all others, it will anonymize all data. But if you have a small number of different values in a column, it can reproduce some of them. You should look at the working algorithm of this tool works, and fine-tune its command line parameters. This tool works fine only with an average amount of data (at least 1000s of rows).","keywords":""},{"title":"clickhouse-odbc-bridge","type":0,"sectionRef":"#","url":"en/operations/utilities/odbc-bridge","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"clickhouse-odbc-bridge","url":"en/operations/utilities/odbc-bridge#usage","content":"clickhouse-server use this tool inside odbc table function and StorageODBC. However it can be used as standalone tool from command line with the following parameters in POST-request URL: connection_string -- ODBC connection string.columns -- columns in ClickHouse NamesAndTypesList format, name in backticks, type as string. Name and type are space separated, rows separated with newline.max_block_size -- optional parameter, sets maximum size of single block. Query is send in post body. Response is returned in RowBinary format. "},{"title":"Example:​","type":1,"pageTitle":"clickhouse-odbc-bridge","url":"en/operations/utilities/odbc-bridge#example","content":"$ clickhouse-odbc-bridge --http-port 9018 --daemon $ curl -d &quot;query=SELECT PageID, ImpID, AdType FROM Keys ORDER BY PageID, ImpID&quot; --data-urlencode &quot;connection_string=DSN=ClickHouse;DATABASE=stat&quot; --data-urlencode &quot;sample_block=columns format version: 1 3 columns: \\`PageID\\` String \\`ImpID\\` String \\`AdType\\` String &quot; &quot;http://localhost:9018/&quot; &gt; result.txt $ cat result.txt 12246623837185725195925621517  "},{"title":"Quick Start","type":0,"sectionRef":"#","url":"en/quick-start","content":"","keywords":"clickhouse install getting started quick start"},{"title":"1. Start ClickHouse​","type":1,"pageTitle":"Quick Start","url":"en/quick-start#1-start-clickhouse","content":"LinuxmacOS The simplest way to download ClickHouse locally is to run the following command. If your operating system is supported, an appropriate ClickHouse binary will be downloaded and made executable: curl https://clickhouse.com/ | sh Run the install command, which defines a collection of useful symlinks along with the files and folders used by ClickHouse - all of which you can see in the output of the install script: sudo ./clickhouse install At the end of the install script, you are prompted for a password for the default user. Feel free to enter a password, or you can optionally leave it blank: Creating log directory /var/log/clickhouse-server. Creating data directory /var/lib/clickhouse. Creating pid directory /var/run/clickhouse-server. chown -R clickhouse:clickhouse '/var/log/clickhouse-server' chown -R clickhouse:clickhouse '/var/run/clickhouse-server' chown clickhouse:clickhouse '/var/lib/clickhouse' Enter password for default user: You should see the following output: ClickHouse has been successfully installed. Start clickhouse-server with: sudo clickhouse start Start clickhouse-client with: clickhouse-client Run the following command to start the ClickHouse server: sudo clickhouse start  "},{"title":"2. Connect to ClickHouse​","type":1,"pageTitle":"Quick Start","url":"en/quick-start#2-connect-to-clickhouse","content":"The ClickHouse server listens for HTTP clients on port 8123 by default. There is a built-in UI for running SQL queries at http://127.0.0.1:8123/play (change the hostname accordingly). Notice in your Play UI that the username was populated with default and the password text field was left empty. If you assigned a password to the default user, enter it into the password field. Try running a query. For example, the following returns the names of the predefined databases: SHOW databases Click the RUN button and the response is displayed in the lower portion of the Play UI: "},{"title":"3. Create a Table​","type":1,"pageTitle":"Quick Start","url":"en/quick-start#3-create-a-table","content":"As in most databases management systems, ClickHouse logically groups tables into databases. Use the CREATE DATABASE command to create a new database in ClickHouse: CREATE DATABASE IF NOT EXISTS helloworld Even the simplest of tables in ClickHouse must specify a table engine. The engine determines details about the table like: how and where the data is stored,which queries are supported, andwhether or not the data is replicated. There are many engines to choose from, but for a simple table on a single-node ClickHouse server,MergeTree is your likely choice. Run the following command to create a table named my_first_table in the helloworld database: CREATE TABLE helloworld.my_first_table ( user_id UInt32, message String, timestamp DateTime, metric Float32 ) ENGINE = MergeTree() PRIMARY KEY (user_id, timestamp)  "},{"title":"A Brief Intro to Primary Keys​","type":1,"pageTitle":"Quick Start","url":"en/quick-start#a-brief-intro-to-primary-keys","content":"Before you go any further, it is important to understand how primary keys work in ClickHouse (the implementation of primary keys might seem unexpected!): primary keys in ClickHouse are not unique for each row in a table The primary key of a ClickHouse table determines how the data is sorted when written to disk. Every 8,192 rows or 10MB of data (referred to as the index granularity) creates an entry in the primary key index file. This granularity concept creates a sparse index that can easily fit in memory, and the granules represent a stripe of the smallest amount of column data that gets processed during SELECT queries. The primary key can be defined using the PRIMARY KEY command. If you define a table without a PRIMARY KEY specified, then the key becomes the tuple specified in the ORDER BY clause. If you specify both a PRIMARY KEY and an ORDER BY, the primary key must be a subset of the sort order. In the example above, my_first_table is a MergeTree table with four columns: user_id: a 32-bit unsigned integermessage: a String data type, which replaces types like VARCHAR, BLOB, CLOB and others from other database systemstimestamp: a DateTime value, which represents an instant in timemetric: a 32-bit floating point number The primary key is also the sorting key, which is a tuple of (user_id, timestamp). Therefore, the data stored in each column file will be sorted by user_id, then timestamp. "},{"title":"4. Insert Data​","type":1,"pageTitle":"Quick Start","url":"en/quick-start#4-insert-data","content":"You can use the familiar INSERT INTO TABLE command with ClickHouse, but it is important to understand that each insert into a MergeTree table causes a part to be created in storage. The best practice with ClickHouse is to insert a large number of rows per batch - tens of thousands or even millions of rows at once. (Don't worry - ClickHouse can easily handle that type of volume!) Even for a simple example, let's insert more than one row at a time: INSERT INTO helloworld.my_first_table (user_id, message, timestamp, metric) VALUES (101, 'Hello, ClickHouse!', now(), -1.0 ), (102, 'Insert a lot of rows per batch', yesterday(), 1.41421 ), (102, 'Sort your data based on your commonly-used queries', today(), 2.718 ), (101, 'Granules are the smallest chunks of data read', now() + 5, 3.14159 ) note Notice the timestamp column is populated using various Date and DateTime functions. ClickHouse has hundreds of useful functions that you can view in the Functions section. Let's verify it worked: SELECT * FROM helloworld.my_first_table You should see the four rows of data that were inserted: "},{"title":"5. The ClickHouse Client​","type":1,"pageTitle":"Quick Start","url":"en/quick-start#5-the-clickhouse-client","content":"LinuxmacOS You can also connect to your ClickHouse server using a command-line tool named clickhouse-client: clickhouse-client  If you get the smiley face prompt, you are ready to run queries! :) Give it a try by running the following query: SELECT * FROM helloworld.my_first_table ORDER BY timestamp Notice the response comes back in a nice table format: SELECT * FROM helloworld.my_first_table ORDER BY timestamp ASC Query id: f7a33012-bc8c-4f0f-9641-260ee1ffe4b8 ┌─user_id─┬─message────────────────────────────────────────────┬───────────timestamp─┬──metric─┐ │ 102 │ Insert a lot of rows per batch │ 2022-03-21 00:00:00 │ 1.41421 │ │ 102 │ Sort your data based on your commonly-used queries │ 2022-03-22 00:00:00 │ 2.718 │ │ 101 │ Hello, ClickHouse! │ 2022-03-22 14:04:09 │ -1 │ │ 101 │ Granules are the smallest chunks of data read │ 2022-03-22 14:04:14 │ 3.14159 │ └─────────┴────────────────────────────────────────────────────┴─────────────────────┴─────────┘ 4 rows in set. Elapsed: 0.008 sec. Add a FORMAT clause to specify one of the many supported output formats of ClickHouse: SELECT * FROM helloworld.my_first_table ORDER BY timestamp FORMAT TabSeparated In the above query, the output is returned as tab-separated: Query id: 3604df1c-acfd-4117-9c56-f86c69721121 102 Insert a lot of rows per batch 2022-03-21 00:00:00 1.41421 102 Sort your data based on your commonly-used queries 2022-03-22 00:00:00 2.718 101 Hello, ClickHouse! 2022-03-22 14:04:09 -1 101 Granules are the smallest chunks of data read 2022-03-22 14:04:14 3.14159 4 rows in set. Elapsed: 0.005 sec. To exit the clickhouse-client, enter the exit command: :) exit Bye.  "},{"title":"6. Insert a CSV file​","type":1,"pageTitle":"Quick Start","url":"en/quick-start#6-insert-a-csv-file","content":"A common task when getting started with a database is to insert some data that you already have in files. We have some sample data online that you can insert that represents clickstream data - it includes a user ID, a URL that was visited, and the timestamp of the event. Suppose we have the following text in a CSV file named data.csv: 102,This is data in a file,2022-02-22 10:43:28,123.45 101,It is comma-separated,2022-02-23 00:00:00,456.78 103,Use FORMAT to specify the format,2022-02-21 10:43:30,678.90 The following command inserts the data into my_first_table: clickhouse-client --query='INSERT INTO helloworld.my_first_table FORMAT CSV' &lt; data.csv Notice the new rows appear in the table now: "},{"title":"What's Next?​","type":1,"pageTitle":"Quick Start","url":"en/quick-start#whats-next","content":"The Tutorial has you insert 2 million rows into a table and write some analytical queriesWe have a list of example datasets with instructions on how to insert themCheck out our 25-minute video on Getting Started with ClickHouseIf your data is coming from an external source, view our collection of integration guides for connecting to message queues, databases, pipelines and moreIf you are using a UI/BI visualization tool, view the user guides for connecting a UI to ClickHouseThe user guide on primary keys is everything you need to know about primary keys and how to define them "},{"title":"Aggregate Functions","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/","content":"","keywords":""},{"title":"NULL Processing​","type":1,"pageTitle":"Aggregate Functions","url":"en/sql-reference/aggregate-functions/#null-processing","content":"During aggregation, all NULLs are skipped. Examples: Consider this table: ┌─x─┬────y─┐ │ 1 │ 2 │ │ 2 │ ᴺᵁᴸᴸ │ │ 3 │ 2 │ │ 3 │ 3 │ │ 3 │ ᴺᵁᴸᴸ │ └───┴──────┘  Let’s say you need to total the values in the y column: SELECT sum(y) FROM t_null_big  ┌─sum(y)─┐ │ 7 │ └────────┘  Now you can use the groupArray function to create an array from the y column: SELECT groupArray(y) FROM t_null_big  ┌─groupArray(y)─┐ │ [2,2,3] │ └───────────────┘  groupArray does not include NULL in the resulting array. "},{"title":"Aggregate Function Combinators","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/combinators","content":"","keywords":""},{"title":"-If​","type":1,"pageTitle":"Aggregate Function Combinators","url":"en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-if","content":"The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument – a condition (Uint8 type). The aggregate function processes only the rows that trigger the condition. If the condition was not triggered even once, it returns a default value (usually zeros or empty strings). Examples: sumIf(column, cond), countIf(cond), avgIf(x, cond), quantilesTimingIf(level1, level2)(x, cond), argMinIf(arg, val, cond) and so on. With conditional aggregate functions, you can calculate aggregates for several conditions at once, without using subqueries and JOINs. For example, conditional aggregate functions can be used to implement the segment comparison functionality. "},{"title":"-Array​","type":1,"pageTitle":"Aggregate Function Combinators","url":"en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-array","content":"The -Array suffix can be appended to any aggregate function. In this case, the aggregate function takes arguments of the ‘Array(T)’ type (arrays) instead of ‘T’ type arguments. If the aggregate function accepts multiple arguments, this must be arrays of equal lengths. When processing arrays, the aggregate function works like the original aggregate function across all array elements. Example 1: sumArray(arr) - Totals all the elements of all ‘arr’ arrays. In this example, it could have been written more simply: sum(arraySum(arr)). Example 2: uniqArray(arr) – Counts the number of unique elements in all ‘arr’ arrays. This could be done an easier way: uniq(arrayJoin(arr)), but it’s not always possible to add ‘arrayJoin’ to a query. -If and -Array can be combined. However, ‘Array’ must come first, then ‘If’. Examples: uniqArrayIf(arr, cond), quantilesTimingArrayIf(level1, level2)(arr, cond). Due to this order, the ‘cond’ argument won’t be an array. "},{"title":"-Map​","type":1,"pageTitle":"Aggregate Function Combinators","url":"en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-map","content":"The -Map suffix can be appended to any aggregate function. This will create an aggregate function which gets Map type as an argument, and aggregates values of each key of the map separately using the specified aggregate function. The result is also of a Map type. Examples: sumMap(map(1,1)), avgMap(map('a', 1)). "},{"title":"-SimpleState​","type":1,"pageTitle":"Aggregate Function Combinators","url":"en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-simplestate","content":"If you apply this combinator, the aggregate function returns the same value but with a different type. This is a SimpleAggregateFunction(...) that can be stored in a table to work with AggregatingMergeTree tables. Syntax &lt;aggFunction&gt;SimpleState(x)  Arguments x — Aggregate function parameters. Returned values The value of an aggregate function with the SimpleAggregateFunction(...) type. Example Query: WITH anySimpleState(number) AS c SELECT toTypeName(c), c FROM numbers(1);  Result: ┌─toTypeName(c)────────────────────────┬─c─┐ │ SimpleAggregateFunction(any, UInt64) │ 0 │ └──────────────────────────────────────┴───┘  "},{"title":"-State​","type":1,"pageTitle":"Aggregate Function Combinators","url":"en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-state","content":"If you apply this combinator, the aggregate function does not return the resulting value (such as the number of unique values for the uniq function), but an intermediate state of the aggregation (for uniq, this is the hash table for calculating the number of unique values). This is an AggregateFunction(...) that can be used for further processing or stored in a table to finish aggregating later. To work with these states, use: AggregatingMergeTree table engine.finalizeAggregation function.runningAccumulate function.-Merge combinator.-MergeState combinator. "},{"title":"-Merge​","type":1,"pageTitle":"Aggregate Function Combinators","url":"en/sql-reference/aggregate-functions/combinators#aggregate_functions_combinators-merge","content":"If you apply this combinator, the aggregate function takes the intermediate aggregation state as an argument, combines the states to finish aggregation, and returns the resulting value. "},{"title":"-MergeState​","type":1,"pageTitle":"Aggregate Function Combinators","url":"en/sql-reference/aggregate-functions/combinators#aggregate_functions_combinators-mergestate","content":"Merges the intermediate aggregation states in the same way as the -Merge combinator. However, it does not return the resulting value, but an intermediate aggregation state, similar to the -State combinator. "},{"title":"-ForEach​","type":1,"pageTitle":"Aggregate Function Combinators","url":"en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-foreach","content":"Converts an aggregate function for tables into an aggregate function for arrays that aggregates the corresponding array items and returns an array of results. For example, sumForEach for the arrays [1, 2], [3, 4, 5]and[6, 7]returns the result [10, 13, 5] after adding together the corresponding array items. "},{"title":"-Distinct​","type":1,"pageTitle":"Aggregate Function Combinators","url":"en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-distinct","content":"Every unique combination of arguments will be aggregated only once. Repeating values are ignored. Examples: sum(DISTINCT x), groupArray(DISTINCT x), corrStableDistinct(DISTINCT x, y) and so on. "},{"title":"-OrDefault​","type":1,"pageTitle":"Aggregate Function Combinators","url":"en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-ordefault","content":"Changes behavior of an aggregate function. If an aggregate function does not have input values, with this combinator it returns the default value for its return data type. Applies to the aggregate functions that can take empty input data. -OrDefault can be used with other combinators. Syntax &lt;aggFunction&gt;OrDefault(x)  Arguments x — Aggregate function parameters. Returned values Returns the default value of an aggregate function’s return type if there is nothing to aggregate. Type depends on the aggregate function used. Example Query: SELECT avg(number), avgOrDefault(number) FROM numbers(0)  Result: ┌─avg(number)─┬─avgOrDefault(number)─┐ │ nan │ 0 │ └─────────────┴──────────────────────┘  Also -OrDefault can be used with another combinators. It is useful when the aggregate function does not accept the empty input. Query: SELECT avgOrDefaultIf(x, x &gt; 10) FROM ( SELECT toDecimal32(1.23, 2) AS x )  Result: ┌─avgOrDefaultIf(x, greater(x, 10))─┐ │ 0.00 │ └───────────────────────────────────┘  "},{"title":"-OrNull​","type":1,"pageTitle":"Aggregate Function Combinators","url":"en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-ornull","content":"Changes behavior of an aggregate function. This combinator converts a result of an aggregate function to the Nullable data type. If the aggregate function does not have values to calculate it returns NULL. -OrNull can be used with other combinators. Syntax &lt;aggFunction&gt;OrNull(x)  Arguments x — Aggregate function parameters. Returned values The result of the aggregate function, converted to the Nullable data type.NULL, if there is nothing to aggregate. Type: Nullable(aggregate function return type). Example Add -orNull to the end of aggregate function. Query: SELECT sumOrNull(number), toTypeName(sumOrNull(number)) FROM numbers(10) WHERE number &gt; 10  Result: ┌─sumOrNull(number)─┬─toTypeName(sumOrNull(number))─┐ │ ᴺᵁᴸᴸ │ Nullable(UInt64) │ └───────────────────┴───────────────────────────────┘  Also -OrNull can be used with another combinators. It is useful when the aggregate function does not accept the empty input. Query: SELECT avgOrNullIf(x, x &gt; 10) FROM ( SELECT toDecimal32(1.23, 2) AS x )  Result: ┌─avgOrNullIf(x, greater(x, 10))─┐ │ ᴺᵁᴸᴸ │ └────────────────────────────────┘  "},{"title":"-Resample​","type":1,"pageTitle":"Aggregate Function Combinators","url":"en/sql-reference/aggregate-functions/combinators#agg-functions-combinator-resample","content":"Lets you divide data into groups, and then separately aggregates the data in those groups. Groups are created by splitting the values from one column into intervals. &lt;aggFunction&gt;Resample(start, end, step)(&lt;aggFunction_params&gt;, resampling_key)  Arguments start — Starting value of the whole required interval for resampling_key values.stop — Ending value of the whole required interval for resampling_key values. The whole interval does not include the stop value [start, stop).step — Step for separating the whole interval into subintervals. The aggFunction is executed over each of those subintervals independently.resampling_key — Column whose values are used for separating data into intervals.aggFunction_params — aggFunction parameters. Returned values Array of aggFunction results for each subinterval. Example Consider the people table with the following data: ┌─name───┬─age─┬─wage─┐ │ John │ 16 │ 10 │ │ Alice │ 30 │ 15 │ │ Mary │ 35 │ 8 │ │ Evelyn │ 48 │ 11.5 │ │ David │ 62 │ 9.9 │ │ Brian │ 60 │ 16 │ └────────┴─────┴──────┘  Let’s get the names of the people whose age lies in the intervals of [30,60) and [60,75). Since we use integer representation for age, we get ages in the [30, 59] and [60,74] intervals. To aggregate names in an array, we use the groupArray aggregate function. It takes one argument. In our case, it’s the name column. The groupArrayResample function should use the age column to aggregate names by age. To define the required intervals, we pass the 30, 75, 30 arguments into the groupArrayResample function. SELECT groupArrayResample(30, 75, 30)(name, age) FROM people  ┌─groupArrayResample(30, 75, 30)(name, age)─────┐ │ [['Alice','Mary','Evelyn'],['David','Brian']] │ └───────────────────────────────────────────────┘  Consider the results. Jonh is out of the sample because he’s too young. Other people are distributed according to the specified age intervals. Now let’s count the total number of people and their average wage in the specified age intervals. SELECT countResample(30, 75, 30)(name, age) AS amount, avgResample(30, 75, 30)(wage, age) AS avg_wage FROM people  ┌─amount─┬─avg_wage──────────────────┐ │ [3,2] │ [11.5,12.949999809265137] │ └────────┴───────────────────────────┘  "},{"title":"List of Aggregate Functions","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/","content":"List of Aggregate Functions Standard aggregate functions: countminmaxsumavganystddevPopstddevSampvarPopvarSampcovarPopcovarSamp ClickHouse-specific aggregate functions: anyHeavyanyLastargMinargMaxavgWeightedtopKtopKWeightedgroupArraygroupUniqArraygroupArrayInsertAtgroupArrayMovingAvggroupArrayMovingSumgroupBitAndgroupBitOrgroupBitXorgroupBitmapgroupBitmapAndgroupBitmapOrgroupBitmapXorsumWithOverflowsumMapminMapmaxMapskewSampskewPopkurtSampkurtPopuniquniqExactuniqCombineduniqCombined64uniqHLL12quantilequantilesquantileExactquantileExactLowquantileExactHighquantileExactWeightedquantileTimingquantileTimingWeightedquantileDeterministicquantileTDigestquantileTDigestWeightedquantileBFloat16quantileBFloat16WeightedsimpleLinearRegressionstochasticLinearRegressionstochasticLogisticRegressioncategoricalInformationValue Original article","keywords":""},{"title":"any","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/any","content":"any Selects the first encountered value. The query can be executed in any order and even in a different order each time, so the result of this function is indeterminate. To get a determinate result, you can use the ‘min’ or ‘max’ function instead of ‘any’. In some cases, you can rely on the order of execution. This applies to cases when SELECT comes from a subquery that uses ORDER BY. When a SELECT query has the GROUP BY clause or at least one aggregate function, ClickHouse (in contrast to MySQL) requires that all expressions in the SELECT, HAVING, and ORDER BY clauses be calculated from keys or from aggregate functions. In other words, each column selected from the table must be used either in keys or inside aggregate functions. To get behavior like in MySQL, you can put the other columns in the any aggregate function.","keywords":""},{"title":"anyHeavy","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/anyheavy","content":"anyHeavy Selects a frequently occurring value using the heavy hitters algorithm. If there is a value that occurs more than in half the cases in each of the query’s execution threads, this value is returned. Normally, the result is nondeterministic. anyHeavy(column) Arguments column – The column name. Example Take the OnTime data set and select any frequently occurring value in the AirlineID column. SELECT anyHeavy(AirlineID) AS res FROM ontime ┌───res─┐ │ 19690 │ └───────┘ ","keywords":""},{"title":"anylast","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/anylast","content":"","keywords":""},{"title":"anyLast​","type":1,"pageTitle":"anylast","url":"en/sql-reference/aggregate-functions/reference/anylast#anylastx","content":"Selects the last value encountered. The result is just as indeterminate as for the any function. "},{"title":"argMax","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/argmax","content":"argMax Calculates the arg value for a maximum val value. If there are several different values of arg for maximum values of val, returns the first of these values encountered. Syntax argMax(arg, val) Arguments arg — Argument.val — Value. Returned value arg value that corresponds to maximum val value. Type: matches arg type. Example Input table: ┌─user─────┬─salary─┐ │ director │ 5000 │ │ manager │ 3000 │ │ worker │ 1000 │ └──────────┴────────┘ Query: SELECT argMax(user, salary) FROM salary; Result: ┌─argMax(user, salary)─┐ │ director │ └──────────────────────┘ ","keywords":""},{"title":"argMin","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/argmin","content":"argMin Calculates the arg value for a minimum val value. If there are several different values of arg for minimum values of val, returns the first of these values encountered. Syntax argMin(arg, val) Arguments arg — Argument.val — Value. Returned value arg value that corresponds to minimum val value. Type: matches arg type. Example Input table: ┌─user─────┬─salary─┐ │ director │ 5000 │ │ manager │ 3000 │ │ worker │ 1000 │ └──────────┴────────┘ Query: SELECT argMin(user, salary) FROM salary Result: ┌─argMin(user, salary)─┐ │ worker │ └──────────────────────┘ ","keywords":""},{"title":"avg","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/avg","content":"avg Calculates the arithmetic mean. Syntax avg(x) Arguments x — input values, must be Integer, Float, or Decimal. Returned value The arithmetic mean, always as Float64.NaN if the input parameter x is empty. Example Query: SELECT avg(x) FROM values('x Int8', 0, 1, 2, 3, 4, 5); Result: ┌─avg(x)─┐ │ 2.5 │ └────────┘ Example Create a temp table: Query: CREATE table test (t UInt8) ENGINE = Memory; Get the arithmetic mean: Query: SELECT avg(t) FROM test; Result: ┌─avg(x)─┐ │ nan │ └────────┘ Original article","keywords":""},{"title":"avgWeighted","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/avgweighted","content":"avgWeighted Calculates the weighted arithmetic mean. Syntax avgWeighted(x, weight) Arguments x — Values.weight — Weights of the values. x and weight must both beInteger,floating-point, orDecimal, but may have different types. Returned value NaN if all the weights are equal to 0 or the supplied weights parameter is empty.Weighted mean otherwise. Return type is always Float64. Example Query: SELECT avgWeighted(x, w) FROM values('x Int8, w Int8', (4, 1), (1, 0), (10, 2)) Result: ┌─avgWeighted(x, weight)─┐ │ 8 │ └────────────────────────┘ Example Query: SELECT avgWeighted(x, w) FROM values('x Int8, w Float64', (4, 1), (1, 0), (10, 2)) Result: ┌─avgWeighted(x, weight)─┐ │ 8 │ └────────────────────────┘ Example Query: SELECT avgWeighted(x, w) FROM values('x Int8, w Int8', (0, 0), (1, 0), (10, 0)) Result: ┌─avgWeighted(x, weight)─┐ │ nan │ └────────────────────────┘ Example Query: CREATE table test (t UInt8) ENGINE = Memory; SELECT avgWeighted(t) FROM test Result: ┌─avgWeighted(x, weight)─┐ │ nan │ └────────────────────────┘ ","keywords":""},{"title":"categoricalInformationValue","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/categoricalinformationvalue","content":"categoricalInformationValue Calculates the value of (P(tag = 1) - P(tag = 0))(log(P(tag = 1)) - log(P(tag = 0))) for each category. categoricalInformationValue(category1, category2, ..., tag) The result indicates how a discrete (categorical) feature [category1, category2, ...] contribute to a learning model which predicting the value of tag.","keywords":""},{"title":"corr","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/corr","content":"corr Syntax: corr(x, y) Calculates the Pearson correlation coefficient: Σ((x - x̅)(y - y̅)) / sqrt(Σ((x - x̅)^2) * Σ((y - y̅)^2)). note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the corrStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"count","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/count","content":"count Counts the number of rows or not-NULL values. ClickHouse supports the following syntaxes for count: count(expr) or COUNT(DISTINCT expr).count() or COUNT(*). The count() syntax is ClickHouse-specific. Arguments The function can take: Zero parameters.One expression. Returned value If the function is called without parameters it counts the number of rows.If the expression is passed, then the function counts how many times this expression returned not null. If the expression returns a Nullable-type value, then the result of count stays not Nullable. The function returns 0 if the expression returned NULL for all the rows. In both cases the type of the returned value is UInt64. Details ClickHouse supports the COUNT(DISTINCT ...) syntax. The behavior of this construction depends on the count_distinct_implementation setting. It defines which of the uniq* functions is used to perform the operation. The default is the uniqExact function. The SELECT count() FROM table query is optimized by default using metadata from MergeTree. If you need to use row-level security, disable optimization using the optimize_trivial_count_query setting. However SELECT count(nullable_column) FROM table query can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only null subcolumn instead of reading and processing the whole column data. The query SELECT count(n) FROM table transforms to SELECT sum(NOT n.null) FROM table. Examples Example 1: SELECT count() FROM t ┌─count()─┐ │ 5 │ └─────────┘ Example 2: SELECT name, value FROM system.settings WHERE name = 'count_distinct_implementation' ┌─name──────────────────────────┬─value─────┐ │ count_distinct_implementation │ uniqExact │ └───────────────────────────────┴───────────┘ SELECT count(DISTINCT num) FROM t ┌─uniqExact(num)─┐ │ 3 │ └────────────────┘ This example shows that count(DISTINCT num) is performed by the uniqExact function according to the count_distinct_implementation setting value.","keywords":""},{"title":"covarSamp","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/covarsamp","content":"covarSamp Calculates the value of Σ((x - x̅)(y - y̅)) / (n - 1). Returns Float64. When n &lt;= 1, returns +∞. note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the covarSampStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"deltaSum","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/deltasum","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"deltaSum","url":"en/sql-reference/aggregate-functions/reference/deltasum#see-also","content":"runningDifference "},{"title":"groupArray","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/grouparray","content":"groupArray Syntax: groupArray(x) or groupArray(max_size)(x) Creates an array of argument values. Values can be added to the array in any (indeterminate) order. The second version (with the max_size parameter) limits the size of the resulting array to max_size elements. For example, groupArray(1)(x) is equivalent to [any (x)]. In some cases, you can still rely on the order of execution. This applies to cases when SELECT comes from a subquery that uses ORDER BY.","keywords":""},{"title":"deltaSumTimestamp","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/deltasumtimestamp","content":"deltaSumTimestamp Adds the difference between consecutive rows. If the difference is negative, it is ignored. This function is primarily for materialized views that are ordered by some time bucket-aligned timestamp, for example, a toStartOfMinute bucket. Because the rows in such a materialized view will all have the same timestamp, it is impossible for them to be merged in the &quot;right&quot; order. This function keeps track of the timestamp of the values it's seen, so it's possible to order the states correctly during merging. To calculate the delta sum across an ordered collection you can simply use the deltaSum function. Syntax deltaSumTimestamp(value, timestamp) Arguments value — Input values, must be some Integer type or Float type or a Date or DateTime.timestamp — The parameter for order values, must be some Integer type or Float type or a Date or DateTime. Returned value Accumulated differences between consecutive values, ordered by the timestamp parameter. Type: Integer or Float or Date or DateTime. Example Query: SELECT deltaSumTimestamp(value, timestamp) FROM (SELECT number AS timestamp, [0, 4, 8, 3, 0, 0, 0, 1, 3, 5][number] AS value FROM numbers(1, 10)); Result: ┌─deltaSumTimestamp(value, timestamp)─┐ │ 13 │ └─────────────────────────────────────┘ ","keywords":""},{"title":"entropy","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/entropy","content":"entropy Calculates Shannon entropy of a column of values. Syntax entropy(val) Arguments val — Column of values of any type. Returned value Shannon entropy. Type: Float64. Example Query: CREATE TABLE entropy (`vals` UInt32,`strings` String) ENGINE = Memory; INSERT INTO entropy VALUES (1, 'A'), (1, 'A'), (1,'A'), (1,'A'), (2,'B'), (2,'B'), (2,'C'), (2,'D'); SELECT entropy(vals), entropy(strings) FROM entropy; Result: ┌─entropy(vals)─┬─entropy(strings)─┐ │ 1 │ 1.75 │ └───────────────┴──────────────────┘ ","keywords":""},{"title":"covarPop","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/covarpop","content":"covarPop Syntax: covarPop(x, y) Calculates the value of Σ((x - x̅)(y - y̅)) / n. note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the covarPopStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"groupArraySample","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/grouparraysample","content":"groupArraySample Creates an array of sample argument values. The size of the resulting array is limited to max_size elements. Argument values are selected and added to the array randomly. Syntax groupArraySample(max_size[, seed])(x) Arguments max_size — Maximum size of the resulting array. UInt64.seed — Seed for the random number generator. Optional. UInt64. Default value: 123456.x — Argument (column name or expression). Returned values Array of randomly selected x arguments. Type: Array. Examples Consider table colors: ┌─id─┬─color──┐ │ 1 │ red │ │ 2 │ blue │ │ 3 │ green │ │ 4 │ white │ │ 5 │ orange │ └────┴────────┘ Query with column name as argument: SELECT groupArraySample(3)(color) as newcolors FROM colors; Result: ┌─newcolors──────────────────┐ │ ['white','blue','green'] │ └────────────────────────────┘ Query with column name and different seed: SELECT groupArraySample(3, 987654321)(color) as newcolors FROM colors; Result: ┌─newcolors──────────────────┐ │ ['red','orange','green'] │ └────────────────────────────┘ Query with expression as argument: SELECT groupArraySample(3)(concat('light-', color)) as newcolors FROM colors; Result: ┌─newcolors───────────────────────────────────┐ │ ['light-blue','light-orange','light-green'] │ └─────────────────────────────────────────────┘ ","keywords":""},{"title":"groupBitmap","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/groupbitmap","content":"groupBitmap Bitmap or Aggregate calculations from a unsigned integer column, return cardinality of type UInt64, if add suffix -State, then return bitmap object. groupBitmap(expr) Arguments expr – An expression that results in UInt* type. Return value Value of the UInt64 type. Example Test data: UserID 1 1 2 3 Query: SELECT groupBitmap(UserID) as num FROM t Result: num 3 ","keywords":""},{"title":"groupArrayInsertAt","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/grouparrayinsertat","content":"groupArrayInsertAt Inserts a value into the array at the specified position. Syntax groupArrayInsertAt(default_x, size)(x, pos) If in one query several values are inserted into the same position, the function behaves in the following ways: If a query is executed in a single thread, the first one of the inserted values is used.If a query is executed in multiple threads, the resulting value is an undetermined one of the inserted values. Arguments x — Value to be inserted. Expression resulting in one of the supported data types.pos — Position at which the specified element x is to be inserted. Index numbering in the array starts from zero. UInt32.default_x — Default value for substituting in empty positions. Optional parameter. Expression resulting in the data type configured for the x parameter. If default_x is not defined, the default values are used.size — Length of the resulting array. Optional parameter. When using this parameter, the default value default_x must be specified. UInt32. Returned value Array with inserted values. Type: Array. Example Query: SELECT groupArrayInsertAt(toString(number), number * 2) FROM numbers(5); Result: ┌─groupArrayInsertAt(toString(number), multiply(number, 2))─┐ │ ['0','','1','','2','','3','','4'] │ └───────────────────────────────────────────────────────────┘ Query: SELECT groupArrayInsertAt('-')(toString(number), number * 2) FROM numbers(5); Result: ┌─groupArrayInsertAt('-')(toString(number), multiply(number, 2))─┐ │ ['0','-','1','-','2','-','3','-','4'] │ └────────────────────────────────────────────────────────────────┘ Query: SELECT groupArrayInsertAt('-', 5)(toString(number), number * 2) FROM numbers(5); Result: ┌─groupArrayInsertAt('-', 5)(toString(number), multiply(number, 2))─┐ │ ['0','-','1','-','2'] │ └───────────────────────────────────────────────────────────────────┘ Multi-threaded insertion of elements into one position. Query: SELECT groupArrayInsertAt(number, 0) FROM numbers_mt(10) SETTINGS max_block_size = 1; As a result of this query you get random integer in the [0,9] range. For example: ┌─groupArrayInsertAt(number, 0)─┐ │ [7] │ └───────────────────────────────┘ ","keywords":""},{"title":"groupArrayMovingSum","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/grouparraymovingsum","content":"groupArrayMovingSum Calculates the moving sum of input values. groupArrayMovingSum(numbers_for_summing) groupArrayMovingSum(window_size)(numbers_for_summing) The function can take the window size as a parameter. If left unspecified, the function takes the window size equal to the number of rows in the column. Arguments numbers_for_summing — Expression resulting in a numeric data type value.window_size — Size of the calculation window. Returned values Array of the same size and type as the input data. Example The sample table: CREATE TABLE t ( `int` UInt8, `float` Float32, `dec` Decimal32(2) ) ENGINE = TinyLog ┌─int─┬─float─┬──dec─┐ │ 1 │ 1.1 │ 1.10 │ │ 2 │ 2.2 │ 2.20 │ │ 4 │ 4.4 │ 4.40 │ │ 7 │ 7.77 │ 7.77 │ └─────┴───────┴──────┘ The queries: SELECT groupArrayMovingSum(int) AS I, groupArrayMovingSum(float) AS F, groupArrayMovingSum(dec) AS D FROM t ┌─I──────────┬─F───────────────────────────────┬─D──────────────────────┐ │ [1,3,7,14] │ [1.1,3.3000002,7.7000003,15.47] │ [1.10,3.30,7.70,15.47] │ └────────────┴─────────────────────────────────┴────────────────────────┘ SELECT groupArrayMovingSum(2)(int) AS I, groupArrayMovingSum(2)(float) AS F, groupArrayMovingSum(2)(dec) AS D FROM t ┌─I──────────┬─F───────────────────────────────┬─D──────────────────────┐ │ [1,3,6,11] │ [1.1,3.3000002,6.6000004,12.17] │ [1.10,3.30,6.60,12.17] │ └────────────┴─────────────────────────────────┴────────────────────────┘ ","keywords":""},{"title":"exponentialmovingaverage","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/exponentialmovingaverage","content":"","keywords":""},{"title":"exponentialMovingAverage​","type":1,"pageTitle":"exponentialmovingaverage","url":"en/sql-reference/aggregate-functions/reference/exponentialmovingaverage#exponential-moving-average","content":"Сalculates the exponential moving average of values for the determined time. Syntax exponentialMovingAverage(x)(value, timestamp)  Each value corresponds to the determinate timestamp. The half-life x is the time lag at which the exponential weights decay by one-half. The function returns a weighted average: the older the time point, the less weight the corresponding value is considered to be. Arguments value — Value. Integer, Float or Decimal.timestamp — Timestamp. Integer, Float or Decimal. Parameters x — Half-life period. Integer, Float or Decimal. Returned values Returnes an exponentially smoothed moving average of the values for the past x time at the latest point of time. Type: Float64. Examples Input table: ┌──temperature─┬─timestamp──┐ │ 95 │ 1 │ │ 95 │ 2 │ │ 95 │ 3 │ │ 96 │ 4 │ │ 96 │ 5 │ │ 96 │ 6 │ │ 96 │ 7 │ │ 97 │ 8 │ │ 97 │ 9 │ │ 97 │ 10 │ │ 97 │ 11 │ │ 98 │ 12 │ │ 98 │ 13 │ │ 98 │ 14 │ │ 98 │ 15 │ │ 99 │ 16 │ │ 99 │ 17 │ │ 99 │ 18 │ │ 100 │ 19 │ │ 100 │ 20 │ └──────────────┴────────────┘  Query: SELECT exponentialMovingAverage(5)(temperature, timestamp);  Result: ┌──exponentialMovingAverage(5)(temperature, timestamp)──┐ │ 92.25779635374204 │ └───────────────────────────────────────────────────────┘  Query: SELECT value, time, round(exp_smooth, 3), bar(exp_smooth, 0, 1, 50) AS bar FROM ( SELECT (number = 0) OR (number &gt;= 25) AS value, number AS time, exponentialMovingAverage(10)(value, time) OVER (Rows BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS exp_smooth FROM numbers(50) )  Result: ┌─value─┬─time─┬─round(exp_smooth, 3)─┬─bar────────────────────────────────────────┐ │ 1 │ 0 │ 0.067 │ ███▎ │ │ 0 │ 1 │ 0.062 │ ███ │ │ 0 │ 2 │ 0.058 │ ██▊ │ │ 0 │ 3 │ 0.054 │ ██▋ │ │ 0 │ 4 │ 0.051 │ ██▌ │ │ 0 │ 5 │ 0.047 │ ██▎ │ │ 0 │ 6 │ 0.044 │ ██▏ │ │ 0 │ 7 │ 0.041 │ ██ │ │ 0 │ 8 │ 0.038 │ █▊ │ │ 0 │ 9 │ 0.036 │ █▋ │ │ 0 │ 10 │ 0.033 │ █▋ │ │ 0 │ 11 │ 0.031 │ █▌ │ │ 0 │ 12 │ 0.029 │ █▍ │ │ 0 │ 13 │ 0.027 │ █▎ │ │ 0 │ 14 │ 0.025 │ █▎ │ │ 0 │ 15 │ 0.024 │ █▏ │ │ 0 │ 16 │ 0.022 │ █ │ │ 0 │ 17 │ 0.021 │ █ │ │ 0 │ 18 │ 0.019 │ ▊ │ │ 0 │ 19 │ 0.018 │ ▊ │ │ 0 │ 20 │ 0.017 │ ▋ │ │ 0 │ 21 │ 0.016 │ ▋ │ │ 0 │ 22 │ 0.015 │ ▋ │ │ 0 │ 23 │ 0.014 │ ▋ │ │ 0 │ 24 │ 0.013 │ ▋ │ │ 1 │ 25 │ 0.079 │ ███▊ │ │ 1 │ 26 │ 0.14 │ ███████ │ │ 1 │ 27 │ 0.198 │ █████████▊ │ │ 1 │ 28 │ 0.252 │ ████████████▌ │ │ 1 │ 29 │ 0.302 │ ███████████████ │ │ 1 │ 30 │ 0.349 │ █████████████████▍ │ │ 1 │ 31 │ 0.392 │ ███████████████████▌ │ │ 1 │ 32 │ 0.433 │ █████████████████████▋ │ │ 1 │ 33 │ 0.471 │ ███████████████████████▌ │ │ 1 │ 34 │ 0.506 │ █████████████████████████▎ │ │ 1 │ 35 │ 0.539 │ ██████████████████████████▊ │ │ 1 │ 36 │ 0.57 │ ████████████████████████████▌ │ │ 1 │ 37 │ 0.599 │ █████████████████████████████▊ │ │ 1 │ 38 │ 0.626 │ ███████████████████████████████▎ │ │ 1 │ 39 │ 0.651 │ ████████████████████████████████▌ │ │ 1 │ 40 │ 0.674 │ █████████████████████████████████▋ │ │ 1 │ 41 │ 0.696 │ ██████████████████████████████████▋ │ │ 1 │ 42 │ 0.716 │ ███████████████████████████████████▋ │ │ 1 │ 43 │ 0.735 │ ████████████████████████████████████▋ │ │ 1 │ 44 │ 0.753 │ █████████████████████████████████████▋ │ │ 1 │ 45 │ 0.77 │ ██████████████████████████████████████▍ │ │ 1 │ 46 │ 0.785 │ ███████████████████████████████████████▎ │ │ 1 │ 47 │ 0.8 │ ███████████████████████████████████████▊ │ │ 1 │ 48 │ 0.813 │ ████████████████████████████████████████▋ │ │ 1 │ 49 │ 0.825 │ █████████████████████████████████████████▎│ └───────┴──────┴──────────────────────┴────────────────────────────────────────────┘  "},{"title":"groupArrayMovingAvg","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/grouparraymovingavg","content":"groupArrayMovingAvg Calculates the moving average of input values. groupArrayMovingAvg(numbers_for_summing) groupArrayMovingAvg(window_size)(numbers_for_summing) The function can take the window size as a parameter. If left unspecified, the function takes the window size equal to the number of rows in the column. Arguments numbers_for_summing — Expression resulting in a numeric data type value.window_size — Size of the calculation window. Returned values Array of the same size and type as the input data. The function uses rounding towards zero. It truncates the decimal places insignificant for the resulting data type. Example The sample table b: CREATE TABLE t ( `int` UInt8, `float` Float32, `dec` Decimal32(2) ) ENGINE = TinyLog ┌─int─┬─float─┬──dec─┐ │ 1 │ 1.1 │ 1.10 │ │ 2 │ 2.2 │ 2.20 │ │ 4 │ 4.4 │ 4.40 │ │ 7 │ 7.77 │ 7.77 │ └─────┴───────┴──────┘ The queries: SELECT groupArrayMovingAvg(int) AS I, groupArrayMovingAvg(float) AS F, groupArrayMovingAvg(dec) AS D FROM t ┌─I─────────┬─F───────────────────────────────────┬─D─────────────────────┐ │ [0,0,1,3] │ [0.275,0.82500005,1.9250001,3.8675] │ [0.27,0.82,1.92,3.86] │ └───────────┴─────────────────────────────────────┴───────────────────────┘ SELECT groupArrayMovingAvg(2)(int) AS I, groupArrayMovingAvg(2)(float) AS F, groupArrayMovingAvg(2)(dec) AS D FROM t ┌─I─────────┬─F────────────────────────────────┬─D─────────────────────┐ │ [0,1,3,5] │ [0.55,1.6500001,3.3000002,6.085] │ [0.55,1.65,3.30,6.08] │ └───────────┴──────────────────────────────────┴───────────────────────┘ ","keywords":""},{"title":"groupBitAnd","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/groupbitand","content":"groupBitAnd Applies bitwise AND for series of numbers. groupBitAnd(expr) Arguments expr – An expression that results in UInt* type. Return value Value of the UInt* type. Example Test data: binary decimal 00101100 = 44 00011100 = 28 00001101 = 13 01010101 = 85 Query: SELECT groupBitAnd(num) FROM t Where num is the column with the test data. Result: binary decimal 00000100 = 4 ","keywords":""},{"title":"groupBitmapAnd","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/groupbitmapand","content":"groupBitmapAnd Calculations the AND of a bitmap column, return cardinality of type UInt64, if add suffix -State, then return bitmap object. groupBitmapAnd(expr) Arguments expr – An expression that results in AggregateFunction(groupBitmap, UInt*) type. Return value Value of the UInt64 type. Example DROP TABLE IF EXISTS bitmap_column_expr_test2; CREATE TABLE bitmap_column_expr_test2 ( tag_id String, z AggregateFunction(groupBitmap, UInt32) ) ENGINE = MergeTree ORDER BY tag_id; INSERT INTO bitmap_column_expr_test2 VALUES ('tag1', bitmapBuild(cast([1,2,3,4,5,6,7,8,9,10] as Array(UInt32)))); INSERT INTO bitmap_column_expr_test2 VALUES ('tag2', bitmapBuild(cast([6,7,8,9,10,11,12,13,14,15] as Array(UInt32)))); INSERT INTO bitmap_column_expr_test2 VALUES ('tag3', bitmapBuild(cast([2,4,6,8,10,12] as Array(UInt32)))); SELECT groupBitmapAnd(z) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%'); ┌─groupBitmapAnd(z)─┐ │ 3 │ └───────────────────┘ SELECT arraySort(bitmapToArray(groupBitmapAndState(z))) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%'); ┌─arraySort(bitmapToArray(groupBitmapAndState(z)))─┐ │ [6,8,10] │ └──────────────────────────────────────────────────┘ ","keywords":""},{"title":"groupBitmapOr","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/groupbitmapor","content":"groupBitmapOr Calculations the OR of a bitmap column, return cardinality of type UInt64, if add suffix -State, then return bitmap object. This is equivalent to groupBitmapMerge. groupBitmapOr(expr) Arguments expr – An expression that results in AggregateFunction(groupBitmap, UInt*) type. Returned value Value of the UInt64 type. Example DROP TABLE IF EXISTS bitmap_column_expr_test2; CREATE TABLE bitmap_column_expr_test2 ( tag_id String, z AggregateFunction(groupBitmap, UInt32) ) ENGINE = MergeTree ORDER BY tag_id; INSERT INTO bitmap_column_expr_test2 VALUES ('tag1', bitmapBuild(cast([1,2,3,4,5,6,7,8,9,10] as Array(UInt32)))); INSERT INTO bitmap_column_expr_test2 VALUES ('tag2', bitmapBuild(cast([6,7,8,9,10,11,12,13,14,15] as Array(UInt32)))); INSERT INTO bitmap_column_expr_test2 VALUES ('tag3', bitmapBuild(cast([2,4,6,8,10,12] as Array(UInt32)))); SELECT groupBitmapOr(z) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%'); ┌─groupBitmapOr(z)─┐ │ 15 │ └──────────────────┘ SELECT arraySort(bitmapToArray(groupBitmapOrState(z))) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%'); ┌─arraySort(bitmapToArray(groupBitmapOrState(z)))─┐ │ [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15] │ └─────────────────────────────────────────────────┘ ","keywords":""},{"title":"groupBitOr","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/groupbitor","content":"groupBitOr Applies bitwise OR for series of numbers. groupBitOr(expr) Arguments expr – An expression that results in UInt* type. Returned value Value of the UInt* type. Example Test data: binary decimal 00101100 = 44 00011100 = 28 00001101 = 13 01010101 = 85 Query: SELECT groupBitOr(num) FROM t Where num is the column with the test data. Result: binary decimal 01111101 = 125 ","keywords":""},{"title":"groupBitXor","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/groupbitxor","content":"groupBitXor Applies bitwise XOR for series of numbers. groupBitXor(expr) Arguments expr – An expression that results in UInt* type. Return value Value of the UInt* type. Example Test data: binary decimal 00101100 = 44 00011100 = 28 00001101 = 13 01010101 = 85 Query: SELECT groupBitXor(num) FROM t Where num is the column with the test data. Result: binary decimal 01101000 = 104 ","keywords":""},{"title":"groupUniqArray","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/groupuniqarray","content":"groupUniqArray Syntax: groupUniqArray(x) or groupUniqArray(max_size)(x) Creates an array from different argument values. Memory consumption is the same as for the uniqExact function. The second version (with the max_size parameter) limits the size of the resulting array to max_size elements. For example, groupUniqArray(1)(x) is equivalent to [any(x)].","keywords":""},{"title":"kurtSamp","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/kurtsamp","content":"kurtSamp Computes the sample kurtosis of a sequence. It represents an unbiased estimate of the kurtosis of a random variable if passed values form its sample. kurtSamp(expr) Arguments expr — Expression returning a number. Returned value The kurtosis of the given distribution. Type — Float64. If n &lt;= 1 (n is a size of the sample), then the function returns nan. Example SELECT kurtSamp(value) FROM series_with_value_column; ","keywords":""},{"title":"kurtPop","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/kurtpop","content":"kurtPop Computes the kurtosis of a sequence. kurtPop(expr) Arguments expr — Expression returning a number. Returned value The kurtosis of the given distribution. Type — Float64 Example SELECT kurtPop(value) FROM series_with_value_column; ","keywords":""},{"title":"maxMap","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/maxmap","content":"maxMap Syntax: maxMap(key, value) or maxMap(Tuple(key, value)) Calculates the maximum from value array according to the keys specified in the key array. Passing a tuple of keys and value arrays is identical to passing two arrays of keys and values. The number of elements in key and value must be the same for each row that is totaled. Returns a tuple of two arrays: keys and values calculated for the corresponding keys. Example: SELECT maxMap(a, b) FROM values('a Array(Int32), b Array(Int64)', ([1, 2], [2, 2]), ([2, 3], [1, 1])) ┌─maxMap(a, b)──────┐ │ ([1,2,3],[2,2,1]) │ └───────────────────┘ ","keywords":""},{"title":"mannWhitneyUTest","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/mannwhitneyutest","content":"mannWhitneyUTest Applies the Mann-Whitney rank test to samples from two populations. Syntax mannWhitneyUTest[(alternative[, continuity_correction])](sample_data, sample_index) Values of both samples are in the sample_data column. If sample_index equals to 0 then the value in that row belongs to the sample from the first population. Otherwise it belongs to the sample from the second population. The null hypothesis is that two populations are stochastically equal. Also one-sided hypothesises can be tested. This test does not assume that data have normal distribution. Arguments sample_data — sample data. Integer, Float or Decimal.sample_index — sample index. Integer. Parameters alternative — alternative hypothesis. (Optional, default: 'two-sided'.) String. 'two-sided';'greater';'less'. continuity_correction — if not 0 then continuity correction in the normal approximation for the p-value is applied. (Optional, default: 1.) UInt64. Returned values Tuple with two elements: calculated U-statistic. Float64.calculated p-value. Float64. Example Input table: ┌─sample_data─┬─sample_index─┐ │ 10 │ 0 │ │ 11 │ 0 │ │ 12 │ 0 │ │ 1 │ 1 │ │ 2 │ 1 │ │ 3 │ 1 │ └─────────────┴──────────────┘ Query: SELECT mannWhitneyUTest('greater')(sample_data, sample_index) FROM mww_ttest; Result: ┌─mannWhitneyUTest('greater')(sample_data, sample_index)─┐ │ (9,0.04042779918503192) │ └────────────────────────────────────────────────────────┘ See Also Mann–Whitney U testStochastic ordering Original article","keywords":""},{"title":"max","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/max","content":"max Aggregate function that calculates the maximum across a group of values. Example: SELECT max(salary) FROM employees; SELECT department, max(salary) FROM employees GROUP BY department; If you need non-aggregate function to choose a maximum of two values, see greatest: SELECT greatest(a, b) FROM table; ","keywords":""},{"title":"groupBitmapXor","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/groupbitmapxor","content":"groupBitmapXor Calculations the XOR of a bitmap column, return cardinality of type UInt64, if add suffix -State, then return bitmap object. groupBitmapOr(expr) Arguments expr – An expression that results in AggregateFunction(groupBitmap, UInt*) type. Returned value Value of the UInt64 type. Example DROP TABLE IF EXISTS bitmap_column_expr_test2; CREATE TABLE bitmap_column_expr_test2 ( tag_id String, z AggregateFunction(groupBitmap, UInt32) ) ENGINE = MergeTree ORDER BY tag_id; INSERT INTO bitmap_column_expr_test2 VALUES ('tag1', bitmapBuild(cast([1,2,3,4,5,6,7,8,9,10] as Array(UInt32)))); INSERT INTO bitmap_column_expr_test2 VALUES ('tag2', bitmapBuild(cast([6,7,8,9,10,11,12,13,14,15] as Array(UInt32)))); INSERT INTO bitmap_column_expr_test2 VALUES ('tag3', bitmapBuild(cast([2,4,6,8,10,12] as Array(UInt32)))); SELECT groupBitmapXor(z) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%'); ┌─groupBitmapXor(z)─┐ │ 10 │ └───────────────────┘ SELECT arraySort(bitmapToArray(groupBitmapXorState(z))) FROM bitmap_column_expr_test2 WHERE like(tag_id, 'tag%'); ┌─arraySort(bitmapToArray(groupBitmapXorState(z)))─┐ │ [1,3,5,6,8,10,11,13,14,15] │ └──────────────────────────────────────────────────┘ ","keywords":""},{"title":"minMap","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/minmap","content":"minMap Syntax: minMap(key, value) or minMap(Tuple(key, value)) Calculates the minimum from value array according to the keys specified in the key array. Passing a tuple of keys and value ​​arrays is identical to passing two arrays of keys and values. The number of elements in key and value must be the same for each row that is totaled. Returns a tuple of two arrays: keys in sorted order, and values calculated for the corresponding keys. Example: SELECT minMap(a, b) FROM values('a Array(Int32), b Array(Int64)', ([1, 2], [2, 2]), ([2, 3], [1, 1])) ┌─minMap(a, b)──────┐ │ ([1,2,3],[2,1,1]) │ └───────────────────┘ ","keywords":""},{"title":"quantile","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/quantile","content":"quantile Computes an approximate quantile of a numeric data sequence. This function applies reservoir sampling with a reservoir size up to 8192 and a random number generator for sampling. The result is non-deterministic. To get an exact quantile, use the quantileExact function. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Note that for an empty numeric sequence, quantile will return NaN, but its quantile* variants will return either NaN or a default value for the sequence type, depending on the variant. Syntax quantile(level)(expr) Alias: median. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime. Returned value Approximate quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Input table: ┌─val─┐ │ 1 │ │ 1 │ │ 2 │ │ 3 │ └─────┘ Query: SELECT quantile(val) FROM t Result: ┌─quantile(val)─┐ │ 1.5 │ └───────────────┘ See Also medianquantiles","keywords":""},{"title":"intervalLengthSum","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/intervalLengthSum","content":"intervalLengthSum Calculates the total length of union of all ranges (segments on numeric axis). Syntax intervalLengthSum(start, end) Arguments start — The starting value of the interval. Int32, Int64, UInt32, UInt64, Float32, Float64, DateTime or Date.end — The ending value of the interval. Int32, Int64, UInt32, UInt64, Float32, Float64, DateTime or Date. note Arguments must be of the same data type. Otherwise, an exception will be thrown. Returned value Total length of union of all ranges (segments on numeric axis). Depending on the type of the argument, the return value may be UInt64 or Float64 type. Examples Input table: ┌─id─┬─start─┬─end─┐ │ a │ 1.1 │ 2.9 │ │ a │ 2.5 │ 3.2 │ │ a │ 4 │ 5 │ └────┴───────┴─────┘ In this example, the arguments of the Float32 type are used. The function returns a value of the Float64 type. Result is the sum of lengths of intervals [1.1, 3.2] (union of [1.1, 2.9] and [2.5, 3.2]) and [4, 5] Query: SELECT id, intervalLengthSum(start, end), toTypeName(intervalLengthSum(start, end)) FROM fl_interval GROUP BY id ORDER BY id; Result: ┌─id─┬─intervalLengthSum(start, end)─┬─toTypeName(intervalLengthSum(start, end))─┐ │ a │ 3.1 │ Float64 │ └────┴───────────────────────────────┴───────────────────────────────────────────┘ Input table: ┌─id─┬───────────────start─┬─────────────────end─┐ │ a │ 2020-01-01 01:12:30 │ 2020-01-01 02:10:10 │ │ a │ 2020-01-01 02:05:30 │ 2020-01-01 02:50:31 │ │ a │ 2020-01-01 03:11:22 │ 2020-01-01 03:23:31 │ └────┴─────────────────────┴─────────────────────┘ In this example, the arguments of the DateTime type are used. The function returns a value in seconds. Query: SELECT id, intervalLengthSum(start, end), toTypeName(intervalLengthSum(start, end)) FROM dt_interval GROUP BY id ORDER BY id; Result: ┌─id─┬─intervalLengthSum(start, end)─┬─toTypeName(intervalLengthSum(start, end))─┐ │ a │ 6610 │ UInt64 │ └────┴───────────────────────────────┴───────────────────────────────────────────┘ Input table: ┌─id─┬──────start─┬────────end─┐ │ a │ 2020-01-01 │ 2020-01-04 │ │ a │ 2020-01-12 │ 2020-01-18 │ └────┴────────────┴────────────┘ In this example, the arguments of the Date type are used. The function returns a value in days. Query: SELECT id, intervalLengthSum(start, end), toTypeName(intervalLengthSum(start, end)) FROM date_interval GROUP BY id ORDER BY id; Result: ┌─id─┬─intervalLengthSum(start, end)─┬─toTypeName(intervalLengthSum(start, end))─┐ │ a │ 9 │ UInt64 │ └────┴───────────────────────────────┴───────────────────────────────────────────┘ ","keywords":""},{"title":"meanZTest","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/meanztest","content":"meanZTest Applies mean z-test to samples from two populations. Syntax meanZTest(population_variance_x, population_variance_y, confidence_level)(sample_data, sample_index) Values of both samples are in the sample_data column. If sample_index equals to 0 then the value in that row belongs to the sample from the first population. Otherwise it belongs to the sample from the second population. The null hypothesis is that means of populations are equal. Normal distribution is assumed. Populations may have unequal variance and the variances are known. Arguments sample_data — Sample data. Integer, Float or Decimal.sample_index — Sample index. Integer. Parameters population_variance_x — Variance for population x. Float.population_variance_y — Variance for population y. Float.confidence_level — Confidence level in order to calculate confidence intervals. Float. Returned values Tuple with four elements: calculated t-statistic. Float64.calculated p-value. Float64.calculated confidence-interval-low. Float64.calculated confidence-interval-high. Float64. Example Input table: ┌─sample_data─┬─sample_index─┐ │ 20.3 │ 0 │ │ 21.9 │ 0 │ │ 22.1 │ 0 │ │ 18.9 │ 1 │ │ 19 │ 1 │ │ 20.3 │ 1 │ └─────────────┴──────────────┘ Query: SELECT meanZTest(0.7, 0.45, 0.95)(sample_data, sample_index) FROM mean_ztest Result: ┌─meanZTest(0.7, 0.45, 0.95)(sample_data, sample_index)────────────────────────────┐ │ (3.2841296025548123,0.0010229786769086013,0.8198428246768334,3.2468238419898365) │ └──────────────────────────────────────────────────────────────────────────────────┘ Original article","keywords":""},{"title":"quantileDeterministic","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/quantiledeterministic","content":"quantileDeterministic Computes an approximate quantile of a numeric data sequence. This function applies reservoir sampling with a reservoir size up to 8192 and deterministic algorithm of sampling. The result is deterministic. To get an exact quantile, use the quantileExact function. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileDeterministic(level)(expr, determinator) Alias: medianDeterministic. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime.determinator — Number whose hash is used instead of a random number generator in the reservoir sampling algorithm to make the result of sampling deterministic. As a determinator you can use any deterministic positive number, for example, a user id or an event id. If the same determinator value occures too often, the function works incorrectly. Returned value Approximate quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Input table: ┌─val─┐ │ 1 │ │ 1 │ │ 2 │ │ 3 │ └─────┘ Query: SELECT quantileDeterministic(val, 1) FROM t Result: ┌─quantileDeterministic(val, 1)─┐ │ 1.5 │ └───────────────────────────────┘ See Also medianquantiles","keywords":""},{"title":"median","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/median","content":"median The median* functions are the aliases for the corresponding quantile* functions. They calculate median of a numeric data sample. Functions: median — Alias for quantile.medianDeterministic — Alias for quantileDeterministic.medianExact — Alias for quantileExact.medianExactWeighted — Alias for quantileExactWeighted.medianTiming — Alias for quantileTiming.medianTimingWeighted — Alias for quantileTimingWeighted.medianTDigest — Alias for quantileTDigest.medianTDigestWeighted — Alias for quantileTDigestWeighted.medianBFloat16 — Alias for quantileBFloat16. Example Input table: ┌─val─┐ │ 1 │ │ 1 │ │ 2 │ │ 3 │ └─────┘ Query: SELECT medianDeterministic(val, 1) FROM t; Result: ┌─medianDeterministic(val, 1)─┐ │ 1.5 │ └─────────────────────────────┘ ","keywords":""},{"title":"quantileExactWeighted","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/quantileexactweighted","content":"quantileExactWeighted Exactly computes the quantile of a numeric data sequence, taking into account the weight of each element. To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Each value is counted with its weight, as if it is present weight times. A hash table is used in the algorithm. Because of this, if the passed values ​​are frequently repeated, the function consumes less RAM than quantileExact. You can use this function instead of quantileExact and specify the weight 1. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileExactWeighted(level)(expr, weight) Alias: medianExactWeighted. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime.weight — Column with weights of sequence members. Weight is a number of value occurrences. Returned value Quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Input table: ┌─n─┬─val─┐ │ 0 │ 3 │ │ 1 │ 2 │ │ 2 │ 1 │ │ 5 │ 4 │ └───┴─────┘ Query: SELECT quantileExactWeighted(n, val) FROM t Result: ┌─quantileExactWeighted(n, val)─┐ │ 1 │ └───────────────────────────────┘ See Also medianquantiles","keywords":""},{"title":"min","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/min","content":"","keywords":""},{"title":"min​","type":1,"pageTitle":"min","url":"en/sql-reference/aggregate-functions/reference/min#agg_function-min","content":"Aggregate function that calculates the minimum across a group of values. Example: SELECT min(salary) FROM employees;  SELECT department, min(salary) FROM employees GROUP BY department;  If you need non-aggregate function to choose a minimum of two values, see least: SELECT least(a, b) FROM table;  "},{"title":"rankCorr","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/rankCorr","content":"rankCorr Computes a rank correlation coefficient. Syntax rankCorr(x, y) Arguments x — Arbitrary value. Float32 or Float64.y — Arbitrary value. Float32 or Float64. Returned value(s) Returns a rank correlation coefficient of the ranks of x and y. The value of the correlation coefficient ranges from -1 to +1. If less than two arguments are passed, the function will return an exception. The value close to +1 denotes a high linear relationship, and with an increase of one random variable, the second random variable also increases. The value close to -1 denotes a high linear relationship, and with an increase of one random variable, the second random variable decreases. The value close or equal to 0 denotes no relationship between the two random variables. Type: Float64. Example Query: SELECT rankCorr(number, number) FROM numbers(100); Result: ┌─rankCorr(number, number)─┐ │ 1 │ └──────────────────────────┘ Query: SELECT roundBankers(rankCorr(exp(number), sin(number)), 3) FROM numbers(100); Result: ┌─roundBankers(rankCorr(exp(number), sin(number)), 3)─┐ │ -0.037 │ └─────────────────────────────────────────────────────┘ See Also Spearman's rank correlation coefficient","keywords":""},{"title":"simpleLinearRegression","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/simplelinearregression","content":"simpleLinearRegression Performs simple (unidimensional) linear regression. simpleLinearRegression(x, y) Parameters: x — Column with dependent variable values.y — Column with explanatory variable values. Returned values: Constants (a, b) of the resulting line y = a*x + b. Examples SELECT arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [0, 1, 2, 3]) ┌─arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [0, 1, 2, 3])─┐ │ (1,0) │ └───────────────────────────────────────────────────────────────────┘ SELECT arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [3, 4, 5, 6]) ┌─arrayReduce('simpleLinearRegression', [0, 1, 2, 3], [3, 4, 5, 6])─┐ │ (1,3) │ └───────────────────────────────────────────────────────────────────┘ ","keywords":""},{"title":"quantiles Functions","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/quantiles","content":"","keywords":""},{"title":"quantiles​","type":1,"pageTitle":"quantiles Functions","url":"en/sql-reference/aggregate-functions/reference/quantiles#quantiles","content":"Syntax: quantiles(level1, level2, …)(x) All the quantile functions also have corresponding quantiles functions: quantiles, quantilesDeterministic, quantilesTiming, quantilesTimingWeighted, quantilesExact, quantilesExactWeighted, quantilesTDigest, quantilesBFloat16. These functions calculate all the quantiles of the listed levels in one pass, and return an array of the resulting values. "},{"title":"quantilesExactExclusive​","type":1,"pageTitle":"quantiles Functions","url":"en/sql-reference/aggregate-functions/reference/quantiles#quantilesexactexclusive","content":"Exactly computes the quantiles of a numeric data sequence. To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective. This function is equivalent to PERCENTILE.EXC Excel function, (type R6). Works more efficiently with sets of levels than quantileExactExclusive. Syntax quantilesExactExclusive(level1, level2, ...)(expr)  Arguments expr — Expression over the column values resulting in numeric data types, Date or DateTime. Parameters level — Levels of quantiles. Possible values: (0, 1) — bounds not included. Float. Returned value Array of quantiles of the specified levels. Type of array values: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: CREATE TABLE num AS numbers(1000); SELECT quantilesExactExclusive(0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999)(x) FROM (SELECT number AS x FROM num);  Result: ┌─quantilesExactExclusive(0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999)(x)─┐ │ [249.25,499.5,749.75,899.9,949.9499999999999,989.99,998.999] │ └─────────────────────────────────────────────────────────────────────┘  "},{"title":"quantilesExactInclusive​","type":1,"pageTitle":"quantiles Functions","url":"en/sql-reference/aggregate-functions/reference/quantiles#quantilesexactinclusive","content":"Exactly computes the quantiles of a numeric data sequence. To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective. This function is equivalent to PERCENTILE.INC Excel function, (type R7). Works more efficiently with sets of levels than quantileExactInclusive. Syntax quantilesExactInclusive(level1, level2, ...)(expr)  Arguments expr — Expression over the column values resulting in numeric data types, Date or DateTime. Parameters level — Levels of quantiles. Possible values: [0, 1] — bounds included. Float. Returned value Array of quantiles of the specified levels. Type of array values: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: CREATE TABLE num AS numbers(1000); SELECT quantilesExactInclusive(0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999)(x) FROM (SELECT number AS x FROM num);  Result: ┌─quantilesExactInclusive(0.25, 0.5, 0.75, 0.9, 0.95, 0.99, 0.999)(x)─┐ │ [249.75,499.5,749.25,899.1,949.05,989.01,998.001] │ └─────────────────────────────────────────────────────────────────────┘  "},{"title":"quantileTimingWeighted","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/quantiletimingweighted","content":"quantileTimingWeighted With the determined precision computes the quantile of a numeric data sequence according to the weight of each sequence member. The result is deterministic (it does not depend on the query processing order). The function is optimized for working with sequences which describe distributions like loading web pages times or backend response times. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileTimingWeighted(level)(expr, weight) Alias: medianTimingWeighted. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median. expr — Expression over a column values returning a Float*-type number. - If negative values are passed to the function, the behavior is undefined. - If the value is greater than 30,000 (a page loading time of more than 30 seconds), it is assumed to be 30,000. weight — Column with weights of sequence elements. Weight is a number of value occurrences. Accuracy The calculation is accurate if: Total number of values does not exceed 5670.Total number of values exceeds 5670, but the page loading time is less than 1024ms. Otherwise, the result of the calculation is rounded to the nearest multiple of 16 ms. note For calculating page loading time quantiles, this function is more effective and accurate than quantile. Returned value Quantile of the specified level. Type: Float32. note If no values are passed to the function (when using quantileTimingIf), NaN is returned. The purpose of this is to differentiate these cases from cases that result in zero. See ORDER BY clause for notes on sorting NaN values. Example Input table: ┌─response_time─┬─weight─┐ │ 68 │ 1 │ │ 104 │ 2 │ │ 112 │ 3 │ │ 126 │ 2 │ │ 138 │ 1 │ │ 162 │ 1 │ └───────────────┴────────┘ Query: SELECT quantileTimingWeighted(response_time, weight) FROM t Result: ┌─quantileTimingWeighted(response_time, weight)─┐ │ 112 │ └───────────────────────────────────────────────┘ quantilesTimingWeighted Same as quantileTimingWeighted, but accept multiple parameters with quantile levels and return an Array filled with many values of that quantiles. Example Input table: ┌─response_time─┬─weight─┐ │ 68 │ 1 │ │ 104 │ 2 │ │ 112 │ 3 │ │ 126 │ 2 │ │ 138 │ 1 │ │ 162 │ 1 │ └───────────────┴────────┘ Query: SELECT quantilesTimingWeighted(0,5, 0.99)(response_time, weight) FROM t Result: ┌─quantilesTimingWeighted(0.5, 0.99)(response_time, weight)─┐ │ [112,162] │ └───────────────────────────────────────────────────────────┘ See Also medianquantiles","keywords":""},{"title":"quantileBFloat16","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/quantilebfloat16","content":"quantileBFloat16 Computes an approximate quantile of a sample consisting of bfloat16 numbers. bfloat16 is a floating-point data type with 1 sign bit, 8 exponent bits and 7 fraction bits. The function converts input values to 32-bit floats and takes the most significant 16 bits. Then it calculates bfloat16 quantile value and converts the result to a 64-bit float by appending zero bits. The function is a fast quantile estimator with a relative error no more than 0.390625%. Syntax quantileBFloat16[(level)](expr) Alias: medianBFloat16 Arguments expr — Column with numeric data. Integer, Float. Parameters level — Level of quantile. Optional. Possible values are in the range from 0 to 1. Default value: 0.5. Float. Returned value Approximate quantile of the specified level. Type: Float64. Example Input table has an integer and a float columns: ┌─a─┬─────b─┐ │ 1 │ 1.001 │ │ 2 │ 1.002 │ │ 3 │ 1.003 │ │ 4 │ 1.004 │ └───┴───────┘ Query to calculate 0.75-quantile (third quartile): SELECT quantileBFloat16(0.75)(a), quantileBFloat16(0.75)(b) FROM example_table; Result: ┌─quantileBFloat16(0.75)(a)─┬─quantileBFloat16(0.75)(b)─┐ │ 3 │ 1 │ └───────────────────────────┴───────────────────────────┘ Note that all floating point values in the example are truncated to 1.0 when converting to bfloat16. quantileBFloat16Weighted Like quantileBFloat16 but takes into account the weight of each sequence member. See Also medianquantiles","keywords":""},{"title":"quantileExact Functions","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/quantileexact","content":"","keywords":""},{"title":"quantileExact​","type":1,"pageTitle":"quantileExact Functions","url":"en/sql-reference/aggregate-functions/reference/quantileexact#quantileexact","content":"Exactly computes the quantile of a numeric data sequence. To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileExact(level)(expr)  Alias: medianExact. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime. Returned value Quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: SELECT quantileExact(number) FROM numbers(10)  Result: ┌─quantileExact(number)─┐ │ 5 │ └───────────────────────┘  "},{"title":"quantileExactLow​","type":1,"pageTitle":"quantileExact Functions","url":"en/sql-reference/aggregate-functions/reference/quantileexact#quantileexactlow","content":"Similar to quantileExact, this computes the exact quantile of a numeric data sequence. To get the exact value, all the passed values are combined into an array, which is then fully sorted. The sorting algorithm's complexity is O(N·log(N)), where N = std::distance(first, last) comparisons. The return value depends on the quantile level and the number of elements in the selection, i.e. if the level is 0.5, then the function returns the lower median value for an even number of elements and the middle median value for an odd number of elements. Median is calculated similarly to the median_low implementation which is used in python. For all other levels, the element at the index corresponding to the value of level * size_of_array is returned. For example: SELECT quantileExactLow(0.1)(number) FROM numbers(10) ┌─quantileExactLow(0.1)(number)─┐ │ 1 │ └───────────────────────────────┘  When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileExactLow(level)(expr)  Alias: medianExactLow. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime. Returned value Quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: SELECT quantileExactLow(number) FROM numbers(10)  Result: ┌─quantileExactLow(number)─┐ │ 4 │ └──────────────────────────┘  "},{"title":"quantileExactHigh​","type":1,"pageTitle":"quantileExact Functions","url":"en/sql-reference/aggregate-functions/reference/quantileexact#quantileexacthigh","content":"Similar to quantileExact, this computes the exact quantile of a numeric data sequence. All the passed values are combined into an array, which is then fully sorted, to get the exact value. The sorting algorithm's complexity is O(N·log(N)), where N = std::distance(first, last) comparisons. The return value depends on the quantile level and the number of elements in the selection, i.e. if the level is 0.5, then the function returns the higher median value for an even number of elements and the middle median value for an odd number of elements. Median is calculated similarly to the median_high implementation which is used in python. For all other levels, the element at the index corresponding to the value of level * size_of_array is returned. This implementation behaves exactly similar to the current quantileExact implementation. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileExactHigh(level)(expr)  Alias: medianExactHigh. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime. Returned value Quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: SELECT quantileExactHigh(number) FROM numbers(10)  Result: ┌─quantileExactHigh(number)─┐ │ 5 │ └───────────────────────────┘  "},{"title":"quantileExactExclusive​","type":1,"pageTitle":"quantileExact Functions","url":"en/sql-reference/aggregate-functions/reference/quantileexact#quantileexactexclusive","content":"Exactly computes the quantile of a numeric data sequence. To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective. This function is equivalent to PERCENTILE.EXC Excel function, (type R6). When using multiple quantileExactExclusive functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantilesExactExclusive function. Syntax quantileExactExclusive(level)(expr)  Arguments expr — Expression over the column values resulting in numeric data types, Date or DateTime. Parameters level — Level of quantile. Optional. Possible values: (0, 1) — bounds not included. Default value: 0.5. At level=0.5 the function calculates median. Float. Returned value Quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: CREATE TABLE num AS numbers(1000); SELECT quantileExactExclusive(0.6)(x) FROM (SELECT number AS x FROM num);  Result: ┌─quantileExactExclusive(0.6)(x)─┐ │ 599.6 │ └────────────────────────────────┘  "},{"title":"quantileExactInclusive​","type":1,"pageTitle":"quantileExact Functions","url":"en/sql-reference/aggregate-functions/reference/quantileexact#quantileexactinclusive","content":"Exactly computes the quantile of a numeric data sequence. To get exact value, all the passed values ​​are combined into an array, which is then partially sorted. Therefore, the function consumes O(n) memory, where n is a number of values that were passed. However, for a small number of values, the function is very effective. This function is equivalent to PERCENTILE.INC Excel function, (type R7). When using multiple quantileExactInclusive functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantilesExactInclusive function. Syntax quantileExactInclusive(level)(expr)  Arguments expr — Expression over the column values resulting in numeric data types, Date or DateTime. Parameters level — Level of quantile. Optional. Possible values: [0, 1] — bounds included. Default value: 0.5. At level=0.5 the function calculates median. Float. Returned value Quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: CREATE TABLE num AS numbers(1000); SELECT quantileExactInclusive(0.6)(x) FROM (SELECT number AS x FROM num);  Result: ┌─quantileExactInclusive(0.6)(x)─┐ │ 599.4 │ └────────────────────────────────┘  See Also medianquantiles "},{"title":"Parametric Aggregate Functions","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/parametric-functions","content":"","keywords":""},{"title":"histogram​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"en/sql-reference/aggregate-functions/parametric-functions#histogram","content":"Calculates an adaptive histogram. It does not guarantee precise results. histogram(number_of_bins)(values)  The functions uses A Streaming Parallel Decision Tree Algorithm. The borders of histogram bins are adjusted as new data enters a function. In common case, the widths of bins are not equal. Arguments values — Expression resulting in input values. Parameters number_of_bins — Upper limit for the number of bins in the histogram. The function automatically calculates the number of bins. It tries to reach the specified number of bins, but if it fails, it uses fewer bins. Returned values Array of Tuples of the following format: ``` [(lower_1, upper_1, height_1), ... (lower_N, upper_N, height_N)] ``` - `lower` — Lower bound of the bin. - `upper` — Upper bound of the bin. - `height` — Calculated height of the bin.  Example SELECT histogram(5)(number + 1) FROM ( SELECT * FROM system.numbers LIMIT 20 )  ┌─histogram(5)(plus(number, 1))───────────────────────────────────────────┐ │ [(1,4.5,4),(4.5,8.5,4),(8.5,12.75,4.125),(12.75,17,4.625),(17,20,3.25)] │ └─────────────────────────────────────────────────────────────────────────┘  You can visualize a histogram with the bar function, for example: WITH histogram(5)(rand() % 100) AS hist SELECT arrayJoin(hist).3 AS height, bar(height, 0, 6, 5) AS bar FROM ( SELECT * FROM system.numbers LIMIT 20 )  ┌─height─┬─bar───┐ │ 2.125 │ █▋ │ │ 3.25 │ ██▌ │ │ 5.625 │ ████▏ │ │ 5.625 │ ████▏ │ │ 3.375 │ ██▌ │ └────────┴───────┘  In this case, you should remember that you do not know the histogram bin borders. "},{"title":"sequenceMatch(pattern)(timestamp, cond1, cond2, …)​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"en/sql-reference/aggregate-functions/parametric-functions#function-sequencematch","content":"Checks whether the sequence contains an event chain that matches the pattern. sequenceMatch(pattern)(timestamp, cond1, cond2, ...)  warning Events that occur at the same second may lay in the sequence in an undefined order affecting the result. Arguments timestamp — Column considered to contain time data. Typical data types are Date and DateTime. You can also use any of the supported UInt data types. cond1, cond2 — Conditions that describe the chain of events. Data type: UInt8. You can pass up to 32 condition arguments. The function takes only the events described in these conditions into account. If the sequence contains data that isn’t described in a condition, the function skips them. Parameters pattern — Pattern string. See Pattern syntax. Returned values 1, if the pattern is matched.0, if the pattern isn’t matched. Type: UInt8.  **Pattern syntax** (?N) — Matches the condition argument at position N. Conditions are numbered in the [1, 32] range. For example, (?1) matches the argument passed to the cond1 parameter. .* — Matches any number of events. You do not need conditional arguments to match this element of the pattern. (?t operator value) — Sets the time in seconds that should separate two events. For example, pattern (?1)(?t&gt;1800)(?2) matches events that occur more than 1800 seconds from each other. An arbitrary number of any events can lay between these events. You can use the &gt;=, &gt;, &lt;, &lt;=, == operators. Examples Consider data in the t table: ┌─time─┬─number─┐ │ 1 │ 1 │ │ 2 │ 3 │ │ 3 │ 2 │ └──────┴────────┘  Perform the query: SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2) FROM t  ┌─sequenceMatch('(?1)(?2)')(time, equals(number, 1), equals(number, 2))─┐ │ 1 │ └───────────────────────────────────────────────────────────────────────┘  The function found the event chain where number 2 follows number 1. It skipped number 3 between them, because the number is not described as an event. If we want to take this number into account when searching for the event chain given in the example, we should make a condition for it. SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2, number = 3) FROM t  ┌─sequenceMatch('(?1)(?2)')(time, equals(number, 1), equals(number, 2), equals(number, 3))─┐ │ 0 │ └──────────────────────────────────────────────────────────────────────────────────────────┘  In this case, the function couldn’t find the event chain matching the pattern, because the event for number 3 occurred between 1 and 2. If in the same case we checked the condition for number 4, the sequence would match the pattern. SELECT sequenceMatch('(?1)(?2)')(time, number = 1, number = 2, number = 4) FROM t  ┌─sequenceMatch('(?1)(?2)')(time, equals(number, 1), equals(number, 2), equals(number, 4))─┐ │ 1 │ └──────────────────────────────────────────────────────────────────────────────────────────┘  See Also sequenceCount "},{"title":"sequenceCount(pattern)(time, cond1, cond2, …)​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"en/sql-reference/aggregate-functions/parametric-functions#function-sequencecount","content":"Counts the number of event chains that matched the pattern. The function searches event chains that do not overlap. It starts to search for the next chain after the current chain is matched. warning Events that occur at the same second may lay in the sequence in an undefined order affecting the result. sequenceCount(pattern)(timestamp, cond1, cond2, ...)  Arguments timestamp — Column considered to contain time data. Typical data types are Date and DateTime. You can also use any of the supported UInt data types. cond1, cond2 — Conditions that describe the chain of events. Data type: UInt8. You can pass up to 32 condition arguments. The function takes only the events described in these conditions into account. If the sequence contains data that isn’t described in a condition, the function skips them. Parameters pattern — Pattern string. See Pattern syntax. Returned values Number of non-overlapping event chains that are matched. Type: UInt64. Example Consider data in the t table: ┌─time─┬─number─┐ │ 1 │ 1 │ │ 2 │ 3 │ │ 3 │ 2 │ │ 4 │ 1 │ │ 5 │ 3 │ │ 6 │ 2 │ └──────┴────────┘  Count how many times the number 2 occurs after the number 1 with any amount of other numbers between them: SELECT sequenceCount('(?1).*(?2)')(time, number = 1, number = 2) FROM t  ┌─sequenceCount('(?1).*(?2)')(time, equals(number, 1), equals(number, 2))─┐ │ 2 │ └─────────────────────────────────────────────────────────────────────────┘  See Also sequenceMatch "},{"title":"windowFunnel​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"en/sql-reference/aggregate-functions/parametric-functions#windowfunnel","content":"Searches for event chains in a sliding time window and calculates the maximum number of events that occurred from the chain. The function works according to the algorithm: The function searches for data that triggers the first condition in the chain and sets the event counter to 1. This is the moment when the sliding window starts. If events from the chain occur sequentially within the window, the counter is incremented. If the sequence of events is disrupted, the counter isn’t incremented. If the data has multiple event chains at varying points of completion, the function will only output the size of the longest chain. Syntax windowFunnel(window, [mode, [mode, ... ]])(timestamp, cond1, cond2, ..., condN)  Arguments timestamp — Name of the column containing the timestamp. Data types supported: Date, DateTime and other unsigned integer types (note that even though timestamp supports the UInt64 type, it’s value can’t exceed the Int64 maximum, which is 2^63 - 1).cond — Conditions or data describing the chain of events. UInt8. Parameters window — Length of the sliding window, it is the time interval between the first and the last condition. The unit of window depends on the timestamp itself and varies. Determined using the expression timestamp of cond1 &lt;= timestamp of cond2 &lt;= ... &lt;= timestamp of condN &lt;= timestamp of cond1 + window.mode — It is an optional argument. One or more modes can be set. 'strict_deduplication' — If the same condition holds for the sequence of events, then such repeating event interrupts further processing.'strict_order' — Don't allow interventions of other events. E.g. in the case of A-&gt;B-&gt;D-&gt;C, it stops finding A-&gt;B-&gt;C at the D and the max event level is 2.'strict_increase' — Apply conditions only to events with strictly increasing timestamps. Returned value The maximum number of consecutive triggered conditions from the chain within the sliding time window. All the chains in the selection are analyzed. Type: Integer. Example Determine if a set period of time is enough for the user to select a phone and purchase it twice in the online store. Set the following chain of events: The user logged in to their account on the store (eventID = 1003).The user searches for a phone (eventID = 1007, product = 'phone').The user placed an order (eventID = 1009).The user made the order again (eventID = 1010). Input table: ┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐ │ 2019-01-28 │ 1 │ 2019-01-29 10:00:00 │ 1003 │ phone │ └────────────┴─────────┴─────────────────────┴─────────┴─────────┘ ┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐ │ 2019-01-31 │ 1 │ 2019-01-31 09:00:00 │ 1007 │ phone │ └────────────┴─────────┴─────────────────────┴─────────┴─────────┘ ┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐ │ 2019-01-30 │ 1 │ 2019-01-30 08:00:00 │ 1009 │ phone │ └────────────┴─────────┴─────────────────────┴─────────┴─────────┘ ┌─event_date─┬─user_id─┬───────────timestamp─┬─eventID─┬─product─┐ │ 2019-02-01 │ 1 │ 2019-02-01 08:00:00 │ 1010 │ phone │ └────────────┴─────────┴─────────────────────┴─────────┴─────────┘  Find out how far the user user_id could get through the chain in a period in January-February of 2019. Query: SELECT level, count() AS c FROM ( SELECT user_id, windowFunnel(6048000000000000)(timestamp, eventID = 1003, eventID = 1009, eventID = 1007, eventID = 1010) AS level FROM trend WHERE (event_date &gt;= '2019-01-01') AND (event_date &lt;= '2019-02-02') GROUP BY user_id ) GROUP BY level ORDER BY level ASC;  Result: ┌─level─┬─c─┐ │ 4 │ 1 │ └───────┴───┘  "},{"title":"retention​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"en/sql-reference/aggregate-functions/parametric-functions#retention","content":"The function takes as arguments a set of conditions from 1 to 32 arguments of type UInt8 that indicate whether a certain condition was met for the event. Any condition can be specified as an argument (as in WHERE). The conditions, except the first, apply in pairs: the result of the second will be true if the first and second are true, of the third if the first and third are true, etc. Syntax retention(cond1, cond2, ..., cond32);  Arguments cond — An expression that returns a UInt8 result (1 or 0). Returned value The array of 1 or 0. 1 — Condition was met for the event.0 — Condition wasn’t met for the event. Type: UInt8. Example Let’s consider an example of calculating the retention function to determine site traffic. 1. Сreate a table to illustrate an example. CREATE TABLE retention_test(date Date, uid Int32) ENGINE = Memory; INSERT INTO retention_test SELECT '2020-01-01', number FROM numbers(5); INSERT INTO retention_test SELECT '2020-01-02', number FROM numbers(10); INSERT INTO retention_test SELECT '2020-01-03', number FROM numbers(15);  Input table: Query: SELECT * FROM retention_test  Result: ┌───────date─┬─uid─┐ │ 2020-01-01 │ 0 │ │ 2020-01-01 │ 1 │ │ 2020-01-01 │ 2 │ │ 2020-01-01 │ 3 │ │ 2020-01-01 │ 4 │ └────────────┴─────┘ ┌───────date─┬─uid─┐ │ 2020-01-02 │ 0 │ │ 2020-01-02 │ 1 │ │ 2020-01-02 │ 2 │ │ 2020-01-02 │ 3 │ │ 2020-01-02 │ 4 │ │ 2020-01-02 │ 5 │ │ 2020-01-02 │ 6 │ │ 2020-01-02 │ 7 │ │ 2020-01-02 │ 8 │ │ 2020-01-02 │ 9 │ └────────────┴─────┘ ┌───────date─┬─uid─┐ │ 2020-01-03 │ 0 │ │ 2020-01-03 │ 1 │ │ 2020-01-03 │ 2 │ │ 2020-01-03 │ 3 │ │ 2020-01-03 │ 4 │ │ 2020-01-03 │ 5 │ │ 2020-01-03 │ 6 │ │ 2020-01-03 │ 7 │ │ 2020-01-03 │ 8 │ │ 2020-01-03 │ 9 │ │ 2020-01-03 │ 10 │ │ 2020-01-03 │ 11 │ │ 2020-01-03 │ 12 │ │ 2020-01-03 │ 13 │ │ 2020-01-03 │ 14 │ └────────────┴─────┘  2. Group users by unique ID uid using the retention function. Query: SELECT uid, retention(date = '2020-01-01', date = '2020-01-02', date = '2020-01-03') AS r FROM retention_test WHERE date IN ('2020-01-01', '2020-01-02', '2020-01-03') GROUP BY uid ORDER BY uid ASC  Result: ┌─uid─┬─r───────┐ │ 0 │ [1,1,1] │ │ 1 │ [1,1,1] │ │ 2 │ [1,1,1] │ │ 3 │ [1,1,1] │ │ 4 │ [1,1,1] │ │ 5 │ [0,0,0] │ │ 6 │ [0,0,0] │ │ 7 │ [0,0,0] │ │ 8 │ [0,0,0] │ │ 9 │ [0,0,0] │ │ 10 │ [0,0,0] │ │ 11 │ [0,0,0] │ │ 12 │ [0,0,0] │ │ 13 │ [0,0,0] │ │ 14 │ [0,0,0] │ └─────┴─────────┘  3. Calculate the total number of site visits per day. Query: SELECT sum(r[1]) AS r1, sum(r[2]) AS r2, sum(r[3]) AS r3 FROM ( SELECT uid, retention(date = '2020-01-01', date = '2020-01-02', date = '2020-01-03') AS r FROM retention_test WHERE date IN ('2020-01-01', '2020-01-02', '2020-01-03') GROUP BY uid )  Result: ┌─r1─┬─r2─┬─r3─┐ │ 5 │ 5 │ 5 │ └────┴────┴────┘  Where: r1- the number of unique visitors who visited the site during 2020-01-01 (the cond1 condition).r2- the number of unique visitors who visited the site during a specific time period between 2020-01-01 and 2020-01-02 (cond1 and cond2 conditions).r3- the number of unique visitors who visited the site during a specific time period between 2020-01-01 and 2020-01-03 (cond1 and cond3 conditions). "},{"title":"uniqUpTo(N)(x)​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"en/sql-reference/aggregate-functions/parametric-functions#uniquptonx","content":"Calculates the number of different argument values ​​if it is less than or equal to N. If the number of different argument values is greater than N, it returns N + 1. Recommended for use with small Ns, up to 10. The maximum value of N is 100. For the state of an aggregate function, it uses the amount of memory equal to 1 + N * the size of one value of bytes. For strings, it stores a non-cryptographic hash of 8 bytes. That is, the calculation is approximated for strings. The function also works for several arguments. It works as fast as possible, except for cases when a large N value is used and the number of unique values is slightly less than N. Usage example: Problem: Generate a report that shows only keywords that produced at least 5 unique users. Solution: Write in the GROUP BY query SearchPhrase HAVING uniqUpTo(4)(UserID) &gt;= 5  "},{"title":"sumMapFiltered(keys_to_keep)(keys, values)​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"en/sql-reference/aggregate-functions/parametric-functions#summapfilteredkeys-to-keepkeys-values","content":"Same behavior as sumMap except that an array of keys is passed as a parameter. This can be especially useful when working with a high cardinality of keys. "},{"title":"sequenceNextNode​","type":1,"pageTitle":"Parametric Aggregate Functions","url":"en/sql-reference/aggregate-functions/parametric-functions#sequenceNextNode","content":"Returns a value of the next event that matched an event chain. Experimental function, SET allow_experimental_funnel_functions = 1 to enable it. Syntax sequenceNextNode(direction, base)(timestamp, event_column, base_condition, event1, event2, event3, ...)  Parameters direction — Used to navigate to directions. forward — Moving forward.backward — Moving backward. base — Used to set the base point. head — Set the base point to the first event.tail — Set the base point to the last event.first_match — Set the base point to the first matched event1.last_match — Set the base point to the last matched event1. Arguments timestamp — Name of the column containing the timestamp. Data types supported: Date, DateTime and other unsigned integer types.event_column — Name of the column containing the value of the next event to be returned. Data types supported: String and Nullable(String).base_condition — Condition that the base point must fulfill.event1, event2, ... — Conditions describing the chain of events. UInt8. Returned values event_column[next_index] — If the pattern is matched and next value exists.NULL - If the pattern isn’t matched or next value doesn't exist. Type: Nullable(String). Example It can be used when events are A-&gt;B-&gt;C-&gt;D-&gt;E and you want to know the event following B-&gt;C, which is D. The query statement searching the event following A-&gt;B: CREATE TABLE test_flow ( dt DateTime, id int, page String) ENGINE = MergeTree() PARTITION BY toYYYYMMDD(dt) ORDER BY id; INSERT INTO test_flow VALUES (1, 1, 'A') (2, 1, 'B') (3, 1, 'C') (4, 1, 'D') (5, 1, 'E'); SELECT id, sequenceNextNode('forward', 'head')(dt, page, page = 'A', page = 'A', page = 'B') as next_flow FROM test_flow GROUP BY id;  Result: ┌─id─┬─next_flow─┐ │ 1 │ C │ └────┴───────────┘  Behavior for forward and head ALTER TABLE test_flow DELETE WHERE 1 = 1 settings mutations_sync = 1; INSERT INTO test_flow VALUES (1, 1, 'Home') (2, 1, 'Gift') (3, 1, 'Exit'); INSERT INTO test_flow VALUES (1, 2, 'Home') (2, 2, 'Home') (3, 2, 'Gift') (4, 2, 'Basket'); INSERT INTO test_flow VALUES (1, 3, 'Gift') (2, 3, 'Home') (3, 3, 'Gift') (4, 3, 'Basket');  SELECT id, sequenceNextNode('forward', 'head')(dt, page, page = 'Home', page = 'Home', page = 'Gift') FROM test_flow GROUP BY id; dt id page 1970-01-01 09:00:01 1 Home // Base point, Matched with Home 1970-01-01 09:00:02 1 Gift // Matched with Gift 1970-01-01 09:00:03 1 Exit // The result 1970-01-01 09:00:01 2 Home // Base point, Matched with Home 1970-01-01 09:00:02 2 Home // Unmatched with Gift 1970-01-01 09:00:03 2 Gift 1970-01-01 09:00:04 2 Basket 1970-01-01 09:00:01 3 Gift // Base point, Unmatched with Home 1970-01-01 09:00:02 3 Home 1970-01-01 09:00:03 3 Gift 1970-01-01 09:00:04 3 Basket  Behavior for backward and tail SELECT id, sequenceNextNode('backward', 'tail')(dt, page, page = 'Basket', page = 'Basket', page = 'Gift') FROM test_flow GROUP BY id; dt id page 1970-01-01 09:00:01 1 Home 1970-01-01 09:00:02 1 Gift 1970-01-01 09:00:03 1 Exit // Base point, Unmatched with Basket 1970-01-01 09:00:01 2 Home 1970-01-01 09:00:02 2 Home // The result 1970-01-01 09:00:03 2 Gift // Matched with Gift 1970-01-01 09:00:04 2 Basket // Base point, Matched with Basket 1970-01-01 09:00:01 3 Gift 1970-01-01 09:00:02 3 Home // The result 1970-01-01 09:00:03 3 Gift // Base point, Matched with Gift 1970-01-01 09:00:04 3 Basket // Base point, Matched with Basket  Behavior for forward and first_match SELECT id, sequenceNextNode('forward', 'first_match')(dt, page, page = 'Gift', page = 'Gift') FROM test_flow GROUP BY id; dt id page 1970-01-01 09:00:01 1 Home 1970-01-01 09:00:02 1 Gift // Base point 1970-01-01 09:00:03 1 Exit // The result 1970-01-01 09:00:01 2 Home 1970-01-01 09:00:02 2 Home 1970-01-01 09:00:03 2 Gift // Base point 1970-01-01 09:00:04 2 Basket The result 1970-01-01 09:00:01 3 Gift // Base point 1970-01-01 09:00:02 3 Home // The result 1970-01-01 09:00:03 3 Gift 1970-01-01 09:00:04 3 Basket  SELECT id, sequenceNextNode('forward', 'first_match')(dt, page, page = 'Gift', page = 'Gift', page = 'Home') FROM test_flow GROUP BY id; dt id page 1970-01-01 09:00:01 1 Home 1970-01-01 09:00:02 1 Gift // Base point 1970-01-01 09:00:03 1 Exit // Unmatched with Home 1970-01-01 09:00:01 2 Home 1970-01-01 09:00:02 2 Home 1970-01-01 09:00:03 2 Gift // Base point 1970-01-01 09:00:04 2 Basket // Unmatched with Home 1970-01-01 09:00:01 3 Gift // Base point 1970-01-01 09:00:02 3 Home // Matched with Home 1970-01-01 09:00:03 3 Gift // The result 1970-01-01 09:00:04 3 Basket  Behavior for backward and last_match SELECT id, sequenceNextNode('backward', 'last_match')(dt, page, page = 'Gift', page = 'Gift') FROM test_flow GROUP BY id; dt id page 1970-01-01 09:00:01 1 Home // The result 1970-01-01 09:00:02 1 Gift // Base point 1970-01-01 09:00:03 1 Exit 1970-01-01 09:00:01 2 Home 1970-01-01 09:00:02 2 Home // The result 1970-01-01 09:00:03 2 Gift // Base point 1970-01-01 09:00:04 2 Basket 1970-01-01 09:00:01 3 Gift 1970-01-01 09:00:02 3 Home // The result 1970-01-01 09:00:03 3 Gift // Base point 1970-01-01 09:00:04 3 Basket  SELECT id, sequenceNextNode('backward', 'last_match')(dt, page, page = 'Gift', page = 'Gift', page = 'Home') FROM test_flow GROUP BY id; dt id page 1970-01-01 09:00:01 1 Home // Matched with Home, the result is null 1970-01-01 09:00:02 1 Gift // Base point 1970-01-01 09:00:03 1 Exit 1970-01-01 09:00:01 2 Home // The result 1970-01-01 09:00:02 2 Home // Matched with Home 1970-01-01 09:00:03 2 Gift // Base point 1970-01-01 09:00:04 2 Basket 1970-01-01 09:00:01 3 Gift // The result 1970-01-01 09:00:02 3 Home // Matched with Home 1970-01-01 09:00:03 3 Gift // Base point 1970-01-01 09:00:04 3 Basket  Behavior for base_condition CREATE TABLE test_flow_basecond ( `dt` DateTime, `id` int, `page` String, `ref` String ) ENGINE = MergeTree PARTITION BY toYYYYMMDD(dt) ORDER BY id; INSERT INTO test_flow_basecond VALUES (1, 1, 'A', 'ref4') (2, 1, 'A', 'ref3') (3, 1, 'B', 'ref2') (4, 1, 'B', 'ref1');  SELECT id, sequenceNextNode('forward', 'head')(dt, page, ref = 'ref1', page = 'A') FROM test_flow_basecond GROUP BY id; dt id page ref 1970-01-01 09:00:01 1 A ref4 // The head can not be base point because the ref column of the head unmatched with 'ref1'. 1970-01-01 09:00:02 1 A ref3 1970-01-01 09:00:03 1 B ref2 1970-01-01 09:00:04 1 B ref1  SELECT id, sequenceNextNode('backward', 'tail')(dt, page, ref = 'ref4', page = 'B') FROM test_flow_basecond GROUP BY id; dt id page ref 1970-01-01 09:00:01 1 A ref4 1970-01-01 09:00:02 1 A ref3 1970-01-01 09:00:03 1 B ref2 1970-01-01 09:00:04 1 B ref1 // The tail can not be base point because the ref column of the tail unmatched with 'ref4'.  SELECT id, sequenceNextNode('forward', 'first_match')(dt, page, ref = 'ref3', page = 'A') FROM test_flow_basecond GROUP BY id; dt id page ref 1970-01-01 09:00:01 1 A ref4 // This row can not be base point because the ref column unmatched with 'ref3'. 1970-01-01 09:00:02 1 A ref3 // Base point 1970-01-01 09:00:03 1 B ref2 // The result 1970-01-01 09:00:04 1 B ref1  SELECT id, sequenceNextNode('backward', 'last_match')(dt, page, ref = 'ref2', page = 'B') FROM test_flow_basecond GROUP BY id; dt id page ref 1970-01-01 09:00:01 1 A ref4 1970-01-01 09:00:02 1 A ref3 // The result 1970-01-01 09:00:03 1 B ref2 // Base point 1970-01-01 09:00:04 1 B ref1 // This row can not be base point because the ref column unmatched with 'ref2'.  "},{"title":"skewPop","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/skewpop","content":"skewPop Computes the skewness of a sequence. skewPop(expr) Arguments expr — Expression returning a number. Returned value The skewness of the given distribution. Type — Float64 Example SELECT skewPop(value) FROM series_with_value_column; ","keywords":""},{"title":"skewSamp","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/skewsamp","content":"skewSamp Computes the sample skewness of a sequence. It represents an unbiased estimate of the skewness of a random variable if passed values form its sample. skewSamp(expr) Arguments expr — Expression returning a number. Returned value The skewness of the given distribution. Type — Float64. If n &lt;= 1 (n is the size of the sample), then the function returns nan. Example SELECT skewSamp(value) FROM series_with_value_column; ","keywords":""},{"title":"sparkbar","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/sparkbar","content":"sparkbar sparkbar The function plots a frequency histogram for values x and the repetition rate y of these values over the interval [min_x, max_x]. If no interval is specified, then the minimum x is used as the interval start, and the maximum x — as the interval end. Syntax sparkbar(width[, min_x, max_x])(x, y) Parameters width — The number of segments. Type: Integer.min_x — The interval start. Optional parameter.max_x — The interval end. Optional parameter. Arguments x — The field with values.y — The field with the frequency of values. Returned value The frequency histogram. Example Query: CREATE TABLE spark_bar_data (`cnt` UInt64,`event_date` Date) ENGINE = MergeTree ORDER BY event_date SETTINGS index_granularity = 8192; INSERT INTO spark_bar_data VALUES(1,'2020-01-01'),(4,'2020-01-02'),(5,'2020-01-03'),(2,'2020-01-04'),(3,'2020-01-05'),(7,'2020-01-06'),(6,'2020-01-07'),(8,'2020-01-08'),(2,'2020-01-11'); SELECT sparkbar(9)(event_date,cnt) FROM spark_bar_data; SELECT sparkbar(9,toDate('2020-01-01'),toDate('2020-01-10'))(event_date,cnt) FROM spark_bar_data; Result: ┌─sparkbar(9)(event_date, cnt)─┐ │ │ │ ▁▅▄▃██▅ ▁ │ │ │ └──────────────────────────────┘ ┌─sparkbar(9, toDate('2020-01-01'), toDate('2020-01-10'))(event_date, cnt)─┐ │ │ │▁▄▄▂▅▇█▁ │ │ │ └──────────────────────────────────────────────────────────────────────────┘ ","keywords":""},{"title":"quantileTiming","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/quantiletiming","content":"quantileTiming With the determined precision computes the quantile of a numeric data sequence. The result is deterministic (it does not depend on the query processing order). The function is optimized for working with sequences which describe distributions like loading web pages times or backend response times. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileTiming(level)(expr) Alias: medianTiming. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median. expr — Expression over a column values returning a Float*-type number. If negative values are passed to the function, the behavior is undefined.If the value is greater than 30,000 (a page loading time of more than 30 seconds), it is assumed to be 30,000. Accuracy The calculation is accurate if: Total number of values does not exceed 5670.Total number of values exceeds 5670, but the page loading time is less than 1024ms. Otherwise, the result of the calculation is rounded to the nearest multiple of 16 ms. note For calculating page loading time quantiles, this function is more effective and accurate than quantile. Returned value Quantile of the specified level. Type: Float32. note If no values are passed to the function (when using quantileTimingIf), NaN is returned. The purpose of this is to differentiate these cases from cases that result in zero. See ORDER BY clause for notes on sorting NaN values. Example Input table: ┌─response_time─┐ │ 72 │ │ 112 │ │ 126 │ │ 145 │ │ 104 │ │ 242 │ │ 313 │ │ 168 │ │ 108 │ └───────────────┘ Query: SELECT quantileTiming(response_time) FROM t Result: ┌─quantileTiming(response_time)─┐ │ 126 │ └───────────────────────────────┘ See Also medianquantiles","keywords":""},{"title":"stddevPop","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/stddevpop","content":"stddevPop The result is equal to the square root of varPop. note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the stddevPopStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"quantileTDigest","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/quantiletdigest","content":"quantileTDigest Computes an approximate quantile of a numeric data sequence using the t-digest algorithm. Memory consumption is log(n), where n is a number of values. The result depends on the order of running the query, and is nondeterministic. The performance of the function is lower than performance of quantile or quantileTiming. In terms of the ratio of State size to precision, this function is much better than quantile. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. Syntax quantileTDigest(level)(expr) Alias: medianTDigest. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime. Returned value Approximate quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: SELECT quantileTDigest(number) FROM numbers(10) Result: ┌─quantileTDigest(number)─┐ │ 4.5 │ └─────────────────────────┘ See Also medianquantiles","keywords":""},{"title":"stddevSamp","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/stddevsamp","content":"stddevSamp The result is equal to the square root of varSamp. note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the stddevSampStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"stochasticLinearRegression","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/stochasticlinearregression","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"stochasticLinearRegression","url":"en/sql-reference/aggregate-functions/reference/stochasticlinearregression#agg_functions-stochasticlinearregression-parameters","content":"There are 4 customizable parameters. They are passed to the function sequentially, but there is no need to pass all four - default values will be used, however good model required some parameter tuning. stochasticLinearRegression(1.0, 1.0, 10, 'SGD')  learning rate is the coefficient on step length, when gradient descent step is performed. Too big learning rate may cause infinite weights of the model. Default is 0.00001.l2 regularization coefficient which may help to prevent overfitting. Default is 0.1.mini-batch size sets the number of elements, which gradients will be computed and summed to perform one step of gradient descent. Pure stochastic descent uses one element, however having small batches(about 10 elements) make gradient steps more stable. Default is 15.method for updating weights, they are: Adam (by default), SGD, Momentum, Nesterov. Momentum and Nesterov require little bit more computations and memory, however they happen to be useful in terms of speed of convergance and stability of stochastic gradient methods. "},{"title":"Usage​","type":1,"pageTitle":"stochasticLinearRegression","url":"en/sql-reference/aggregate-functions/reference/stochasticlinearregression#agg_functions-stochasticlinearregression-usage","content":"stochasticLinearRegression is used in two steps: fitting the model and predicting on new data. In order to fit the model and save its state for later usage we use -State combinator, which basically saves the state (model weights, etc). To predict we use function evalMLMethod, which takes a state as an argument as well as features to predict on.  1. Fitting Such query may be used. CREATE TABLE IF NOT EXISTS train_data ( param1 Float64, param2 Float64, target Float64 ) ENGINE = Memory; CREATE TABLE your_model ENGINE = Memory AS SELECT stochasticLinearRegressionState(0.1, 0.0, 5, 'SGD')(target, param1, param2) AS state FROM train_data;  Here we also need to insert data into train_data table. The number of parameters is not fixed, it depends only on number of arguments, passed into linearRegressionState. They all must be numeric values. Note that the column with target value(which we would like to learn to predict) is inserted as the first argument. 2. Predicting After saving a state into the table, we may use it multiple times for prediction, or even merge with other states and create new even better models. WITH (SELECT state FROM your_model) AS model SELECT evalMLMethod(model, param1, param2) FROM test_data  The query will return a column of predicted values. Note that first argument of evalMLMethod is AggregateFunctionState object, next are columns of features. test_data is a table like train_data but may not contain target value. "},{"title":"Notes​","type":1,"pageTitle":"stochasticLinearRegression","url":"en/sql-reference/aggregate-functions/reference/stochasticlinearregression#agg_functions-stochasticlinearregression-notes","content":"To merge two models user may create such query:sql SELECT state1 + state2 FROM your_modelswhere your_models table contains both models. This query will return new AggregateFunctionState object. User may fetch weights of the created model for its own purposes without saving the model if no -State combinator is used.sql SELECT stochasticLinearRegression(0.01)(target, param1, param2) FROM train_dataSuch query will fit the model and return its weights - first are weights, which correspond to the parameters of the model, the last one is bias. So in the example above the query will return a column with 3 values. See Also stochasticLogisticRegressionDifference between linear and logistic regressions "},{"title":"quantileTDigestWeighted","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/quantiletdigestweighted","content":"quantileTDigestWeighted Computes an approximate quantile of a numeric data sequence using the t-digest algorithm. The function takes into account the weight of each sequence member. The maximum error is 1%. Memory consumption is log(n), where n is a number of values. The performance of the function is lower than performance of quantile or quantileTiming. In terms of the ratio of State size to precision, this function is much better than quantile. The result depends on the order of running the query, and is nondeterministic. When using multiple quantile* functions with different levels in a query, the internal states are not combined (that is, the query works less efficiently than it could). In this case, use the quantiles function. note Using quantileTDigestWeighted is not recommended for tiny data sets and can lead to significat error. In this case, consider possibility of using quantileTDigest instead. Syntax quantileTDigestWeighted(level)(expr, weight) Alias: medianTDigestWeighted. Arguments level — Level of quantile. Optional parameter. Constant floating-point number from 0 to 1. We recommend using a level value in the range of [0.01, 0.99]. Default value: 0.5. At level=0.5 the function calculates median.expr — Expression over the column values resulting in numeric data types, Date or DateTime.weight — Column with weights of sequence elements. Weight is a number of value occurrences. Returned value Approximate quantile of the specified level. Type: Float64 for numeric data type input.Date if input values have the Date type.DateTime if input values have the DateTime type. Example Query: SELECT quantileTDigestWeighted(number, 1) FROM numbers(10) Result: ┌─quantileTDigestWeighted(number, 1)─┐ │ 4.5 │ └────────────────────────────────────┘ See Also medianquantiles","keywords":""},{"title":"stochasticLogisticRegression","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/stochasticlogisticregression","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"stochasticLogisticRegression","url":"en/sql-reference/aggregate-functions/reference/stochasticlogisticregression#agg_functions-stochasticlogisticregression-parameters","content":"Parameters are exactly the same as in stochasticLinearRegression:learning rate, l2 regularization coefficient, mini-batch size, method for updating weights. For more information see parameters. stochasticLogisticRegression(1.0, 1.0, 10, 'SGD')  1. Fitting See the `Fitting` section in the [stochasticLinearRegression](#stochasticlinearregression-usage-fitting) description. Predicted labels have to be in \\[-1, 1\\].  2. Predicting Using saved state we can predict probability of object having label `1`. ``` sql WITH (SELECT state FROM your_model) AS model SELECT evalMLMethod(model, param1, param2) FROM test_data ``` The query will return a column of probabilities. Note that first argument of `evalMLMethod` is `AggregateFunctionState` object, next are columns of features. We can also set a bound of probability, which assigns elements to different labels. ``` sql SELECT ans &lt; 1.1 AND ans &gt; 0.5 FROM (WITH (SELECT state FROM your_model) AS model SELECT evalMLMethod(model, param1, param2) AS ans FROM test_data) ``` Then the result will be labels. `test_data` is a table like `train_data` but may not contain target value.  See Also stochasticLinearRegressionDifference between linear and logistic regressions. "},{"title":"studentTTest","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/studentttest","content":"studentTTest Applies Student's t-test to samples from two populations. Syntax studentTTest([confidence_level])(sample_data, sample_index) Values of both samples are in the sample_data column. If sample_index equals to 0 then the value in that row belongs to the sample from the first population. Otherwise it belongs to the sample from the second population. The null hypothesis is that means of populations are equal. Normal distribution with equal variances is assumed. Arguments sample_data — Sample data. Integer, Float or Decimal.sample_index — Sample index. Integer. Parameters confidence_level — Confidence level in order to calculate confidence intervals. Float. Returned values Tuple with two or four elements (if the optional confidence_level is specified): calculated t-statistic. Float64.calculated p-value. Float64.[calculated confidence-interval-low.][Float64](/docs/en/sql-reference/data-types/float).[calculated confidence-interval-high.][Float64](/docs/en/sql-reference/data-types/float). Example Input table: ┌─sample_data─┬─sample_index─┐ │ 20.3 │ 0 │ │ 21.1 │ 0 │ │ 21.9 │ 1 │ │ 21.7 │ 0 │ │ 19.9 │ 1 │ │ 21.8 │ 1 │ └─────────────┴──────────────┘ Query: SELECT studentTTest(sample_data, sample_index) FROM student_ttest; Result: ┌─studentTTest(sample_data, sample_index)───┐ │ (-0.21739130434783777,0.8385421208415731) │ └───────────────────────────────────────────┘ See Also Student's t-testwelchTTest function Original article","keywords":""},{"title":"sum","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/sum","content":"sum Calculates the sum. Only works for numbers.","keywords":""},{"title":"sumCount","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/sumcount","content":"sumCount Calculates the sum of the numbers and counts the number of rows at the same time. The function is used by ClickHouse query optimizer: if there are multiple sum, count or avg functions in a query, they can be replaced to single sumCount function to reuse the calculations. The function is rarely needed to use explicitly. Syntax sumCount(x) Arguments x — Input value, must be Integer, Float, or Decimal. Returned value Tuple (sum, count), where sum is the sum of numbers and count is the number of rows with not-NULL values. Type: Tuple. Example Query: CREATE TABLE s_table (x Int8) Engine = Log; INSERT INTO s_table SELECT number FROM numbers(0, 20); INSERT INTO s_table VALUES (NULL); SELECT sumCount(x) from s_table; Result: ┌─sumCount(x)─┐ │ (190,20) │ └─────────────┘ See also optimize_syntax_fuse_functions setting.","keywords":""},{"title":"sumKahan","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/sumkahan","content":"sumKahan Calculates the sum of the numbers with Kahan compensated summation algorithmSlower than sum function. The compensation works only for Float types. Syntax sumKahan(x) Arguments x — Input value, must be Integer, Float, or Decimal. Returned value the sum of numbers, with type Integer, Float, or Decimal depends on type of input arguments Example Query: SELECT sum(0.1), sumKahan(0.1) FROM numbers(10); Result: ┌───────────sum(0.1)─┬─sumKahan(0.1)─┐ │ 0.9999999999999999 │ 1 │ └────────────────────┴───────────────┘ ","keywords":""},{"title":"sumMap","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/summap","content":"sumMap Syntax: sumMap(key, value) or sumMap(Tuple(key, value)) Totals the value array according to the keys specified in the key array. Passing tuple of keys and values arrays is a synonym to passing two arrays of keys and values. The number of elements in key and value must be the same for each row that is totaled. Returns a tuple of two arrays: keys in sorted order, and values ​​summed for the corresponding keys. Example: CREATE TABLE sum_map( date Date, timeslot DateTime, statusMap Nested( status UInt16, requests UInt64 ), statusMapTuple Tuple(Array(Int32), Array(Int32)) ) ENGINE = Log; INSERT INTO sum_map VALUES ('2000-01-01', '2000-01-01 00:00:00', [1, 2, 3], [10, 10, 10], ([1, 2, 3], [10, 10, 10])), ('2000-01-01', '2000-01-01 00:00:00', [3, 4, 5], [10, 10, 10], ([3, 4, 5], [10, 10, 10])), ('2000-01-01', '2000-01-01 00:01:00', [4, 5, 6], [10, 10, 10], ([4, 5, 6], [10, 10, 10])), ('2000-01-01', '2000-01-01 00:01:00', [6, 7, 8], [10, 10, 10], ([6, 7, 8], [10, 10, 10])); SELECT timeslot, sumMap(statusMap.status, statusMap.requests), sumMap(statusMapTuple) FROM sum_map GROUP BY timeslot ┌────────────timeslot─┬─sumMap(statusMap.status, statusMap.requests)─┬─sumMap(statusMapTuple)─────────┐ │ 2000-01-01 00:00:00 │ ([1,2,3,4,5],[10,10,20,10,10]) │ ([1,2,3,4,5],[10,10,20,10,10]) │ │ 2000-01-01 00:01:00 │ ([4,5,6,7,8],[10,10,20,10,10]) │ ([4,5,6,7,8],[10,10,20,10,10]) │ └─────────────────────┴──────────────────────────────────────────────┴────────────────────────────────┘ ","keywords":""},{"title":"sumWithOverflow","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/sumwithoverflow","content":"sumWithOverflow Computes the sum of the numbers, using the same data type for the result as for the input parameters. If the sum exceeds the maximum value for this data type, it is calculated with overflow. Only works for numbers.","keywords":""},{"title":"topK","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/topk","content":"topK Returns an array of the approximately most frequent values in the specified column. The resulting array is sorted in descending order of approximate frequency of values (not by the values themselves). Implements the Filtered Space-Saving algorithm for analyzing TopK, based on the reduce-and-combine algorithm from Parallel Space Saving. topK(N)(column) This function does not provide a guaranteed result. In certain situations, errors might occur and it might return frequent values that aren’t the most frequent values. We recommend using the N &lt; 10 value; performance is reduced with large N values. Maximum value of N = 65536. Arguments N – The number of elements to return. If the parameter is omitted, default value 10 is used. Arguments x – The value to calculate frequency. Example Take the OnTime data set and select the three most frequently occurring values in the AirlineID column. SELECT topK(3)(AirlineID) AS res FROM ontime ┌─res─────────────────┐ │ [19393,19790,19805] │ └─────────────────────┘ ","keywords":""},{"title":"topKWeighted","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/topkweighted","content":"topKWeighted Returns an array of the approximately most frequent values in the specified column. The resulting array is sorted in descending order of approximate frequency of values (not by the values themselves). Additionally, the weight of the value is taken into account. Syntax topKWeighted(N)(x, weight) Arguments N — The number of elements to return.x — The value.weight — The weight. Every value is accounted weight times for frequency calculation. UInt64. Returned value Returns an array of the values with maximum approximate sum of weights. Example Query: SELECT topKWeighted(10)(number, number) FROM numbers(1000) Result: ┌─topKWeighted(10)(number, number)──────────┐ │ [999,998,997,996,995,994,993,992,991,990] │ └───────────────────────────────────────────┘ See Also topK","keywords":""},{"title":"uniq","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/uniq","content":"uniq Calculates the approximate number of different values of the argument. uniq(x[, ...]) Arguments The function takes a variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types. Returned value A UInt64-type number. Implementation details Function: Calculates a hash for all parameters in the aggregate, then uses it in calculations. Uses an adaptive sampling algorithm. For the calculation state, the function uses a sample of element hash values up to 65536. This algorithm is very accurate and very efficient on the CPU. When the query contains several of these functions, using uniq is almost as fast as using other aggregate functions. Provides the result deterministically (it does not depend on the query processing order). We recommend using this function in almost all scenarios. See Also uniqCombineduniqCombined64uniqHLL12uniqExactuniqTheta","keywords":""},{"title":"uniqCombined64","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/uniqcombined64","content":"uniqCombined64 Same as uniqCombined, but uses 64-bit hash for all data types.","keywords":""},{"title":"uniqCombined","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/uniqcombined","content":"uniqCombined Calculates the approximate number of different argument values. uniqCombined(HLL_precision)(x[, ...]) The uniqCombined function is a good choice for calculating the number of different values. Arguments The function takes a variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types. HLL_precision is the base-2 logarithm of the number of cells in HyperLogLog. Optional, you can use the function as uniqCombined(x[, ...]). The default value for HLL_precision is 17, which is effectively 96 KiB of space (2^17 cells, 6 bits each). Returned value A number UInt64-type number. Implementation details Function: Calculates a hash (64-bit hash for String and 32-bit otherwise) for all parameters in the aggregate, then uses it in calculations. Uses a combination of three algorithms: array, hash table, and HyperLogLog with an error correction table. For a small number of distinct elements, an array is used. When the set size is larger, a hash table is used. For a larger number of elements, HyperLogLog is used, which will occupy a fixed amount of memory. Provides the result deterministically (it does not depend on the query processing order). note Since it uses 32-bit hash for non-String type, the result will have very high error for cardinalities significantly larger than UINT_MAX (error will raise quickly after a few tens of billions of distinct values), hence in this case you should use uniqCombined64 Compared to the uniq function, the uniqCombined: Consumes several times less memory.Calculates with several times higher accuracy.Usually has slightly lower performance. In some scenarios, uniqCombined can perform better than uniq, for example, with distributed queries that transmit a large number of aggregation states over the network. See Also uniquniqCombined64uniqHLL12uniqExactuniqTheta","keywords":""},{"title":"uniqExact","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/uniqexact","content":"uniqExact Calculates the exact number of different argument values. uniqExact(x[, ...]) Use the uniqExact function if you absolutely need an exact result. Otherwise use the uniq function. The uniqExact function uses more memory than uniq, because the size of the state has unbounded growth as the number of different values increases. Arguments The function takes a variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types. See Also uniquniqCombineduniqHLL12uniqTheta","keywords":""},{"title":"uniqHLL12","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/uniqhll12","content":"uniqHLL12 Calculates the approximate number of different argument values, using the HyperLogLog algorithm. uniqHLL12(x[, ...]) Arguments The function takes a variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types. Returned value A UInt64-type number. Implementation details Function: Calculates a hash for all parameters in the aggregate, then uses it in calculations. Uses the HyperLogLog algorithm to approximate the number of different argument values. 2^12 5-bit cells are used. The size of the state is slightly more than 2.5 KB. The result is not very accurate (up to ~10% error) for small data sets (&lt;10K elements). However, the result is fairly accurate for high-cardinality data sets (10K-100M), with a maximum error of ~1.6%. Starting from 100M, the estimation error increases, and the function will return very inaccurate results for data sets with extremely high cardinality (1B+ elements). Provides the determinate result (it does not depend on the query processing order). We do not recommend using this function. In most cases, use the uniq or uniqCombined function. See Also uniquniqCombineduniqExactuniqTheta","keywords":""},{"title":"uniqTheta","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/uniqthetasketch","content":"uniqTheta Calculates the approximate number of different argument values, using the Theta Sketch Framework. uniqTheta(x[, ...]) Arguments The function takes a variable number of parameters. Parameters can be Tuple, Array, Date, DateTime, String, or numeric types. Returned value A UInt64-type number. Implementation details Function: Calculates a hash for all parameters in the aggregate, then uses it in calculations. Uses the KMV algorithm to approximate the number of different argument values. 4096(2^12) 64-bit sketch are used. The size of the state is about 41 KB. The relative error is 3.125% (95% confidence), see the relative error table for detail. See Also uniquniqCombineduniqCombined64uniqHLL12uniqExact","keywords":""},{"title":"varPop(x)","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/varpop","content":"varPop(x) Calculates the amount Σ((x - x̅)^2) / n, where n is the sample size and x̅is the average value of x. In other words, dispersion for a set of values. Returns Float64. note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the varPopStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"Data Types","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/","content":"Data Types ClickHouse can store various kinds of data in table cells. This section describes the supported data types and special considerations for using and/or implementing them if any. You can check whether data type name is case-sensitive in the system.data_type_families table. Original article","keywords":""},{"title":"Boolean Values","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/boolean","content":"Boolean Values Since https://github.com/ClickHouse/ClickHouse/commit/4076ae77b46794e73594a9f400200088ed1e7a6e , there be a separate type for boolean values. For versions before that, there is no separate type for boolean values. Use UInt8 type, restricted to the values 0 or 1. Original article","keywords":""},{"title":"varSamp","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/varsamp","content":"varSamp Calculates the amount Σ((x - x̅)^2) / (n - 1), where n is the sample size and x̅is the average value of x. It represents an unbiased estimate of the variance of a random variable if passed values form its sample. Returns Float64. When n &lt;= 1, returns +∞. note This function uses a numerically unstable algorithm. If you need numerical stability in calculations, use the varSampStable function. It works slower but provides a lower computational error.","keywords":""},{"title":"Date","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/date","content":"Date A date. Stored in two bytes as the number of days since 1970-01-01 (unsigned). Allows storing values from just after the beginning of the Unix Epoch to the upper threshold defined by a constant at the compilation stage (currently, this is until the year 2149, but the final fully-supported year is 2148). Supported range of values: [1970-01-01, 2149-06-06]. The date value is stored without the time zone. Example Creating a table with a Date-type column and inserting data into it: CREATE TABLE dt ( `timestamp` Date, `event_id` UInt8 ) ENGINE = TinyLog; INSERT INTO dt VALUES (1546300800, 1), ('2019-01-01', 2); SELECT * FROM dt; ┌──timestamp─┬─event_id─┐ │ 2019-01-01 │ 1 │ │ 2019-01-01 │ 2 │ └────────────┴──────────┘ See Also Functions for working with dates and timesOperators for working with dates and timesDateTime data type","keywords":""},{"title":"Date32","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/date32","content":"Date32 A date. Supports the date range same with Datetime64. Stored in four bytes as the number of days since 1925-01-01. Allows storing values till 2283-11-11. Examples Creating a table with a Date32-type column and inserting data into it: CREATE TABLE new ( `timestamp` Date32, `event_id` UInt8 ) ENGINE = TinyLog; INSERT INTO new VALUES (4102444800, 1), ('2100-01-01', 2); SELECT * FROM new; ┌──timestamp─┬─event_id─┐ │ 2100-01-01 │ 1 │ │ 2100-01-01 │ 2 │ └────────────┴──────────┘ See Also toDate32toDate32OrZerotoDate32OrNull","keywords":""},{"title":"welchTTest","type":0,"sectionRef":"#","url":"en/sql-reference/aggregate-functions/reference/welchttest","content":"welchTTest Applies Welch's t-test to samples from two populations. Syntax welchTTest([confidence_level])(sample_data, sample_index) Values of both samples are in the sample_data column. If sample_index equals to 0 then the value in that row belongs to the sample from the first population. Otherwise it belongs to the sample from the second population. The null hypothesis is that means of populations are equal. Normal distribution is assumed. Populations may have unequal variance. Arguments sample_data — Sample data. Integer, Float or Decimal.sample_index — Sample index. Integer. Parameters confidence_level — Confidence level in order to calculate confidence intervals. Float. Returned values Tuple with two or four elements (if the optional confidence_level is specified) calculated t-statistic. Float64.calculated p-value. Float64.[calculated confidence-interval-low.][Float64](/docs/en/sql-reference/data-types/float).[calculated confidence-interval-high.][Float64](/docs/en/sql-reference/data-types/float). Example Input table: ┌─sample_data─┬─sample_index─┐ │ 20.3 │ 0 │ │ 22.1 │ 0 │ │ 21.9 │ 0 │ │ 18.9 │ 1 │ │ 20.3 │ 1 │ │ 19 │ 1 │ └─────────────┴──────────────┘ Query: SELECT welchTTest(sample_data, sample_index) FROM welch_ttest; Result: ┌─welchTTest(sample_data, sample_index)─────┐ │ (2.7988719532211235,0.051807360348581945) │ └───────────────────────────────────────────┘ See Also Welch's t-teststudentTTest function Original article","keywords":""},{"title":"AggregateFunction","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/aggregatefunction","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"AggregateFunction","url":"en/sql-reference/data-types/aggregatefunction#usage","content":""},{"title":"Data Insertion​","type":1,"pageTitle":"AggregateFunction","url":"en/sql-reference/data-types/aggregatefunction#data-insertion","content":"To insert data, use INSERT SELECT with aggregate -State- functions. Function examples uniqState(UserID) quantilesState(0.5, 0.9)(SendTiming)  In contrast to the corresponding functions uniq and quantiles, -State- functions return the state, instead of the final value. In other words, they return a value of AggregateFunction type. In the results of SELECT query, the values of AggregateFunction type have implementation-specific binary representation for all of the ClickHouse output formats. If dump data into, for example, TabSeparated format with SELECT query, then this dump can be loaded back using INSERT query. "},{"title":"Data Selection​","type":1,"pageTitle":"AggregateFunction","url":"en/sql-reference/data-types/aggregatefunction#data-selection","content":"When selecting data from AggregatingMergeTree table, use GROUP BY clause and the same aggregate functions as when inserting data, but using -Mergesuffix. An aggregate function with -Merge suffix takes a set of states, combines them, and returns the result of complete data aggregation. For example, the following two queries return the same result: SELECT uniq(UserID) FROM table SELECT uniqMerge(state) FROM (SELECT uniqState(UserID) AS state FROM table GROUP BY RegionID)  "},{"title":"Usage Example​","type":1,"pageTitle":"AggregateFunction","url":"en/sql-reference/data-types/aggregatefunction#usage-example","content":"See AggregatingMergeTree engine description. Original article "},{"title":"Domains","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/domains/","content":"","keywords":""},{"title":"Extra Features of Domains​","type":1,"pageTitle":"Domains","url":"en/sql-reference/data-types/domains/#extra-features-of-domains","content":"Explicit column type name in SHOW CREATE TABLE or DESCRIBE TABLEInput from human-friendly format with INSERT INTO domain_table(domain_column) VALUES(...)Output to human-friendly format for SELECT domain_column FROM domain_tableLoading data from an external source in the human-friendly format: INSERT INTO domain_table FORMAT CSV ... "},{"title":"Limitations​","type":1,"pageTitle":"Domains","url":"en/sql-reference/data-types/domains/#limitations","content":"Can’t convert index column of base type to domain type via ALTER TABLE.Can’t implicitly convert string values into domain values when inserting data from another column or table.Domain adds no constrains on stored values. Original article "},{"title":"Fixedstring","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/fixedstring","content":"Fixedstring A fixed-length string of N bytes (neither characters nor code points). To declare a column of FixedString type, use the following syntax: &lt;column_name&gt; FixedString(N) Where N is a natural number. The FixedString type is efficient when data has the length of precisely N bytes. In all other cases, it is likely to reduce efficiency. Examples of the values that can be efficiently stored in FixedString-typed columns: The binary representation of IP addresses (FixedString(16) for IPv6).Language codes (ru_RU, en_US … ).Currency codes (USD, RUB … ).Binary representation of hashes (FixedString(16) for MD5, FixedString(32) for SHA256). To store UUID values, use the UUID data type. When inserting the data, ClickHouse: Complements a string with null bytes if the string contains fewer than N bytes.Throws the Too large value for FixedString(N) exception if the string contains more than N bytes. When selecting the data, ClickHouse does not remove the null bytes at the end of the string. If you use the WHERE clause, you should add null bytes manually to match the FixedString value. The following example illustrates how to use the WHERE clause with FixedString. Let’s consider the following table with the single FixedString(2) column: ┌─name──┐ │ b │ └───────┘ The query SELECT * FROM FixedStringTable WHERE a = 'b' does not return any data as a result. We should complement the filter pattern with null bytes. SELECT * FROM FixedStringTable WHERE a = 'b\\0' ┌─a─┐ │ b │ └───┘ This behaviour differs from MySQL for the CHAR type (where strings are padded with spaces, and the spaces are removed for output). Note that the length of the FixedString(N) value is constant. The length function returns N even if the FixedString(N) value is filled only with null bytes, but the empty function returns 1 in this case. Original article","keywords":""},{"title":"Array(t)","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/array","content":"","keywords":""},{"title":"Creating an Array​","type":1,"pageTitle":"Array(t)","url":"en/sql-reference/data-types/array#creating-an-array","content":"You can use a function to create an array: array(T)  You can also use square brackets. []  Example of creating an array: SELECT array(1, 2) AS x, toTypeName(x)  ┌─x─────┬─toTypeName(array(1, 2))─┐ │ [1,2] │ Array(UInt8) │ └───────┴─────────────────────────┘  SELECT [1, 2] AS x, toTypeName(x)  ┌─x─────┬─toTypeName([1, 2])─┐ │ [1,2] │ Array(UInt8) │ └───────┴────────────────────┘  "},{"title":"Working with Data Types​","type":1,"pageTitle":"Array(t)","url":"en/sql-reference/data-types/array#working-with-data-types","content":"The maximum size of an array is limited to one million elements. When creating an array on the fly, ClickHouse automatically defines the argument type as the narrowest data type that can store all the listed arguments. If there are any Nullable or literal NULL values, the type of an array element also becomes Nullable. If ClickHouse couldn’t determine the data type, it generates an exception. For instance, this happens when trying to create an array with strings and numbers simultaneously (SELECT array(1, 'a')). Examples of automatic data type detection: SELECT array(1, 2, NULL) AS x, toTypeName(x)  ┌─x──────────┬─toTypeName(array(1, 2, NULL))─┐ │ [1,2,NULL] │ Array(Nullable(UInt8)) │ └────────────┴───────────────────────────────┘  If you try to create an array of incompatible data types, ClickHouse throws an exception: SELECT array(1, 'a')  Received exception from server (version 1.1.54388): Code: 386. DB::Exception: Received from localhost:9000, 127.0.0.1. DB::Exception: There is no supertype for types UInt8, String because some of them are String/FixedString and some of them are not.  "},{"title":"Array Size​","type":1,"pageTitle":"Array(t)","url":"en/sql-reference/data-types/array#array-size","content":"It is possible to find the size of an array by using the size0 subcolumn without reading the whole column. For multi-dimensional arrays you can use sizeN-1, where N is the wanted dimension. Example Query: CREATE TABLE t_arr (`arr` Array(Array(Array(UInt32)))) ENGINE = MergeTree ORDER BY tuple(); INSERT INTO t_arr VALUES ([[[12, 13, 0, 1],[12]]]); SELECT arr.size0, arr.size1, arr.size2 FROM t_arr;  Result: ┌─arr.size0─┬─arr.size1─┬─arr.size2─┐ │ 1 │ [2] │ [[4,1]] │ └───────────┴───────────┴───────────┘  "},{"title":"Float32, Float64","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/float","content":"","keywords":""},{"title":"Using Floating-point Numbers​","type":1,"pageTitle":"Float32, Float64","url":"en/sql-reference/data-types/float#using-floating-point-numbers","content":"Computations with floating-point numbers might produce a rounding error. SELECT 1 - 0.9  ┌───────minus(1, 0.9)─┐ │ 0.09999999999999998 │ └─────────────────────┘  The result of the calculation depends on the calculation method (the processor type and architecture of the computer system).Floating-point calculations might result in numbers such as infinity (Inf) and “not-a-number” (NaN). This should be taken into account when processing the results of calculations.When parsing floating-point numbers from text, the result might not be the nearest machine-representable number. "},{"title":"NaN and Inf​","type":1,"pageTitle":"Float32, Float64","url":"en/sql-reference/data-types/float#data_type-float-nan-inf","content":"In contrast to standard SQL, ClickHouse supports the following categories of floating-point numbers: Inf – Infinity. SELECT 0.5 / 0  ┌─divide(0.5, 0)─┐ │ inf │ └────────────────┘  -Inf — Negative infinity. SELECT -0.5 / 0  ┌─divide(-0.5, 0)─┐ │ -inf │ └─────────────────┘  NaN — Not a number. SELECT 0 / 0  ┌─divide(0, 0)─┐ │ nan │ └──────────────┘  See the rules for NaN sorting in the section ORDER BY clause. Original article "},{"title":"UInt8, UInt16, UInt32, UInt64, UInt128, UInt256, Int8, Int16, Int32, Int64, Int128, Int256","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/int-uint","content":"","keywords":""},{"title":"Int Ranges​","type":1,"pageTitle":"UInt8, UInt16, UInt32, UInt64, UInt128, UInt256, Int8, Int16, Int32, Int64, Int128, Int256","url":"en/sql-reference/data-types/int-uint#int-ranges","content":"Int8 — [-128 : 127]Int16 — [-32768 : 32767]Int32 — [-2147483648 : 2147483647]Int64 — [-9223372036854775808 : 9223372036854775807]Int128 — [-170141183460469231731687303715884105728 : 170141183460469231731687303715884105727]Int256 — [-57896044618658097711785492504343953926634992332820282019728792003956564819968 : 57896044618658097711785492504343953926634992332820282019728792003956564819967] Aliases: Int8 — TINYINT, BOOL, BOOLEAN, INT1.Int16 — SMALLINT, INT2.Int32 — INT, INT4, INTEGER.Int64 — BIGINT. "},{"title":"UInt Ranges​","type":1,"pageTitle":"UInt8, UInt16, UInt32, UInt64, UInt128, UInt256, Int8, Int16, Int32, Int64, Int128, Int256","url":"en/sql-reference/data-types/int-uint#uint-ranges","content":"UInt8 — [0 : 255]UInt16 — [0 : 65535]UInt32 — [0 : 4294967295]UInt64 — [0 : 18446744073709551615]UInt128 — [0 : 340282366920938463463374607431768211455]UInt256 — [0 : 115792089237316195423570985008687907853269984665640564039457584007913129639935] Original article "},{"title":"LowCardinality Data Type","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/lowcardinality","content":"","keywords":""},{"title":"Syntax​","type":1,"pageTitle":"LowCardinality Data Type","url":"en/sql-reference/data-types/lowcardinality#lowcardinality-syntax","content":"LowCardinality(data_type)  Parameters data_type — String, FixedString, Date, DateTime, and numbers excepting Decimal. LowCardinality is not efficient for some data types, see the allow_suspicious_low_cardinality_types setting description. "},{"title":"Description​","type":1,"pageTitle":"LowCardinality Data Type","url":"en/sql-reference/data-types/lowcardinality#lowcardinality-dscr","content":"LowCardinality is a superstructure that changes a data storage method and rules of data processing. ClickHouse applies dictionary coding to LowCardinality-columns. Operating with dictionary encoded data significantly increases performance of SELECT queries for many applications. The efficiency of using LowCardinality data type depends on data diversity. If a dictionary contains less than 10,000 distinct values, then ClickHouse mostly shows higher efficiency of data reading and storing. If a dictionary contains more than 100,000 distinct values, then ClickHouse can perform worse in comparison with using ordinary data types. Consider using LowCardinality instead of Enum when working with strings. LowCardinality provides more flexibility in use and often reveals the same or higher efficiency. "},{"title":"Example​","type":1,"pageTitle":"LowCardinality Data Type","url":"en/sql-reference/data-types/lowcardinality#example","content":"Create a table with a LowCardinality-column: CREATE TABLE lc_t ( `id` UInt16, `strings` LowCardinality(String) ) ENGINE = MergeTree() ORDER BY id  "},{"title":"Related Settings and Functions​","type":1,"pageTitle":"LowCardinality Data Type","url":"en/sql-reference/data-types/lowcardinality#related-settings-and-functions","content":"Settings: low_cardinality_max_dictionary_sizelow_cardinality_use_single_dictionary_for_partlow_cardinality_allow_in_native_formatallow_suspicious_low_cardinality_typesoutput_format_arrow_low_cardinality_as_dictionary Functions: toLowCardinality "},{"title":"See Also​","type":1,"pageTitle":"LowCardinality Data Type","url":"en/sql-reference/data-types/lowcardinality#see-also","content":"Reducing ClickHouse Storage Cost with the Low Cardinality Type – Lessons from an Instana Engineer.String Optimization (video presentation in Russian). Slides in English. "},{"title":"Datetime","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/datetime","content":"","keywords":""},{"title":"Usage Remarks​","type":1,"pageTitle":"Datetime","url":"en/sql-reference/data-types/datetime#usage-remarks","content":"The point in time is saved as a Unix timestamp, regardless of the time zone or daylight saving time. The time zone affects how the values of the DateTime type values are displayed in text format and how the values specified as strings are parsed (‘2020-01-01 05:00:01’). Timezone agnostic unix timestamp is stored in tables, and the timezone is used to transform it to text format or back during data import/export or to make calendar calculations on the values (example: toDate, toHour functions et cetera). The time zone is not stored in the rows of the table (or in resultset), but is stored in the column metadata. A list of supported time zones can be found in the IANA Time Zone Database and also can be queried by SELECT * FROM system.time_zones. The list is also available at Wikipedia. You can explicitly set a time zone for DateTime-type columns when creating a table. Example: DateTime('UTC'). If the time zone isn’t set, ClickHouse uses the value of the timezone parameter in the server settings or the operating system settings at the moment of the ClickHouse server start. The clickhouse-client applies the server time zone by default if a time zone isn’t explicitly set when initializing the data type. To use the client time zone, run clickhouse-client with the --use_client_time_zone parameter. ClickHouse outputs values depending on the value of the date_time_output_format setting. YYYY-MM-DD hh:mm:ss text format by default. Additionaly you can change the output with the formatDateTime function. When inserting data into ClickHouse, you can use different formats of date and time strings, depending on the value of the date_time_input_format setting. "},{"title":"Examples​","type":1,"pageTitle":"Datetime","url":"en/sql-reference/data-types/datetime#examples","content":"1. Creating a table with a DateTime-type column and inserting data into it: CREATE TABLE dt ( `timestamp` DateTime('Asia/Istanbul'), `event_id` UInt8 ) ENGINE = TinyLog;  INSERT INTO dt Values (1546300800, 1), ('2019-01-01 00:00:00', 2);  SELECT * FROM dt;  ┌───────────timestamp─┬─event_id─┐ │ 2019-01-01 03:00:00 │ 1 │ │ 2019-01-01 00:00:00 │ 2 │ └─────────────────────┴──────────┘  When inserting datetime as an integer, it is treated as Unix Timestamp (UTC). 1546300800 represents '2019-01-01 00:00:00' UTC. However, as timestamp column has Asia/Istanbul (UTC+3) timezone specified, when outputting as string the value will be shown as '2019-01-01 03:00:00'When inserting string value as datetime, it is treated as being in column timezone. '2019-01-01 00:00:00' will be treated as being in Asia/Istanbul timezone and saved as 1546290000. 2. Filtering on DateTime values SELECT * FROM dt WHERE timestamp = toDateTime('2019-01-01 00:00:00', 'Asia/Istanbul')  ┌───────────timestamp─┬─event_id─┐ │ 2019-01-01 00:00:00 │ 2 │ └─────────────────────┴──────────┘  DateTime column values can be filtered using a string value in WHERE predicate. It will be converted to DateTime automatically: SELECT * FROM dt WHERE timestamp = '2019-01-01 00:00:00'  ┌───────────timestamp─┬─event_id─┐ │ 2019-01-01 03:00:00 │ 1 │ └─────────────────────┴──────────┘  3. Getting a time zone for a DateTime-type column: SELECT toDateTime(now(), 'Asia/Istanbul') AS column, toTypeName(column) AS x  ┌──────────────column─┬─x─────────────────────────┐ │ 2019-10-16 04:12:04 │ DateTime('Asia/Istanbul') │ └─────────────────────┴───────────────────────────┘  4. Timezone conversion SELECT toDateTime(timestamp, 'Europe/London') as lon_time, toDateTime(timestamp, 'Asia/Istanbul') as mos_time FROM dt  ┌───────────lon_time──┬────────────mos_time─┐ │ 2019-01-01 00:00:00 │ 2019-01-01 03:00:00 │ │ 2018-12-31 21:00:00 │ 2019-01-01 00:00:00 │ └─────────────────────┴─────────────────────┘  As timezone conversion only changes the metadata, the operation has no computation cost. "},{"title":"Limitations on timezones support​","type":1,"pageTitle":"Datetime","url":"en/sql-reference/data-types/datetime#limitations-on-timezones-support","content":"Some timezones may not be supported completely. There are a few cases: If the offset from UTC is not a multiple of 15 minutes, the calculation of hours and minutes can be incorrect. For example, the time zone in Monrovia, Liberia has offset UTC -0:44:30 before 7 Jan 1972. If you are doing calculations on the historical time in Monrovia timezone, the time processing functions may give incorrect results. The results after 7 Jan 1972 will be correct nevertheless. If the time transition (due to daylight saving time or for other reasons) was performed at a point of time that is not a multiple of 15 minutes, you can also get incorrect results at this specific day. Non-monotonic calendar dates. For example, in Happy Valley - Goose Bay, the time was transitioned one hour backwards at 00:01:00 7 Nov 2010 (one minute after midnight). So after 6th Nov has ended, people observed a whole one minute of 7th Nov, then time was changed back to 23:01 6th Nov and after another 59 minutes the 7th Nov started again. ClickHouse does not (yet) support this kind of fun. During these days the results of time processing functions may be slightly incorrect. Similar issue exists for Casey Antarctic station in year 2010. They changed time three hours back at 5 Mar, 02:00. If you are working in antarctic station, please don't afraid to use ClickHouse. Just make sure you set timezone to UTC or be aware of inaccuracies. Time shifts for multiple days. Some pacific islands changed their timezone offset from UTC+14 to UTC-12. That's alright but some inaccuracies may present if you do calculations with their timezone for historical time points at the days of conversion. "},{"title":"See Also​","type":1,"pageTitle":"Datetime","url":"en/sql-reference/data-types/datetime#see-also","content":"Type conversion functionsFunctions for working with dates and timesFunctions for working with arraysThe date_time_input_format settingThe date_time_output_format settingThe timezone server configuration parameterOperators for working with dates and timesThe Date data type Original article "},{"title":"Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S), Decimal256(S)","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/decimal","content":"","keywords":""},{"title":"Parameters​","type":1,"pageTitle":"Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S), Decimal256(S)","url":"en/sql-reference/data-types/decimal#parameters","content":"P - precision. Valid range: [ 1 : 76 ]. Determines how many decimal digits number can have (including fraction).S - scale. Valid range: [ 0 : P ]. Determines how many decimal digits fraction can have. Depending on P parameter value Decimal(P, S) is a synonym for: P from [ 1 : 9 ] - for Decimal32(S)P from [ 10 : 18 ] - for Decimal64(S)P from [ 19 : 38 ] - for Decimal128(S)P from [ 39 : 76 ] - for Decimal256(S) "},{"title":"Decimal Value Ranges​","type":1,"pageTitle":"Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S), Decimal256(S)","url":"en/sql-reference/data-types/decimal#decimal-value-ranges","content":"Decimal32(S) - ( -1 * 10^(9 - S), 1 * 10^(9 - S) )Decimal64(S) - ( -1 * 10^(18 - S), 1 * 10^(18 - S) )Decimal128(S) - ( -1 * 10^(38 - S), 1 * 10^(38 - S) )Decimal256(S) - ( -1 * 10^(76 - S), 1 * 10^(76 - S) ) For example, Decimal32(4) can contain numbers from -99999.9999 to 99999.9999 with 0.0001 step. "},{"title":"Internal Representation​","type":1,"pageTitle":"Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S), Decimal256(S)","url":"en/sql-reference/data-types/decimal#internal-representation","content":"Internally data is represented as normal signed integers with respective bit width. Real value ranges that can be stored in memory are a bit larger than specified above, which are checked only on conversion from a string. Because modern CPUs do not support 128-bit integers natively, operations on Decimal128 are emulated. Because of this Decimal128 works significantly slower than Decimal32/Decimal64. "},{"title":"Operations and Result Type​","type":1,"pageTitle":"Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S), Decimal256(S)","url":"en/sql-reference/data-types/decimal#operations-and-result-type","content":"Binary operations on Decimal result in wider result type (with any order of arguments). Decimal64(S1) &lt;op&gt; Decimal32(S2) -&gt; Decimal64(S)Decimal128(S1) &lt;op&gt; Decimal32(S2) -&gt; Decimal128(S)Decimal128(S1) &lt;op&gt; Decimal64(S2) -&gt; Decimal128(S)Decimal256(S1) &lt;op&gt; Decimal&lt;32|64|128&gt;(S2) -&gt; Decimal256(S) Rules for scale: add, subtract: S = max(S1, S2).multuply: S = S1 + S2.divide: S = S1. For similar operations between Decimal and integers, the result is Decimal of the same size as an argument. Operations between Decimal and Float32/Float64 are not defined. If you need them, you can explicitly cast one of argument using toDecimal32, toDecimal64, toDecimal128 or toFloat32, toFloat64 builtins. Keep in mind that the result will lose precision and type conversion is a computationally expensive operation. Some functions on Decimal return result as Float64 (for example, var or stddev). Intermediate calculations might still be performed in Decimal, which might lead to different results between Float64 and Decimal inputs with the same values. "},{"title":"Overflow Checks​","type":1,"pageTitle":"Decimal(P, S), Decimal32(S), Decimal64(S), Decimal128(S), Decimal256(S)","url":"en/sql-reference/data-types/decimal#overflow-checks","content":"During calculations on Decimal, integer overflows might happen. Excessive digits in a fraction are discarded (not rounded). Excessive digits in integer part will lead to an exception. SELECT toDecimal32(2, 4) AS x, x / 3  ┌──────x─┬─divide(toDecimal32(2, 4), 3)─┐ │ 2.0000 │ 0.6666 │ └────────┴──────────────────────────────┘  SELECT toDecimal32(4.2, 8) AS x, x * x  DB::Exception: Scale is out of bounds.  SELECT toDecimal32(4.2, 8) AS x, 6 * x  DB::Exception: Decimal math overflow.  Overflow checks lead to operations slowdown. If it is known that overflows are not possible, it makes sense to disable checks using decimal_check_overflow setting. When checks are disabled and overflow happens, the result will be incorrect: SET decimal_check_overflow = 0; SELECT toDecimal32(4.2, 8) AS x, 6 * x  ┌──────────x─┬─multiply(6, toDecimal32(4.2, 8))─┐ │ 4.20000000 │ -17.74967296 │ └────────────┴──────────────────────────────────┘  Overflow checks happen not only on arithmetic operations but also on value comparison: SELECT toDecimal32(1, 8) &lt; 100  DB::Exception: Can't compare.  See also isDecimalOverflowcountDigits Original article "},{"title":"Datetime64","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/datetime64","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"Datetime64","url":"en/sql-reference/data-types/datetime64#examples","content":"Creating a table with DateTime64-type column and inserting data into it: CREATE TABLE dt ( `timestamp` DateTime64(3, 'Asia/Istanbul'), `event_id` UInt8 ) ENGINE = TinyLog;  INSERT INTO dt Values (1546300800000, 1), ('2019-01-01 00:00:00', 2);  SELECT * FROM dt;  ┌───────────────timestamp─┬─event_id─┐ │ 2019-01-01 03:00:00.000 │ 1 │ │ 2019-01-01 00:00:00.000 │ 2 │ └─────────────────────────┴──────────┘  When inserting datetime as an integer, it is treated as an appropriately scaled Unix Timestamp (UTC). 1546300800000 (with precision 3) represents '2019-01-01 00:00:00' UTC. However, as timestamp column has Asia/Istanbul (UTC+3) timezone specified, when outputting as a string the value will be shown as '2019-01-01 03:00:00'.When inserting string value as datetime, it is treated as being in column timezone. '2019-01-01 00:00:00' will be treated as being in Asia/Istanbul timezone and stored as 1546290000000. Filtering on DateTime64 values SELECT * FROM dt WHERE timestamp = toDateTime64('2019-01-01 00:00:00', 3, 'Asia/Istanbul');  ┌───────────────timestamp─┬─event_id─┐ │ 2019-01-01 00:00:00.000 │ 2 │ └─────────────────────────┴──────────┘  Unlike DateTime, DateTime64 values are not converted from String automatically. Getting a time zone for a DateTime64-type value: SELECT toDateTime64(now(), 3, 'Asia/Istanbul') AS column, toTypeName(column) AS x;  ┌──────────────────column─┬─x──────────────────────────────┐ │ 2019-10-16 04:12:04.000 │ DateTime64(3, 'Asia/Istanbul') │ └─────────────────────────┴────────────────────────────────┘  Timezone conversion SELECT toDateTime64(timestamp, 3, 'Europe/London') as lon_time, toDateTime64(timestamp, 3, 'Asia/Istanbul') as mos_time FROM dt;  ┌───────────────lon_time──┬────────────────mos_time─┐ │ 2019-01-01 00:00:00.000 │ 2019-01-01 03:00:00.000 │ │ 2018-12-31 21:00:00.000 │ 2019-01-01 00:00:00.000 │ └─────────────────────────┴─────────────────────────┘  See Also Type conversion functionsFunctions for working with dates and timesFunctions for working with arraysThe date_time_input_format settingThe date_time_output_format settingThe timezone server configuration parameterOperators for working with dates and timesDate data typeDateTime data type "},{"title":"Nested Data Structures","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/nested-data-structures/","content":"Nested Data Structures Original article","keywords":""},{"title":"ipv4","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/domains/ipv4","content":"","keywords":""},{"title":"IPv4​","type":1,"pageTitle":"ipv4","url":"en/sql-reference/data-types/domains/ipv4#ipv4","content":"IPv4 is a domain based on UInt32 type and serves as a typed replacement for storing IPv4 values. It provides compact storage with the human-friendly input-output format and column type information on inspection. "},{"title":"Basic Usage​","type":1,"pageTitle":"ipv4","url":"en/sql-reference/data-types/domains/ipv4#basic-usage","content":"CREATE TABLE hits (url String, from IPv4) ENGINE = MergeTree() ORDER BY url; DESCRIBE TABLE hits;  ┌─name─┬─type───┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┐ │ url │ String │ │ │ │ │ │ from │ IPv4 │ │ │ │ │ └──────┴────────┴──────────────┴────────────────────┴─────────┴──────────────────┘  OR you can use IPv4 domain as a key: CREATE TABLE hits (url String, from IPv4) ENGINE = MergeTree() ORDER BY from;  IPv4 domain supports custom input format as IPv4-strings: INSERT INTO hits (url, from) VALUES ('https://wikipedia.org', '116.253.40.133')('https://clickhouse.com', '183.247.232.58')('https://clickhouse.com/docs/en/', '116.106.34.242'); SELECT * FROM hits;  ┌─url────────────────────────────────┬───────────from─┐ │ https://clickhouse.com/docs/en/ │ 116.106.34.242 │ │ https://wikipedia.org │ 116.253.40.133 │ │ https://clickhouse.com │ 183.247.232.58 │ └────────────────────────────────────┴────────────────┘  Values are stored in compact binary form: SELECT toTypeName(from), hex(from) FROM hits LIMIT 1;  ┌─toTypeName(from)─┬─hex(from)─┐ │ IPv4 │ B7F7E83A │ └──────────────────┴───────────┘  Domain values are not implicitly convertible to types other than UInt32. If you want to convert IPv4 value to a string, you have to do that explicitly with IPv4NumToString() function: SELECT toTypeName(s), IPv4NumToString(from) as s FROM hits LIMIT 1;  ┌─toTypeName(IPv4NumToString(from))─┬─s──────────────┐ │ String │ 183.247.232.58 │ └───────────────────────────────────┴────────────────┘  Or cast to a UInt32 value: SELECT toTypeName(i), CAST(from as UInt32) as i FROM hits LIMIT 1;  ┌─toTypeName(CAST(from, 'UInt32'))─┬──────────i─┐ │ UInt32 │ 3086477370 │ └──────────────────────────────────┴────────────┘  Original article "},{"title":"ipv6","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/domains/ipv6","content":"","keywords":""},{"title":"IPv6​","type":1,"pageTitle":"ipv6","url":"en/sql-reference/data-types/domains/ipv6#ipv6","content":"IPv6 is a domain based on FixedString(16) type and serves as a typed replacement for storing IPv6 values. It provides compact storage with the human-friendly input-output format and column type information on inspection. "},{"title":"Basic Usage​","type":1,"pageTitle":"ipv6","url":"en/sql-reference/data-types/domains/ipv6#basic-usage","content":"CREATE TABLE hits (url String, from IPv6) ENGINE = MergeTree() ORDER BY url; DESCRIBE TABLE hits;  ┌─name─┬─type───┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┐ │ url │ String │ │ │ │ │ │ from │ IPv6 │ │ │ │ │ └──────┴────────┴──────────────┴────────────────────┴─────────┴──────────────────┘  OR you can use IPv6 domain as a key: CREATE TABLE hits (url String, from IPv6) ENGINE = MergeTree() ORDER BY from;  IPv6 domain supports custom input as IPv6-strings: INSERT INTO hits (url, from) VALUES ('https://wikipedia.org', '2a02:aa08:e000:3100::2')('https://clickhouse.com', '2001:44c8:129:2632:33:0:252:2')('https://clickhouse.com/docs/en/', '2a02:e980:1e::1'); SELECT * FROM hits;  ┌─url────────────────────────────────┬─from──────────────────────────┐ │ https://clickhouse.com │ 2001:44c8:129:2632:33:0:252:2 │ │ https://clickhouse.com/docs/en/ │ 2a02:e980:1e::1 │ │ https://wikipedia.org │ 2a02:aa08:e000:3100::2 │ └────────────────────────────────────┴───────────────────────────────┘  Values are stored in compact binary form: SELECT toTypeName(from), hex(from) FROM hits LIMIT 1;  ┌─toTypeName(from)─┬─hex(from)────────────────────────┐ │ IPv6 │ 200144C8012926320033000002520002 │ └──────────────────┴──────────────────────────────────┘  Domain values are not implicitly convertible to types other than FixedString(16). If you want to convert IPv6 value to a string, you have to do that explicitly with IPv6NumToString() function: SELECT toTypeName(s), IPv6NumToString(from) as s FROM hits LIMIT 1;  ┌─toTypeName(IPv6NumToString(from))─┬─s─────────────────────────────┐ │ String │ 2001:44c8:129:2632:33:0:252:2 │ └───────────────────────────────────┴───────────────────────────────┘  Or cast to a FixedString(16) value: SELECT toTypeName(i), CAST(from as FixedString(16)) as i FROM hits LIMIT 1;  ┌─toTypeName(CAST(from, 'FixedString(16)'))─┬─i───────┐ │ FixedString(16) │ ��� │ └───────────────────────────────────────────┴─────────┘  Original article "},{"title":"Enum","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/enum","content":"","keywords":""},{"title":"Usage Examples​","type":1,"pageTitle":"Enum","url":"en/sql-reference/data-types/enum#usage-examples","content":"Here we create a table with an Enum8('hello' = 1, 'world' = 2) type column: CREATE TABLE t_enum ( x Enum('hello' = 1, 'world' = 2) ) ENGINE = TinyLog  Similarly, you could omit numbers. ClickHouse will assign consecutive numbers automatically. Numbers are assigned starting from 1 by default. CREATE TABLE t_enum ( x Enum('hello', 'world') ) ENGINE = TinyLog  You can also specify legal starting number for the first name. CREATE TABLE t_enum ( x Enum('hello' = 1, 'world') ) ENGINE = TinyLog  CREATE TABLE t_enum ( x Enum8('hello' = -129, 'world') ) ENGINE = TinyLog  Exception on server: Code: 69. DB::Exception: Value -129 for element 'hello' exceeds range of Enum8.  Column x can only store values that are listed in the type definition: 'hello' or 'world'. If you try to save any other value, ClickHouse will raise an exception. 8-bit size for this Enum is chosen automatically. INSERT INTO t_enum VALUES ('hello'), ('world'), ('hello')  Ok.  INSERT INTO t_enum values('a')  Exception on client: Code: 49. DB::Exception: Unknown element 'a' for type Enum('hello' = 1, 'world' = 2)  When you query data from the table, ClickHouse outputs the string values from Enum. SELECT * FROM t_enum  ┌─x─────┐ │ hello │ │ world │ │ hello │ └───────┘  If you need to see the numeric equivalents of the rows, you must cast the Enum value to integer type. SELECT CAST(x, 'Int8') FROM t_enum  ┌─CAST(x, 'Int8')─┐ │ 1 │ │ 2 │ │ 1 │ └─────────────────┘  To create an Enum value in a query, you also need to use CAST. SELECT toTypeName(CAST('a', 'Enum(\\'a\\' = 1, \\'b\\' = 2)'))  ┌─toTypeName(CAST('a', 'Enum(\\'a\\' = 1, \\'b\\' = 2)'))─┐ │ Enum8('a' = 1, 'b' = 2) │ └─────────────────────────────────────────────────────┘  "},{"title":"General Rules and Usage​","type":1,"pageTitle":"Enum","url":"en/sql-reference/data-types/enum#general-rules-and-usage","content":"Each of the values is assigned a number in the range -128 ... 127 for Enum8 or in the range -32768 ... 32767 for Enum16. All the strings and numbers must be different. An empty string is allowed. If this type is specified (in a table definition), numbers can be in an arbitrary order. However, the order does not matter. Neither the string nor the numeric value in an Enum can be NULL. An Enum can be contained in Nullable type. So if you create a table using the query CREATE TABLE t_enum_nullable ( x Nullable( Enum8('hello' = 1, 'world' = 2) ) ) ENGINE = TinyLog  it can store not only 'hello' and 'world', but NULL, as well. INSERT INTO t_enum_nullable Values('hello'),('world'),(NULL)  In RAM, an Enum column is stored in the same way as Int8 or Int16 of the corresponding numerical values. When reading in text form, ClickHouse parses the value as a string and searches for the corresponding string from the set of Enum values. If it is not found, an exception is thrown. When reading in text format, the string is read and the corresponding numeric value is looked up. An exception will be thrown if it is not found. When writing in text form, it writes the value as the corresponding string. If column data contains garbage (numbers that are not from the valid set), an exception is thrown. When reading and writing in binary form, it works the same way as for Int8 and Int16 data types. The implicit default value is the value with the lowest number. During ORDER BY, GROUP BY, IN, DISTINCT and so on, Enums behave the same way as the corresponding numbers. For example, ORDER BY sorts them numerically. Equality and comparison operators work the same way on Enums as they do on the underlying numeric values. Enum values cannot be compared with numbers. Enums can be compared to a constant string. If the string compared to is not a valid value for the Enum, an exception will be thrown. The IN operator is supported with the Enum on the left-hand side and a set of strings on the right-hand side. The strings are the values of the corresponding Enum. Most numeric and string operations are not defined for Enum values, e.g. adding a number to an Enum or concatenating a string to an Enum. However, the Enum has a natural toString function that returns its string value. Enum values are also convertible to numeric types using the toT function, where T is a numeric type. When T corresponds to the enum’s underlying numeric type, this conversion is zero-cost. The Enum type can be changed without cost using ALTER, if only the set of values is changed. It is possible to both add and remove members of the Enum using ALTER (removing is safe only if the removed value has never been used in the table). As a safeguard, changing the numeric value of a previously defined Enum member will throw an exception. Using ALTER, it is possible to change an Enum8 to an Enum16 or vice versa, just like changing an Int8 to Int16. Original article "},{"title":"Geo Data Types","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/geo","content":"","keywords":""},{"title":"Point​","type":1,"pageTitle":"Geo Data Types","url":"en/sql-reference/data-types/geo#point-data-type","content":"Point is represented by its X and Y coordinates, stored as a Tuple(Float64, Float64). Example Query: SET allow_experimental_geo_types = 1; CREATE TABLE geo_point (p Point) ENGINE = Memory(); INSERT INTO geo_point VALUES((10, 10)); SELECT p, toTypeName(p) FROM geo_point;  Result: ┌─p─────┬─toTypeName(p)─┐ │ (10,10) │ Point │ └───────┴───────────────┘  "},{"title":"Ring​","type":1,"pageTitle":"Geo Data Types","url":"en/sql-reference/data-types/geo#ring-data-type","content":"Ring is a simple polygon without holes stored as an array of points: Array(Point). Example Query: SET allow_experimental_geo_types = 1; CREATE TABLE geo_ring (r Ring) ENGINE = Memory(); INSERT INTO geo_ring VALUES([(0, 0), (10, 0), (10, 10), (0, 10)]); SELECT r, toTypeName(r) FROM geo_ring;  Result: ┌─r─────────────────────────────┬─toTypeName(r)─┐ │ [(0,0),(10,0),(10,10),(0,10)] │ Ring │ └───────────────────────────────┴───────────────┘  "},{"title":"Polygon​","type":1,"pageTitle":"Geo Data Types","url":"en/sql-reference/data-types/geo#polygon-data-type","content":"Polygon is a polygon with holes stored as an array of rings: Array(Ring). First element of outer array is the outer shape of polygon and all the following elements are holes. Example This is a polygon with one hole: SET allow_experimental_geo_types = 1; CREATE TABLE geo_polygon (pg Polygon) ENGINE = Memory(); INSERT INTO geo_polygon VALUES([[(20, 20), (50, 20), (50, 50), (20, 50)], [(30, 30), (50, 50), (50, 30)]]); SELECT pg, toTypeName(pg) FROM geo_polygon;  Result: ┌─pg────────────────────────────────────────────────────────────┬─toTypeName(pg)─┐ │ [[(20,20),(50,20),(50,50),(20,50)],[(30,30),(50,50),(50,30)]] │ Polygon │ └───────────────────────────────────────────────────────────────┴────────────────┘  "},{"title":"MultiPolygon​","type":1,"pageTitle":"Geo Data Types","url":"en/sql-reference/data-types/geo#multipolygon-data-type","content":"MultiPolygon consists of multiple polygons and is stored as an array of polygons: Array(Polygon). Example This multipolygon consists of two separate polygons — the first one without holes, and the second with one hole: SET allow_experimental_geo_types = 1; CREATE TABLE geo_multipolygon (mpg MultiPolygon) ENGINE = Memory(); INSERT INTO geo_multipolygon VALUES([[[(0, 0), (10, 0), (10, 10), (0, 10)]], [[(20, 20), (50, 20), (50, 50), (20, 50)],[(30, 30), (50, 50), (50, 30)]]]); SELECT mpg, toTypeName(mpg) FROM geo_multipolygon;  Result: ┌─mpg─────────────────────────────────────────────────────────────────────────────────────────────┬─toTypeName(mpg)─┐ │ [[[(0,0),(10,0),(10,10),(0,10)]],[[(20,20),(50,20),(50,50),(20,50)],[(30,30),(50,50),(50,30)]]] │ MultiPolygon │ └─────────────────────────────────────────────────────────────────────────────────────────────────┴─────────────────┘  Original article "},{"title":"Map(key, value)","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/map","content":"","keywords":""},{"title":"Convert Tuple to Map Type​","type":1,"pageTitle":"Map(key, value)","url":"en/sql-reference/data-types/map#map-and-tuple","content":"You can cast Tuple() as Map() using CAST function: SELECT CAST(([1, 2, 3], ['Ready', 'Steady', 'Go']), 'Map(UInt8, String)') AS map;  ┌─map───────────────────────────┐ │ {1:'Ready',2:'Steady',3:'Go'} │ └───────────────────────────────┘  "},{"title":"Map.keys and Map.values Subcolumns​","type":1,"pageTitle":"Map(key, value)","url":"en/sql-reference/data-types/map#map-subcolumns","content":"To optimize Map column processing, in some cases you can use the keys and values subcolumns instead of reading the whole column. Example Query: CREATE TABLE t_map (`a` Map(String, UInt64)) ENGINE = Memory; INSERT INTO t_map VALUES (map('key1', 1, 'key2', 2, 'key3', 3)); SELECT a.keys FROM t_map; SELECT a.values FROM t_map;  Result: ┌─a.keys─────────────────┐ │ ['key1','key2','key3'] │ └────────────────────────┘ ┌─a.values─┐ │ [1,2,3] │ └──────────┘  See Also map() functionCAST() function Original article "},{"title":"ANSI SQL Compatibility of ClickHouse SQL Dialect","type":0,"sectionRef":"#","url":"en/sql-reference/ansi","content":"","keywords":""},{"title":"Differences in Behaviour​","type":1,"pageTitle":"ANSI SQL Compatibility of ClickHouse SQL Dialect","url":"en/sql-reference/ansi#differences-in-behaviour","content":"The following table lists cases when query feature works in ClickHouse, but behaves not as specified in ANSI SQL. Feature ID\tFeature Name\tDifferenceE011\tNumeric data types\tNumeric literal with period is interpreted as approximate (Float64) instead of exact (Decimal) E051-05\tSelect items can be renamed\tItem renames have a wider visibility scope than just the SELECT result E141-01\tNOT NULL constraints\tNOT NULL is implied for table columns by default E011-04\tArithmetic operators\tClickHouse overflows instead of checked arithmetic and changes the result data type based on custom rules "},{"title":"Feature Status​","type":1,"pageTitle":"ANSI SQL Compatibility of ClickHouse SQL Dialect","url":"en/sql-reference/ansi#feature-status","content":"Feature ID\tFeature Name\tStatus\tCommentE011\tNumeric data types\tPartial E011-01\tINTEGER and SMALLINT data types\tYes E011-02\tREAL, DOUBLE PRECISION and FLOAT data types data types\tYes E011-03\tDECIMAL and NUMERIC data types\tYes E011-04\tArithmetic operators\tYes E011-05\tNumeric comparison\tYes E011-06\tImplicit casting among the numeric data types\tNo\tANSI SQL allows arbitrary implicit cast between numeric types, while ClickHouse relies on functions having multiple overloads instead of implicit cast E021\tCharacter string types\tPartial E021-01\tCHARACTER data type\tYes E021-02\tCHARACTER VARYING data type\tYes E021-03\tCharacter literals\tYes E021-04\tCHARACTER_LENGTH function\tPartial\tNo USING clause E021-05\tOCTET_LENGTH function\tNo\tLENGTH behaves similarly E021-06\tSUBSTRING\tPartial\tNo support for SIMILAR and ESCAPE clauses, no SUBSTRING_REGEX variant E021-07\tCharacter concatenation\tPartial\tNo COLLATE clause E021-08\tUPPER and LOWER functions\tYes E021-09\tTRIM function\tYes E021-10\tImplicit casting among the fixed-length and variable-length character string types\tPartial\tANSI SQL allows arbitrary implicit cast between string types, while ClickHouse relies on functions having multiple overloads instead of implicit cast E021-11\tPOSITION function\tPartial\tNo support for IN and USING clauses, no POSITION_REGEX variant E021-12\tCharacter comparison\tYes E031\tIdentifiers\tPartial E031-01\tDelimited identifiers\tPartial\tUnicode literal support is limited E031-02\tLower case identifiers\tYes E031-03\tTrailing underscore\tYes E051\tBasic query specification\tPartial E051-01\tSELECT DISTINCT\tYes E051-02\tGROUP BY clause\tYes E051-04\tGROUP BY can contain columns not in &lt;select list&gt;\tYes E051-05\tSelect items can be renamed\tYes E051-06\tHAVING clause\tYes E051-07\tQualified * in select list\tYes E051-08\tCorrelation name in the FROM clause\tYes E051-09\tRename columns in the FROM clause\tNo E061\tBasic predicates and search conditions\tPartial E061-01\tComparison predicate\tYes E061-02\tBETWEEN predicate\tPartial\tNo SYMMETRIC and ASYMMETRIC clause E061-03\tIN predicate with list of values\tYes E061-04\tLIKE predicate\tYes E061-05\tLIKE predicate: ESCAPE clause\tNo E061-06\tNULL predicate\tYes E061-07\tQuantified comparison predicate\tNo E061-08\tEXISTS predicate\tNo E061-09\tSubqueries in comparison predicate\tYes E061-11\tSubqueries in IN predicate\tYes E061-12\tSubqueries in quantified comparison predicate\tNo E061-13\tCorrelated subqueries\tNo E061-14\tSearch condition\tYes E071\tBasic query expressions\tPartial E071-01\tUNION DISTINCT table operator\tYes E071-02\tUNION ALL table operator\tYes E071-03\tEXCEPT DISTINCT table operator\tNo E071-05\tColumns combined via table operators need not have exactly the same data type\tYes E071-06\tTable operators in subqueries\tYes E081\tBasic privileges\tYes E081-01\tSELECT privilege at the table level\tYes E081-02\tDELETE privilege E081-03\tINSERT privilege at the table level\tYes E081-04\tUPDATE privilege at the table level\tYes E081-05\tUPDATE privilege at the column level E081-06\tREFERENCES privilege at the table level E081-07\tREFERENCES privilege at the column level E081-08\tWITH GRANT OPTION\tYes E081-09\tUSAGE privilege E081-10\tEXECUTE privilege E091\tSet functions\tYes E091-01\tAVG\tYes E091-02\tCOUNT\tYes E091-03\tMAX\tYes E091-04\tMIN\tYes E091-05\tSUM\tYes E091-06\tALL quantifier\tYes E091-07\tDISTINCT quantifier\tYes\tNot all aggregate functions supported E101\tBasic data manipulation\tPartial E101-01\tINSERT statement\tYes\tNote: primary key in ClickHouse does not imply the UNIQUE constraint E101-03\tSearched UPDATE statement\tPartial\tThere’s an ALTER UPDATE statement for batch data modification E101-04\tSearched DELETE statement\tPartial\tThere’s an ALTER DELETE statement for batch data removal E111\tSingle row SELECT statement\tNo E121\tBasic cursor support\tNo E121-01\tDECLARE CURSOR\tNo E121-02\tORDER BY columns need not be in select list\tYes E121-03\tValue expressions in ORDER BY clause\tYes E121-04\tOPEN statement\tNo E121-06\tPositioned UPDATE statement\tNo E121-07\tPositioned DELETE statement\tNo E121-08\tCLOSE statement\tNo E121-10\tFETCH statement: implicit NEXT\tNo E121-17\tWITH HOLD cursors\tNo E131\tNull value support (nulls in lieu of values)\tYes\tSome restrictions apply E141\tBasic integrity constraints\tPartial E141-01\tNOT NULL constraints\tYes\tNote: NOT NULL is implied for table columns by default E141-02\tUNIQUE constraint of NOT NULL columns\tNo E141-03\tPRIMARY KEY constraints\tPartial E141-04\tBasic FOREIGN KEY constraint with the NO ACTION default for both referential delete action and referential update action\tNo E141-06\tCHECK constraint\tYes E141-07\tColumn defaults\tYes E141-08\tNOT NULL inferred on PRIMARY KEY\tYes E141-10\tNames in a foreign key can be specified in any order\tNo E151\tTransaction support\tNo E151-01\tCOMMIT statement\tNo E151-02\tROLLBACK statement\tNo E152\tBasic SET TRANSACTION statement\tNo E152-01\tSET TRANSACTION statement: ISOLATION LEVEL SERIALIZABLE clause\tNo E152-02\tSET TRANSACTION statement: READ ONLY and READ WRITE clauses\tNo E153\tUpdatable queries with subqueries\tYes E161\tSQL comments using leading double minus\tYes E171\tSQLSTATE support\tNo E182\tHost language binding\tNo F031\tBasic schema manipulation\tPartial F031-01\tCREATE TABLE statement to create persistent base tables\tPartial\tNo SYSTEM VERSIONING, ON COMMIT, GLOBAL, LOCAL, PRESERVE, DELETE, REF IS, WITH OPTIONS, UNDER, LIKE, PERIOD FOR clauses and no support for user resolved data types F031-02\tCREATE VIEW statement\tPartial\tNo RECURSIVE, CHECK, UNDER, WITH OPTIONS clauses and no support for user resolved data types F031-03\tGRANT statement\tYes F031-04\tALTER TABLE statement: ADD COLUMN clause\tYes\tNo support for GENERATED clause and system time period F031-13\tDROP TABLE statement: RESTRICT clause\tNo F031-16\tDROP VIEW statement: RESTRICT clause\tNo F031-19\tREVOKE statement: RESTRICT clause\tNo F041\tBasic joined table\tPartial F041-01\tInner join (but not necessarily the INNER keyword)\tYes F041-02\tINNER keyword\tYes F041-03\tLEFT OUTER JOIN\tYes F041-04\tRIGHT OUTER JOIN\tYes F041-05\tOuter joins can be nested\tYes F041-07\tThe inner table in a left or right outer join can also be used in an inner join\tYes F041-08\tAll comparison operators are supported (rather than just =)\tNo F051\tBasic date and time\tPartial F051-01\tDATE data type (including support of DATE literal)\tYes F051-02\tTIME data type (including support of TIME literal) with fractional seconds precision of at least 0\tNo F051-03\tTIMESTAMP data type (including support of TIMESTAMP literal) with fractional seconds precision of at least 0 and 6\tYes F051-04\tComparison predicate on DATE, TIME, and TIMESTAMP data types\tYes F051-05\tExplicit CAST between datetime types and character string types\tYes F051-06\tCURRENT_DATE\tNo\ttoday() is similar F051-07\tLOCALTIME\tNo\tnow() is similar F051-08\tLOCALTIMESTAMP\tNo F081\tUNION and EXCEPT in views\tPartial F131\tGrouped operations\tPartial F131-01\tWHERE, GROUP BY, and HAVING clauses supported in queries with grouped views\tYes F131-02\tMultiple tables supported in queries with grouped views\tYes F131-03\tSet functions supported in queries with grouped views\tYes F131-04\tSubqueries with GROUP BY and HAVING clauses and grouped views\tYes F131-05\tSingle row SELECT with GROUP BY and HAVING clauses and grouped views\tNo F181\tMultiple module support\tNo F201\tCAST function\tYes F221\tExplicit defaults\tNo F261\tCASE expression\tYes F261-01\tSimple CASE\tYes F261-02\tSearched CASE\tYes F261-03\tNULLIF\tYes F261-04\tCOALESCE\tYes F311\tSchema definition statement\tPartial F311-01\tCREATE SCHEMA\tPartial\tSee CREATE DATABASE F311-02\tCREATE TABLE for persistent base tables\tYes F311-03\tCREATE VIEW\tYes F311-04\tCREATE VIEW: WITH CHECK OPTION\tNo F311-05\tGRANT statement\tYes F471\tScalar subquery values\tYes F481\tExpanded NULL predicate\tYes F812\tBasic flagging\tNo S011\tDistinct data types T321\tBasic SQL-invoked routines\tNo T321-01\tUser-defined functions with no overloading\tNo T321-02\tUser-defined stored procedures with no overloading\tNo T321-03\tFunction invocation\tNo T321-04\tCALL statement\tNo T321-05\tRETURN statement\tNo T631\tIN predicate with one list element\tYes\t "},{"title":"Nested","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/nested-data-structures/nested","content":"","keywords":""},{"title":"Nested(name1 Type1, Name2 Type2, …)​","type":1,"pageTitle":"Nested","url":"en/sql-reference/data-types/nested-data-structures/nested#nestedname1-type1-name2-type2","content":"A nested data structure is like a table inside a cell. The parameters of a nested data structure – the column names and types – are specified the same way as in a CREATE TABLE query. Each table row can correspond to any number of rows in a nested data structure. Example: CREATE TABLE test.visits ( CounterID UInt32, StartDate Date, Sign Int8, IsNew UInt8, VisitID UInt64, UserID UInt64, ... Goals Nested ( ID UInt32, Serial UInt32, EventTime DateTime, Price Int64, OrderID String, CurrencyID UInt32 ), ... ) ENGINE = CollapsingMergeTree(StartDate, intHash32(UserID), (CounterID, StartDate, intHash32(UserID), VisitID), 8192, Sign)  This example declares the Goals nested data structure, which contains data about conversions (goals reached). Each row in the ‘visits’ table can correspond to zero or any number of conversions. When flatten_nested is set to 0 (which is not by default), arbitrary levels of nesting are supported. In most cases, when working with a nested data structure, its columns are specified with column names separated by a dot. These columns make up an array of matching types. All the column arrays of a single nested data structure have the same length. Example: SELECT Goals.ID, Goals.EventTime FROM test.visits WHERE CounterID = 101500 AND length(Goals.ID) &lt; 5 LIMIT 10  ┌─Goals.ID───────────────────────┬─Goals.EventTime───────────────────────────────────────────────────────────────────────────┐ │ [1073752,591325,591325] │ ['2014-03-17 16:38:10','2014-03-17 16:38:48','2014-03-17 16:42:27'] │ │ [1073752] │ ['2014-03-17 00:28:25'] │ │ [1073752] │ ['2014-03-17 10:46:20'] │ │ [1073752,591325,591325,591325] │ ['2014-03-17 13:59:20','2014-03-17 22:17:55','2014-03-17 22:18:07','2014-03-17 22:18:51'] │ │ [] │ [] │ │ [1073752,591325,591325] │ ['2014-03-17 11:37:06','2014-03-17 14:07:47','2014-03-17 14:36:21'] │ │ [] │ [] │ │ [] │ [] │ │ [591325,1073752] │ ['2014-03-17 00:46:05','2014-03-17 00:46:05'] │ │ [1073752,591325,591325,591325] │ ['2014-03-17 13:28:33','2014-03-17 13:30:26','2014-03-17 18:51:21','2014-03-17 18:51:45'] │ └────────────────────────────────┴───────────────────────────────────────────────────────────────────────────────────────────┘  It is easiest to think of a nested data structure as a set of multiple column arrays of the same length. The only place where a SELECT query can specify the name of an entire nested data structure instead of individual columns is the ARRAY JOIN clause. For more information, see “ARRAY JOIN clause”. Example: SELECT Goal.ID, Goal.EventTime FROM test.visits ARRAY JOIN Goals AS Goal WHERE CounterID = 101500 AND length(Goals.ID) &lt; 5 LIMIT 10  ┌─Goal.ID─┬──────Goal.EventTime─┐ │ 1073752 │ 2014-03-17 16:38:10 │ │ 591325 │ 2014-03-17 16:38:48 │ │ 591325 │ 2014-03-17 16:42:27 │ │ 1073752 │ 2014-03-17 00:28:25 │ │ 1073752 │ 2014-03-17 10:46:20 │ │ 1073752 │ 2014-03-17 13:59:20 │ │ 591325 │ 2014-03-17 22:17:55 │ │ 591325 │ 2014-03-17 22:18:07 │ │ 591325 │ 2014-03-17 22:18:51 │ │ 1073752 │ 2014-03-17 11:37:06 │ └─────────┴─────────────────────┘  You can’t perform SELECT for an entire nested data structure. You can only explicitly list individual columns that are part of it. For an INSERT query, you should pass all the component column arrays of a nested data structure separately (as if they were individual column arrays). During insertion, the system checks that they have the same length. For a DESCRIBE query, the columns in a nested data structure are listed separately in the same way. The ALTER query for elements in a nested data structure has limitations. Original article "},{"title":"Multiword Types","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/multiword-types","content":"","keywords":""},{"title":"Multiword Types Support​","type":1,"pageTitle":"Multiword Types","url":"en/sql-reference/data-types/multiword-types#multiword-types-support","content":"Multiword types\tSimple typesDOUBLE PRECISION\tFloat64 CHAR LARGE OBJECT\tString CHAR VARYING\tString CHARACTER LARGE OBJECT\tString CHARACTER VARYING\tString NCHAR LARGE OBJECT\tString NCHAR VARYING\tString NATIONAL CHARACTER LARGE OBJECT\tString NATIONAL CHARACTER VARYING\tString NATIONAL CHAR VARYING\tString NATIONAL CHARACTER\tString NATIONAL CHAR\tString BINARY LARGE OBJECT\tString BINARY VARYING\tString Original article "},{"title":"Nullable(typename)","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/nullable","content":"","keywords":""},{"title":"Storage Features​","type":1,"pageTitle":"Nullable(typename)","url":"en/sql-reference/data-types/nullable#storage-features","content":"To store Nullable type values in a table column, ClickHouse uses a separate file with NULL masks in addition to normal file with values. Entries in masks file allow ClickHouse to distinguish between NULL and a default value of corresponding data type for each table row. Because of an additional file, Nullable column consumes additional storage space compared to a similar normal one. note Using Nullable almost always negatively affects performance, keep this in mind when designing your databases. "},{"title":"Finding NULL​","type":1,"pageTitle":"Nullable(typename)","url":"en/sql-reference/data-types/nullable#finding-null","content":"It is possible to find NULL values in a column by using null subcolumn without reading the whole column. It returns 1 if the corresponding value is NULL and 0 otherwise. Example Query: CREATE TABLE nullable (`n` Nullable(UInt32)) ENGINE = MergeTree ORDER BY tuple(); INSERT INTO nullable VALUES (1) (NULL) (2) (NULL); SELECT n.null FROM nullable;  Result: ┌─n.null─┐ │ 0 │ │ 1 │ │ 0 │ │ 1 │ └────────┘  "},{"title":"Usage Example​","type":1,"pageTitle":"Nullable(typename)","url":"en/sql-reference/data-types/nullable#usage-example","content":"CREATE TABLE t_null(x Int8, y Nullable(Int8)) ENGINE TinyLog  INSERT INTO t_null VALUES (1, NULL), (2, 3)  SELECT x + y FROM t_null  ┌─plus(x, y)─┐ │ ᴺᵁᴸᴸ │ │ 5 │ └────────────┘  Original article "},{"title":"SimpleAggregateFunction","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/simpleaggregatefunction","content":"SimpleAggregateFunction SimpleAggregateFunction(name, types_of_arguments…) data type stores current value of the aggregate function, and does not store its full state as AggregateFunction does. This optimization can be applied to functions for which the following property holds: the result of applying a function f to a row set S1 UNION ALL S2 can be obtained by applying f to parts of the row set separately, and then again applying f to the results: f(S1 UNION ALL S2) = f(f(S1) UNION ALL f(S2)). This property guarantees that partial aggregation results are enough to compute the combined one, so we do not have to store and process any extra data. The common way to produce an aggregate function value is by calling the aggregate function with the -SimpleState suffix. The following aggregate functions are supported: anyanyLastminmaxsumsumWithOverflowgroupBitAndgroupBitOrgroupBitXorgroupArrayArraygroupUniqArrayArraysumMapminMapmaxMap note Values of the SimpleAggregateFunction(func, Type) look and stored the same way as Type, so you do not need to apply functions with -Merge/-State suffixes. SimpleAggregateFunction has better performance than AggregateFunction with same aggregation function. Parameters Name of the aggregate function.Types of the aggregate function arguments. Example CREATE TABLE simple (id UInt64, val SimpleAggregateFunction(sum, Double)) ENGINE=AggregatingMergeTree ORDER BY id; Original article","keywords":""},{"title":"Special Data Types","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/special-data-types/","content":"Special Data Types Special data type values can’t be serialized for saving in a table or output in query results, but can be used as an intermediate result during query execution. Original article","keywords":""},{"title":"Expression","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/special-data-types/expression","content":"Expression Expressions are used for representing lambdas in high-order functions. Original article","keywords":""},{"title":"Interval","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/special-data-types/interval","content":"","keywords":""},{"title":"Usage Remarks​","type":1,"pageTitle":"Interval","url":"en/sql-reference/data-types/special-data-types/interval#data-type-interval-usage-remarks","content":"You can use Interval-type values in arithmetical operations with Date and DateTime-type values. For example, you can add 4 days to the current time: SELECT now() as current_date_time, current_date_time + INTERVAL 4 DAY  ┌───current_date_time─┬─plus(now(), toIntervalDay(4))─┐ │ 2019-10-23 10:58:45 │ 2019-10-27 10:58:45 │ └─────────────────────┴───────────────────────────────┘  Intervals with different types can’t be combined. You can’t use intervals like 4 DAY 1 HOUR. Specify intervals in units that are smaller or equal to the smallest unit of the interval, for example, the interval 1 day and an hour interval can be expressed as 25 HOUR or 90000 SECOND. You can’t perform arithmetical operations with Interval-type values, but you can add intervals of different types consequently to values in Date or DateTime data types. For example: SELECT now() AS current_date_time, current_date_time + INTERVAL 4 DAY + INTERVAL 3 HOUR  ┌───current_date_time─┬─plus(plus(now(), toIntervalDay(4)), toIntervalHour(3))─┐ │ 2019-10-23 11:16:28 │ 2019-10-27 14:16:28 │ └─────────────────────┴────────────────────────────────────────────────────────┘  The following query causes an exception: select now() AS current_date_time, current_date_time + (INTERVAL 4 DAY + INTERVAL 3 HOUR)  Received exception from server (version 19.14.1): Code: 43. DB::Exception: Received from localhost:9000. DB::Exception: Wrong argument types for function plus: if one argument is Interval, then another must be Date or DateTime..  "},{"title":"See Also​","type":1,"pageTitle":"Interval","url":"en/sql-reference/data-types/special-data-types/interval#see-also","content":"INTERVAL operatortoInterval type conversion functions "},{"title":"Nothing","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/special-data-types/nothing","content":"Nothing The only purpose of this data type is to represent cases where a value is not expected. So you can’t create a Nothing type value. For example, literal NULL has type of Nullable(Nothing). See more about Nullable. The Nothing type can also used to denote empty arrays: SELECT toTypeName(array()) ┌─toTypeName(array())─┐ │ Array(Nothing) │ └─────────────────────┘ Original article","keywords":""},{"title":"Set","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/special-data-types/set","content":"Set Used for the right half of an IN expression. Original article","keywords":""},{"title":"String","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/string","content":"","keywords":""},{"title":"Encodings​","type":1,"pageTitle":"String","url":"en/sql-reference/data-types/string#encodings","content":"ClickHouse does not have the concept of encodings. Strings can contain an arbitrary set of bytes, which are stored and output as-is. If you need to store texts, we recommend using UTF-8 encoding. At the very least, if your terminal uses UTF-8 (as recommended), you can read and write your values without making conversions. Similarly, certain functions for working with strings have separate variations that work under the assumption that the string contains a set of bytes representing a UTF-8 encoded text. For example, the length function calculates the string length in bytes, while the lengthUTF8 function calculates the string length in Unicode code points, assuming that the value is UTF-8 encoded. Original article "},{"title":"Tuple(t1, T2, …)","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/tuple","content":"","keywords":""},{"title":"Creating a Tuple​","type":1,"pageTitle":"Tuple(t1, T2, …)","url":"en/sql-reference/data-types/tuple#creating-a-tuple","content":"You can use a function to create a tuple: tuple(T1, T2, ...)  Example of creating a tuple: SELECT tuple(1,'a') AS x, toTypeName(x)  ┌─x───────┬─toTypeName(tuple(1, 'a'))─┐ │ (1,'a') │ Tuple(UInt8, String) │ └─────────┴───────────────────────────┘  "},{"title":"Working with Data Types​","type":1,"pageTitle":"Tuple(t1, T2, …)","url":"en/sql-reference/data-types/tuple#working-with-data-types","content":"When creating a tuple on the fly, ClickHouse automatically detects the type of each argument as the minimum of the types which can store the argument value. If the argument is NULL, the type of the tuple element is Nullable. Example of automatic data type detection: SELECT tuple(1, NULL) AS x, toTypeName(x)  ┌─x────────┬─toTypeName(tuple(1, NULL))──────┐ │ (1,NULL) │ Tuple(UInt8, Nullable(Nothing)) │ └──────────┴─────────────────────────────────┘  "},{"title":"Addressing Tuple Elements​","type":1,"pageTitle":"Tuple(t1, T2, …)","url":"en/sql-reference/data-types/tuple#addressing-tuple-elements","content":"It is possible to read elements of named tuples using indexes and names: CREATE TABLE named_tuples (`a` Tuple(s String, i Int64)) ENGINE = Memory; INSERT INTO named_tuples VALUES (('y', 10)), (('x',-10)); SELECT a.s FROM named_tuples; SELECT a.2 FROM named_tuples;  Result: ┌─a.s─┐ │ y │ │ x │ └─────┘ ┌─tupleElement(a, 2)─┐ │ 10 │ │ -10 │ └────────────────────┘  Original article "},{"title":"UUID","type":0,"sectionRef":"#","url":"en/sql-reference/data-types/uuid","content":"","keywords":""},{"title":"How to Generate​","type":1,"pageTitle":"UUID","url":"en/sql-reference/data-types/uuid#how-to-generate","content":"To generate the UUID value, ClickHouse provides the generateUUIDv4 function. "},{"title":"Usage Example​","type":1,"pageTitle":"UUID","url":"en/sql-reference/data-types/uuid#usage-example","content":"Example 1 This example demonstrates creating a table with the UUID type column and inserting a value into the table. CREATE TABLE t_uuid (x UUID, y String) ENGINE=TinyLog  INSERT INTO t_uuid SELECT generateUUIDv4(), 'Example 1'  SELECT * FROM t_uuid  ┌────────────────────────────────────x─┬─y─────────┐ │ 417ddc5d-e556-4d27-95dd-a34d84e46a50 │ Example 1 │ └──────────────────────────────────────┴───────────┘  Example 2 In this example, the UUID column value is not specified when inserting a new record. INSERT INTO t_uuid (y) VALUES ('Example 2')  SELECT * FROM t_uuid  ┌────────────────────────────────────x─┬─y─────────┐ │ 417ddc5d-e556-4d27-95dd-a34d84e46a50 │ Example 1 │ │ 00000000-0000-0000-0000-000000000000 │ Example 2 │ └──────────────────────────────────────┴───────────┘  "},{"title":"Restrictions​","type":1,"pageTitle":"UUID","url":"en/sql-reference/data-types/uuid#restrictions","content":"The UUID data type only supports functions which String data type also supports (for example, min, max, and count). The UUID data type is not supported by arithmetic operations (for example, abs) or aggregate functions, such as sum and avg. Original article "},{"title":"Dictionaries","type":0,"sectionRef":"#","url":"en/sql-reference/dictionaries/","content":"Dictionaries A dictionary is a mapping (key -&gt; attributes) that is convenient for various types of reference lists. ClickHouse supports special functions for working with dictionaries that can be used in queries. It is easier and more efficient to use dictionaries with functions than a JOIN with reference tables. ClickHouse supports: Built-in dictionaries with a specific set of functions.Plug-in (external) dictionaries with a set of functions.","keywords":""},{"title":"External Dictionaries","type":0,"sectionRef":"#","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts","content":"","keywords":""},{"title":"See Also​","type":1,"pageTitle":"External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts#ext-dicts-see-also","content":"Configuring an External DictionaryStoring Dictionaries in MemoryDictionary UpdatesSources of External DictionariesDictionary Key and FieldsFunctions for Working with External Dictionaries "},{"title":"Configuring an External Dictionary","type":0,"sectionRef":"#","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict","content":"Configuring an External Dictionary If dictionary is configured using xml file, than dictionary configuration has the following structure: &lt;dictionary&gt; &lt;name&gt;dict_name&lt;/name&gt; &lt;structure&gt; &lt;!-- Complex key configuration --&gt; &lt;/structure&gt; &lt;source&gt; &lt;!-- Source configuration --&gt; &lt;/source&gt; &lt;layout&gt; &lt;!-- Memory layout configuration --&gt; &lt;/layout&gt; &lt;lifetime&gt; &lt;!-- Lifetime of dictionary in memory --&gt; &lt;/lifetime&gt; &lt;/dictionary&gt; Corresponding DDL-query has the following structure: CREATE DICTIONARY dict_name ( ... -- attributes ) PRIMARY KEY ... -- complex or single key configuration SOURCE(...) -- Source configuration LAYOUT(...) -- Memory layout configuration LIFETIME(...) -- Lifetime of dictionary in memory name – The identifier that can be used to access the dictionary. Use the characters [a-zA-Z0-9_\\-].source — Source of the dictionary.layout — Dictionary layout in memory.structure — Structure of the dictionary . A key and attributes that can be retrieved by this key.lifetime — Frequency of dictionary updates.","keywords":""},{"title":"Hierarchical Dictionaries","type":0,"sectionRef":"#","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-hierarchical","content":"Hierarchical Dictionaries ClickHouse supports hierarchical dictionaries with a numeric key. Look at the following hierarchical structure: 0 (Common parent) │ ├── 1 (Russia) │ │ │ └── 2 (Moscow) │ │ │ └── 3 (Center) │ └── 4 (Great Britain) │ └── 5 (London) This hierarchy can be expressed as the following dictionary table. region_id\tparent_region\tregion_name1\t0\tRussia 2\t1\tMoscow 3\t2\tCenter 4\t0\tGreat Britain 5\t4\tLondon This table contains a column parent_region that contains the key of the nearest parent for the element. ClickHouse supports the hierarchical property for external dictionary attributes. This property allows you to configure the hierarchical dictionary similar to described above. The dictGetHierarchy function allows you to get the parent chain of an element. For our example, the structure of dictionary can be the following: &lt;dictionary&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;region_id&lt;/name&gt; &lt;/id&gt; &lt;attribute&gt; &lt;name&gt;parent_region&lt;/name&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;null_value&gt;0&lt;/null_value&gt; &lt;hierarchical&gt;true&lt;/hierarchical&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;region_name&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;/dictionary&gt; ","keywords":""},{"title":"Internal Dictionaries","type":0,"sectionRef":"#","url":"en/sql-reference/dictionaries/internal-dicts","content":"Internal Dictionaries ClickHouse contains a built-in feature for working with a geobase. This allows you to: Use a region’s ID to get its name in the desired language.Use a region’s ID to get the ID of a city, area, federal district, country, or continent.Check whether a region is part of another region.Get a chain of parent regions. All the functions support “translocality,” the ability to simultaneously use different perspectives on region ownership. For more information, see the section “Functions for working with web analytics dictionaries”. The internal dictionaries are disabled in the default package. To enable them, uncomment the parameters path_to_regions_hierarchy_file and path_to_regions_names_files in the server configuration file. The geobase is loaded from text files. Place the regions_hierarchy*.txt files into the path_to_regions_hierarchy_file directory. This configuration parameter must contain the path to the regions_hierarchy.txt file (the default regional hierarchy), and the other files (regions_hierarchy_ua.txt) must be located in the same directory. Put the regions_names_*.txt files in the path_to_regions_names_files directory. You can also create these files yourself. The file format is as follows: regions_hierarchy*.txt: TabSeparated (no header), columns: region ID (UInt32)parent region ID (UInt32)region type (UInt8): 1 - continent, 3 - country, 4 - federal district, 5 - region, 6 - city; other types do not have valuespopulation (UInt32) — optional column regions_names_*.txt: TabSeparated (no header), columns: region ID (UInt32)region name (String) — Can’t contain tabs or line feeds, even escaped ones. A flat array is used for storing in RAM. For this reason, IDs shouldn’t be more than a million. Dictionaries can be updated without restarting the server. However, the set of available dictionaries is not updated. For updates, the file modification times are checked. If a file has changed, the dictionary is updated. The interval to check for changes is configured in the builtin_dictionaries_reload_interval parameter. Dictionary updates (other than loading at first use) do not block queries. During updates, queries use the old versions of dictionaries. If an error occurs during an update, the error is written to the server log, and queries continue using the old version of dictionaries. We recommend periodically updating the dictionaries with the geobase. During an update, generate new files and write them to a separate location. When everything is ready, rename them to the files used by the server. There are also functions for working with OS identifiers and search engines, but they shouldn’t be used.","keywords":""},{"title":"Distributed DDL Queries (ON CLUSTER Clause)","type":0,"sectionRef":"#","url":"en/sql-reference/distributed-ddl","content":"Distributed DDL Queries (ON CLUSTER Clause) By default the CREATE, DROP, ALTER, and RENAME queries affect only the current server where they are executed. In a cluster setup, it is possible to run such queries in a distributed manner with the ON CLUSTER clause. For example, the following query creates the all_hits Distributed table on each host in cluster: CREATE TABLE IF NOT EXISTS all_hits ON CLUSTER cluster (p Date, i Int32) ENGINE = Distributed(cluster, default, hits) In order to run these queries correctly, each host must have the same cluster definition (to simplify syncing configs, you can use substitutions from ZooKeeper). They must also connect to the ZooKeeper servers. The local version of the query will eventually be executed on each host in the cluster, even if some hosts are currently not available. warning The order for executing queries within a single host is guaranteed.","keywords":""},{"title":"arrayJoin function","type":0,"sectionRef":"#","url":"en/sql-reference/functions/array-join","content":"arrayJoin function This is a very unusual function. Normal functions do not change a set of rows, but just change the values in each row (map). Aggregate functions compress a set of rows (fold or reduce). The ‘arrayJoin’ function takes each row and generates a set of rows (unfold). This function takes an array as an argument, and propagates the source row to multiple rows for the number of elements in the array. All the values in columns are simply copied, except the values in the column where this function is applied; it is replaced with the corresponding array value. A query can use multiple arrayJoin functions. In this case, the transformation is performed multiple times. Note the ARRAY JOIN syntax in the SELECT query, which provides broader possibilities. Example: SELECT arrayJoin([1, 2, 3] AS src) AS dst, 'Hello', src ┌─dst─┬─\\'Hello\\'─┬─src─────┐ │ 1 │ Hello │ [1,2,3] │ │ 2 │ Hello │ [1,2,3] │ │ 3 │ Hello │ [1,2,3] │ └─────┴───────────┴─────────┘ ","keywords":""},{"title":"Comparison Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/comparison-functions","content":"","keywords":""},{"title":"equals, a = b and a == b operator​","type":1,"pageTitle":"Comparison Functions","url":"en/sql-reference/functions/comparison-functions#function-equals","content":""},{"title":"notEquals, a != b and a \\<> b operator​","type":1,"pageTitle":"Comparison Functions","url":"en/sql-reference/functions/comparison-functions#function-notequals","content":""},{"title":"less, \\< operator​","type":1,"pageTitle":"Comparison Functions","url":"en/sql-reference/functions/comparison-functions#function-less","content":""},{"title":"greater, > operator​","type":1,"pageTitle":"Comparison Functions","url":"en/sql-reference/functions/comparison-functions#function-greater","content":""},{"title":"lessOrEquals, \\<= operator​","type":1,"pageTitle":"Comparison Functions","url":"en/sql-reference/functions/comparison-functions#function-lessorequals","content":""},{"title":"greaterOrEquals, >= operator​","type":1,"pageTitle":"Comparison Functions","url":"en/sql-reference/functions/comparison-functions#function-greaterorequals","content":""},{"title":"Dictionary Updates","type":0,"sectionRef":"#","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-lifetime","content":"Dictionary Updates ClickHouse periodically updates the dictionaries. The update interval for fully downloaded dictionaries and the invalidation interval for cached dictionaries are defined in the lifetime tag in seconds. Dictionary updates (other than loading for first use) do not block queries. During updates, the old version of a dictionary is used. If an error occurs during an update, the error is written to the server log, and queries continue using the old version of dictionaries. Example of settings: &lt;dictionary&gt; ... &lt;lifetime&gt;300&lt;/lifetime&gt; ... &lt;/dictionary&gt; or CREATE DICTIONARY (...) ... LIFETIME(300) ... Setting &lt;lifetime&gt;0&lt;/lifetime&gt; (LIFETIME(0)) prevents dictionaries from updating. You can set a time interval for updates, and ClickHouse will choose a uniformly random time within this range. This is necessary in order to distribute the load on the dictionary source when updating on a large number of servers. Example of settings: &lt;dictionary&gt; ... &lt;lifetime&gt; &lt;min&gt;300&lt;/min&gt; &lt;max&gt;360&lt;/max&gt; &lt;/lifetime&gt; ... &lt;/dictionary&gt; or LIFETIME(MIN 300 MAX 360) If &lt;min&gt;0&lt;/min&gt; and &lt;max&gt;0&lt;/max&gt;, ClickHouse does not reload the dictionary by timeout. In this case, ClickHouse can reload the dictionary earlier if the dictionary configuration file was changed or the SYSTEM RELOAD DICTIONARY command was executed. When updating the dictionaries, the ClickHouse server applies different logic depending on the type of source: For a text file, it checks the time of modification. If the time differs from the previously recorded time, the dictionary is updated.For MySQL source, the time of modification is checked using a SHOW TABLE STATUS query (in case of MySQL 8 you need to disable meta-information caching in MySQL by set global information_schema_stats_expiry=0).Dictionaries from other sources are updated every time by default. For other sources (ODBC, PostgreSQL, ClickHouse, etc), you can set up a query that will update the dictionaries only if they really changed, rather than each time. To do this, follow these steps: The dictionary table must have a field that always changes when the source data is updated.The settings of the source must specify a query that retrieves the changing field. The ClickHouse server interprets the query result as a row, and if this row has changed relative to its previous state, the dictionary is updated. Specify the query in the &lt;invalidate_query&gt; field in the settings for the source. Example of settings: &lt;dictionary&gt; ... &lt;odbc&gt; ... &lt;invalidate_query&gt;SELECT update_time FROM dictionary_source where id = 1&lt;/invalidate_query&gt; &lt;/odbc&gt; ... &lt;/dictionary&gt; or ... SOURCE(ODBC(... invalidate_query 'SELECT update_time FROM dictionary_source where id = 1')) ... For Cache, ComplexKeyCache, SSDCache, and SSDComplexKeyCache dictionaries both synchronious and asynchronious updates are supported. It is also possible for Flat, Hashed, ComplexKeyHashed dictionaries to only request data that was changed after the previous update. If update_field is specified as part of the dictionary source configuration, value of the previous update time in seconds will be added to the data request. Depends on source type (Executable, HTTP, MySQL, PostgreSQL, ClickHouse, or ODBC) different logic will be applied to update_field before request data from an external source. If the source is HTTP then update_field will be added as a query parameter with the last update time as the parameter value.If the source is Executable then update_field will be added as an executable script argument with the last update time as the argument value.If the source is ClickHouse, MySQL, PostgreSQL, ODBC there will be an additional part of WHERE, where update_field is compared as greater or equal with the last update time. If update_field option is set, additional option update_lag can be set. Value of update_lag option is subtracted from previous update time before request updated data. Example of settings: &lt;dictionary&gt; ... &lt;clickhouse&gt; ... &lt;update_field&gt;added_time&lt;/update_field&gt; &lt;update_lag&gt;15&lt;/update_lag&gt; &lt;/clickhouse&gt; ... &lt;/dictionary&gt; or ... SOURCE(CLICKHOUSE(... update_field 'added_time' update_lag 15)) ... ","keywords":""},{"title":"Functions for Working with Files","type":0,"sectionRef":"#","url":"en/sql-reference/functions/files","content":"","keywords":""},{"title":"file​","type":1,"pageTitle":"Functions for Working with Files","url":"en/sql-reference/functions/files#file","content":"Reads file as a String. The file content is not parsed, so any information is read as one string and placed into the specified column. Syntax file(path)  Arguments path — The relative path to the file from user_files_path. Path to file support following wildcards: *, ?, {abc,def} and {N..M} where N, M — numbers, 'abc', 'def' — strings. Example Inserting data from files a.txt and b.txt into a table as strings: Query: INSERT INTO table SELECT file('a.txt'), file('b.txt');  See Also user_files_pathfile "},{"title":"Polygon dictionaries","type":0,"sectionRef":"#","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-polygon","content":"Polygon dictionaries Polygon dictionaries allow you to efficiently search for the polygon containing specified points. For example: defining a city area by geographical coordinates. Example of a polygon dictionary configuration: &lt;dictionary&gt; &lt;structure&gt; &lt;key&gt; &lt;name&gt;key&lt;/name&gt; &lt;type&gt;Array(Array(Array(Array(Float64))))&lt;/type&gt; &lt;/key&gt; &lt;attribute&gt; &lt;name&gt;name&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;value&lt;/name&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;null_value&gt;0&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;layout&gt; &lt;polygon&gt; &lt;store_polygon_key_column&gt;1&lt;/store_polygon_key_column&gt; &lt;/polygon&gt; &lt;/layout&gt; ... &lt;/dictionary&gt; The corresponding DDL-query: CREATE DICTIONARY polygon_dict_name ( key Array(Array(Array(Array(Float64)))), name String, value UInt64 ) PRIMARY KEY key LAYOUT(POLYGON(STORE_POLYGON_KEY_COLUMN 1)) ... When configuring the polygon dictionary, the key must have one of two types: A simple polygon. It is an array of points.MultiPolygon. It is an array of polygons. Each polygon is a two-dimensional array of points. The first element of this array is the outer boundary of the polygon, and subsequent elements specify areas to be excluded from it. Points can be specified as an array or a tuple of their coordinates. In the current implementation, only two-dimensional points are supported. The user can upload their own data in all formats supported by ClickHouse. There are 3 types of in-memory storage available: POLYGON_SIMPLE. This is a naive implementation, where a linear pass through all polygons is made for each query, and membership is checked for each one without using additional indexes. POLYGON_INDEX_EACH. A separate index is built for each polygon, which allows you to quickly check whether it belongs in most cases (optimized for geographical regions). Also, a grid is superimposed on the area under consideration, which significantly narrows the number of polygons under consideration. The grid is created by recursively dividing the cell into 16 equal parts and is configured with two parameters. The division stops when the recursion depth reaches MAX_DEPTH or when the cell crosses no more than MIN_INTERSECTIONS polygons. To respond to the query, there is a corresponding cell, and the index for the polygons stored in it is accessed alternately. POLYGON_INDEX_CELL. This placement also creates the grid described above. The same options are available. For each sheet cell, an index is built on all pieces of polygons that fall into it, which allows you to quickly respond to a request. POLYGON. Synonym to POLYGON_INDEX_CELL. Dictionary queries are carried out using standard functions for working with external dictionaries. An important difference is that here the keys will be the points for which you want to find the polygon containing them. Example Example of working with the dictionary defined above: CREATE TABLE points ( x Float64, y Float64 ) ... SELECT tuple(x, y) AS key, dictGet(dict_name, 'name', key), dictGet(dict_name, 'value', key) FROM points ORDER BY x, y; As a result of executing the last command for each point in the 'points' table, a minimum area polygon containing this point will be found, and the requested attributes will be output. Example You can read columns from polygon dictionaries via SELECT query, just turn on the store_polygon_key_column = 1 in the dictionary configuration or corresponding DDL-query. Query: CREATE TABLE polygons_test_table ( key Array(Array(Array(Tuple(Float64, Float64)))), name String ) ENGINE = TinyLog; INSERT INTO polygons_test_table VALUES ([[[(3, 1), (0, 1), (0, -1), (3, -1)]]], 'Value'); CREATE DICTIONARY polygons_test_dictionary ( key Array(Array(Array(Tuple(Float64, Float64)))), name String ) PRIMARY KEY key SOURCE(CLICKHOUSE(TABLE 'polygons_test_table')) LAYOUT(POLYGON(STORE_POLYGON_KEY_COLUMN 1)) LIFETIME(0); SELECT * FROM polygons_test_dictionary; Result: ┌─key─────────────────────────────┬─name──┐ │ [[[(3,1),(0,1),(0,-1),(3,-1)]]] │ Value │ └─────────────────────────────────┴───────┘ ","keywords":""},{"title":"Dictionary Key and Fields","type":0,"sectionRef":"#","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure","content":"","keywords":""},{"title":"Key​","type":1,"pageTitle":"Dictionary Key and Fields","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure#ext_dict_structure-key","content":"ClickHouse supports the following types of keys: Numeric key. UInt64. Defined in the &lt;id&gt; tag or using PRIMARY KEY keyword.Composite key. Set of values of different types. Defined in the tag &lt;key&gt; or PRIMARY KEY keyword. An xml structure can contain either &lt;id&gt; or &lt;key&gt;. DDL-query must contain single PRIMARY KEY. warning You must not describe key as an attribute. "},{"title":"Numeric Key​","type":1,"pageTitle":"Dictionary Key and Fields","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure#numeric-key","content":"Type: UInt64. Configuration example: &lt;id&gt; &lt;name&gt;Id&lt;/name&gt; &lt;/id&gt;  Configuration fields: name – The name of the column with keys. For DDL-query: CREATE DICTIONARY ( Id UInt64, ... ) PRIMARY KEY Id ...  PRIMARY KEY – The name of the column with keys. "},{"title":"Composite Key​","type":1,"pageTitle":"Dictionary Key and Fields","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure#composite-key","content":"The key can be a tuple from any types of fields. The layout in this case must be complex_key_hashed or complex_key_cache. tip A composite key can consist of a single element. This makes it possible to use a string as the key, for instance. The key structure is set in the element &lt;key&gt;. Key fields are specified in the same format as the dictionary attributes. Example: &lt;structure&gt; &lt;key&gt; &lt;attribute&gt; &lt;name&gt;field1&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;field2&lt;/name&gt; &lt;type&gt;UInt32&lt;/type&gt; &lt;/attribute&gt; ... &lt;/key&gt; ...  or CREATE DICTIONARY ( field1 String, field2 String ... ) PRIMARY KEY field1, field2 ...  For a query to the dictGet* function, a tuple is passed as the key. Example: dictGetString('dict_name', 'attr_name', tuple('string for field1', num_for_field2)). "},{"title":"Attributes​","type":1,"pageTitle":"Dictionary Key and Fields","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-structure#ext_dict_structure-attributes","content":"Configuration example: &lt;structure&gt; ... &lt;attribute&gt; &lt;name&gt;Name&lt;/name&gt; &lt;type&gt;ClickHouseDataType&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;expression&gt;rand64()&lt;/expression&gt; &lt;hierarchical&gt;true&lt;/hierarchical&gt; &lt;injective&gt;true&lt;/injective&gt; &lt;is_object_id&gt;true&lt;/is_object_id&gt; &lt;/attribute&gt; &lt;/structure&gt;  or CREATE DICTIONARY somename ( Name ClickHouseDataType DEFAULT '' EXPRESSION rand64() HIERARCHICAL INJECTIVE IS_OBJECT_ID )  Configuration fields: Tag\tDescription\tRequiredname\tColumn name.\tYes type\tClickHouse data type: UInt8, UInt16, UInt32, UInt64, Int8, Int16, Int32, Int64, Float32, Float64, UUID, Decimal32, Decimal64, Decimal128, Decimal256,Date, Date32, DateTime, DateTime64, String, Array. ClickHouse tries to cast value from dictionary to the specified data type. For example, for MySQL, the field might be TEXT, VARCHAR, or BLOB in the MySQL source table, but it can be uploaded as String in ClickHouse. Nullable is currently supported for Flat, Hashed, ComplexKeyHashed, Direct, ComplexKeyDirect, RangeHashed, Polygon, Cache, ComplexKeyCache, SSDCache, SSDComplexKeyCache dictionaries. In IPTrie dictionaries Nullable types are not supported.\tYes null_value\tDefault value for a non-existing element. In the example, it is an empty string. NULL value can be used only for the Nullable types (see the previous line with types description).\tYes expression\tExpression that ClickHouse executes on the value. The expression can be a column name in the remote SQL database. Thus, you can use it to create an alias for the remote column. Default value: no expression.\tNo hierarchical\tIf true, the attribute contains the value of a parent key for the current key. See Hierarchical Dictionaries. Default value: false.\tNo injective\tFlag that shows whether the id -&gt; attribute image is injective. If true, ClickHouse can automatically place after the GROUP BY clause the requests to dictionaries with injection. Usually it significantly reduces the amount of such requests. Default value: false.\tNo is_object_id\tFlag that shows whether the query is executed for a MongoDB document by ObjectID. Default value: false.\tNo See Also Functions for working with external dictionaries. "},{"title":"Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/","content":"","keywords":""},{"title":"Strong Typing​","type":1,"pageTitle":"Functions","url":"en/sql-reference/functions/#strong-typing","content":"In contrast to standard SQL, ClickHouse has strong typing. In other words, it does not make implicit conversions between types. Each function works for a specific set of types. This means that sometimes you need to use type conversion functions. "},{"title":"Common Subexpression Elimination​","type":1,"pageTitle":"Functions","url":"en/sql-reference/functions/#common-subexpression-elimination","content":"All expressions in a query that have the same AST (the same record or same result of syntactic parsing) are considered to have identical values. Such expressions are concatenated and executed once. Identical subqueries are also eliminated this way. "},{"title":"Types of Results​","type":1,"pageTitle":"Functions","url":"en/sql-reference/functions/#types-of-results","content":"All functions return a single return as the result (not several values, and not zero values). The type of result is usually defined only by the types of arguments, not by the values. Exceptions are the tupleElement function (the a.N operator), and the toFixedString function. "},{"title":"Constants​","type":1,"pageTitle":"Functions","url":"en/sql-reference/functions/#constants","content":"For simplicity, certain functions can only work with constants for some arguments. For example, the right argument of the LIKE operator must be a constant. Almost all functions return a constant for constant arguments. The exception is functions that generate random numbers. The ‘now’ function returns different values for queries that were run at different times, but the result is considered a constant, since constancy is only important within a single query. A constant expression is also considered a constant (for example, the right half of the LIKE operator can be constructed from multiple constants). Functions can be implemented in different ways for constant and non-constant arguments (different code is executed). But the results for a constant and for a true column containing only the same value should match each other. "},{"title":"NULL Processing​","type":1,"pageTitle":"Functions","url":"en/sql-reference/functions/#null-processing","content":"Functions have the following behaviors: If at least one of the arguments of the function is NULL, the function result is also NULL.Special behavior that is specified individually in the description of each function. In the ClickHouse source code, these functions have UseDefaultImplementationForNulls=false. "},{"title":"Constancy​","type":1,"pageTitle":"Functions","url":"en/sql-reference/functions/#constancy","content":"Functions can’t change the values of their arguments – any changes are returned as the result. Thus, the result of calculating separate functions does not depend on the order in which the functions are written in the query. "},{"title":"Higher-order functions, -> operator and lambda(params, expr) function​","type":1,"pageTitle":"Functions","url":"en/sql-reference/functions/#higher-order-functions","content":"Higher-order functions can only accept lambda functions as their functional argument. To pass a lambda function to a higher-order function use -&gt; operator. The left side of the arrow has a formal parameter, which is any ID, or multiple formal parameters – any IDs in a tuple. The right side of the arrow has an expression that can use these formal parameters, as well as any table columns. Examples: x -&gt; 2 * x str -&gt; str != Referer  A lambda function that accepts multiple arguments can also be passed to a higher-order function. In this case, the higher-order function is passed several arrays of identical length that these arguments will correspond to. For some functions the first argument (the lambda function) can be omitted. In this case, identical mapping is assumed. "},{"title":"SQL User Defined Functions​","type":1,"pageTitle":"Functions","url":"en/sql-reference/functions/#user-defined-functions","content":"Custom functions from lambda expressions can be created using the CREATE FUNCTION statement. To delete these functions use the DROP FUNCTION statement. "},{"title":"Executable User Defined Functions​","type":1,"pageTitle":"Functions","url":"en/sql-reference/functions/#executable-user-defined-functions","content":"ClickHouse can call any external executable program or script to process data. The configuration of executable user defined functions can be located in one or more xml-files. The path to the configuration is specified in the user_defined_executable_functions_config parameter. A function configuration contains the following settings: name - a function name.command - script name to execute or command if execute_direct is false.argument - argument description with the type, and optional name of an argument. Each argument is described in a separate setting. Specifying name is necessary if argument names are part of serialization for user defined function format like Native or JSONEachRow. Default argument name value is c + argument_number.format - a format in which arguments are passed to the command.return_type - the type of a returned value.return_name - name of retuned value. Specifying return name is necessary if return name is part of serialization for user defined function format like Native or JSONEachRow. Optional. Default value is result.type - an executable type. If type is set to executable then single command is started. If it is set to executable_pool then a pool of commands is created.max_command_execution_time - maximum execution time in seconds for processing block of data. This setting is valid for executable_pool commands only. Optional. Default value is 10.command_termination_timeout - time in seconds during which a command should finish after its pipe is closed. After that time SIGTERM is sent to the process executing the command. Optional. Default value is 10.command_read_timeout - timeout for reading data from command stdout in milliseconds. Default value 10000. Optional parameter.command_write_timeout - timeout for writing data to command stdin in milliseconds. Default value 10000. Optional parameter.pool_size - the size of a command pool. Optional. Default value is 16.send_chunk_header - controls whether to send row count before sending a chunk of data to process. Optional. Default value is false.execute_direct - If execute_direct = 1, then command will be searched inside user_scripts folder specified by user_scripts_path. Additional script arguments can be specified using whitespace separator. Example: script_name arg1 arg2. If execute_direct = 0, command is passed as argument for bin/sh -c. Default value is 1. Optional parameter.lifetime - the reload interval of a function in seconds. If it is set to 0 then the function is not reloaded. Default value is 0. Optional parameter. The command must read arguments from STDIN and must output the result to STDOUT. The command must process arguments iteratively. That is after processing a chunk of arguments it must wait for the next chunk. Example Creating test_function using XML configuration. File test_function.xml. &lt;functions&gt; &lt;function&gt; &lt;type&gt;executable&lt;/type&gt; &lt;name&gt;test_function_python&lt;/name&gt; &lt;return_type&gt;String&lt;/return_type&gt; &lt;argument&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;name&gt;value&lt;/name&gt; &lt;/argument&gt; &lt;format&gt;TabSeparated&lt;/format&gt; &lt;command&gt;test_function.py&lt;/command&gt; &lt;/function&gt; &lt;/functions&gt;  Script file inside user_scripts folder test_function.py. #!/usr/bin/python3 import sys if __name__ == '__main__': for line in sys.stdin: print(&quot;Value &quot; + line, end='') sys.stdout.flush()  Query: SELECT test_function_python(toUInt64(2));  Result: ┌─test_function_python(2)─┐ │ Value 2 │ └─────────────────────────┘  Creating test_function_sum manually specifying execute_direct to 0 using XML configuration. File test_function.xml. &lt;functions&gt; &lt;function&gt; &lt;type&gt;executable&lt;/type&gt; &lt;name&gt;test_function_sum&lt;/name&gt; &lt;return_type&gt;UInt64&lt;/return_type&gt; &lt;argument&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;name&gt;lhs&lt;/name&gt; &lt;/argument&gt; &lt;argument&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;name&gt;rhs&lt;/name&gt; &lt;/argument&gt; &lt;format&gt;TabSeparated&lt;/format&gt; &lt;command&gt;cd /; clickhouse-local --input-format TabSeparated --output-format TabSeparated --structure 'x UInt64, y UInt64' --query &quot;SELECT x + y FROM table&quot;&lt;/command&gt; &lt;execute_direct&gt;0&lt;/execute_direct&gt; &lt;/function&gt; &lt;/functions&gt;  Query: SELECT test_function_sum(2, 2);  Result: ┌─test_function_sum(2, 2)─┐ │ 4 │ └─────────────────────────┘  Creating test_function_sum_json with named arguments and format JSONEachRow using XML configuration. File test_function.xml. &lt;function&gt; &lt;type&gt;executable&lt;/type&gt; &lt;name&gt;test_function_sum_json&lt;/name&gt; &lt;return_type&gt;UInt64&lt;/return_type&gt; &lt;return_name&gt;result_name&lt;/return_name&gt; &lt;argument&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;name&gt;argument_1&lt;/name&gt; &lt;/argument&gt; &lt;argument&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;name&gt;argument_2&lt;/name&gt; &lt;/argument&gt; &lt;format&gt;JSONEachRow&lt;/format&gt; &lt;command&gt;test_function_sum_json.py&lt;/command&gt; &lt;/function&gt;  Script file inside user_scripts folder test_function_sum_json.py. #!/usr/bin/python3 import sys import json if __name__ == '__main__': for line in sys.stdin: value = json.loads(line) first_arg = int(value['argument_1']) second_arg = int(value['argument_2']) result = {'result_name': first_arg + second_arg} print(json.dumps(result), end='\\n') sys.stdout.flush()  Query: SELECT test_function_sum_json(2, 2);  Result: ┌─test_function_sum_json(2, 2)─┐ │ 4 │ └──────────────────────────────┘  "},{"title":"Error Handling​","type":1,"pageTitle":"Functions","url":"en/sql-reference/functions/#error-handling","content":"Some functions might throw an exception if the data is invalid. In this case, the query is canceled and an error text is returned to the client. For distributed processing, when an exception occurs on one of the servers, the other servers also attempt to abort the query. "},{"title":"Evaluation of Argument Expressions​","type":1,"pageTitle":"Functions","url":"en/sql-reference/functions/#evaluation-of-argument-expressions","content":"In almost all programming languages, one of the arguments might not be evaluated for certain operators. This is usually the operators &amp;&amp;, ||, and ?:. But in ClickHouse, arguments of functions (operators) are always evaluated. This is because entire parts of columns are evaluated at once, instead of calculating each row separately. "},{"title":"Performing Functions for Distributed Query Processing​","type":1,"pageTitle":"Functions","url":"en/sql-reference/functions/#performing-functions-for-distributed-query-processing","content":"For distributed query processing, as many stages of query processing as possible are performed on remote servers, and the rest of the stages (merging intermediate results and everything after that) are performed on the requestor server. This means that functions can be performed on different servers. For example, in the query SELECT f(sum(g(x))) FROM distributed_table GROUP BY h(y), if a distributed_table has at least two shards, the functions ‘g’ and ‘h’ are performed on remote servers, and the function ‘f’ is performed on the requestor server.if a distributed_table has only one shard, all the ‘f’, ‘g’, and ‘h’ functions are performed on this shard’s server. The result of a function usually does not depend on which server it is performed on. However, sometimes this is important. For example, functions that work with dictionaries use the dictionary that exists on the server they are running on. Another example is the hostName function, which returns the name of the server it is running on in order to make GROUP BY by servers in a SELECT query. If a function in a query is performed on the requestor server, but you need to perform it on remote servers, you can wrap it in an ‘any’ aggregate function or add it to a key in GROUP BY. "},{"title":"Arithmetic Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/arithmetic-functions","content":"","keywords":""},{"title":"plus(a, b), a + b operator​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#plusa-b-a-b-operator","content":"Calculates the sum of the numbers. You can also add integer numbers with a date or date and time. In the case of a date, adding an integer means adding the corresponding number of days. For a date with time, it means adding the corresponding number of seconds. "},{"title":"minus(a, b), a - b operator​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#minusa-b-a-b-operator","content":"Calculates the difference. The result is always signed. You can also calculate integer numbers from a date or date with time. The idea is the same – see above for ‘plus’. "},{"title":"multiply(a, b), a * b operator​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#multiplya-b-a-b-operator","content":"Calculates the product of the numbers. "},{"title":"divide(a, b), a / b operator​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#dividea-b-a-b-operator","content":"Calculates the quotient of the numbers. The result type is always a floating-point type. It is not integer division. For integer division, use the ‘intDiv’ function. When dividing by zero you get ‘inf’, ‘-inf’, or ‘nan’. "},{"title":"intDiv(a, b)​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#intdiva-b","content":"Calculates the quotient of the numbers. Divides into integers, rounding down (by the absolute value). An exception is thrown when dividing by zero or when dividing a minimal negative number by minus one. "},{"title":"intDivOrZero(a, b)​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#intdivorzeroa-b","content":"Differs from ‘intDiv’ in that it returns zero when dividing by zero or when dividing a minimal negative number by minus one. "},{"title":"modulo(a, b), a % b operator​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#modulo","content":"Calculates the remainder after division. If arguments are floating-point numbers, they are pre-converted to integers by dropping the decimal portion. The remainder is taken in the same sense as in C++. Truncated division is used for negative numbers. An exception is thrown when dividing by zero or when dividing a minimal negative number by minus one. "},{"title":"moduloOrZero(a, b)​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#modulo-or-zero","content":"Differs from modulo in that it returns zero when the divisor is zero. "},{"title":"negate(a), -a operator​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#negatea-a-operator","content":"Calculates a number with the reverse sign. The result is always signed. "},{"title":"abs(a)​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#arithm_func-abs","content":"Calculates the absolute value of the number (a). That is, if a \\&lt; 0, it returns -a. For unsigned types it does not do anything. For signed integer types, it returns an unsigned number. "},{"title":"gcd(a, b)​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#gcda-b","content":"Returns the greatest common divisor of the numbers. An exception is thrown when dividing by zero or when dividing a minimal negative number by minus one. "},{"title":"lcm(a, b)​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#lcma-b","content":"Returns the least common multiple of the numbers. An exception is thrown when dividing by zero or when dividing a minimal negative number by minus one. "},{"title":"max2​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#max2","content":"Compares two values and returns the maximum. The returned value is converted to Float64. Syntax max2(value1, value2)  Arguments value1 — First value. Int/UInt or Float.value2 — Second value. Int/UInt or Float. Returned value The maximum of two values. Type: Float. Example Query: SELECT max2(-1, 2);  Result: ┌─max2(-1, 2)─┐ │ 2 │ └─────────────┘  "},{"title":"min2​","type":1,"pageTitle":"Arithmetic Functions","url":"en/sql-reference/functions/arithmetic-functions#min2","content":"Compares two values and returns the minimum. The returned value is converted to Float64. Syntax max2(value1, value2)  Arguments value1 — First value. Int/UInt or Float.value2 — Second value. Int/UInt or Float. Returned value The minimum of two values. Type: Float. Example Query: SELECT min2(-1, 2);  Result: ┌─min2(-1, 2)─┐ │ -1 │ └─────────────┘  "},{"title":"Bit Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/bit-functions","content":"","keywords":""},{"title":"bitAnd(a, b)​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bitanda-b","content":""},{"title":"bitOr(a, b)​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bitora-b","content":""},{"title":"bitXor(a, b)​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bitxora-b","content":""},{"title":"bitNot(a)​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bitnota","content":""},{"title":"bitShiftLeft(a, b)​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bitshiftlefta-b","content":"Shifts the binary representation of a value to the left by a specified number of bit positions. A FixedString or a String is treated as a single multibyte value. Bits of a FixedString value are lost as they are shifted out. On the contrary, a String value is extended with additional bytes, so no bits are lost. Syntax bitShiftLeft(a, b)  Arguments a — A value to shift. Integer types, String or FixedString.b — The number of shift positions. Unsigned integer types, 64 bit types or less are allowed. Returned value Shifted value. The type of the returned value is the same as the type of the input value. Example In the following queries bin and hex functions are used to show bits of shifted values. SELECT 99 AS a, bin(a), bitShiftLeft(a, 2) AS a_shifted, bin(a_shifted); SELECT 'abc' AS a, hex(a), bitShiftLeft(a, 4) AS a_shifted, hex(a_shifted); SELECT toFixedString('abc', 3) AS a, hex(a), bitShiftLeft(a, 4) AS a_shifted, hex(a_shifted);  Result: ┌──a─┬─bin(99)──┬─a_shifted─┬─bin(bitShiftLeft(99, 2))─┐ │ 99 │ 01100011 │ 140 │ 10001100 │ └────┴──────────┴───────────┴──────────────────────────┘ ┌─a───┬─hex('abc')─┬─a_shifted─┬─hex(bitShiftLeft('abc', 4))─┐ │ abc │ 616263 │ &amp;0 │ 06162630 │ └─────┴────────────┴───────────┴─────────────────────────────┘ ┌─a───┬─hex(toFixedString('abc', 3))─┬─a_shifted─┬─hex(bitShiftLeft(toFixedString('abc', 3), 4))─┐ │ abc │ 616263 │ &amp;0 │ 162630 │ └─────┴──────────────────────────────┴───────────┴───────────────────────────────────────────────┘  "},{"title":"bitShiftRight(a, b)​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bitshiftrighta-b","content":"Shifts the binary representation of a value to the right by a specified number of bit positions. A FixedString or a String is treated as a single multibyte value. Note that the length of a String value is reduced as bits are shifted out. Syntax bitShiftRight(a, b)  Arguments a — A value to shift. Integer types, String or FixedString.b — The number of shift positions. Unsigned integer types, 64 bit types or less are allowed. Returned value Shifted value. The type of the returned value is the same as the type of the input value. Example Query: SELECT 101 AS a, bin(a), bitShiftRight(a, 2) AS a_shifted, bin(a_shifted); SELECT 'abc' AS a, hex(a), bitShiftRight(a, 12) AS a_shifted, hex(a_shifted); SELECT toFixedString('abc', 3) AS a, hex(a), bitShiftRight(a, 12) AS a_shifted, hex(a_shifted);  Result: ┌───a─┬─bin(101)─┬─a_shifted─┬─bin(bitShiftRight(101, 2))─┐ │ 101 │ 01100101 │ 25 │ 00011001 │ └─────┴──────────┴───────────┴────────────────────────────┘ ┌─a───┬─hex('abc')─┬─a_shifted─┬─hex(bitShiftRight('abc', 12))─┐ │ abc │ 616263 │ │ 0616 │ └─────┴────────────┴───────────┴───────────────────────────────┘ ┌─a───┬─hex(toFixedString('abc', 3))─┬─a_shifted─┬─hex(bitShiftRight(toFixedString('abc', 3), 12))─┐ │ abc │ 616263 │ │ 000616 │ └─────┴──────────────────────────────┴───────────┴─────────────────────────────────────────────────┘  "},{"title":"bitRotateLeft(a, b)​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bitrotatelefta-b","content":""},{"title":"bitRotateRight(a, b)​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bitrotaterighta-b","content":""},{"title":"bitSlice(s, offset, length)​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bitslices-offset-length","content":"Returns a substring starting with the bit from the ‘offset’ index that is ‘length’ bits long. bits indexing starts from 1 Syntax bitSlice(s, offset[, length])  Arguments s — s is String or FixedString.offset — The start index with bit, A positive value indicates an offset on the left, and a negative value is an indent on the right. Numbering of the bits begins with 1.length — The length of substring with bit. If you specify a negative value, the function returns an open substring [offset, array_length - length]. If you omit the value, the function returns the substring [offset, the_end_string]. If length exceeds s, it will be truncate.If length isn't multiple of 8, will fill 0 on the right. Returned value The substring. String Example Query: select bin('Hello'), bin(bitSlice('Hello', 1, 8)) select bin('Hello'), bin(bitSlice('Hello', 1, 2)) select bin('Hello'), bin(bitSlice('Hello', 1, 9)) select bin('Hello'), bin(bitSlice('Hello', -4, 8))  Result: ┌─bin('Hello')─────────────────────────────┬─bin(bitSlice('Hello', 1, 8))─┐ │ 0100100001100101011011000110110001101111 │ 01001000 │ └──────────────────────────────────────────┴──────────────────────────────┘ ┌─bin('Hello')─────────────────────────────┬─bin(bitSlice('Hello', 1, 2))─┐ │ 0100100001100101011011000110110001101111 │ 01000000 │ └──────────────────────────────────────────┴──────────────────────────────┘ ┌─bin('Hello')─────────────────────────────┬─bin(bitSlice('Hello', 1, 9))─┐ │ 0100100001100101011011000110110001101111 │ 0100100000000000 │ └──────────────────────────────────────────┴──────────────────────────────┘ ┌─bin('Hello')─────────────────────────────┬─bin(bitSlice('Hello', -4, 8))─┐ │ 0100100001100101011011000110110001101111 │ 11110000 │ └──────────────────────────────────────────┴───────────────────────────────┘  "},{"title":"bitTest​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bittest","content":"Takes any integer and converts it into binary form, returns the value of a bit at specified position. The countdown starts from 0 from the right to the left. Syntax SELECT bitTest(number, index)  Arguments number – Integer number.index – Position of bit. Returned values Returns a value of bit at specified position. Type: UInt8. Example For example, the number 43 in base-2 (binary) numeral system is 101011. Query: SELECT bitTest(43, 1);  Result: ┌─bitTest(43, 1)─┐ │ 1 │ └────────────────┘  Another example: Query: SELECT bitTest(43, 2);  Result: ┌─bitTest(43, 2)─┐ │ 0 │ └────────────────┘  "},{"title":"bitTestAll​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bittestall","content":"Returns result of logical conjuction (AND operator) of all bits at given positions. The countdown starts from 0 from the right to the left. The conjuction for bitwise operations: 0 AND 0 = 0 0 AND 1 = 0 1 AND 0 = 0 1 AND 1 = 1 Syntax SELECT bitTestAll(number, index1, index2, index3, index4, ...)  Arguments number – Integer number.index1, index2, index3, index4 – Positions of bit. For example, for set of positions (index1, index2, index3, index4) is true if and only if all of its positions are true (index1 ⋀ index2, ⋀ index3 ⋀ index4). Returned values Returns result of logical conjuction. Type: UInt8. Example For example, the number 43 in base-2 (binary) numeral system is 101011. Query: SELECT bitTestAll(43, 0, 1, 3, 5);  Result: ┌─bitTestAll(43, 0, 1, 3, 5)─┐ │ 1 │ └────────────────────────────┘  Another example: Query: SELECT bitTestAll(43, 0, 1, 3, 5, 2);  Result: ┌─bitTestAll(43, 0, 1, 3, 5, 2)─┐ │ 0 │ └───────────────────────────────┘  "},{"title":"bitTestAny​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bittestany","content":"Returns result of logical disjunction (OR operator) of all bits at given positions. The countdown starts from 0 from the right to the left. The disjunction for bitwise operations: 0 OR 0 = 0 0 OR 1 = 1 1 OR 0 = 1 1 OR 1 = 1 Syntax SELECT bitTestAny(number, index1, index2, index3, index4, ...)  Arguments number – Integer number.index1, index2, index3, index4 – Positions of bit. Returned values Returns result of logical disjuction. Type: UInt8. Example For example, the number 43 in base-2 (binary) numeral system is 101011. Query: SELECT bitTestAny(43, 0, 2);  Result: ┌─bitTestAny(43, 0, 2)─┐ │ 1 │ └──────────────────────┘  Another example: Query: SELECT bitTestAny(43, 4, 2);  Result: ┌─bitTestAny(43, 4, 2)─┐ │ 0 │ └──────────────────────┘  "},{"title":"bitCount​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bitcount","content":"Calculates the number of bits set to one in the binary representation of a number. Syntax bitCount(x)  Arguments x — Integer or floating-point number. The function uses the value representation in memory. It allows supporting floating-point numbers. Returned value Number of bits set to one in the input number. The function does not convert input value to a larger type (sign extension). So, for example, bitCount(toUInt8(-1)) = 8. Type: UInt8. Example Take for example the number 333. Its binary representation: 0000000101001101. Query: SELECT bitCount(333);  Result: ┌─bitCount(333)─┐ │ 5 │ └───────────────┘  "},{"title":"bitHammingDistance​","type":1,"pageTitle":"Bit Functions","url":"en/sql-reference/functions/bit-functions#bithammingdistance","content":"Returns the Hamming Distance between the bit representations of two integer values. Can be used with SimHash functions for detection of semi-duplicate strings. The smaller is the distance, the more likely those strings are the same. Syntax bitHammingDistance(int1, int2)  Arguments int1 — First integer value. Int64.int2 — Second integer value. Int64. Returned value The Hamming distance. Type: UInt8. Examples Query: SELECT bitHammingDistance(111, 121);  Result: ┌─bitHammingDistance(111, 121)─┐ │ 3 │ └──────────────────────────────┘  With SimHash: SELECT bitHammingDistance(ngramSimHash('cat ate rat'), ngramSimHash('rat ate cat'));  Result: ┌─bitHammingDistance(ngramSimHash('cat ate rat'), ngramSimHash('rat ate cat'))─┐ │ 5 │ └──────────────────────────────────────────────────────────────────────────────┘  "},{"title":"Geo Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/geo/","content":"Geo Functions Original article","keywords":""},{"title":"Conditional Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/conditional-functions","content":"","keywords":""},{"title":"if​","type":1,"pageTitle":"Conditional Functions","url":"en/sql-reference/functions/conditional-functions#if","content":"Controls conditional branching. Unlike most systems, ClickHouse always evaluate both expressions then and else. Syntax if(cond, then, else)  If the condition cond evaluates to a non-zero value, returns the result of the expression then, and the result of the expression else, if present, is skipped. If the cond is zero or NULL, then the result of the then expression is skipped and the result of the else expression, if present, is returned. You can use the short_circuit_function_evaluation setting to calculate the if function according to a short scheme. If this setting is enabled, then expression is evaluated only on rows where cond is true, else expression – where cond is false. For example, an exception about division by zero is not thrown when executing the query SELECT if(number = 0, 0, intDiv(42, number)) FROM numbers(10), because intDiv(42, number) will be evaluated only for numbers that doesn't satisfy condition number = 0. Arguments cond – The condition for evaluation that can be zero or not. The type is UInt8, Nullable(UInt8) or NULL.then – The expression to return if condition is met.else – The expression to return if condition is not met. Returned values The function executes then and else expressions and returns its result, depending on whether the condition cond ended up being zero or not. Example Query: SELECT if(1, plus(2, 2), plus(2, 6));  Result: ┌─plus(2, 2)─┐ │ 4 │ └────────────┘  Query: SELECT if(0, plus(2, 2), plus(2, 6));  Result: ┌─plus(2, 6)─┐ │ 8 │ └────────────┘  then and else must have the lowest common type. Example: Take this LEFT_RIGHT table: SELECT * FROM LEFT_RIGHT ┌─left─┬─right─┐ │ ᴺᵁᴸᴸ │ 4 │ │ 1 │ 3 │ │ 2 │ 2 │ │ 3 │ 1 │ │ 4 │ ᴺᵁᴸᴸ │ └──────┴───────┘  The following query compares left and right values: SELECT left, right, if(left &lt; right, 'left is smaller than right', 'right is greater or equal than left') AS is_smaller FROM LEFT_RIGHT WHERE isNotNull(left) AND isNotNull(right) ┌─left─┬─right─┬─is_smaller──────────────────────────┐ │ 1 │ 3 │ left is smaller than right │ │ 2 │ 2 │ right is greater or equal than left │ │ 3 │ 1 │ right is greater or equal than left │ └──────┴───────┴─────────────────────────────────────┘  Note: NULL values are not used in this example, check NULL values in conditionals section. "},{"title":"Ternary Operator​","type":1,"pageTitle":"Conditional Functions","url":"en/sql-reference/functions/conditional-functions#ternary-operator","content":"It works same as if function. Syntax: cond ? then : else Returns then if the cond evaluates to be true (greater than zero), otherwise returns else. cond must be of type of UInt8, and then and else must have the lowest common type. then and else can be NULL See also ifNotFinite. "},{"title":"multiIf​","type":1,"pageTitle":"Conditional Functions","url":"en/sql-reference/functions/conditional-functions#multiif","content":"Allows you to write the CASE operator more compactly in the query. Syntax multiIf(cond_1, then_1, cond_2, then_2, ..., else)  You can use the short_circuit_function_evaluation setting to calculate the multiIf function according to a short scheme. If this setting is enabled, then_i expression is evaluated only on rows where ((NOT cond_1) AND (NOT cond_2) AND ... AND (NOT cond_{i-1}) AND cond_i) is true, cond_i will be evaluated only on rows where ((NOT cond_1) AND (NOT cond_2) AND ... AND (NOT cond_{i-1})) is true. For example, an exception about division by zero is not thrown when executing the query SELECT multiIf(number = 2, intDiv(1, number), number = 5) FROM numbers(10). Arguments cond_N — The condition for the function to return then_N.then_N — The result of the function when executed.else — The result of the function if none of the conditions is met. The function accepts 2N+1 parameters. Returned values The function returns one of the values then_N or else, depending on the conditions cond_N. Example Again using LEFT_RIGHT table. SELECT left, right, multiIf(left &lt; right, 'left is smaller', left &gt; right, 'left is greater', left = right, 'Both equal', 'Null value') AS result FROM LEFT_RIGHT ┌─left─┬─right─┬─result──────────┐ │ ᴺᵁᴸᴸ │ 4 │ Null value │ │ 1 │ 3 │ left is smaller │ │ 2 │ 2 │ Both equal │ │ 3 │ 1 │ left is greater │ │ 4 │ ᴺᵁᴸᴸ │ Null value │ └──────┴───────┴─────────────────┘  "},{"title":"Using Conditional Results Directly​","type":1,"pageTitle":"Conditional Functions","url":"en/sql-reference/functions/conditional-functions#using-conditional-results-directly","content":"Conditionals always result to 0, 1 or NULL. So you can use conditional results directly like this: SELECT left &lt; right AS is_small FROM LEFT_RIGHT ┌─is_small─┐ │ ᴺᵁᴸᴸ │ │ 1 │ │ 0 │ │ 0 │ │ ᴺᵁᴸᴸ │ └──────────┘  "},{"title":"NULL Values in Conditionals​","type":1,"pageTitle":"Conditional Functions","url":"en/sql-reference/functions/conditional-functions#null-values-in-conditionals","content":"When NULL values are involved in conditionals, the result will also be NULL. SELECT NULL &lt; 1, 2 &lt; NULL, NULL &lt; NULL, NULL = NULL ┌─less(NULL, 1)─┬─less(2, NULL)─┬─less(NULL, NULL)─┬─equals(NULL, NULL)─┐ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ └───────────────┴───────────────┴──────────────────┴────────────────────┘  So you should construct your queries carefully if the types are Nullable. The following example demonstrates this by failing to add equals condition to multiIf. SELECT left, right, multiIf(left &lt; right, 'left is smaller', left &gt; right, 'right is smaller', 'Both equal') AS faulty_result FROM LEFT_RIGHT ┌─left─┬─right─┬─faulty_result────┐ │ ᴺᵁᴸᴸ │ 4 │ Both equal │ │ 1 │ 3 │ left is smaller │ │ 2 │ 2 │ Both equal │ │ 3 │ 1 │ right is smaller │ │ 4 │ ᴺᵁᴸᴸ │ Both equal │ └──────┴───────┴──────────────────┘  "},{"title":"Functions for Working with Nullable Values","type":0,"sectionRef":"#","url":"en/sql-reference/functions/functions-for-nulls","content":"","keywords":""},{"title":"isNull​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"en/sql-reference/functions/functions-for-nulls#isnull","content":"Checks whether the argument is NULL. isNull(x)  Alias: ISNULL. Arguments x — A value with a non-compound data type. Returned value 1 if x is NULL.0 if x is not NULL. Example Input table ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ │ 2 │ 3 │ └───┴──────┘  Query SELECT x FROM t_null WHERE isNull(y);  ┌─x─┐ │ 1 │ └───┘  "},{"title":"isNotNull​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"en/sql-reference/functions/functions-for-nulls#isnotnull","content":"Checks whether the argument is NULL. isNotNull(x)  Arguments: x — A value with a non-compound data type. Returned value 0 if x is NULL.1 if x is not NULL. Example Input table ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ │ 2 │ 3 │ └───┴──────┘  Query SELECT x FROM t_null WHERE isNotNull(y);  ┌─x─┐ │ 2 │ └───┘  "},{"title":"coalesce​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"en/sql-reference/functions/functions-for-nulls#coalesce","content":"Checks from left to right whether NULL arguments were passed and returns the first non-NULL argument. coalesce(x,...)  Arguments: Any number of parameters of a non-compound type. All parameters must be compatible by data type. Returned values The first non-NULL argument.NULL, if all arguments are NULL. Example Consider a list of contacts that may specify multiple ways to contact a customer. ┌─name─────┬─mail─┬─phone─────┬──icq─┐ │ client 1 │ ᴺᵁᴸᴸ │ 123-45-67 │ 123 │ │ client 2 │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ └──────────┴──────┴───────────┴──────┘  The mail and phone fields are of type String, but the icq field is UInt32, so it needs to be converted to String. Get the first available contact method for the customer from the contact list: SELECT name, coalesce(mail, phone, CAST(icq,'Nullable(String)')) FROM aBook;  ┌─name─────┬─coalesce(mail, phone, CAST(icq, 'Nullable(String)'))─┐ │ client 1 │ 123-45-67 │ │ client 2 │ ᴺᵁᴸᴸ │ └──────────┴──────────────────────────────────────────────────────┘  "},{"title":"ifNull​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"en/sql-reference/functions/functions-for-nulls#ifnull","content":"Returns an alternative value if the main argument is NULL. ifNull(x,alt)  Arguments: x — The value to check for NULL.alt — The value that the function returns if x is NULL. Returned values The value x, if x is not NULL.The value alt, if x is NULL. Example SELECT ifNull('a', 'b');  ┌─ifNull('a', 'b')─┐ │ a │ └──────────────────┘  SELECT ifNull(NULL, 'b');  ┌─ifNull(NULL, 'b')─┐ │ b │ └───────────────────┘  "},{"title":"nullIf​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"en/sql-reference/functions/functions-for-nulls#nullif","content":"Returns NULL if the arguments are equal. nullIf(x, y)  Arguments: x, y — Values for comparison. They must be compatible types, or ClickHouse will generate an exception. Returned values NULL, if the arguments are equal.The x value, if the arguments are not equal. Example SELECT nullIf(1, 1);  ┌─nullIf(1, 1)─┐ │ ᴺᵁᴸᴸ │ └──────────────┘  SELECT nullIf(1, 2);  ┌─nullIf(1, 2)─┐ │ 1 │ └──────────────┘  "},{"title":"assumeNotNull​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"en/sql-reference/functions/functions-for-nulls#assumenotnull","content":"Results in an equivalent non-Nullable value for a Nullable type. In case the original value is NULL the result is undetermined. See also ifNull and coalesce functions. assumeNotNull(x)  Arguments: x — The original value. Returned values The original value from the non-Nullable type, if it is not NULL.Implementation specific result if the original value was NULL. Example Consider the t_null table. SHOW CREATE TABLE t_null;  ┌─statement─────────────────────────────────────────────────────────────────┐ │ CREATE TABLE default.t_null ( x Int8, y Nullable(Int8)) ENGINE = TinyLog │ └───────────────────────────────────────────────────────────────────────────┘  ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ │ 2 │ 3 │ └───┴──────┘  Apply the assumeNotNull function to the y column. SELECT assumeNotNull(y) FROM t_null;  ┌─assumeNotNull(y)─┐ │ 0 │ │ 3 │ └──────────────────┘  SELECT toTypeName(assumeNotNull(y)) FROM t_null;  ┌─toTypeName(assumeNotNull(y))─┐ │ Int8 │ │ Int8 │ └──────────────────────────────┘  "},{"title":"toNullable​","type":1,"pageTitle":"Functions for Working with Nullable Values","url":"en/sql-reference/functions/functions-for-nulls#tonullable","content":"Converts the argument type to Nullable. toNullable(x)  Arguments: x — The value of any non-compound type. Returned value The input value with a Nullable type. Example SELECT toTypeName(10);  ┌─toTypeName(10)─┐ │ UInt8 │ └────────────────┘  SELECT toTypeName(toNullable(10));  ┌─toTypeName(toNullable(10))─┐ │ Nullable(UInt8) │ └────────────────────────────┘  "},{"title":"Functions for Working with Geographical Coordinates","type":0,"sectionRef":"#","url":"en/sql-reference/functions/geo/coordinates","content":"","keywords":""},{"title":"greatCircleDistance​","type":1,"pageTitle":"Functions for Working with Geographical Coordinates","url":"en/sql-reference/functions/geo/coordinates#greatcircledistance","content":"Calculates the distance between two points on the Earth’s surface using the great-circle formula. greatCircleDistance(lon1Deg, lat1Deg, lon2Deg, lat2Deg)  Input parameters lon1Deg — Longitude of the first point in degrees. Range: [-180°, 180°].lat1Deg — Latitude of the first point in degrees. Range: [-90°, 90°].lon2Deg — Longitude of the second point in degrees. Range: [-180°, 180°].lat2Deg — Latitude of the second point in degrees. Range: [-90°, 90°]. Positive values correspond to North latitude and East longitude, and negative values correspond to South latitude and West longitude. Returned value The distance between two points on the Earth’s surface, in meters. Generates an exception when the input parameter values fall outside of the range. Example SELECT greatCircleDistance(55.755831, 37.617673, -55.755831, -37.617673)  ┌─greatCircleDistance(55.755831, 37.617673, -55.755831, -37.617673)─┐ │ 14132374.194975413 │ └───────────────────────────────────────────────────────────────────┘  "},{"title":"geoDistance​","type":1,"pageTitle":"Functions for Working with Geographical Coordinates","url":"en/sql-reference/functions/geo/coordinates#geodistance","content":"Similar to greatCircleDistance but calculates the distance on WGS-84 ellipsoid instead of sphere. This is more precise approximation of the Earth Geoid. The performance is the same as for greatCircleDistance (no performance drawback). It is recommended to use geoDistance to calculate the distances on Earth. Technical note: for close enough points we calculate the distance using planar approximation with the metric on the tangent plane at the midpoint of the coordinates. "},{"title":"greatCircleAngle​","type":1,"pageTitle":"Functions for Working with Geographical Coordinates","url":"en/sql-reference/functions/geo/coordinates#greatcircleangle","content":"Calculates the central angle between two points on the Earth’s surface using the great-circle formula. greatCircleAngle(lon1Deg, lat1Deg, lon2Deg, lat2Deg)  Input parameters lon1Deg — Longitude of the first point in degrees.lat1Deg — Latitude of the first point in degrees.lon2Deg — Longitude of the second point in degrees.lat2Deg — Latitude of the second point in degrees. Returned value The central angle between two points in degrees. Example SELECT greatCircleAngle(0, 0, 45, 0) AS arc  ┌─arc─┐ │ 45 │ └─────┘  "},{"title":"pointInEllipses​","type":1,"pageTitle":"Functions for Working with Geographical Coordinates","url":"en/sql-reference/functions/geo/coordinates#pointinellipses","content":"Checks whether the point belongs to at least one of the ellipses. Coordinates are geometric in the Cartesian coordinate system. pointInEllipses(x, y, x₀, y₀, a₀, b₀,...,xₙ, yₙ, aₙ, bₙ)  Input parameters x, y — Coordinates of a point on the plane.xᵢ, yᵢ — Coordinates of the center of the i-th ellipsis.aᵢ, bᵢ — Axes of the i-th ellipsis in units of x, y coordinates. The input parameters must be 2+4⋅n, where n is the number of ellipses. Returned values 1 if the point is inside at least one of the ellipses; 0if it is not. Example SELECT pointInEllipses(10., 10., 10., 9.1, 1., 0.9999)  ┌─pointInEllipses(10., 10., 10., 9.1, 1., 0.9999)─┐ │ 1 │ └─────────────────────────────────────────────────┘  "},{"title":"pointInPolygon​","type":1,"pageTitle":"Functions for Working with Geographical Coordinates","url":"en/sql-reference/functions/geo/coordinates#pointinpolygon","content":"Checks whether the point belongs to the polygon on the plane. pointInPolygon((x, y), [(a, b), (c, d) ...], ...)  Input values (x, y) — Coordinates of a point on the plane. Data type — Tuple — A tuple of two numbers.[(a, b), (c, d) ...] — Polygon vertices. Data type — Array. Each vertex is represented by a pair of coordinates (a, b). Vertices should be specified in a clockwise or counterclockwise order. The minimum number of vertices is 3. The polygon must be constant.The function also supports polygons with holes (cut out sections). In this case, add polygons that define the cut out sections using additional arguments of the function. The function does not support non-simply-connected polygons. Returned values 1 if the point is inside the polygon, 0 if it is not. If the point is on the polygon boundary, the function may return either 0 or 1. Example SELECT pointInPolygon((3., 3.), [(6, 0), (8, 4), (5, 8), (0, 2)]) AS res  ┌─res─┐ │ 1 │ └─────┘  Original article "},{"title":"Functions for Working with Geohash","type":0,"sectionRef":"#","url":"en/sql-reference/functions/geo/geohash","content":"","keywords":""},{"title":"geohashEncode​","type":1,"pageTitle":"Functions for Working with Geohash","url":"en/sql-reference/functions/geo/geohash#geohashencode","content":"Encodes latitude and longitude as a geohash-string. geohashEncode(longitude, latitude, [precision])  Input values longitude - longitude part of the coordinate you want to encode. Floating in range[-180°, 180°]latitude - latitude part of the coordinate you want to encode. Floating in range [-90°, 90°]precision - Optional, length of the resulting encoded string, defaults to 12. Integer in range [1, 12]. Any value less than 1 or greater than 12 is silently converted to 12. Returned values alphanumeric String of encoded coordinate (modified version of the base32-encoding alphabet is used). Example SELECT geohashEncode(-5.60302734375, 42.593994140625, 0) AS res;  ┌─res──────────┐ │ ezs42d000000 │ └──────────────┘  "},{"title":"geohashDecode​","type":1,"pageTitle":"Functions for Working with Geohash","url":"en/sql-reference/functions/geo/geohash#geohashdecode","content":"Decodes any geohash-encoded string into longitude and latitude. Input values encoded string - geohash-encoded string. Returned values (longitude, latitude) - 2-tuple of Float64 values of longitude and latitude. Example SELECT geohashDecode('ezs42') AS res;  ┌─res─────────────────────────────┐ │ (-5.60302734375,42.60498046875) │ └─────────────────────────────────┘  "},{"title":"geohashesInBox​","type":1,"pageTitle":"Functions for Working with Geohash","url":"en/sql-reference/functions/geo/geohash#geohashesinbox","content":"Returns an array of geohash-encoded strings of given precision that fall inside and intersect boundaries of given box, basically a 2D grid flattened into array. Syntax geohashesInBox(longitude_min, latitude_min, longitude_max, latitude_max, precision)  Arguments longitude_min — Minimum longitude. Range: [-180°, 180°]. Type: Float.latitude_min — Minimum latitude. Range: [-90°, 90°]. Type: Float.longitude_max — Maximum longitude. Range: [-180°, 180°]. Type: Float.latitude_max — Maximum latitude. Range: [-90°, 90°]. Type: Float.precision — Geohash precision. Range: [1, 12]. Type: UInt8. note All coordinate parameters must be of the same type: either Float32 or Float64. Returned values Array of precision-long strings of geohash-boxes covering provided area, you should not rely on order of items.[] - Empty array if minimum latitude and longitude values aren’t less than corresponding maximum values. Type: Array(String). note Function throws an exception if resulting array is over 10’000’000 items long. Example Query: SELECT geohashesInBox(24.48, 40.56, 24.785, 40.81, 4) AS thasos;  Result: ┌─thasos──────────────────────────────────────┐ │ ['sx1q','sx1r','sx32','sx1w','sx1x','sx38'] │ └─────────────────────────────────────────────┘  Original article "},{"title":"Encoding Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/encoding-functions","content":"","keywords":""},{"title":"char​","type":1,"pageTitle":"Encoding Functions","url":"en/sql-reference/functions/encoding-functions#char","content":"Returns the string with the length as the number of passed arguments and each byte has the value of corresponding argument. Accepts multiple arguments of numeric types. If the value of argument is out of range of UInt8 data type, it is converted to UInt8 with possible rounding and overflow. Syntax char(number_1, [number_2, ..., number_n]);  Arguments number_1, number_2, ..., number_n — Numerical arguments interpreted as integers. Types: Int, Float. Returned value a string of given bytes. Type: String. Example Query: SELECT char(104.1, 101, 108.9, 108.9, 111) AS hello;  Result: ┌─hello─┐ │ hello │ └───────┘  You can construct a string of arbitrary encoding by passing the corresponding bytes. Here is example for UTF-8: Query: SELECT char(0xD0, 0xBF, 0xD1, 0x80, 0xD0, 0xB8, 0xD0, 0xB2, 0xD0, 0xB5, 0xD1, 0x82) AS hello;  Result: ┌─hello──┐ │ привет │ └────────┘  Query: SELECT char(0xE4, 0xBD, 0xA0, 0xE5, 0xA5, 0xBD) AS hello;  Result: ┌─hello─┐ │ 你好 │ └───────┘  "},{"title":"hex​","type":1,"pageTitle":"Encoding Functions","url":"en/sql-reference/functions/encoding-functions#hex","content":"Returns a string containing the argument’s hexadecimal representation. Alias: HEX. Syntax hex(arg)  The function is using uppercase letters A-F and not using any prefixes (like 0x) or suffixes (like h). For integer arguments, it prints hex digits (“nibbles”) from the most significant to least significant (big-endian or “human-readable” order). It starts with the most significant non-zero byte (leading zero bytes are omitted) but always prints both digits of every byte even if the leading digit is zero. Values of type Date and DateTime are formatted as corresponding integers (the number of days since Epoch for Date and the value of Unix Timestamp for DateTime). For String and FixedString, all bytes are simply encoded as two hexadecimal numbers. Zero bytes are not omitted. Values of Float and Decimal types are encoded as their representation in memory. As we support little-endian architecture, they are encoded in little-endian. Zero leading/trailing bytes are not omitted. Values of UUID type are encoded as big-endian order string. Arguments arg — A value to convert to hexadecimal. Types: String, UInt, Float, Decimal, Date or DateTime. Returned value A string with the hexadecimal representation of the argument. Type: String. Examples Query: SELECT hex(1);  Result: 01  Query: SELECT hex(toFloat32(number)) AS hex_presentation FROM numbers(15, 2);  Result: ┌─hex_presentation─┐ │ 00007041 │ │ 00008041 │ └──────────────────┘  Query: SELECT hex(toFloat64(number)) AS hex_presentation FROM numbers(15, 2);  Result: ┌─hex_presentation─┐ │ 0000000000002E40 │ │ 0000000000003040 │ └──────────────────┘  Query: SELECT lower(hex(toUUID('61f0c404-5cb3-11e7-907b-a6006ad3dba0'))) as uuid_hex  Result: ┌─uuid_hex─────────────────────────┐ │ 61f0c4045cb311e7907ba6006ad3dba0 │ └──────────────────────────────────┘  "},{"title":"unhex​","type":1,"pageTitle":"Encoding Functions","url":"en/sql-reference/functions/encoding-functions#unhexstr","content":"Performs the opposite operation of hex. It interprets each pair of hexadecimal digits (in the argument) as a number and converts it to the byte represented by the number. The return value is a binary string (BLOB). If you want to convert the result to a number, you can use the reverse and reinterpretAs&lt;Type&gt; functions. note If unhex is invoked from within the clickhouse-client, binary strings display using UTF-8. Alias: UNHEX. Syntax unhex(arg)  Arguments arg — A string containing any number of hexadecimal digits. Type: String. Supports both uppercase and lowercase letters A-F. The number of hexadecimal digits does not have to be even. If it is odd, the last digit is interpreted as the least significant half of the 00-0F byte. If the argument string contains anything other than hexadecimal digits, some implementation-defined result is returned (an exception isn’t thrown). For a numeric argument the inverse of hex(N) is not performed by unhex(). Returned value A binary string (BLOB). Type: String. Example Query: SELECT unhex('303132'), UNHEX('4D7953514C');  Result: ┌─unhex('303132')─┬─unhex('4D7953514C')─┐ │ 012 │ MySQL │ └─────────────────┴─────────────────────┘  Query: SELECT reinterpretAsUInt64(reverse(unhex('FFF'))) AS num;  Result: ┌──num─┐ │ 4095 │ └──────┘  "},{"title":"bin​","type":1,"pageTitle":"Encoding Functions","url":"en/sql-reference/functions/encoding-functions#bin","content":"Returns a string containing the argument’s binary representation. Syntax bin(arg)  Alias: BIN. For integer arguments, it prints bin digits from the most significant to least significant (big-endian or “human-readable” order). It starts with the most significant non-zero byte (leading zero bytes are omitted) but always prints eight digits of every byte if the leading digit is zero. Values of type Date and DateTime are formatted as corresponding integers (the number of days since Epoch for Date and the value of Unix Timestamp for DateTime). For String and FixedString, all bytes are simply encoded as eight binary numbers. Zero bytes are not omitted. Values of Float and Decimal types are encoded as their representation in memory. As we support little-endian architecture, they are encoded in little-endian. Zero leading/trailing bytes are not omitted. Values of UUID type are encoded as big-endian order string. Arguments arg — A value to convert to binary. String, FixedString, UInt, Float, Decimal, Date, or DateTime. Returned value A string with the binary representation of the argument. Type: String. Examples Query: SELECT bin(14);  Result: ┌─bin(14)──┐ │ 00001110 │ └──────────┘  Query: SELECT bin(toFloat32(number)) AS bin_presentation FROM numbers(15, 2);  Result: ┌─bin_presentation─────────────────┐ │ 00000000000000000111000001000001 │ │ 00000000000000001000000001000001 │ └──────────────────────────────────┘  Query: SELECT bin(toFloat64(number)) AS bin_presentation FROM numbers(15, 2);  Result: ┌─bin_presentation─────────────────────────────────────────────────┐ │ 0000000000000000000000000000000000000000000000000010111001000000 │ │ 0000000000000000000000000000000000000000000000000011000001000000 │ └──────────────────────────────────────────────────────────────────┘  Query: SELECT bin(toUUID('61f0c404-5cb3-11e7-907b-a6006ad3dba0')) as bin_uuid  Result: ┌─bin_uuid─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ 01100001111100001100010000000100010111001011001100010001111001111001000001111011101001100000000001101010110100111101101110100000 │ └──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"unbin​","type":1,"pageTitle":"Encoding Functions","url":"en/sql-reference/functions/encoding-functions#unbinstr","content":"Interprets each pair of binary digits (in the argument) as a number and converts it to the byte represented by the number. The functions performs the opposite operation to bin. Syntax unbin(arg)  Alias: UNBIN. For a numeric argument unbin() does not return the inverse of bin(). If you want to convert the result to a number, you can use the reverse and reinterpretAs&lt;Type&gt; functions. note If unbin is invoked from within the clickhouse-client, binary strings are displayed using UTF-8. Supports binary digits 0 and 1. The number of binary digits does not have to be multiples of eight. If the argument string contains anything other than binary digits, some implementation-defined result is returned (an exception isn’t thrown). Arguments arg — A string containing any number of binary digits. String. Returned value A binary string (BLOB). Type: String. Examples Query: SELECT UNBIN('001100000011000100110010'), UNBIN('0100110101111001010100110101000101001100');  Result: ┌─unbin('001100000011000100110010')─┬─unbin('0100110101111001010100110101000101001100')─┐ │ 012 │ MySQL │ └───────────────────────────────────┴───────────────────────────────────────────────────┘  Query: SELECT reinterpretAsUInt64(reverse(unbin('1110'))) AS num;  Result: ┌─num─┐ │ 14 │ └─────┘  "},{"title":"UUIDStringToNum(str)​","type":1,"pageTitle":"Encoding Functions","url":"en/sql-reference/functions/encoding-functions#uuidstringtonumstr","content":"Accepts a string containing 36 characters in the format 123e4567-e89b-12d3-a456-426655440000, and returns it as a set of bytes in a FixedString(16). "},{"title":"UUIDNumToString(str)​","type":1,"pageTitle":"Encoding Functions","url":"en/sql-reference/functions/encoding-functions#uuidnumtostringstr","content":"Accepts a FixedString(16) value. Returns a string containing 36 characters in text format. "},{"title":"bitmaskToList(num)​","type":1,"pageTitle":"Encoding Functions","url":"en/sql-reference/functions/encoding-functions#bitmasktolistnum","content":"Accepts an integer. Returns a string containing the list of powers of two that total the source number when summed. They are comma-separated without spaces in text format, in ascending order. "},{"title":"bitmaskToArray(num)​","type":1,"pageTitle":"Encoding Functions","url":"en/sql-reference/functions/encoding-functions#bitmasktoarraynum","content":"Accepts an integer. Returns an array of UInt64 numbers containing the list of powers of two that total the source number when summed. Numbers in the array are in ascending order. "},{"title":"bitPositionsToArray(num)​","type":1,"pageTitle":"Encoding Functions","url":"en/sql-reference/functions/encoding-functions#bitpositionstoarraynum","content":"Accepts an integer and converts it to an unsigned integer. Returns an array of UInt64 numbers containing the list of positions of bits of arg that equal 1, in ascending order. Syntax bitPositionsToArray(arg)  Arguments arg — Integer value. Int/UInt. Returned value An array containing a list of positions of bits that equal 1, in ascending order. Type: Array(UInt64). Example Query: SELECT bitPositionsToArray(toInt8(1)) AS bit_positions;  Result: ┌─bit_positions─┐ │ [0] │ └───────────────┘  Query: SELECT bitPositionsToArray(toInt8(-1)) AS bit_positions;  Result: ┌─bit_positions─────┐ │ [0,1,2,3,4,5,6,7] │ └───────────────────┘  "},{"title":"Encryption functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/encryption-functions","content":"","keywords":""},{"title":"encrypt​","type":1,"pageTitle":"Encryption functions","url":"en/sql-reference/functions/encryption-functions#encrypt","content":"This function encrypts data using these modes: aes-128-ecb, aes-192-ecb, aes-256-ecbaes-128-cbc, aes-192-cbc, aes-256-cbcaes-128-cfb1, aes-192-cfb1, aes-256-cfb1aes-128-cfb8, aes-192-cfb8, aes-256-cfb8aes-128-cfb128, aes-192-cfb128, aes-256-cfb128aes-128-ofb, aes-192-ofb, aes-256-ofbaes-128-gcm, aes-192-gcm, aes-256-gcm Syntax encrypt('mode', 'plaintext', 'key' [, iv, aad])  Arguments mode — Encryption mode. String.plaintext — Text thats need to be encrypted. String.key — Encryption key. String.iv — Initialization vector. Required for -gcm modes, optinal for others. String.aad — Additional authenticated data. It isn't encrypted, but it affects decryption. Works only in -gcm modes, for others would throw an exception. String. Returned value Ciphertext binary string. String. Examples Create this table: Query: CREATE TABLE encryption_test ( `comment` String, `secret` String ) ENGINE = Memory;  Insert some data (please avoid storing the keys/ivs in the database as this undermines the whole concept of encryption), also storing 'hints' is unsafe too and used only for illustrative purposes: Query: INSERT INTO encryption_test VALUES('aes-256-cfb128 no IV', encrypt('aes-256-cfb128', 'Secret', '12345678910121314151617181920212')),\\ ('aes-256-cfb128 no IV, different key', encrypt('aes-256-cfb128', 'Secret', 'keykeykeykeykeykeykeykeykeykeyke')),\\ ('aes-256-cfb128 with IV', encrypt('aes-256-cfb128', 'Secret', '12345678910121314151617181920212', 'iviviviviviviviv')),\\ ('aes-256-cbc no IV', encrypt('aes-256-cbc', 'Secret', '12345678910121314151617181920212'));  Query: SELECT comment, hex(secret) FROM encryption_test;  Result: ┌─comment─────────────────────────────┬─hex(secret)──────────────────────┐ │ aes-256-cfb128 no IV │ B4972BDC4459 │ │ aes-256-cfb128 no IV, different key │ 2FF57C092DC9 │ │ aes-256-cfb128 with IV │ 5E6CB398F653 │ │ aes-256-cbc no IV │ 1BC0629A92450D9E73A00E7D02CF4142 │ └─────────────────────────────────────┴──────────────────────────────────┘  Example with -gcm: Query: INSERT INTO encryption_test VALUES('aes-256-gcm', encrypt('aes-256-gcm', 'Secret', '12345678910121314151617181920212', 'iviviviviviviviv')), \\ ('aes-256-gcm with AAD', encrypt('aes-256-gcm', 'Secret', '12345678910121314151617181920212', 'iviviviviviviviv', 'aad')); SELECT comment, hex(secret) FROM encryption_test WHERE comment LIKE '%gcm%';  Result: ┌─comment──────────────┬─hex(secret)──────────────────────────────────┐ │ aes-256-gcm │ A8A3CCBC6426CFEEB60E4EAE03D3E94204C1B09E0254 │ │ aes-256-gcm with AAD │ A8A3CCBC6426D9A1017A0A932322F1852260A4AD6837 │ └──────────────────────┴──────────────────────────────────────────────┘  "},{"title":"aes_encrypt_mysql​","type":1,"pageTitle":"Encryption functions","url":"en/sql-reference/functions/encryption-functions#aes_encrypt_mysql","content":"Compatible with mysql encryption and resulting ciphertext can be decrypted with AES_DECRYPT function. Will produce the same ciphertext as encrypt on equal inputs. But when key or iv are longer than they should normally be, aes_encrypt_mysql will stick to what MySQL's aes_encrypt does: 'fold' key and ignore excess bits of iv. Supported encryption modes: aes-128-ecb, aes-192-ecb, aes-256-ecbaes-128-cbc, aes-192-cbc, aes-256-cbcaes-128-cfb1, aes-192-cfb1, aes-256-cfb1aes-128-cfb8, aes-192-cfb8, aes-256-cfb8aes-128-cfb128, aes-192-cfb128, aes-256-cfb128aes-128-ofb, aes-192-ofb, aes-256-ofb Syntax aes_encrypt_mysql('mode', 'plaintext', 'key' [, iv])  Arguments mode — Encryption mode. String.plaintext — Text that needs to be encrypted. String.key — Encryption key. If key is longer than required by mode, MySQL-specific key folding is performed. String.iv — Initialization vector. Optional, only first 16 bytes are taken into account String. Returned value Ciphertext binary string. String. Examples Given equal input encrypt and aes_encrypt_mysql produce the same ciphertext: Query: SELECT encrypt('aes-256-cfb128', 'Secret', '12345678910121314151617181920212', 'iviviviviviviviv') = aes_encrypt_mysql('aes-256-cfb128', 'Secret', '12345678910121314151617181920212', 'iviviviviviviviv') AS ciphertexts_equal;  Result: ┌─ciphertexts_equal─┐ │ 1 │ └───────────────────┘  But encrypt fails when key or iv is longer than expected: Query: SELECT encrypt('aes-256-cfb128', 'Secret', '123456789101213141516171819202122', 'iviviviviviviviv123');  Result: Received exception from server (version 21.1.2): Code: 36. DB::Exception: Received from localhost:9000. DB::Exception: Invalid key size: 33 expected 32: While processing encrypt('aes-256-cfb128', 'Secret', '123456789101213141516171819202122', 'iviviviviviviviv123').  While aes_encrypt_mysql produces MySQL-compatitalbe output: Query: SELECT hex(aes_encrypt_mysql('aes-256-cfb128', 'Secret', '123456789101213141516171819202122', 'iviviviviviviviv123')) AS ciphertext;  Result: ┌─ciphertext───┐ │ 24E9E4966469 │ └──────────────┘  Notice how supplying even longer IV produces the same result Query: SELECT hex(aes_encrypt_mysql('aes-256-cfb128', 'Secret', '123456789101213141516171819202122', 'iviviviviviviviv123456')) AS ciphertext  Result: ┌─ciphertext───┐ │ 24E9E4966469 │ └──────────────┘  Which is binary equal to what MySQL produces on same inputs: mysql&gt; SET block_encryption_mode='aes-256-cfb128'; Query OK, 0 rows affected (0.00 sec) mysql&gt; SELECT aes_encrypt('Secret', '123456789101213141516171819202122', 'iviviviviviviviv123456') as ciphertext; +------------------------+ | ciphertext | +------------------------+ | 0x24E9E4966469 | +------------------------+ 1 row in set (0.00 sec)  "},{"title":"decrypt​","type":1,"pageTitle":"Encryption functions","url":"en/sql-reference/functions/encryption-functions#decrypt","content":"This function decrypts ciphertext into a plaintext using these modes: aes-128-ecb, aes-192-ecb, aes-256-ecbaes-128-cbc, aes-192-cbc, aes-256-cbcaes-128-cfb1, aes-192-cfb1, aes-256-cfb1aes-128-cfb8, aes-192-cfb8, aes-256-cfb8aes-128-cfb128, aes-192-cfb128, aes-256-cfb128aes-128-ofb, aes-192-ofb, aes-256-ofbaes-128-gcm, aes-192-gcm, aes-256-gcm Syntax decrypt('mode', 'ciphertext', 'key' [, iv, aad])  Arguments mode — Decryption mode. String.ciphertext — Encrypted text that needs to be decrypted. String.key — Decryption key. String.iv — Initialization vector. Required for -gcm modes, optinal for others. String.aad — Additional authenticated data. Won't decrypt if this value is incorrect. Works only in -gcm modes, for others would throw an exception. String. Returned value Decrypted String. String. Examples Re-using table from encrypt. Query: SELECT comment, hex(secret) FROM encryption_test;  Result: ┌─comment──────────────┬─hex(secret)──────────────────────────────────┐ │ aes-256-gcm │ A8A3CCBC6426CFEEB60E4EAE03D3E94204C1B09E0254 │ │ aes-256-gcm with AAD │ A8A3CCBC6426D9A1017A0A932322F1852260A4AD6837 │ └──────────────────────┴──────────────────────────────────────────────┘ ┌─comment─────────────────────────────┬─hex(secret)──────────────────────┐ │ aes-256-cfb128 no IV │ B4972BDC4459 │ │ aes-256-cfb128 no IV, different key │ 2FF57C092DC9 │ │ aes-256-cfb128 with IV │ 5E6CB398F653 │ │ aes-256-cbc no IV │ 1BC0629A92450D9E73A00E7D02CF4142 │ └─────────────────────────────────────┴──────────────────────────────────┘  Now let's try to decrypt all that data. Query: SELECT comment, decrypt('aes-256-cfb128', secret, '12345678910121314151617181920212') as plaintext FROM encryption_test  Result: ┌─comment─────────────────────────────┬─plaintext─┐ │ aes-256-cfb128 no IV │ Secret │ │ aes-256-cfb128 no IV, different key │ �4� � │ │ aes-256-cfb128 with IV │ ���6�~ │ │aes-256-cbc no IV │ �2*4�h3c�4w��@ └─────────────────────────────────────┴───────────┘  Notice how only a portion of the data was properly decrypted, and the rest is gibberish since either mode, key, or iv were different upon encryption. "},{"title":"aes_decrypt_mysql​","type":1,"pageTitle":"Encryption functions","url":"en/sql-reference/functions/encryption-functions#aes_decrypt_mysql","content":"Compatible with mysql encryption and decrypts data encrypted with AES_ENCRYPT function. Will produce same plaintext as decrypt on equal inputs. But when key or iv are longer than they should normally be, aes_decrypt_mysql will stick to what MySQL's aes_decrypt does: 'fold' key and ignore excess bits of IV. Supported decryption modes: aes-128-ecb, aes-192-ecb, aes-256-ecbaes-128-cbc, aes-192-cbc, aes-256-cbcaes-128-cfb1, aes-192-cfb1, aes-256-cfb1aes-128-cfb8, aes-192-cfb8, aes-256-cfb8aes-128-cfb128, aes-192-cfb128, aes-256-cfb128aes-128-ofb, aes-192-ofb, aes-256-ofb Syntax aes_decrypt_mysql('mode', 'ciphertext', 'key' [, iv])  Arguments mode — Decryption mode. String.ciphertext — Encrypted text that needs to be decrypted. String.key — Decryption key. String.iv — Initialization vector. Optinal. String. Returned value Decrypted String. String. Examples Let's decrypt data we've previously encrypted with MySQL: mysql&gt; SET block_encryption_mode='aes-256-cfb128'; Query OK, 0 rows affected (0.00 sec) mysql&gt; SELECT aes_encrypt('Secret', '123456789101213141516171819202122', 'iviviviviviviviv123456') as ciphertext; +------------------------+ | ciphertext | +------------------------+ | 0x24E9E4966469 | +------------------------+ 1 row in set (0.00 sec)  Query: SELECT aes_decrypt_mysql('aes-256-cfb128', unhex('24E9E4966469'), '123456789101213141516171819202122', 'iviviviviviviviv123456') AS plaintext  Result: ┌─plaintext─┐ │ Secret │ └───────────┘  Original article "},{"title":"Functions for Working with S2 Index","type":0,"sectionRef":"#","url":"en/sql-reference/functions/geo/s2","content":"","keywords":""},{"title":"geoToS2​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"en/sql-reference/functions/geo/s2#geotos2","content":"Returns S2 point index corresponding to the provided coordinates (longitude, latitude). Syntax geoToS2(lon, lat)  Arguments lon — Longitude. Float64.lat — Latitude. Float64. Returned values S2 point index. Type: UInt64. Example Query: SELECT geoToS2(37.79506683, 55.71290588) AS s2Index;  Result: ┌─────────────s2Index─┐ │ 4704772434919038107 │ └─────────────────────┘  "},{"title":"s2ToGeo​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"en/sql-reference/functions/geo/s2#s2togeo","content":"Returns geo coordinates (longitude, latitude) corresponding to the provided S2 point index. Syntax s2ToGeo(s2index)  Arguments s2index — S2 Index. UInt64. Returned values A tuple consisting of two values: tuple(lon,lat). Type: lon — Float64. lat — Float64. Example Query: SELECT s2ToGeo(4704772434919038107) AS s2Coodrinates;  Result: ┌─s2Coodrinates────────────────────────┐ │ (37.79506681471008,55.7129059052841) │ └──────────────────────────────────────┘  "},{"title":"s2GetNeighbors​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"en/sql-reference/functions/geo/s2#s2getneighbors","content":"Returns S2 neighbor indixes corresponding to the provided S2. Each cell in the S2 system is a quadrilateral bounded by four geodesics. So, each cell has 4 neighbors. Syntax s2GetNeighbors(s2index)  Arguments s2index — S2 Index. UInt64. Returned values An array consisting of 4 neighbor indexes: array[s2index1, s2index3, s2index2, s2index4]. Type: UInt64. Example Query: SELECT s2GetNeighbors(5074766849661468672) AS s2Neighbors;  Result: ┌─s2Neighbors───────────────────────────────────────────────────────────────────────┐ │ [5074766987100422144,5074766712222515200,5074767536856236032,5074767261978329088] │ └───────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"s2CellsIntersect​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"en/sql-reference/functions/geo/s2#s2cellsintersect","content":"Determines if the two provided S2 cells intersect or not. Syntax s2CellsIntersect(s2index1, s2index2)  Arguments siIndex1, s2index2 — S2 Index. UInt64. Returned values 1 — If the cells intersect.0 — If the cells don't intersect. Type: UInt8. Example Query: SELECT s2CellsIntersect(9926595209846587392, 9926594385212866560) AS intersect;  Result: ┌─intersect─┐ │ 1 │ └───────────┘  "},{"title":"s2CapContains​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"en/sql-reference/functions/geo/s2#s2capcontains","content":"Determines if a cap contains a S2 point. A cap represents a part of the sphere that has been cut off by a plane. It is defined by a point on a sphere and a radius in degrees. Syntax s2CapContains(center, degrees, point)  Arguments center — S2 point index corresponding to the cap. UInt64.degrees — Radius of the cap in degrees. Float64.point — S2 point index. UInt64. Returned values 1 — If the cap contains the S2 point index.0 — If the cap doesn't contain the S2 point index. Type: UInt8. Example Query: SELECT s2CapContains(1157339245694594829, 1.0, 1157347770437378819) AS capContains;  Result: ┌─capContains─┐ │ 1 │ └─────────────┘  "},{"title":"s2CapUnion​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"en/sql-reference/functions/geo/s2#s2capunion","content":"Determines the smallest cap that contains the given two input caps. A cap represents a portion of the sphere that has been cut off by a plane. It is defined by a point on a sphere and a radius in degrees. Syntax s2CapUnion(center1, radius1, center2, radius2)  Arguments center1, center2 — S2 point indixes corresponding to the two input caps. UInt64.radius1, radius2 — Radius of the two input caps in degrees. Float64. Returned values center — S2 point index corresponding the center of the smallest cap containing the two input caps. Type: UInt64.radius — Radius of the smallest cap containing the two input caps. Type: Float64. Example Query: SELECT s2CapUnion(3814912406305146967, 1.0, 1157347770437378819, 1.0) AS capUnion;  Result: ┌─capUnion───────────────────────────────┐ │ (4534655147792050737,60.2088283994957) │ └────────────────────────────────────────┘  "},{"title":"s2RectAdd​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"en/sql-reference/functions/geo/s2#s2rectadd","content":"Increases the size of the bounding rectangle to include the given S2 point. In the S2 system, a rectangle is represented by a type of S2Region called a S2LatLngRect that represents a rectangle in latitude-longitude space. Syntax s2RectAdd(s2pointLow, s2pointHigh, s2Point)  Arguments s2PointLow — Low S2 point index corresponding to the rectangle. UInt64.s2PointHigh — High S2 point index corresponding to the rectangle. UInt64.s2Point — Target S2 point index that the bound rectangle should be grown to include. UInt64. Returned values s2PointLow — Low S2 cell id corresponding to the grown rectangle. Type: UInt64.s2PointHigh — Hight S2 cell id corresponding to the grown rectangle. Type: UInt64. Example Query: SELECT s2RectAdd(5178914411069187297, 5177056748191934217, 5179056748191934217) AS rectAdd;  Result: ┌─rectAdd───────────────────────────────────┐ │ (5179062030687166815,5177056748191934217) │ └───────────────────────────────────────────┘  "},{"title":"s2RectContains​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"en/sql-reference/functions/geo/s2#s2rectcontains","content":"Determines if a given rectangle contains a S2 point. In the S2 system, a rectangle is represented by a type of S2Region called a S2LatLngRect that represents a rectangle in latitude-longitude space. Syntax s2RectContains(s2PointLow, s2PointHi, s2Point)  Arguments s2PointLow — Low S2 point index corresponding to the rectangle. UInt64.s2PointHigh — High S2 point index corresponding to the rectangle. UInt64.s2Point — Target S2 point index. UInt64. Returned values 1 — If the rectangle contains the given S2 point.0 — If the rectangle doesn't contain the given S2 point. Example Query: SELECT s2RectContains(5179062030687166815, 5177056748191934217, 5177914411069187297) AS rectContains;  Result: ┌─rectContains─┐ │ 0 │ └──────────────┘  "},{"title":"s2RectUinion​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"en/sql-reference/functions/geo/s2#s2rectunion","content":"Returns the smallest rectangle containing the union of this rectangle and the given rectangle. In the S2 system, a rectangle is represented by a type of S2Region called a S2LatLngRect that represents a rectangle in latitude-longitude space. Syntax s2RectUnion(s2Rect1PointLow, s2Rect1PointHi, s2Rect2PointLow, s2Rect2PointHi)  Arguments s2Rect1PointLow, s2Rect1PointHi — Low and High S2 point indexes corresponding to the first rectangle. UInt64.s2Rect2PointLow, s2Rect2PointHi — Low and High S2 point indexes corresponding to the second rectangle. UInt64. Returned values s2UnionRect2PointLow — Low S2 cell id corresponding to the union rectangle. Type: UInt64.s2UnionRect2PointHi — High S2 cell id corresponding to the union rectangle. Type: UInt64. Example Query: SELECT s2RectUnion(5178914411069187297, 5177056748191934217, 5179062030687166815, 5177056748191934217) AS rectUnion;  Result: ┌─rectUnion─────────────────────────────────┐ │ (5179062030687166815,5177056748191934217) │ └───────────────────────────────────────────┘  "},{"title":"s2RectIntersection​","type":1,"pageTitle":"Functions for Working with S2 Index","url":"en/sql-reference/functions/geo/s2#s2rectintersection","content":"Returns the smallest rectangle containing the intersection of this rectangle and the given rectangle. In the S2 system, a rectangle is represented by a type of S2Region called a S2LatLngRect that represents a rectangle in latitude-longitude space. Syntax s2RectIntersection(s2Rect1PointLow, s2Rect1PointHi, s2Rect2PointLow, s2Rect2PointHi)  Arguments s2Rect1PointLow, s2Rect1PointHi — Low and High S2 point indexes corresponding to the first rectangle. UInt64.s2Rect2PointLow, s2Rect2PointHi — Low and High S2 point indexes corresponding to the second rectangle. UInt64. Returned values s2UnionRect2PointLow — Low S2 cell id corresponding to the rectangle containing the intersection of the given rectangles. Type: UInt64.s2UnionRect2PointHi — High S2 cell id corresponding to the rectangle containing the intersection of the given rectangles. Type: UInt64. Example Query: SELECT s2RectIntersection(5178914411069187297, 5177056748191934217, 5179062030687166815, 5177056748191934217) AS rectIntersection;  Result: ┌─rectIntersection──────────────────────────┐ │ (5178914411069187297,5177056748191934217) │ └───────────────────────────────────────────┘  "},{"title":"ext-dict-functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/ext-dict-functions","content":"","keywords":""},{"title":"dictGet, dictGetOrDefault, dictGetOrNull​","type":1,"pageTitle":"ext-dict-functions","url":"en/sql-reference/functions/ext-dict-functions#dictget","content":"Retrieves values from an external dictionary. dictGet('dict_name', attr_names, id_expr) dictGetOrDefault('dict_name', attr_names, id_expr, default_value_expr) dictGetOrNull('dict_name', attr_name, id_expr)  Arguments dict_name — Name of the dictionary. String literal.attr_names — Name of the column of the dictionary, String literal, or tuple of column names, Tuple(String literal).id_expr — Key value. Expression returning dictionary key-type value or Tuple-type value depending on the dictionary configuration.default_value_expr — Values returned if the dictionary does not contain a row with the id_expr key. Expression or Tuple(Expression), returning the value (or values) in the data types configured for the attr_names attribute. Returned value If ClickHouse parses the attribute successfully in the attribute’s data type, functions return the value of the dictionary attribute that corresponds to id_expr. If there is no the key, corresponding to id_expr, in the dictionary, then: - `dictGet` returns the content of the `&lt;null_value&gt;` element specified for the attribute in the dictionary configuration. - `dictGetOrDefault` returns the value passed as the `default_value_expr` parameter. - `dictGetOrNull` returns `NULL` in case key was not found in dictionary.  ClickHouse throws an exception if it cannot parse the value of the attribute or the value does not match the attribute data type. Example for simple key dictionary Create a text file ext-dict-test.csv containing the following: 1,1 2,2  The first column is id, the second column is c1. Configure the external dictionary: &lt;clickhouse&gt; &lt;dictionary&gt; &lt;name&gt;ext-dict-test&lt;/name&gt; &lt;source&gt; &lt;file&gt; &lt;path&gt;/path-to/ext-dict-test.csv&lt;/path&gt; &lt;format&gt;CSV&lt;/format&gt; &lt;/file&gt; &lt;/source&gt; &lt;layout&gt; &lt;flat /&gt; &lt;/layout&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;id&lt;/name&gt; &lt;/id&gt; &lt;attribute&gt; &lt;name&gt;c1&lt;/name&gt; &lt;type&gt;UInt32&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;lifetime&gt;0&lt;/lifetime&gt; &lt;/dictionary&gt; &lt;/clickhouse&gt;  Perform the query: SELECT dictGetOrDefault('ext-dict-test', 'c1', number + 1, toUInt32(number * 10)) AS val, toTypeName(val) AS type FROM system.numbers LIMIT 3;  ┌─val─┬─type───┐ │ 1 │ UInt32 │ │ 2 │ UInt32 │ │ 20 │ UInt32 │ └─────┴────────┘  Example for complex key dictionary Create a text file ext-dict-mult.csv containing the following: 1,1,'1' 2,2,'2' 3,3,'3'  The first column is id, the second is c1, the third is c2. Configure the external dictionary: &lt;clickhouse&gt; &lt;dictionary&gt; &lt;name&gt;ext-dict-mult&lt;/name&gt; &lt;source&gt; &lt;file&gt; &lt;path&gt;/path-to/ext-dict-mult.csv&lt;/path&gt; &lt;format&gt;CSV&lt;/format&gt; &lt;/file&gt; &lt;/source&gt; &lt;layout&gt; &lt;flat /&gt; &lt;/layout&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;id&lt;/name&gt; &lt;/id&gt; &lt;attribute&gt; &lt;name&gt;c1&lt;/name&gt; &lt;type&gt;UInt32&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;c2&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;lifetime&gt;0&lt;/lifetime&gt; &lt;/dictionary&gt; &lt;/clickhouse&gt;  Perform the query: SELECT dictGet('ext-dict-mult', ('c1','c2'), number) AS val, toTypeName(val) AS type FROM system.numbers LIMIT 3;  ┌─val─────┬─type──────────────────┐ │ (1,'1') │ Tuple(UInt8, String) │ │ (2,'2') │ Tuple(UInt8, String) │ │ (3,'3') │ Tuple(UInt8, String) │ └─────────┴───────────────────────┘  Example for range key dictionary Input table: CREATE TABLE range_key_dictionary_source_table ( key UInt64, start_date Date, end_date Date, value String, value_nullable Nullable(String) ) ENGINE = TinyLog(); INSERT INTO range_key_dictionary_source_table VALUES(1, toDate('2019-05-20'), toDate('2019-05-20'), 'First', 'First'); INSERT INTO range_key_dictionary_source_table VALUES(2, toDate('2019-05-20'), toDate('2019-05-20'), 'Second', NULL); INSERT INTO range_key_dictionary_source_table VALUES(3, toDate('2019-05-20'), toDate('2019-05-20'), 'Third', 'Third');  Create the external dictionary: CREATE DICTIONARY range_key_dictionary ( key UInt64, start_date Date, end_date Date, value String, value_nullable Nullable(String) ) PRIMARY KEY key SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() TABLE 'range_key_dictionary_source_table')) LIFETIME(MIN 1 MAX 1000) LAYOUT(RANGE_HASHED()) RANGE(MIN start_date MAX end_date);  Perform the query: SELECT (number, toDate('2019-05-20')), dictHas('range_key_dictionary', number, toDate('2019-05-20')), dictGetOrNull('range_key_dictionary', 'value', number, toDate('2019-05-20')), dictGetOrNull('range_key_dictionary', 'value_nullable', number, toDate('2019-05-20')), dictGetOrNull('range_key_dictionary', ('value', 'value_nullable'), number, toDate('2019-05-20')) FROM system.numbers LIMIT 5 FORMAT TabSeparated;  Result: (0,'2019-05-20') 0 \\N \\N (NULL,NULL) (1,'2019-05-20') 1 First First ('First','First') (2,'2019-05-20') 1 Second \\N ('Second',NULL) (3,'2019-05-20') 1 Third Third ('Third','Third') (4,'2019-05-20') 0 \\N \\N (NULL,NULL)  See Also External Dictionaries "},{"title":"dictHas​","type":1,"pageTitle":"ext-dict-functions","url":"en/sql-reference/functions/ext-dict-functions#dicthas","content":"Checks whether a key is present in a dictionary. dictHas('dict_name', id_expr)  Arguments dict_name — Name of the dictionary. String literal.id_expr — Key value. Expression returning dictionary key-type value or Tuple-type value depending on the dictionary configuration. Returned value 0, if there is no key.1, if there is a key. Type: UInt8. "},{"title":"dictGetHierarchy​","type":1,"pageTitle":"ext-dict-functions","url":"en/sql-reference/functions/ext-dict-functions#dictgethierarchy","content":"Creates an array, containing all the parents of a key in the hierarchical dictionary. Syntax dictGetHierarchy('dict_name', key)  Arguments dict_name — Name of the dictionary. String literal.key — Key value. Expression returning a UInt64-type value. Returned value Parents for the key. Type: Array(UInt64). "},{"title":"dictIsIn​","type":1,"pageTitle":"ext-dict-functions","url":"en/sql-reference/functions/ext-dict-functions#dictisin","content":"Checks the ancestor of a key through the whole hierarchical chain in the dictionary. dictIsIn('dict_name', child_id_expr, ancestor_id_expr)  Arguments dict_name — Name of the dictionary. String literal.child_id_expr — Key to be checked. Expression returning a UInt64-type value.ancestor_id_expr — Alleged ancestor of the child_id_expr key. Expression returning a UInt64-type value. Returned value 0, if child_id_expr is not a child of ancestor_id_expr.1, if child_id_expr is a child of ancestor_id_expr or if child_id_expr is an ancestor_id_expr. Type: UInt8. "},{"title":"dictGetChildren​","type":1,"pageTitle":"ext-dict-functions","url":"en/sql-reference/functions/ext-dict-functions#dictgetchildren","content":"Returns first-level children as an array of indexes. It is the inverse transformation for dictGetHierarchy. Syntax dictGetChildren(dict_name, key)  Arguments dict_name — Name of the dictionary. String literal.key — Key value. Expression returning a UInt64-type value. Returned values First-level descendants for the key. Type: Array(UInt64). Example Consider the hierarchic dictionary: ┌─id─┬─parent_id─┐ │ 1 │ 0 │ │ 2 │ 1 │ │ 3 │ 1 │ │ 4 │ 2 │ └────┴───────────┘  First-level children: SELECT dictGetChildren('hierarchy_flat_dictionary', number) FROM system.numbers LIMIT 4;  ┌─dictGetChildren('hierarchy_flat_dictionary', number)─┐ │ [1] │ │ [2,3] │ │ [4] │ │ [] │ └──────────────────────────────────────────────────────┘  "},{"title":"dictGetDescendant​","type":1,"pageTitle":"ext-dict-functions","url":"en/sql-reference/functions/ext-dict-functions#dictgetdescendant","content":"Returns all descendants as if dictGetChildren function was applied level times recursively. Syntax dictGetDescendants(dict_name, key, level)  Arguments dict_name — Name of the dictionary. String literal.key — Key value. Expression returning a UInt64-type value.level — Hierarchy level. If level = 0 returns all descendants to the end. UInt8. Returned values Descendants for the key. Type: Array(UInt64). Example Consider the hierarchic dictionary: ┌─id─┬─parent_id─┐ │ 1 │ 0 │ │ 2 │ 1 │ │ 3 │ 1 │ │ 4 │ 2 │ └────┴───────────┘  All descendants: SELECT dictGetDescendants('hierarchy_flat_dictionary', number) FROM system.numbers LIMIT 4;  ┌─dictGetDescendants('hierarchy_flat_dictionary', number)─┐ │ [1,2,3,4] │ │ [2,3,4] │ │ [4] │ │ [] │ └─────────────────────────────────────────────────────────┘  First-level descendants: SELECT dictGetDescendants('hierarchy_flat_dictionary', number, 1) FROM system.numbers LIMIT 4;  ┌─dictGetDescendants('hierarchy_flat_dictionary', number, 1)─┐ │ [1] │ │ [2,3] │ │ [4] │ │ [] │ └────────────────────────────────────────────────────────────┘  "},{"title":"Other Functions​","type":1,"pageTitle":"ext-dict-functions","url":"en/sql-reference/functions/ext-dict-functions#ext_dict_functions-other","content":"ClickHouse supports specialized functions that convert dictionary attribute values to a specific data type regardless of the dictionary configuration. Functions: dictGetInt8, dictGetInt16, dictGetInt32, dictGetInt64dictGetUInt8, dictGetUInt16, dictGetUInt32, dictGetUInt64dictGetFloat32, dictGetFloat64dictGetDatedictGetDateTimedictGetUUIDdictGetString All these functions have the OrDefault modification. For example, dictGetDateOrDefault. Syntax: dictGet[Type]('dict_name', 'attr_name', id_expr) dictGet[Type]OrDefault('dict_name', 'attr_name', id_expr, default_value_expr)  Arguments dict_name — Name of the dictionary. String literal.attr_name — Name of the column of the dictionary. String literal.id_expr — Key value. Expression returning a UInt64 or Tuple-type value depending on the dictionary configuration.default_value_expr — Value returned if the dictionary does not contain a row with the id_expr key. Expression returning the value in the data type configured for the attr_name attribute. Returned value If ClickHouse parses the attribute successfully in the attribute’s data type, functions return the value of the dictionary attribute that corresponds to id_expr. If there is no requested id_expr in the dictionary then: - `dictGet[Type]` returns the content of the `&lt;null_value&gt;` element specified for the attribute in the dictionary configuration. - `dictGet[Type]OrDefault` returns the value passed as the `default_value_expr` parameter.  ClickHouse throws an exception if it cannot parse the value of the attribute or the value does not match the attribute data type. "},{"title":"Bitmap Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/bitmap-functions","content":"","keywords":""},{"title":"bitmapBuild​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmap_functions-bitmapbuild","content":"Build a bitmap from unsigned integer array. bitmapBuild(array)  Arguments array – Unsigned integer array. Example SELECT bitmapBuild([1, 2, 3, 4, 5]) AS res, toTypeName(res);  ┌─res─┬─toTypeName(bitmapBuild([1, 2, 3, 4, 5]))─────┐ │ │ AggregateFunction(groupBitmap, UInt8) │ └─────┴──────────────────────────────────────────────┘  "},{"title":"bitmapToArray​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmaptoarray","content":"Convert bitmap to integer array. bitmapToArray(bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapToArray(bitmapBuild([1, 2, 3, 4, 5])) AS res;  ┌─res─────────┐ │ [1,2,3,4,5] │ └─────────────┘  "},{"title":"bitmapSubsetInRange​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmap-functions-bitmapsubsetinrange","content":"Return subset in specified range (not include the range_end). bitmapSubsetInRange(bitmap, range_start, range_end)  Arguments bitmap – Bitmap object.range_start – Range start point. Type: UInt32.range_end – Range end point (excluded). Type: UInt32. Example SELECT bitmapToArray(bitmapSubsetInRange(bitmapBuild([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,100,200,500]), toUInt32(30), toUInt32(200))) AS res;  ┌─res───────────────┐ │ [30,31,32,33,100] │ └───────────────────┘  "},{"title":"bitmapSubsetLimit​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmapsubsetlimit","content":"Creates a subset of bitmap with n elements taken between range_start and cardinality_limit. Syntax bitmapSubsetLimit(bitmap, range_start, cardinality_limit)  Arguments bitmap – Bitmap object.range_start – The subset starting point. Type: UInt32.cardinality_limit – The subset cardinality upper limit. Type: UInt32. Returned value The subset. Type: Bitmap object. Example Query: SELECT bitmapToArray(bitmapSubsetLimit(bitmapBuild([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,100,200,500]), toUInt32(30), toUInt32(200))) AS res;  Result: ┌─res───────────────────────┐ │ [30,31,32,33,100,200,500] │ └───────────────────────────┘  "},{"title":"subBitmap​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#subbitmap","content":"Returns the bitmap elements, starting from the offset position. The number of returned elements is limited by the cardinality_limit parameter. Analog of the substring) string function, but for bitmap. Syntax subBitmap(bitmap, offset, cardinality_limit)  Arguments bitmap – The bitmap. Type: Bitmap object.offset – The position of the first element of the subset. Type: UInt32.cardinality_limit – The maximum number of elements in the subset. Type: UInt32. Returned value The subset. Type: Bitmap object. Example Query: SELECT bitmapToArray(subBitmap(bitmapBuild([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,100,200,500]), toUInt32(10), toUInt32(10))) AS res;  Result: ┌─res─────────────────────────────┐ │ [10,11,12,13,14,15,16,17,18,19] │ └─────────────────────────────────┘  "},{"title":"bitmapContains​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmap_functions-bitmapcontains","content":"Checks whether the bitmap contains an element. bitmapContains(haystack, needle)  Arguments haystack – Bitmap object, where the function searches.needle – Value that the function searches. Type: UInt32. Returned values 0 — If haystack does not contain needle.1 — If haystack contains needle. Type: UInt8. Example SELECT bitmapContains(bitmapBuild([1,5,7,9]), toUInt32(9)) AS res;  ┌─res─┐ │ 1 │ └─────┘  "},{"title":"bitmapHasAny​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmaphasany","content":"Checks whether two bitmaps have intersection by some elements. bitmapHasAny(bitmap1, bitmap2)  If you are sure that bitmap2 contains strictly one element, consider using the bitmapContains function. It works more efficiently. Arguments bitmap* – Bitmap object. Return values 1, if bitmap1 and bitmap2 have one similar element at least.0, otherwise. Example SELECT bitmapHasAny(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;  ┌─res─┐ │ 1 │ └─────┘  "},{"title":"bitmapHasAll​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmaphasall","content":"Analogous to hasAll(array, array) returns 1 if the first bitmap contains all the elements of the second one, 0 otherwise. If the second argument is an empty bitmap then returns 1. bitmapHasAll(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapHasAll(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;  ┌─res─┐ │ 0 │ └─────┘  "},{"title":"bitmapCardinality​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmapcardinality","content":"Retrun bitmap cardinality of type UInt64. bitmapCardinality(bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapCardinality(bitmapBuild([1, 2, 3, 4, 5])) AS res;  ┌─res─┐ │ 5 │ └─────┘  "},{"title":"bitmapMin​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmapmin","content":"Retrun the smallest value of type UInt64 in the set, UINT32_MAX if the set is empty. bitmapMin(bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapMin(bitmapBuild([1, 2, 3, 4, 5])) AS res;   ┌─res─┐ │ 1 │ └─────┘  "},{"title":"bitmapMax​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmapmax","content":"Retrun the greatest value of type UInt64 in the set, 0 if the set is empty. bitmapMax(bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapMax(bitmapBuild([1, 2, 3, 4, 5])) AS res;   ┌─res─┐ │ 5 │ └─────┘  "},{"title":"bitmapTransform​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmaptransform","content":"Transform an array of values in a bitmap to another array of values, the result is a new bitmap. bitmapTransform(bitmap, from_array, to_array)  Arguments bitmap – Bitmap object.from_array – UInt32 array. For idx in range [0, from_array.size()), if bitmap contains from_array[idx], then replace it with to_array[idx]. Note that the result depends on array ordering if there are common elements between from_array and to_array.to_array – UInt32 array, its size shall be the same to from_array. Example SELECT bitmapToArray(bitmapTransform(bitmapBuild([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]), cast([5,999,2] as Array(UInt32)), cast([2,888,20] as Array(UInt32)))) AS res;   ┌─res───────────────────┐ │ [1,3,4,6,7,8,9,10,20] │ └───────────────────────┘  "},{"title":"bitmapAnd​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmapand","content":"Two bitmap and calculation, the result is a new bitmap. bitmapAnd(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapToArray(bitmapAnd(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res;  ┌─res─┐ │ [3] │ └─────┘  "},{"title":"bitmapOr​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmapor","content":"Two bitmap or calculation, the result is a new bitmap. bitmapOr(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapToArray(bitmapOr(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res;  ┌─res─────────┐ │ [1,2,3,4,5] │ └─────────────┘  "},{"title":"bitmapXor​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmapxor","content":"Two bitmap xor calculation, the result is a new bitmap. bitmapXor(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapToArray(bitmapXor(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res;  ┌─res───────┐ │ [1,2,4,5] │ └───────────┘  "},{"title":"bitmapAndnot​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmapandnot","content":"Two bitmap andnot calculation, the result is a new bitmap. bitmapAndnot(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapToArray(bitmapAndnot(bitmapBuild([1,2,3]),bitmapBuild([3,4,5]))) AS res;  ┌─res───┐ │ [1,2] │ └───────┘  "},{"title":"bitmapAndCardinality​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmapandcardinality","content":"Two bitmap and calculation, return cardinality of type UInt64. bitmapAndCardinality(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapAndCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;  ┌─res─┐ │ 1 │ └─────┘  "},{"title":"bitmapOrCardinality​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmaporcardinality","content":"Two bitmap or calculation, return cardinality of type UInt64. bitmapOrCardinality(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapOrCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;  ┌─res─┐ │ 5 │ └─────┘  "},{"title":"bitmapXorCardinality​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmapxorcardinality","content":"Two bitmap xor calculation, return cardinality of type UInt64. bitmapXorCardinality(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapXorCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;  ┌─res─┐ │ 4 │ └─────┘  "},{"title":"bitmapAndnotCardinality​","type":1,"pageTitle":"Bitmap Functions","url":"en/sql-reference/functions/bitmap-functions#bitmapandnotcardinality","content":"Two bitmap andnot calculation, return cardinality of type UInt64. bitmapAndnotCardinality(bitmap,bitmap)  Arguments bitmap – Bitmap object. Example SELECT bitmapAndnotCardinality(bitmapBuild([1,2,3]),bitmapBuild([3,4,5])) AS res;  ┌─res─┐ │ 2 │ └─────┘  "},{"title":"Functions for Implementing the IN Operator","type":0,"sectionRef":"#","url":"en/sql-reference/functions/in-functions","content":"","keywords":""},{"title":"in, notIn, globalIn, globalNotIn​","type":1,"pageTitle":"Functions for Implementing the IN Operator","url":"en/sql-reference/functions/in-functions#in-functions","content":"See the section IN operators. "},{"title":"Introspection Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/introspection","content":"","keywords":""},{"title":"addressToLine​","type":1,"pageTitle":"Introspection Functions","url":"en/sql-reference/functions/introspection#addresstoline","content":"Converts virtual memory address inside ClickHouse server process to the filename and the line number in ClickHouse source code. If you use official ClickHouse packages, you need to install the clickhouse-common-static-dbg package. Syntax addressToLine(address_of_binary_instruction)  Arguments address_of_binary_instruction (UInt64) — Address of instruction in a running process. Returned value Source code filename and the line number in this file delimited by colon. For example, `/build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:199`, where `199` is a line number. Name of a binary, if the function couldn’t find the debug information. Empty string, if the address is not valid. Type: String. Example Enabling introspection functions: SET allow_introspection_functions=1;  Selecting the first string from the trace_log system table: SELECT * FROM system.trace_log LIMIT 1 \\G;  Row 1: ────── event_date: 2019-11-19 event_time: 2019-11-19 18:57:23 revision: 54429 timer_type: Real thread_number: 48 query_id: 421b6855-1858-45a5-8f37-f383409d6d72 trace: [140658411141617,94784174532828,94784076370703,94784076372094,94784076361020,94784175007680,140658411116251,140658403895439]  The trace field contains the stack trace at the moment of sampling. Getting the source code filename and the line number for a single address: SELECT addressToLine(94784076370703) \\G;  Row 1: ────── addressToLine(94784076370703): /build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:199  Applying the function to the whole stack trace: SELECT arrayStringConcat(arrayMap(x -&gt; addressToLine(x), trace), '\\n') AS trace_source_code_lines FROM system.trace_log LIMIT 1 \\G  The arrayMap function allows to process each individual element of the trace array by the addressToLine function. The result of this processing you see in the trace_source_code_lines column of output. Row 1: ────── trace_source_code_lines: /lib/x86_64-linux-gnu/libpthread-2.27.so /usr/lib/debug/usr/bin/clickhouse /build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:199 /build/obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:155 /usr/include/c++/9/bits/atomic_base.h:551 /usr/lib/debug/usr/bin/clickhouse /lib/x86_64-linux-gnu/libpthread-2.27.so /build/glibc-OTsEL5/glibc-2.27/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97  "},{"title":"addressToLineWithInlines​","type":1,"pageTitle":"Introspection Functions","url":"en/sql-reference/functions/introspection#addresstolinewithinlines","content":"Similar to addressToLine, but it will return an Array with all inline functions, and will be much slower as a price. If you use official ClickHouse packages, you need to install the clickhouse-common-static-dbg package. Syntax addressToLineWithInlines(address_of_binary_instruction)  Arguments address_of_binary_instruction (UInt64) — Address of instruction in a running process. Returned value Array which first element is source code filename and the line number in this file delimited by colon. And from second element, inline functions' source code filename and line number and function name are listed. Array with single element which is name of a binary, if the function couldn’t find the debug information. Empty array, if the address is not valid. Type: Array(String). Example Enabling introspection functions: SET allow_introspection_functions=1;  Applying the function to address. SELECT addressToLineWithInlines(531055181::UInt64);  ┌─addressToLineWithInlines(CAST('531055181', 'UInt64'))────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ ['./src/Functions/addressToLineWithInlines.cpp:98','./build_normal_debug/./src/Functions/addressToLineWithInlines.cpp:176:DB::(anonymous namespace)::FunctionAddressToLineWithInlines::implCached(unsigned long) const'] │ └──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  Applying the function to the whole stack trace: SELECT ta, addressToLineWithInlines(arrayJoin(trace) as ta) FROM system.trace_log WHERE query_id = '5e173544-2020-45de-b645-5deebe2aae54';  The arrayJoin functions will split array to rows. ┌────────ta─┬─addressToLineWithInlines(arrayJoin(trace))───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ 365497529 │ ['./build_normal_debug/./contrib/libcxx/include/string_view:252'] │ │ 365593602 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:191'] │ │ 365593866 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365592528 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365591003 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:477'] │ │ 365590479 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:442'] │ │ 365590600 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:457'] │ │ 365598941 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365607098 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365590571 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:451'] │ │ 365598941 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365607098 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365590571 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:451'] │ │ 365598941 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365607098 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365590571 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:451'] │ │ 365598941 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:0'] │ │ 365597289 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:807'] │ │ 365599840 │ ['./build_normal_debug/./src/Common/Dwarf.cpp:1118'] │ │ 531058145 │ ['./build_normal_debug/./src/Functions/addressToLineWithInlines.cpp:152'] │ │ 531055181 │ ['./src/Functions/addressToLineWithInlines.cpp:98','./build_normal_debug/./src/Functions/addressToLineWithInlines.cpp:176:DB::(anonymous namespace)::FunctionAddressToLineWithInlines::implCached(unsigned long) const'] │ │ 422333613 │ ['./build_normal_debug/./src/Functions/IFunctionAdaptors.h:21'] │ │ 586866022 │ ['./build_normal_debug/./src/Functions/IFunction.cpp:216'] │ │ 586869053 │ ['./build_normal_debug/./src/Functions/IFunction.cpp:264'] │ │ 586873237 │ ['./build_normal_debug/./src/Functions/IFunction.cpp:334'] │ │ 597901620 │ ['./build_normal_debug/./src/Interpreters/ExpressionActions.cpp:601'] │ │ 597898534 │ ['./build_normal_debug/./src/Interpreters/ExpressionActions.cpp:718'] │ │ 630442912 │ ['./build_normal_debug/./src/Processors/Transforms/ExpressionTransform.cpp:23'] │ │ 546354050 │ ['./build_normal_debug/./src/Processors/ISimpleTransform.h:38'] │ │ 626026993 │ ['./build_normal_debug/./src/Processors/ISimpleTransform.cpp:89'] │ │ 626294022 │ ['./build_normal_debug/./src/Processors/Executors/ExecutionThreadContext.cpp:45'] │ │ 626293730 │ ['./build_normal_debug/./src/Processors/Executors/ExecutionThreadContext.cpp:63'] │ │ 626169525 │ ['./build_normal_debug/./src/Processors/Executors/PipelineExecutor.cpp:213'] │ │ 626170308 │ ['./build_normal_debug/./src/Processors/Executors/PipelineExecutor.cpp:178'] │ │ 626166348 │ ['./build_normal_debug/./src/Processors/Executors/PipelineExecutor.cpp:329'] │ │ 626163461 │ ['./build_normal_debug/./src/Processors/Executors/PipelineExecutor.cpp:84'] │ │ 626323536 │ ['./build_normal_debug/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:85'] │ │ 626323277 │ ['./build_normal_debug/./src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:112'] │ │ 626323133 │ ['./build_normal_debug/./contrib/libcxx/include/type_traits:3682'] │ │ 626323041 │ ['./build_normal_debug/./contrib/libcxx/include/tuple:1415'] │ └───────────┴──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"addressToSymbol​","type":1,"pageTitle":"Introspection Functions","url":"en/sql-reference/functions/introspection#addresstosymbol","content":"Converts virtual memory address inside ClickHouse server process to the symbol from ClickHouse object files. Syntax addressToSymbol(address_of_binary_instruction)  Arguments address_of_binary_instruction (UInt64) — Address of instruction in a running process. Returned value Symbol from ClickHouse object files.Empty string, if the address is not valid. Type: String. Example Enabling introspection functions: SET allow_introspection_functions=1;  Selecting the first string from the trace_log system table: SELECT * FROM system.trace_log LIMIT 1 \\G;  Row 1: ────── event_date: 2019-11-20 event_time: 2019-11-20 16:57:59 revision: 54429 timer_type: Real thread_number: 48 query_id: 724028bf-f550-45aa-910d-2af6212b94ac trace: [94138803686098,94138815010911,94138815096522,94138815101224,94138815102091,94138814222988,94138806823642,94138814457211,94138806823642,94138814457211,94138806823642,94138806795179,94138806796144,94138753770094,94138753771646,94138753760572,94138852407232,140399185266395,140399178045583]  The trace field contains the stack trace at the moment of sampling. Getting a symbol for a single address: SELECT addressToSymbol(94138803686098) \\G;  Row 1: ────── addressToSymbol(94138803686098): _ZNK2DB24IAggregateFunctionHelperINS_20AggregateFunctionSumImmNS_24AggregateFunctionSumDataImEEEEE19addBatchSinglePlaceEmPcPPKNS_7IColumnEPNS_5ArenaE  Applying the function to the whole stack trace: SELECT arrayStringConcat(arrayMap(x -&gt; addressToSymbol(x), trace), '\\n') AS trace_symbols FROM system.trace_log LIMIT 1 \\G  The arrayMap function allows to process each individual element of the trace array by the addressToSymbols function. The result of this processing you see in the trace_symbols column of output. Row 1: ────── trace_symbols: _ZNK2DB24IAggregateFunctionHelperINS_20AggregateFunctionSumImmNS_24AggregateFunctionSumDataImEEEEE19addBatchSinglePlaceEmPcPPKNS_7IColumnEPNS_5ArenaE _ZNK2DB10Aggregator21executeWithoutKeyImplERPcmPNS0_28AggregateFunctionInstructionEPNS_5ArenaE _ZN2DB10Aggregator14executeOnBlockESt6vectorIN3COWINS_7IColumnEE13immutable_ptrIS3_EESaIS6_EEmRNS_22AggregatedDataVariantsERS1_IPKS3_SaISC_EERS1_ISE_SaISE_EERb _ZN2DB10Aggregator14executeOnBlockERKNS_5BlockERNS_22AggregatedDataVariantsERSt6vectorIPKNS_7IColumnESaIS9_EERS6_ISB_SaISB_EERb _ZN2DB10Aggregator7executeERKSt10shared_ptrINS_17IBlockInputStreamEERNS_22AggregatedDataVariantsE _ZN2DB27AggregatingBlockInputStream8readImplEv _ZN2DB17IBlockInputStream4readEv _ZN2DB26ExpressionBlockInputStream8readImplEv _ZN2DB17IBlockInputStream4readEv _ZN2DB26ExpressionBlockInputStream8readImplEv _ZN2DB17IBlockInputStream4readEv _ZN2DB28AsynchronousBlockInputStream9calculateEv _ZNSt17_Function_handlerIFvvEZN2DB28AsynchronousBlockInputStream4nextEvEUlvE_E9_M_invokeERKSt9_Any_data _ZN14ThreadPoolImplI20ThreadFromGlobalPoolE6workerESt14_List_iteratorIS0_E _ZZN20ThreadFromGlobalPoolC4IZN14ThreadPoolImplIS_E12scheduleImplIvEET_St8functionIFvvEEiSt8optionalImEEUlvE1_JEEEOS4_DpOT0_ENKUlvE_clEv _ZN14ThreadPoolImplISt6threadE6workerESt14_List_iteratorIS0_E execute_native_thread_routine start_thread clone  "},{"title":"demangle​","type":1,"pageTitle":"Introspection Functions","url":"en/sql-reference/functions/introspection#demangle","content":"Converts a symbol that you can get using the addressToSymbol function to the C++ function name. Syntax demangle(symbol)  Arguments symbol (String) — Symbol from an object file. Returned value Name of the C++ function.Empty string if a symbol is not valid. Type: String. Example Enabling introspection functions: SET allow_introspection_functions=1;  Selecting the first string from the trace_log system table: SELECT * FROM system.trace_log LIMIT 1 \\G;  Row 1: ────── event_date: 2019-11-20 event_time: 2019-11-20 16:57:59 revision: 54429 timer_type: Real thread_number: 48 query_id: 724028bf-f550-45aa-910d-2af6212b94ac trace: [94138803686098,94138815010911,94138815096522,94138815101224,94138815102091,94138814222988,94138806823642,94138814457211,94138806823642,94138814457211,94138806823642,94138806795179,94138806796144,94138753770094,94138753771646,94138753760572,94138852407232,140399185266395,140399178045583]  The trace field contains the stack trace at the moment of sampling. Getting a function name for a single address: SELECT demangle(addressToSymbol(94138803686098)) \\G;  Row 1: ────── demangle(addressToSymbol(94138803686098)): DB::IAggregateFunctionHelper&lt;DB::AggregateFunctionSum&lt;unsigned long, unsigned long, DB::AggregateFunctionSumData&lt;unsigned long&gt; &gt; &gt;::addBatchSinglePlace(unsigned long, char*, DB::IColumn const**, DB::Arena*) const  Applying the function to the whole stack trace: SELECT arrayStringConcat(arrayMap(x -&gt; demangle(addressToSymbol(x)), trace), '\\n') AS trace_functions FROM system.trace_log LIMIT 1 \\G  The arrayMap function allows to process each individual element of the trace array by the demangle function. The result of this processing you see in the trace_functions column of output. Row 1: ────── trace_functions: DB::IAggregateFunctionHelper&lt;DB::AggregateFunctionSum&lt;unsigned long, unsigned long, DB::AggregateFunctionSumData&lt;unsigned long&gt; &gt; &gt;::addBatchSinglePlace(unsigned long, char*, DB::IColumn const**, DB::Arena*) const DB::Aggregator::executeWithoutKeyImpl(char*&amp;, unsigned long, DB::Aggregator::AggregateFunctionInstruction*, DB::Arena*) const DB::Aggregator::executeOnBlock(std::vector&lt;COW&lt;DB::IColumn&gt;::immutable_ptr&lt;DB::IColumn&gt;, std::allocator&lt;COW&lt;DB::IColumn&gt;::immutable_ptr&lt;DB::IColumn&gt; &gt; &gt;, unsigned long, DB::AggregatedDataVariants&amp;, std::vector&lt;DB::IColumn const*, std::allocator&lt;DB::IColumn const*&gt; &gt;&amp;, std::vector&lt;std::vector&lt;DB::IColumn const*, std::allocator&lt;DB::IColumn const*&gt; &gt;, std::allocator&lt;std::vector&lt;DB::IColumn const*, std::allocator&lt;DB::IColumn const*&gt; &gt; &gt; &gt;&amp;, bool&amp;) DB::Aggregator::executeOnBlock(DB::Block const&amp;, DB::AggregatedDataVariants&amp;, std::vector&lt;DB::IColumn const*, std::allocator&lt;DB::IColumn const*&gt; &gt;&amp;, std::vector&lt;std::vector&lt;DB::IColumn const*, std::allocator&lt;DB::IColumn const*&gt; &gt;, std::allocator&lt;std::vector&lt;DB::IColumn const*, std::allocator&lt;DB::IColumn const*&gt; &gt; &gt; &gt;&amp;, bool&amp;) DB::Aggregator::execute(std::shared_ptr&lt;DB::IBlockInputStream&gt; const&amp;, DB::AggregatedDataVariants&amp;) DB::AggregatingBlockInputStream::readImpl() DB::IBlockInputStream::read() DB::ExpressionBlockInputStream::readImpl() DB::IBlockInputStream::read() DB::ExpressionBlockInputStream::readImpl() DB::IBlockInputStream::read() DB::AsynchronousBlockInputStream::calculate() std::_Function_handler&lt;void (), DB::AsynchronousBlockInputStream::next()::{lambda()#1}&gt;::_M_invoke(std::_Any_data const&amp;) ThreadPoolImpl&lt;ThreadFromGlobalPool&gt;::worker(std::_List_iterator&lt;ThreadFromGlobalPool&gt;) ThreadFromGlobalPool::ThreadFromGlobalPool&lt;ThreadPoolImpl&lt;ThreadFromGlobalPool&gt;::scheduleImpl&lt;void&gt;(std::function&lt;void ()&gt;, int, std::optional&lt;unsigned long&gt;)::{lambda()#3}&gt;(ThreadPoolImpl&lt;ThreadFromGlobalPool&gt;::scheduleImpl&lt;void&gt;(std::function&lt;void ()&gt;, int, std::optional&lt;unsigned long&gt;)::{lambda()#3}&amp;&amp;)::{lambda()#1}::operator()() const ThreadPoolImpl&lt;std::thread&gt;::worker(std::_List_iterator&lt;std::thread&gt;) execute_native_thread_routine start_thread clone  "},{"title":"tid​","type":1,"pageTitle":"Introspection Functions","url":"en/sql-reference/functions/introspection#tid","content":"Returns id of the thread, in which current Block is processed. Syntax tid()  Returned value Current thread id. Uint64. Example Query: SELECT tid();  Result: ┌─tid()─┐ │ 3878 │ └───────┘  "},{"title":"logTrace​","type":1,"pageTitle":"Introspection Functions","url":"en/sql-reference/functions/introspection#logtrace","content":"Emits trace log message to server log for each Block. Syntax logTrace('message')  Arguments message — Message that is emitted to server log. String. Returned value Always returns 0. Example Query: SELECT logTrace('logTrace message');  Result: ┌─logTrace('logTrace message')─┐ │ 0 │ └──────────────────────────────┘  "},{"title":"Functions for Working with Dates and Times","type":0,"sectionRef":"#","url":"en/sql-reference/functions/date-time-functions","content":"","keywords":""},{"title":"timeZone​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#timezone","content":"Returns the timezone of the server. If it is executed in the context of a distributed table, then it generates a normal column with values relevant to each shard. Otherwise it produces a constant value. Syntax timeZone()  Alias: timezone. Returned value Timezone. Type: String. "},{"title":"toTimeZone​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#totimezone","content":"Converts time or date and time to the specified time zone. The time zone is an attribute of the Date and DateTime data types. The internal value (number of seconds) of the table field or of the resultset's column does not change, the column's type changes and its string representation changes accordingly. Syntax toTimezone(value, timezone)  Alias: toTimezone. Arguments value — Time or date and time. DateTime64.timezone — Timezone for the returned value. String. This argument is a constant, because toTimezone changes the timezone of a column (timezone is an attribute of DateTime* types). Returned value Date and time. Type: DateTime. Example Query: SELECT toDateTime('2019-01-01 00:00:00', 'UTC') AS time_utc, toTypeName(time_utc) AS type_utc, toInt32(time_utc) AS int32utc, toTimeZone(time_utc, 'Asia/Yekaterinburg') AS time_yekat, toTypeName(time_yekat) AS type_yekat, toInt32(time_yekat) AS int32yekat, toTimeZone(time_utc, 'US/Samoa') AS time_samoa, toTypeName(time_samoa) AS type_samoa, toInt32(time_samoa) AS int32samoa FORMAT Vertical;  Result: Row 1: ────── time_utc: 2019-01-01 00:00:00 type_utc: DateTime('UTC') int32utc: 1546300800 time_yekat: 2019-01-01 05:00:00 type_yekat: DateTime('Asia/Yekaterinburg') int32yekat: 1546300800 time_samoa: 2018-12-31 13:00:00 type_samoa: DateTime('US/Samoa') int32samoa: 1546300800  toTimeZone(time_utc, 'Asia/Yekaterinburg') changes the DateTime('UTC') type to DateTime('Asia/Yekaterinburg'). The value (Unixtimestamp) 1546300800 stays the same, but the string representation (the result of the toString() function) changes from time_utc: 2019-01-01 00:00:00 to time_yekat: 2019-01-01 05:00:00. "},{"title":"timeZoneOf​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#timezoneof","content":"Returns the timezone name of DateTime or DateTime64 data types. Syntax timeZoneOf(value)  Alias: timezoneOf. Arguments value — Date and time. DateTime or DateTime64. Returned value Timezone name. Type: String. Example Query: SELECT timezoneOf(now());  Result: ┌─timezoneOf(now())─┐ │ Etc/UTC │ └───────────────────┘  "},{"title":"timeZoneOffset​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#timezoneoffset","content":"Returns a timezone offset in seconds from UTC. The function takes into account daylight saving time and historical timezone changes at the specified date and time.IANA timezone database is used to calculate the offset. Syntax timeZoneOffset(value)  Alias: timezoneOffset. Arguments value — Date and time. DateTime or DateTime64. Returned value Offset from UTC in seconds. Type: Int32. Example Query: SELECT toDateTime('2021-04-21 10:20:30', 'America/New_York') AS Time, toTypeName(Time) AS Type, timeZoneOffset(Time) AS Offset_in_seconds, (Offset_in_seconds / 3600) AS Offset_in_hours;  Result: ┌────────────────Time─┬─Type─────────────────────────┬─Offset_in_seconds─┬─Offset_in_hours─┐ │ 2021-04-21 10:20:30 │ DateTime('America/New_York') │ -14400 │ -4 │ └─────────────────────┴──────────────────────────────┴───────────────────┴─────────────────┘  "},{"title":"toYear​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#toyear","content":"Converts a date or date with time to a UInt16 number containing the year number (AD). Alias: YEAR. "},{"title":"toQuarter​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#toquarter","content":"Converts a date or date with time to a UInt8 number containing the quarter number. Alias: QUARTER. "},{"title":"toMonth​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tomonth","content":"Converts a date or date with time to a UInt8 number containing the month number (1-12). Alias: MONTH. "},{"title":"toDayOfYear​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#todayofyear","content":"Converts a date or date with time to a UInt16 number containing the number of the day of the year (1-366). Alias: DAYOFYEAR. "},{"title":"toDayOfMonth​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#todayofmonth","content":"Converts a date or date with time to a UInt8 number containing the number of the day of the month (1-31). Aliases: DAYOFMONTH, DAY. "},{"title":"toDayOfWeek​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#todayofweek","content":"Converts a date or date with time to a UInt8 number containing the number of the day of the week (Monday is 1, and Sunday is 7). Alias: DAYOFWEEK. "},{"title":"toHour​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tohour","content":"Converts a date with time to a UInt8 number containing the number of the hour in 24-hour time (0-23). This function assumes that if clocks are moved ahead, it is by one hour and occurs at 2 a.m., and if clocks are moved back, it is by one hour and occurs at 3 a.m. (which is not always true – even in Moscow the clocks were twice changed at a different time). Alias: HOUR. "},{"title":"toMinute​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tominute","content":"Converts a date with time to a UInt8 number containing the number of the minute of the hour (0-59). Alias: MINUTE. "},{"title":"toSecond​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tosecond","content":"Converts a date with time to a UInt8 number containing the number of the second in the minute (0-59). Leap seconds are not accounted for. Alias: SECOND. "},{"title":"toUnixTimestamp​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#to-unix-timestamp","content":"For DateTime argument: converts value to the number with type UInt32 -- Unix Timestamp (https://en.wikipedia.org/wiki/Unix_time). For String argument: converts the input string to the datetime according to the timezone (optional second argument, server timezone is used by default) and returns the corresponding unix timestamp. Syntax toUnixTimestamp(datetime) toUnixTimestamp(str, [timezone])  Returned value Returns the unix timestamp. Type: UInt32. Example Query: SELECT toUnixTimestamp('2017-11-05 08:07:47', 'Asia/Tokyo') AS unix_timestamp  Result: ┌─unix_timestamp─┐ │ 1509836867 │ └────────────────┘  note The return type toStartOf* functions described below is Date or DateTime. Though these functions can take DateTime64 as an argument, passing them a DateTime64 that is out of the normal range (years 1925 - 2283) will give an incorrect result. "},{"title":"toStartOfYear​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tostartofyear","content":"Rounds down a date or date with time to the first day of the year. Returns the date. "},{"title":"toStartOfISOYear​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tostartofisoyear","content":"Rounds down a date or date with time to the first day of ISO year. Returns the date. "},{"title":"toStartOfQuarter​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tostartofquarter","content":"Rounds down a date or date with time to the first day of the quarter. The first day of the quarter is either 1 January, 1 April, 1 July, or 1 October. Returns the date. "},{"title":"toStartOfMonth​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tostartofmonth","content":"Rounds down a date or date with time to the first day of the month. Returns the date. note The behavior of parsing incorrect dates is implementation specific. ClickHouse may return zero date, throw an exception or do “natural” overflow. "},{"title":"toMonday​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tomonday","content":"Rounds down a date or date with time to the nearest Monday. Returns the date. "},{"title":"toStartOfWeek(t[,mode])​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tostartofweektmode","content":"Rounds down a date or date with time to the nearest Sunday or Monday by mode. Returns the date. The mode argument works exactly like the mode argument to toWeek(). For the single-argument syntax, a mode value of 0 is used. "},{"title":"toStartOfDay​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tostartofday","content":"Rounds down a date with time to the start of the day. "},{"title":"toStartOfHour​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tostartofhour","content":"Rounds down a date with time to the start of the hour. "},{"title":"toStartOfMinute​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tostartofminute","content":"Rounds down a date with time to the start of the minute. "},{"title":"toStartOfSecond​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tostartofsecond","content":"Truncates sub-seconds. Syntax toStartOfSecond(value, [timezone])  Arguments value — Date and time. DateTime64.timezone — Timezone for the returned value (optional). If not specified, the function uses the timezone of the value parameter. String. Returned value Input value without sub-seconds. Type: DateTime64. Examples Query without timezone: WITH toDateTime64('2020-01-01 10:20:30.999', 3) AS dt64 SELECT toStartOfSecond(dt64);  Result: ┌───toStartOfSecond(dt64)─┐ │ 2020-01-01 10:20:30.000 │ └─────────────────────────┘  Query with timezone: WITH toDateTime64('2020-01-01 10:20:30.999', 3) AS dt64 SELECT toStartOfSecond(dt64, 'Asia/Istanbul');  Result: ┌─toStartOfSecond(dt64, 'Asia/Istanbul')─┐ │ 2020-01-01 13:20:30.000 │ └────────────────────────────────────────┘  See also Timezone server configuration parameter. "},{"title":"toStartOfFiveMinutes​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tostartoffiveminutes","content":"Rounds down a date with time to the start of the five-minute interval. "},{"title":"toStartOfTenMinutes​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tostartoftenminutes","content":"Rounds down a date with time to the start of the ten-minute interval. "},{"title":"toStartOfFifteenMinutes​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tostartoffifteenminutes","content":"Rounds down the date with time to the start of the fifteen-minute interval. "},{"title":"toStartOfInterval(time_or_data, INTERVAL x unit [, time_zone])​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tostartofintervaltime-or-data-interval-x-unit-time-zone","content":"This is a generalization of other functions named toStartOf*. For example,toStartOfInterval(t, INTERVAL 1 year) returns the same as toStartOfYear(t),toStartOfInterval(t, INTERVAL 1 month) returns the same as toStartOfMonth(t),toStartOfInterval(t, INTERVAL 1 day) returns the same as toStartOfDay(t),toStartOfInterval(t, INTERVAL 15 minute) returns the same as toStartOfFifteenMinutes(t) etc. "},{"title":"toTime​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#totime","content":"Converts a date with time to a certain fixed date, while preserving the time. "},{"title":"toRelativeYearNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#torelativeyearnum","content":"Converts a date with time or date to the number of the year, starting from a certain fixed point in the past. "},{"title":"toRelativeQuarterNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#torelativequarternum","content":"Converts a date with time or date to the number of the quarter, starting from a certain fixed point in the past. "},{"title":"toRelativeMonthNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#torelativemonthnum","content":"Converts a date with time or date to the number of the month, starting from a certain fixed point in the past. "},{"title":"toRelativeWeekNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#torelativeweeknum","content":"Converts a date with time or date to the number of the week, starting from a certain fixed point in the past. "},{"title":"toRelativeDayNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#torelativedaynum","content":"Converts a date with time or date to the number of the day, starting from a certain fixed point in the past. "},{"title":"toRelativeHourNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#torelativehournum","content":"Converts a date with time or date to the number of the hour, starting from a certain fixed point in the past. "},{"title":"toRelativeMinuteNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#torelativeminutenum","content":"Converts a date with time or date to the number of the minute, starting from a certain fixed point in the past. "},{"title":"toRelativeSecondNum​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#torelativesecondnum","content":"Converts a date with time or date to the number of the second, starting from a certain fixed point in the past. "},{"title":"toISOYear​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#toisoyear","content":"Converts a date or date with time to a UInt16 number containing the ISO Year number. "},{"title":"toISOWeek​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#toisoweek","content":"Converts a date or date with time to a UInt8 number containing the ISO Week number. "},{"title":"toWeek(date[,mode])​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#toweekdatemode","content":"This function returns the week number for date or datetime. The two-argument form of toWeek() enables you to specify whether the week starts on Sunday or Monday and whether the return value should be in the range from 0 to 53 or from 1 to 53. If the mode argument is omitted, the default mode is 0.toISOWeek()is a compatibility function that is equivalent to toWeek(date,3). The following table describes how the mode argument works. Mode\tFirst day of week\tRange\tWeek 1 is the first week …0\tSunday\t0-53\twith a Sunday in this year 1\tMonday\t0-53\twith 4 or more days this year 2\tSunday\t1-53\twith a Sunday in this year 3\tMonday\t1-53\twith 4 or more days this year 4\tSunday\t0-53\twith 4 or more days this year 5\tMonday\t0-53\twith a Monday in this year 6\tSunday\t1-53\twith 4 or more days this year 7\tMonday\t1-53\twith a Monday in this year 8\tSunday\t1-53\tcontains January 1 9\tMonday\t1-53\tcontains January 1 For mode values with a meaning of “with 4 or more days this year,” weeks are numbered according to ISO 8601:1988: If the week containing January 1 has 4 or more days in the new year, it is week 1. Otherwise, it is the last week of the previous year, and the next week is week 1. For mode values with a meaning of “contains January 1”, the week contains January 1 is week 1. It does not matter how many days in the new year the week contained, even if it contained only one day. toWeek(date, [, mode][, Timezone])  Arguments date – Date or DateTime.mode – Optional parameter, Range of values is [0,9], default is 0.Timezone – Optional parameter, it behaves like any other conversion function. Example SELECT toDate('2016-12-27') AS date, toWeek(date) AS week0, toWeek(date,1) AS week1, toWeek(date,9) AS week9;  ┌───────date─┬─week0─┬─week1─┬─week9─┐ │ 2016-12-27 │ 52 │ 52 │ 1 │ └────────────┴───────┴───────┴───────┘  "},{"title":"toYearWeek(date[,mode])​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#toyearweekdatemode","content":"Returns year and week for a date. The year in the result may be different from the year in the date argument for the first and the last week of the year. The mode argument works exactly like the mode argument to toWeek(). For the single-argument syntax, a mode value of 0 is used. toISOYear()is a compatibility function that is equivalent to intDiv(toYearWeek(date,3),100). Example SELECT toDate('2016-12-27') AS date, toYearWeek(date) AS yearWeek0, toYearWeek(date,1) AS yearWeek1, toYearWeek(date,9) AS yearWeek9;  ┌───────date─┬─yearWeek0─┬─yearWeek1─┬─yearWeek9─┐ │ 2016-12-27 │ 201652 │ 201652 │ 201701 │ └────────────┴───────────┴───────────┴───────────┘  "},{"title":"date_trunc​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#date_trunc","content":"Truncates date and time data to the specified part of date. Syntax date_trunc(unit, value[, timezone])  Alias: dateTrunc. Arguments unit — The type of interval to truncate the result. String Literal. Possible values: secondminutehourdayweekmonthquarteryear value — Date and time. DateTime or DateTime64. timezone — Timezone name for the returned value (optional). If not specified, the function uses the timezone of the value parameter. String. Returned value Value, truncated to the specified part of date. Type: Datetime. Example Query without timezone: SELECT now(), date_trunc('hour', now());  Result: ┌───────────────now()─┬─date_trunc('hour', now())─┐ │ 2020-09-28 10:40:45 │ 2020-09-28 10:00:00 │ └─────────────────────┴───────────────────────────┘  Query with the specified timezone: SELECT now(), date_trunc('hour', now(), 'Asia/Istanbul');  Result: ┌───────────────now()─┬─date_trunc('hour', now(), 'Asia/Istanbul')─┐ │ 2020-09-28 10:46:26 │ 2020-09-28 13:00:00 │ └─────────────────────┴────────────────────────────────────────────┘  See Also toStartOfInterval "},{"title":"date_add​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#date_add","content":"Adds the time interval or date interval to the provided date or date with time. Syntax date_add(unit, value, date)  Aliases: dateAdd, DATE_ADD. Arguments unit — The type of interval to add. String. Possible values: secondminutehourdayweekmonthquarteryear value — Value of interval to add. Int. date — The date or date with time to which value is added. Date or DateTime. Returned value Date or date with time obtained by adding value, expressed in unit, to date. Type: Date or DateTime. Example Query: SELECT date_add(YEAR, 3, toDate('2018-01-01'));  Result: ┌─plus(toDate('2018-01-01'), toIntervalYear(3))─┐ │ 2021-01-01 │ └───────────────────────────────────────────────┘  "},{"title":"date_diff​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#date_diff","content":"Returns the difference between two dates or dates with time values. Syntax date_diff('unit', startdate, enddate, [timezone])  Aliases: dateDiff, DATE_DIFF. Arguments unit — The type of interval for result. String. Possible values: secondminutehourdayweekmonthquarteryear startdate — The first time value to subtract (the subtrahend). Date or DateTime. enddate — The second time value to subtract from (the minuend). Date or DateTime. timezone — Timezone name (optional). If specified, it is applied to both startdate and enddate. If not specified, timezones of startdate and enddate are used. If they are not the same, the result is unspecified. String. Returned value Difference between enddate and startdate expressed in unit. Type: Int. Example Query: SELECT dateDiff('hour', toDateTime('2018-01-01 22:00:00'), toDateTime('2018-01-02 23:00:00'));  Result: ┌─dateDiff('hour', toDateTime('2018-01-01 22:00:00'), toDateTime('2018-01-02 23:00:00'))─┐ │ 25 │ └────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"date_sub​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#date_sub","content":"Subtracts the time interval or date interval from the provided date or date with time. Syntax date_sub(unit, value, date)  Aliases: dateSub, DATE_SUB. Arguments unit — The type of interval to subtract. String. Possible values: secondminutehourdayweekmonthquarteryear value — Value of interval to subtract. Int. date — The date or date with time from which value is subtracted. Date or DateTime. Returned value Date or date with time obtained by subtracting value, expressed in unit, from date. Type: Date or DateTime. Example Query: SELECT date_sub(YEAR, 3, toDate('2018-01-01'));  Result: ┌─minus(toDate('2018-01-01'), toIntervalYear(3))─┐ │ 2015-01-01 │ └────────────────────────────────────────────────┘  "},{"title":"timestamp_add​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#timestamp_add","content":"Adds the specified time value with the provided date or date time value. Syntax timestamp_add(date, INTERVAL value unit)  Aliases: timeStampAdd, TIMESTAMP_ADD. Arguments date — Date or date with time. Date or DateTime. value — Value of interval to add. Int. unit — The type of interval to add. String. Possible values: secondminutehourdayweekmonthquarteryear Returned value Date or date with time with the specified value expressed in unit added to date. Type: Date or DateTime. Example Query: select timestamp_add(toDate('2018-01-01'), INTERVAL 3 MONTH);  Result: ┌─plus(toDate('2018-01-01'), toIntervalMonth(3))─┐ │ 2018-04-01 │ └────────────────────────────────────────────────┘  "},{"title":"timestamp_sub​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#timestamp_sub","content":"Subtracts the time interval from the provided date or date with time. Syntax timestamp_sub(unit, value, date)  Aliases: timeStampSub, TIMESTAMP_SUB. Arguments unit — The type of interval to subtract. String. Possible values: secondminutehourdayweekmonthquarteryear value — Value of interval to subtract. Int. date — Date or date with time. Date or DateTime. Returned value Date or date with time obtained by subtracting value, expressed in unit, from date. Type: Date or DateTime. Example Query: select timestamp_sub(MONTH, 5, toDateTime('2018-12-18 01:02:03'));  Result: ┌─minus(toDateTime('2018-12-18 01:02:03'), toIntervalMonth(5))─┐ │ 2018-07-18 01:02:03 │ └──────────────────────────────────────────────────────────────┘  "},{"title":"now​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#now","content":"Returns the current date and time. Syntax now([timezone])  Arguments timezone — Timezone name for the returned value (optional). String. Returned value Current date and time. Type: Datetime. Example Query without timezone: SELECT now();  Result: ┌───────────────now()─┐ │ 2020-10-17 07:42:09 │ └─────────────────────┘  Query with the specified timezone: SELECT now('Asia/Istanbul');  Result: ┌─now('Asia/Istanbul')─┐ │ 2020-10-17 10:42:23 │ └──────────────────────┘  "},{"title":"today​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#today","content":"Accepts zero arguments and returns the current date at one of the moments of request execution. The same as ‘toDate(now())’. "},{"title":"yesterday​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#yesterday","content":"Accepts zero arguments and returns yesterday’s date at one of the moments of request execution. The same as ‘today() - 1’. "},{"title":"timeSlot​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#timeslot","content":"Rounds the time to the half hour. "},{"title":"toYYYYMM​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#toyyyymm","content":"Converts a date or date with time to a UInt32 number containing the year and month number (YYYY * 100 + MM). "},{"title":"toYYYYMMDD​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#toyyyymmdd","content":"Converts a date or date with time to a UInt32 number containing the year and month number (YYYY * 10000 + MM * 100 + DD). "},{"title":"toYYYYMMDDhhmmss​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#toyyyymmddhhmmss","content":"Converts a date or date with time to a UInt64 number containing the year and month number (YYYY * 10000000000 + MM * 100000000 + DD * 1000000 + hh * 10000 + mm * 100 + ss). "},{"title":"addYears, addMonths, addWeeks, addDays, addHours, addMinutes, addSeconds, addQuarters​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#addyears-addmonths-addweeks-adddays-addhours-addminutes-addseconds-addquarters","content":"Function adds a Date/DateTime interval to a Date/DateTime and then return the Date/DateTime. For example: WITH toDate('2018-01-01') AS date, toDateTime('2018-01-01 00:00:00') AS date_time SELECT addYears(date, 1) AS add_years_with_date, addYears(date_time, 1) AS add_years_with_date_time  ┌─add_years_with_date─┬─add_years_with_date_time─┐ │ 2019-01-01 │ 2019-01-01 00:00:00 │ └─────────────────────┴──────────────────────────┘  "},{"title":"subtractYears, subtractMonths, subtractWeeks, subtractDays, subtractHours, subtractMinutes, subtractSeconds, subtractQuarters​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#subtractyears-subtractmonths-subtractweeks-subtractdays-subtracthours-subtractminutes-subtractseconds-subtractquarters","content":"Function subtract a Date/DateTime interval to a Date/DateTime and then return the Date/DateTime. For example: WITH toDate('2019-01-01') AS date, toDateTime('2019-01-01 00:00:00') AS date_time SELECT subtractYears(date, 1) AS subtract_years_with_date, subtractYears(date_time, 1) AS subtract_years_with_date_time  ┌─subtract_years_with_date─┬─subtract_years_with_date_time─┐ │ 2018-01-01 │ 2018-01-01 00:00:00 │ └──────────────────────────┴───────────────────────────────┘  "},{"title":"timeSlots(StartTime, Duration,[, Size])​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#timeslotsstarttime-duration-size","content":"For a time interval starting at ‘StartTime’ and continuing for ‘Duration’ seconds, it returns an array of moments in time, consisting of points from this interval rounded down to the ‘Size’ in seconds. ‘Size’ is an optional parameter: a constant UInt32, set to 1800 by default. For example, timeSlots(toDateTime('2012-01-01 12:20:00'), 600) = [toDateTime('2012-01-01 12:00:00'), toDateTime('2012-01-01 12:30:00')]. This is necessary for searching for pageviews in the corresponding session. "},{"title":"formatDateTime​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#formatdatetime","content":"Formats a Time according to the given Format string. Format is a constant expression, so you cannot have multiple formats for a single result column. Syntax formatDateTime(Time, Format\\[, Timezone\\])  Returned value(s) Returns time and date values according to the determined format. Replacement fieldsUsing replacement fields, you can define a pattern for the resulting string. “Example” column shows formatting result for 2018-01-02 22:33:44. Placeholder\tDescription\tExample%C\tyear divided by 100 and truncated to integer (00-99)\t20 %d\tday of the month, zero-padded (01-31)\t02 %D\tShort MM/DD/YY date, equivalent to %m/%d/%y\t01/02/18 %e\tday of the month, space-padded ( 1-31) 2 %F\tshort YYYY-MM-DD date, equivalent to %Y-%m-%d\t2018-01-02 %G\tfour-digit year format for ISO week number, calculated from the week-based year defined by the ISO 8601 standard, normally useful only with %V\t2018 %g\ttwo-digit year format, aligned to ISO 8601, abbreviated from four-digit notation\t18 %H\thour in 24h format (00-23)\t22 %I\thour in 12h format (01-12)\t10 %j\tday of the year (001-366)\t002 %m\tmonth as a decimal number (01-12)\t01 %M\tminute (00-59)\t33 %n\tnew-line character (‘’) %p\tAM or PM designation\tPM %Q\tQuarter (1-4)\t1 %R\t24-hour HH:MM time, equivalent to %H:%M\t22:33 %S\tsecond (00-59)\t44 %t\thorizontal-tab character (’) %T\tISO 8601 time format (HH:MM:SS), equivalent to %H:%M:%S\t22:33:44 %u\tISO 8601 weekday as number with Monday as 1 (1-7)\t2 %V\tISO 8601 week number (01-53)\t01 %w\tweekday as a decimal number with Sunday as 0 (0-6)\t2 %y\tYear, last two digits (00-99)\t18 %Y\tYear\t2018 %%\ta % sign\t% Example Query: SELECT formatDateTime(toDate('2010-01-04'), '%g')  Result: ┌─formatDateTime(toDate('2010-01-04'), '%g')─┐ │ 10 │ └────────────────────────────────────────────┘  "},{"title":"dateName​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#dataname","content":"Returns specified part of date. Syntax dateName(date_part, date)  Arguments date_part — Date part. Possible values: 'year', 'quarter', 'month', 'week', 'dayofyear', 'day', 'weekday', 'hour', 'minute', 'second'. String.date — Date. Date, DateTime or DateTime64.timezone — Timezone. Optional. String. Returned value The specified part of date. Type: String Example Query: WITH toDateTime('2021-04-14 11:22:33') AS date_value SELECT dateName('year', date_value), dateName('month', date_value), dateName('day', date_value);  Result: ┌─dateName('year', date_value)─┬─dateName('month', date_value)─┬─dateName('day', date_value)─┐ │ 2021 │ April │ 14 │ └──────────────────────────────┴───────────────────────────────┴─────────────────────────────  "},{"title":"FROM_UNIXTIME​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#fromunixfime","content":"Function converts Unix timestamp to a calendar date and a time of a day. When there is only a single argument of Integer type, it acts in the same way as toDateTime and return DateTime type. Example: Query: SELECT FROM_UNIXTIME(423543535);  Result: ┌─FROM_UNIXTIME(423543535)─┐ │ 1983-06-04 10:58:55 │ └──────────────────────────┘  When there are two arguments: first is an Integer or DateTime, second is a constant format string — it acts in the same way as formatDateTime and return String type. For example: SELECT FROM_UNIXTIME(1234334543, '%Y-%m-%d %R:%S') AS DateTime;  ┌─DateTime────────────┐ │ 2009-02-11 14:42:23 │ └─────────────────────┘  "},{"title":"toModifiedJulianDay​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tomodifiedjulianday","content":"Converts a Proleptic Gregorian calendar date in text form YYYY-MM-DD to a Modified Julian Day number in Int32. This function supports date from 0000-01-01 to 9999-12-31. It raises an exception if the argument cannot be parsed as a date, or the date is invalid. Syntax toModifiedJulianDay(date)  Arguments date — Date in text form. String or FixedString. Returned value Modified Julian Day number. Type: Int32. Example Query: SELECT toModifiedJulianDay('2020-01-01');  Result: ┌─toModifiedJulianDay('2020-01-01')─┐ │ 58849 │ └───────────────────────────────────┘  "},{"title":"toModifiedJulianDayOrNull​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#tomodifiedjuliandayornull","content":"Similar to toModifiedJulianDay(), but instead of raising exceptions it returns NULL. Syntax toModifiedJulianDayOrNull(date)  Arguments date — Date in text form. String or FixedString. Returned value Modified Julian Day number. Type: Nullable(Int32). Example Query: SELECT toModifiedJulianDayOrNull('2020-01-01');  Result: ┌─toModifiedJulianDayOrNull('2020-01-01')─┐ │ 58849 │ └─────────────────────────────────────────┘  "},{"title":"fromModifiedJulianDay​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#frommodifiedjulianday","content":"Converts a Modified Julian Day number to a Proleptic Gregorian calendar date in text form YYYY-MM-DD. This function supports day number from -678941 to 2973119 (which represent 0000-01-01 and 9999-12-31 respectively). It raises an exception if the day number is outside of the supported range. Syntax fromModifiedJulianDay(day)  Arguments day — Modified Julian Day number. Any integral types. Returned value Date in text form. Type: String Example Query: SELECT fromModifiedJulianDay(58849);  Result: ┌─fromModifiedJulianDay(58849)─┐ │ 2020-01-01 │ └──────────────────────────────┘  "},{"title":"fromModifiedJulianDayOrNull​","type":1,"pageTitle":"Functions for Working with Dates and Times","url":"en/sql-reference/functions/date-time-functions#frommodifiedjuliandayornull","content":"Similar to fromModifiedJulianDayOrNull(), but instead of raising exceptions it returns NULL. Syntax fromModifiedJulianDayOrNull(day)  Arguments day — Modified Julian Day number. Any integral types. Returned value Date in text form. Type: Nullable(String) Example Query: SELECT fromModifiedJulianDayOrNull(58849);  Result: ┌─fromModifiedJulianDayOrNull(58849)─┐ │ 2020-01-01 │ └────────────────────────────────────┘  "},{"title":"Storing Dictionaries in Memory","type":0,"sectionRef":"#","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout","content":"","keywords":""},{"title":"Ways to Store Dictionaries in Memory​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#ways-to-store-dictionaries-in-memory","content":"flathashedsparse_hashedcomplex_key_hashedcomplex_key_sparse_hashedhashed_arraycomplex_key_hashed_arrayrange_hashedcomplex_key_range_hashedcachecomplex_key_cachessd_cachecomplex_key_ssd_cachedirectcomplex_key_directip_trie "},{"title":"flat​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#flat","content":"The dictionary is completely stored in memory in the form of flat arrays. How much memory does the dictionary use? The amount is proportional to the size of the largest key (in space used). The dictionary key has the UInt64 type and the value is limited to max_array_size (by default — 500,000). If a larger key is discovered when creating the dictionary, ClickHouse throws an exception and does not create the dictionary. Dictionary flat arrays initial size is controlled by initial_array_size setting (by default — 1024). All types of sources are supported. When updating, data (from a file or from a table) is read in it entirety. This method provides the best performance among all available methods of storing the dictionary. Configuration example: &lt;layout&gt; &lt;flat&gt; &lt;initial_array_size&gt;50000&lt;/initial_array_size&gt; &lt;max_array_size&gt;5000000&lt;/max_array_size&gt; &lt;/flat&gt; &lt;/layout&gt;  or LAYOUT(FLAT(INITIAL_ARRAY_SIZE 50000 MAX_ARRAY_SIZE 5000000))  "},{"title":"hashed​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#dicts-external_dicts_dict_layout-hashed","content":"The dictionary is completely stored in memory in the form of a hash table. The dictionary can contain any number of elements with any identifiers In practice, the number of keys can reach tens of millions of items. If preallocate is true (default is false) the hash table will be preallocated (this will make the dictionary load faster). But note that you should use it only if: The source support an approximate number of elements (for now it is supported only by the ClickHouse source).There are no duplicates in the data (otherwise it may increase memory usage for the hashtable). All types of sources are supported. When updating, data (from a file or from a table) is read in its entirety. Configuration example: &lt;layout&gt; &lt;hashed&gt; &lt;preallocate&gt;0&lt;/preallocate&gt; &lt;/hashed&gt; &lt;/layout&gt;  or LAYOUT(HASHED(PREALLOCATE 0))  "},{"title":"sparse_hashed​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#dicts-external_dicts_dict_layout-sparse_hashed","content":"Similar to hashed, but uses less memory in favor more CPU usage. It will be also preallocated so as hashed (with preallocate set to true), and note that it is even more significant for sparse_hashed. Configuration example: &lt;layout&gt; &lt;sparse_hashed /&gt; &lt;/layout&gt;  or LAYOUT(SPARSE_HASHED([PREALLOCATE 0]))  "},{"title":"complex_key_hashed​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-hashed","content":"This type of storage is for use with composite keys. Similar to hashed. Configuration example: &lt;layout&gt; &lt;complex_key_hashed /&gt; &lt;/layout&gt;  or LAYOUT(COMPLEX_KEY_HASHED())  "},{"title":"complex_key_sparse_hashed​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-sparse-hashed","content":"This type of storage is for use with composite keys. Similar to sparse_hashed. Configuration example: &lt;layout&gt; &lt;complex_key_sparse_hashed /&gt; &lt;/layout&gt;  or LAYOUT(COMPLEX_KEY_SPARSE_HASHED())  "},{"title":"hashed_array​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#dicts-external_dicts_dict_layout-hashed-array","content":"The dictionary is completely stored in memory. Each attribute is stored in an array. The key attribute is stored in the form of a hashed table where value is an index in the attributes array. The dictionary can contain any number of elements with any identifiers. In practice, the number of keys can reach tens of millions of items. All types of sources are supported. When updating, data (from a file or from a table) is read in its entirety. Configuration example: &lt;layout&gt; &lt;hashed_array&gt; &lt;/hashed_array&gt; &lt;/layout&gt;  or LAYOUT(HASHED_ARRAY())  "},{"title":"complex_key_hashed_array​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-hashed-array","content":"This type of storage is for use with composite keys. Similar to hashed_array. Configuration example: &lt;layout&gt; &lt;complex_key_hashed_array /&gt; &lt;/layout&gt;  or LAYOUT(COMPLEX_KEY_HASHED_ARRAY())  "},{"title":"range_hashed​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#range-hashed","content":"The dictionary is stored in memory in the form of a hash table with an ordered array of ranges and their corresponding values. This storage method works the same way as hashed and allows using date/time (arbitrary numeric type) ranges in addition to the key. Example: The table contains discounts for each advertiser in the format: +---------|-------------|-------------|------+ | advertiser id | discount start date | discount end date | amount | +===============+=====================+===================+========+ | 123 | 2015-01-01 | 2015-01-15 | 0.15 | +---------|-------------|-------------|------+ | 123 | 2015-01-16 | 2015-01-31 | 0.25 | +---------|-------------|-------------|------+ | 456 | 2015-01-01 | 2015-01-15 | 0.05 | +---------|-------------|-------------|------+  To use a sample for date ranges, define the range_min and range_max elements in the structure. These elements must contain elements name and type (if type is not specified, the default type will be used - Date). type can be any numeric type (Date / DateTime / UInt64 / Int32 / others). warning Values of range_min and range_max should fit in Int64 type. Example: &lt;structure&gt; &lt;id&gt; &lt;name&gt;Id&lt;/name&gt; &lt;/id&gt; &lt;range_min&gt; &lt;name&gt;first&lt;/name&gt; &lt;type&gt;Date&lt;/type&gt; &lt;/range_min&gt; &lt;range_max&gt; &lt;name&gt;last&lt;/name&gt; &lt;type&gt;Date&lt;/type&gt; &lt;/range_max&gt; ...  or CREATE DICTIONARY somedict ( id UInt64, first Date, last Date ) PRIMARY KEY id LAYOUT(RANGE_HASHED()) RANGE(MIN first MAX last)  To work with these dictionaries, you need to pass an additional argument to the dictGetT function, for which a range is selected: dictGetT('dict_name', 'attr_name', id, date)  This function returns the value for the specified ids and the date range that includes the passed date. Details of the algorithm: If the id is not found or a range is not found for the id, it returns the default value for the dictionary.If there are overlapping ranges, it returns value for any (random) range.If the range delimiter is NULL or an invalid date (such as 1900-01-01), the range is open. The range can be open on both sides. Configuration example: &lt;clickhouse&gt; &lt;dictionary&gt; ... &lt;layout&gt; &lt;range_hashed /&gt; &lt;/layout&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;Abcdef&lt;/name&gt; &lt;/id&gt; &lt;range_min&gt; &lt;name&gt;StartTimeStamp&lt;/name&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;/range_min&gt; &lt;range_max&gt; &lt;name&gt;EndTimeStamp&lt;/name&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;/range_max&gt; &lt;attribute&gt; &lt;name&gt;XXXType&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value /&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;/dictionary&gt; &lt;/clickhouse&gt;  or CREATE DICTIONARY somedict( Abcdef UInt64, StartTimeStamp UInt64, EndTimeStamp UInt64, XXXType String DEFAULT '' ) PRIMARY KEY Abcdef RANGE(MIN StartTimeStamp MAX EndTimeStamp)  "},{"title":"complex_key_range_hashed​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-range-hashed","content":"The dictionary is stored in memory in the form of a hash table with an ordered array of ranges and their corresponding values (see range_hashed). This type of storage is for use with composite keys. Configuration example: CREATE DICTIONARY range_dictionary ( CountryID UInt64, CountryKey String, StartDate Date, EndDate Date, Tax Float64 DEFAULT 0.2 ) PRIMARY KEY CountryID, CountryKey SOURCE(CLICKHOUSE(TABLE 'date_table')) LIFETIME(MIN 1 MAX 1000) LAYOUT(COMPLEX_KEY_RANGE_HASHED()) RANGE(MIN StartDate MAX EndDate);  "},{"title":"cache​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#cache","content":"The dictionary is stored in a cache that has a fixed number of cells. These cells contain frequently used elements. When searching for a dictionary, the cache is searched first. For each block of data, all keys that are not found in the cache or are outdated are requested from the source using SELECT attrs... FROM db.table WHERE id IN (k1, k2, ...). The received data is then written to the cache. If keys are not found in dictionary, then update cache task is created and added into update queue. Update queue properties can be controlled with settings max_update_queue_size, update_queue_push_timeout_milliseconds, query_wait_timeout_milliseconds, max_threads_for_updates. For cache dictionaries, the expiration lifetime of data in the cache can be set. If more time than lifetime has passed since loading the data in a cell, the cell’s value is not used and key becomes expired. The key is re-requested the next time it needs to be used. This behaviour can be configured with setting allow_read_expired_keys. This is the least effective of all the ways to store dictionaries. The speed of the cache depends strongly on correct settings and the usage scenario. A cache type dictionary performs well only when the hit rates are high enough (recommended 99% and higher). You can view the average hit rate in the system.dictionaries table. If setting allow_read_expired_keys is set to 1, by default 0. Then dictionary can support asynchronous updates. If a client requests keys and all of them are in cache, but some of them are expired, then dictionary will return expired keys for a client and request them asynchronously from the source. To improve cache performance, use a subquery with LIMIT, and call the function with the dictionary externally. Supported sources: MySQL, ClickHouse, executable, HTTP. Example of settings: &lt;layout&gt; &lt;cache&gt; &lt;!-- The size of the cache, in number of cells. Rounded up to a power of two. --&gt; &lt;size_in_cells&gt;1000000000&lt;/size_in_cells&gt; &lt;!-- Allows to read expired keys. --&gt; &lt;allow_read_expired_keys&gt;0&lt;/allow_read_expired_keys&gt; &lt;!-- Max size of update queue. --&gt; &lt;max_update_queue_size&gt;100000&lt;/max_update_queue_size&gt; &lt;!-- Max timeout in milliseconds for push update task into queue. --&gt; &lt;update_queue_push_timeout_milliseconds&gt;10&lt;/update_queue_push_timeout_milliseconds&gt; &lt;!-- Max wait timeout in milliseconds for update task to complete. --&gt; &lt;query_wait_timeout_milliseconds&gt;60000&lt;/query_wait_timeout_milliseconds&gt; &lt;!-- Max threads for cache dictionary update. --&gt; &lt;max_threads_for_updates&gt;4&lt;/max_threads_for_updates&gt; &lt;/cache&gt; &lt;/layout&gt;  or LAYOUT(CACHE(SIZE_IN_CELLS 1000000000))  Set a large enough cache size. You need to experiment to select the number of cells: Set some value.Run queries until the cache is completely full.Assess memory consumption using the system.dictionaries table.Increase or decrease the number of cells until the required memory consumption is reached. warning Do not use ClickHouse as a source, because it is slow to process queries with random reads. "},{"title":"complex_key_cache​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-cache","content":"This type of storage is for use with composite keys. Similar to cache. "},{"title":"ssd_cache​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#ssd-cache","content":"Similar to cache, but stores data on SSD and index in RAM. All cache dictionary settings related to update queue can also be applied to SSD cache dictionaries. &lt;layout&gt; &lt;ssd_cache&gt; &lt;!-- Size of elementary read block in bytes. Recommended to be equal to SSD's page size. --&gt; &lt;block_size&gt;4096&lt;/block_size&gt; &lt;!-- Max cache file size in bytes. --&gt; &lt;file_size&gt;16777216&lt;/file_size&gt; &lt;!-- Size of RAM buffer in bytes for reading elements from SSD. --&gt; &lt;read_buffer_size&gt;131072&lt;/read_buffer_size&gt; &lt;!-- Size of RAM buffer in bytes for aggregating elements before flushing to SSD. --&gt; &lt;write_buffer_size&gt;1048576&lt;/write_buffer_size&gt; &lt;!-- Path where cache file will be stored. --&gt; &lt;path&gt;/var/lib/clickhouse/user_files/test_dict&lt;/path&gt; &lt;/ssd_cache&gt; &lt;/layout&gt;  or LAYOUT(SSD_CACHE(BLOCK_SIZE 4096 FILE_SIZE 16777216 READ_BUFFER_SIZE 1048576 PATH '/var/lib/clickhouse/user_files/test_dict'))  "},{"title":"complex_key_ssd_cache​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-ssd-cache","content":"This type of storage is for use with composite keys. Similar to ssd_cache. "},{"title":"direct​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#direct","content":"The dictionary is not stored in memory and directly goes to the source during the processing of a request. The dictionary key has the UInt64 type. All types of sources, except local files, are supported. Configuration example: &lt;layout&gt; &lt;direct /&gt; &lt;/layout&gt;  or LAYOUT(DIRECT())  "},{"title":"complex_key_direct​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#complex-key-direct","content":"This type of storage is for use with composite keys. Similar to direct. "},{"title":"ip_trie​","type":1,"pageTitle":"Storing Dictionaries in Memory","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-layout#ip-trie","content":"This type of storage is for mapping network prefixes (IP addresses) to metadata such as ASN. Example: The table contains network prefixes and their corresponding AS number and country code:  +-----------|-----|------+ | prefix | asn | cca2 | +=================+=======+========+ | 202.79.32.0/20 | 17501 | NP | +-----------|-----|------+ | 2620:0:870::/48 | 3856 | US | +-----------|-----|------+ | 2a02:6b8:1::/48 | 13238 | RU | +-----------|-----|------+ | 2001:db8::/32 | 65536 | ZZ | +-----------|-----|------+  When using this type of layout, the structure must have a composite key. Example: &lt;structure&gt; &lt;key&gt; &lt;attribute&gt; &lt;name&gt;prefix&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;/attribute&gt; &lt;/key&gt; &lt;attribute&gt; &lt;name&gt;asn&lt;/name&gt; &lt;type&gt;UInt32&lt;/type&gt; &lt;null_value /&gt; &lt;/attribute&gt; &lt;attribute&gt; &lt;name&gt;cca2&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value&gt;??&lt;/null_value&gt; &lt;/attribute&gt; ... &lt;/structure&gt; &lt;layout&gt; &lt;ip_trie&gt; &lt;!-- Key attribute `prefix` can be retrieved via dictGetString. --&gt; &lt;!-- This option increases memory usage. --&gt; &lt;access_to_key_from_attributes&gt;true&lt;/access_to_key_from_attributes&gt; &lt;/ip_trie&gt; &lt;/layout&gt;  or CREATE DICTIONARY somedict ( prefix String, asn UInt32, cca2 String DEFAULT '??' ) PRIMARY KEY prefix  The key must have only one String type attribute that contains an allowed IP prefix. Other types are not supported yet. For queries, you must use the same functions (dictGetT with a tuple) as for dictionaries with composite keys: dictGetT('dict_name', 'attr_name', tuple(ip))  The function takes either UInt32 for IPv4, or FixedString(16) for IPv6: dictGetString('prefix', 'asn', tuple(IPv6StringToNum('2001:db8::1')))  Other types are not supported yet. The function returns the attribute for the prefix that corresponds to this IP address. If there are overlapping prefixes, the most specific one is returned. Data must completely fit into RAM. "},{"title":"Functions for Working with IPv4 and IPv6 Addresses","type":0,"sectionRef":"#","url":"en/sql-reference/functions/ip-address-functions","content":"","keywords":""},{"title":"IPv4NumToString(num)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#ipv4numtostringnum","content":"Takes a UInt32 number. Interprets it as an IPv4 address in big endian. Returns a string containing the corresponding IPv4 address in the format A.B.C.d (dot-separated numbers in decimal form). Alias: INET_NTOA. "},{"title":"IPv4StringToNum(s)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#ipv4stringtonums","content":"The reverse function of IPv4NumToString. If the IPv4 address has an invalid format, it throws exception. Alias: INET_ATON. "},{"title":"IPv4StringToNumOrDefault(s)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#ipv4stringtonums","content":"Same as IPv4StringToNum, but if the IPv4 address has an invalid format, it returns 0. "},{"title":"IPv4StringToNumOrNull(s)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#ipv4stringtonums","content":"Same as IPv4StringToNum, but if the IPv4 address has an invalid format, it returns null. "},{"title":"IPv4NumToStringClassC(num)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#ipv4numtostringclasscnum","content":"Similar to IPv4NumToString, but using xxx instead of the last octet. Example: SELECT IPv4NumToStringClassC(ClientIP) AS k, count() AS c FROM test.hits GROUP BY k ORDER BY c DESC LIMIT 10  ┌─k──────────────┬─────c─┐ │ 83.149.9.xxx │ 26238 │ │ 217.118.81.xxx │ 26074 │ │ 213.87.129.xxx │ 25481 │ │ 83.149.8.xxx │ 24984 │ │ 217.118.83.xxx │ 22797 │ │ 78.25.120.xxx │ 22354 │ │ 213.87.131.xxx │ 21285 │ │ 78.25.121.xxx │ 20887 │ │ 188.162.65.xxx │ 19694 │ │ 83.149.48.xxx │ 17406 │ └────────────────┴───────┘  Since using ‘xxx’ is highly unusual, this may be changed in the future. We recommend that you do not rely on the exact format of this fragment. "},{"title":"IPv6NumToString(x)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#ipv6numtostringx","content":"Accepts a FixedString(16) value containing the IPv6 address in binary format. Returns a string containing this address in text format. IPv6-mapped IPv4 addresses are output in the format ::ffff:111.222.33.44. Alias: INET6_NTOA. Examples: SELECT IPv6NumToString(toFixedString(unhex('2A0206B8000000000000000000000011'), 16)) AS addr;  ┌─addr─────────┐ │ 2a02:6b8::11 │ └──────────────┘  SELECT IPv6NumToString(ClientIP6 AS k), count() AS c FROM hits_all WHERE EventDate = today() AND substring(ClientIP6, 1, 12) != unhex('00000000000000000000FFFF') GROUP BY k ORDER BY c DESC LIMIT 10  ┌─IPv6NumToString(ClientIP6)──────────────┬─────c─┐ │ 2a02:2168:aaa:bbbb::2 │ 24695 │ │ 2a02:2698:abcd:abcd:abcd:abcd:8888:5555 │ 22408 │ │ 2a02:6b8:0:fff::ff │ 16389 │ │ 2a01:4f8:111:6666::2 │ 16016 │ │ 2a02:2168:888:222::1 │ 15896 │ │ 2a01:7e00::ffff:ffff:ffff:222 │ 14774 │ │ 2a02:8109:eee:ee:eeee:eeee:eeee:eeee │ 14443 │ │ 2a02:810b:8888:888:8888:8888:8888:8888 │ 14345 │ │ 2a02:6b8:0:444:4444:4444:4444:4444 │ 14279 │ │ 2a01:7e00::ffff:ffff:ffff:ffff │ 13880 │ └─────────────────────────────────────────┴───────┘  SELECT IPv6NumToString(ClientIP6 AS k), count() AS c FROM hits_all WHERE EventDate = today() GROUP BY k ORDER BY c DESC LIMIT 10  ┌─IPv6NumToString(ClientIP6)─┬──────c─┐ │ ::ffff:94.26.111.111 │ 747440 │ │ ::ffff:37.143.222.4 │ 529483 │ │ ::ffff:5.166.111.99 │ 317707 │ │ ::ffff:46.38.11.77 │ 263086 │ │ ::ffff:79.105.111.111 │ 186611 │ │ ::ffff:93.92.111.88 │ 176773 │ │ ::ffff:84.53.111.33 │ 158709 │ │ ::ffff:217.118.11.22 │ 154004 │ │ ::ffff:217.118.11.33 │ 148449 │ │ ::ffff:217.118.11.44 │ 148243 │ └────────────────────────────┴────────┘  "},{"title":"IPv6StringToNum​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#ipv6stringtonums","content":"The reverse function of IPv6NumToString. If the IPv6 address has an invalid format, it throws exception. If the input string contains a valid IPv4 address, returns its IPv6 equivalent. HEX can be uppercase or lowercase. Alias: INET6_ATON. Syntax IPv6StringToNum(string)  Argument string — IP address. String. Returned value IPv6 address in binary format. Type: FixedString(16). Example Query: SELECT addr, cutIPv6(IPv6StringToNum(addr), 0, 0) FROM (SELECT ['notaddress', '127.0.0.1', '1111::ffff'] AS addr) ARRAY JOIN addr;  Result: ┌─addr───────┬─cutIPv6(IPv6StringToNum(addr), 0, 0)─┐ │ notaddress │ :: │ │ 127.0.0.1 │ ::ffff:127.0.0.1 │ │ 1111::ffff │ 1111::ffff │ └────────────┴──────────────────────────────────────┘  See Also cutIPv6. "},{"title":"IPv6StringToNumOrDefault(s)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#ipv6stringtonums","content":"Same as IPv6StringToNum, but if the IPv6 address has an invalid format, it returns 0. "},{"title":"IPv6StringToNumOrNull(s)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#ipv6stringtonums","content":"Same as IPv6StringToNum, but if the IPv6 address has an invalid format, it returns null. "},{"title":"IPv4ToIPv6(x)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#ipv4toipv6x","content":"Takes a UInt32 number. Interprets it as an IPv4 address in big endian. Returns a FixedString(16) value containing the IPv6 address in binary format. Examples: SELECT IPv6NumToString(IPv4ToIPv6(IPv4StringToNum('192.168.0.1'))) AS addr;  ┌─addr───────────────┐ │ ::ffff:192.168.0.1 │ └────────────────────┘  "},{"title":"cutIPv6(x, bytesToCutForIPv6, bytesToCutForIPv4)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#cutipv6x-bytestocutforipv6-bytestocutforipv4","content":"Accepts a FixedString(16) value containing the IPv6 address in binary format. Returns a string containing the address of the specified number of bytes removed in text format. For example: WITH IPv6StringToNum('2001:0DB8:AC10:FE01:FEED:BABE:CAFE:F00D') AS ipv6, IPv4ToIPv6(IPv4StringToNum('192.168.0.1')) AS ipv4 SELECT cutIPv6(ipv6, 2, 0), cutIPv6(ipv4, 0, 2)  ┌─cutIPv6(ipv6, 2, 0)─────────────────┬─cutIPv6(ipv4, 0, 2)─┐ │ 2001:db8:ac10:fe01:feed:babe:cafe:0 │ ::ffff:192.168.0.0 │ └─────────────────────────────────────┴─────────────────────┘  "},{"title":"IPv4CIDRToRange(ipv4, Cidr),​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#ipv4cidrtorangeipv4-cidr","content":"Accepts an IPv4 and an UInt8 value containing the CIDR. Return a tuple with two IPv4 containing the lower range and the higher range of the subnet. SELECT IPv4CIDRToRange(toIPv4('192.168.5.2'), 16);  ┌─IPv4CIDRToRange(toIPv4('192.168.5.2'), 16)─┐ │ ('192.168.0.0','192.168.255.255') │ └────────────────────────────────────────────┘  "},{"title":"IPv6CIDRToRange(ipv6, Cidr),​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#ipv6cidrtorangeipv6-cidr","content":"Accepts an IPv6 and an UInt8 value containing the CIDR. Return a tuple with two IPv6 containing the lower range and the higher range of the subnet. SELECT IPv6CIDRToRange(toIPv6('2001:0db8:0000:85a3:0000:0000:ac1f:8001'), 32);  ┌─IPv6CIDRToRange(toIPv6('2001:0db8:0000:85a3:0000:0000:ac1f:8001'), 32)─┐ │ ('2001:db8::','2001:db8:ffff:ffff:ffff:ffff:ffff:ffff') │ └────────────────────────────────────────────────────────────────────────┘  "},{"title":"toIPv4(string)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#toipv4string","content":"An alias to IPv4StringToNum() that takes a string form of IPv4 address and returns value of IPv4 type, which is binary equal to value returned by IPv4StringToNum(). WITH '171.225.130.45' as IPv4_string SELECT toTypeName(IPv4StringToNum(IPv4_string)), toTypeName(toIPv4(IPv4_string))  ┌─toTypeName(IPv4StringToNum(IPv4_string))─┬─toTypeName(toIPv4(IPv4_string))─┐ │ UInt32 │ IPv4 │ └──────────────────────────────────────────┴─────────────────────────────────┘  WITH '171.225.130.45' as IPv4_string SELECT hex(IPv4StringToNum(IPv4_string)), hex(toIPv4(IPv4_string))  ┌─hex(IPv4StringToNum(IPv4_string))─┬─hex(toIPv4(IPv4_string))─┐ │ ABE1822D │ ABE1822D │ └───────────────────────────────────┴──────────────────────────┘  "},{"title":"toIPv4OrDefault(string)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#toipv4ordefaultstring","content":"Same as toIPv4, but if the IPv4 address has an invalid format, it returns 0. "},{"title":"toIPv4OrNull(string)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#toipv4ornullstring","content":"Same as toIPv4, but if the IPv4 address has an invalid format, it returns null. "},{"title":"toIPv6​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#toipv6string","content":"Converts a string form of IPv6 address to IPv6 type. If the IPv6 address has an invalid format, returns an empty value. Similar to IPv6StringToNum function, which converts IPv6 address to binary format. If the input string contains a valid IPv4 address, then the IPv6 equivalent of the IPv4 address is returned. Syntax toIPv6(string)  Argument string — IP address. String Returned value IP address. Type: IPv6. Examples Query: WITH '2001:438:ffff::407d:1bc1' AS IPv6_string SELECT hex(IPv6StringToNum(IPv6_string)), hex(toIPv6(IPv6_string));  Result: ┌─hex(IPv6StringToNum(IPv6_string))─┬─hex(toIPv6(IPv6_string))─────────┐ │ 20010438FFFF000000000000407D1BC1 │ 20010438FFFF000000000000407D1BC1 │ └───────────────────────────────────┴──────────────────────────────────┘  Query: SELECT toIPv6('127.0.0.1');  Result: ┌─toIPv6('127.0.0.1')─┐ │ ::ffff:127.0.0.1 │ └─────────────────────┘  "},{"title":"IPv6StringToNumOrDefault(s)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#toipv6ordefaultstring","content":"Same as toIPv6, but if the IPv6 address has an invalid format, it returns 0. "},{"title":"IPv6StringToNumOrNull(s)​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#toipv6ornullstring","content":"Same as toIPv6, but if the IPv6 address has an invalid format, it returns null. "},{"title":"isIPv4String​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#isipv4string","content":"Determines whether the input string is an IPv4 address or not. If string is IPv6 address returns 0. Syntax isIPv4String(string)  Arguments string — IP address. String. Returned value 1 if string is IPv4 address, 0 otherwise. Type: UInt8. Examples Query: SELECT addr, isIPv4String(addr) FROM ( SELECT ['0.0.0.0', '127.0.0.1', '::ffff:127.0.0.1'] AS addr ) ARRAY JOIN addr;  Result: ┌─addr─────────────┬─isIPv4String(addr)─┐ │ 0.0.0.0 │ 1 │ │ 127.0.0.1 │ 1 │ │ ::ffff:127.0.0.1 │ 0 │ └──────────────────┴────────────────────┘  "},{"title":"isIPv6String​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#isipv6string","content":"Determines whether the input string is an IPv6 address or not. If string is IPv4 address returns 0. Syntax isIPv6String(string)  Arguments string — IP address. String. Returned value 1 if string is IPv6 address, 0 otherwise. Type: UInt8. Examples Query: SELECT addr, isIPv6String(addr) FROM ( SELECT ['::', '1111::ffff', '::ffff:127.0.0.1', '127.0.0.1'] AS addr ) ARRAY JOIN addr;  Result: ┌─addr─────────────┬─isIPv6String(addr)─┐ │ :: │ 1 │ │ 1111::ffff │ 1 │ │ ::ffff:127.0.0.1 │ 1 │ │ 127.0.0.1 │ 0 │ └──────────────────┴────────────────────┘  "},{"title":"isIPAddressInRange​","type":1,"pageTitle":"Functions for Working with IPv4 and IPv6 Addresses","url":"en/sql-reference/functions/ip-address-functions#isipaddressinrange","content":"Determines if an IP address is contained in a network represented in the CIDR notation. Returns 1 if true, or 0 otherwise. Syntax isIPAddressInRange(address, prefix)  This function accepts both IPv4 and IPv6 addresses (and networks) represented as strings. It returns 0 if the IP version of the address and the CIDR don't match. Arguments address — An IPv4 or IPv6 address. String.prefix — An IPv4 or IPv6 network prefix in CIDR. String. Returned value 1 or 0. Type: UInt8. Example Query: SELECT isIPAddressInRange('127.0.0.1', '127.0.0.0/8');  Result: ┌─isIPAddressInRange('127.0.0.1', '127.0.0.0/8')─┐ │ 1 │ └────────────────────────────────────────────────┘  Query: SELECT isIPAddressInRange('127.0.0.1', 'ffff::/16');  Result: ┌─isIPAddressInRange('127.0.0.1', 'ffff::/16')─┐ │ 0 │ └──────────────────────────────────────────────┘  Query: SELECT isIPAddressInRange('::ffff:192.168.0.1', '::ffff:192.168.0.4/128');  Result: ┌─isIPAddressInRange('::ffff:192.168.0.1', '::ffff:192.168.0.4/128')─┐ │ 0 │ └────────────────────────────────────────────────────────────────────┘  "},{"title":"Functions for Working with JSON","type":0,"sectionRef":"#","url":"en/sql-reference/functions/json-functions","content":"","keywords":""},{"title":"visitParamHas(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#visitparamhasparams-name","content":"Checks whether there is a field with the name name. Alias: simpleJSONHas. "},{"title":"visitParamExtractUInt(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#visitparamextractuintparams-name","content":"Parses UInt64 from the value of the field named name. If this is a string field, it tries to parse a number from the beginning of the string. If the field does not exist, or it exists but does not contain a number, it returns 0. Alias: simpleJSONExtractUInt. "},{"title":"visitParamExtractInt(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#visitparamextractintparams-name","content":"The same as for Int64. Alias: simpleJSONExtractInt. "},{"title":"visitParamExtractFloat(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#visitparamextractfloatparams-name","content":"The same as for Float64. Alias: simpleJSONExtractFloat. "},{"title":"visitParamExtractBool(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#visitparamextractboolparams-name","content":"Parses a true/false value. The result is UInt8. Alias: simpleJSONExtractBool. "},{"title":"visitParamExtractRaw(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#visitparamextractrawparams-name","content":"Returns the value of a field, including separators. Alias: simpleJSONExtractRaw. Examples: visitParamExtractRaw('{&quot;abc&quot;:&quot;\\\\n\\\\u0000&quot;}', 'abc') = '&quot;\\\\n\\\\u0000&quot;'; visitParamExtractRaw('{&quot;abc&quot;:{&quot;def&quot;:[1,2,3]}}', 'abc') = '{&quot;def&quot;:[1,2,3]}';  "},{"title":"visitParamExtractString(params, name)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#visitparamextractstringparams-name","content":"Parses the string in double quotes. The value is unescaped. If unescaping failed, it returns an empty string. Alias: simpleJSONExtractString. Examples: visitParamExtractString('{&quot;abc&quot;:&quot;\\\\n\\\\u0000&quot;}', 'abc') = '\\n\\0'; visitParamExtractString('{&quot;abc&quot;:&quot;\\\\u263a&quot;}', 'abc') = '☺'; visitParamExtractString('{&quot;abc&quot;:&quot;\\\\u263&quot;}', 'abc') = ''; visitParamExtractString('{&quot;abc&quot;:&quot;hello}', 'abc') = '';  There is currently no support for code points in the format \\uXXXX\\uYYYY that are not from the basic multilingual plane (they are converted to CESU-8 instead of UTF-8). The following functions are based on simdjson designed for more complex JSON parsing requirements. The assumption 2 mentioned above still applies. "},{"title":"isValidJSON(json)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#isvalidjsonjson","content":"Checks that passed string is a valid json. Examples: SELECT isValidJSON('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}') = 1 SELECT isValidJSON('not a json') = 0  "},{"title":"JSONHas(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#jsonhasjson-indices-or-keys","content":"If the value exists in the JSON document, 1 will be returned. If the value does not exist, 0 will be returned. Examples: SELECT JSONHas('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b') = 1 SELECT JSONHas('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b', 4) = 0  indices_or_keys is a list of zero or more arguments each of them can be either string or integer. String = access object member by key.Positive integer = access the n-th member/key from the beginning.Negative integer = access the n-th member/key from the end. Minimum index of the element is 1. Thus the element 0 does not exist. You may use integers to access both JSON arrays and JSON objects. So, for example: SELECT JSONExtractKey('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 1) = 'a' SELECT JSONExtractKey('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 2) = 'b' SELECT JSONExtractKey('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', -1) = 'b' SELECT JSONExtractKey('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', -2) = 'a' SELECT JSONExtractString('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 1) = 'hello'  "},{"title":"JSONLength(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#jsonlengthjson-indices-or-keys","content":"Return the length of a JSON array or a JSON object. If the value does not exist or has a wrong type, 0 will be returned. Examples: SELECT JSONLength('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b') = 3 SELECT JSONLength('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}') = 2  "},{"title":"JSONType(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#jsontypejson-indices-or-keys","content":"Return the type of a JSON value. If the value does not exist, Null will be returned. Examples: SELECT JSONType('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}') = 'Object' SELECT JSONType('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'a') = 'String' SELECT JSONType('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b') = 'Array'  "},{"title":"JSONExtractUInt(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#jsonextractuintjson-indices-or-keys","content":""},{"title":"JSONExtractInt(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#jsonextractintjson-indices-or-keys","content":""},{"title":"JSONExtractFloat(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#jsonextractfloatjson-indices-or-keys","content":""},{"title":"JSONExtractBool(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#jsonextractbooljson-indices-or-keys","content":"Parses a JSON and extract a value. These functions are similar to visitParam functions. If the value does not exist or has a wrong type, 0 will be returned. Examples: SELECT JSONExtractInt('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b', 1) = -100 SELECT JSONExtractFloat('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b', 2) = 200.0 SELECT JSONExtractUInt('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b', -1) = 300  "},{"title":"JSONExtractString(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#jsonextractstringjson-indices-or-keys","content":"Parses a JSON and extract a string. This function is similar to visitParamExtractString functions. If the value does not exist or has a wrong type, an empty string will be returned. The value is unescaped. If unescaping failed, it returns an empty string. Examples: SELECT JSONExtractString('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'a') = 'hello' SELECT JSONExtractString('{&quot;abc&quot;:&quot;\\\\n\\\\u0000&quot;}', 'abc') = '\\n\\0' SELECT JSONExtractString('{&quot;abc&quot;:&quot;\\\\u263a&quot;}', 'abc') = '☺' SELECT JSONExtractString('{&quot;abc&quot;:&quot;\\\\u263&quot;}', 'abc') = '' SELECT JSONExtractString('{&quot;abc&quot;:&quot;hello}', 'abc') = ''  "},{"title":"JSONExtract(json[, indices_or_keys…], Return_type)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#jsonextractjson-indices-or-keys-return-type","content":"Parses a JSON and extract a value of the given ClickHouse data type. This is a generalization of the previous JSONExtract&lt;type&gt; functions. This meansJSONExtract(..., 'String') returns exactly the same as JSONExtractString(),JSONExtract(..., 'Float64') returns exactly the same as JSONExtractFloat(). Examples: SELECT JSONExtract('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'Tuple(String, Array(Float64))') = ('hello',[-100,200,300]) SELECT JSONExtract('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'Tuple(b Array(Float64), a String)') = ([-100,200,300],'hello') SELECT JSONExtract('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b', 'Array(Nullable(Int8))') = [-100, NULL, NULL] SELECT JSONExtract('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b', 4, 'Nullable(Int64)') = NULL SELECT JSONExtract('{&quot;passed&quot;: true}', 'passed', 'UInt8') = 1 SELECT JSONExtract('{&quot;day&quot;: &quot;Thursday&quot;}', 'day', 'Enum8(\\'Sunday\\' = 0, \\'Monday\\' = 1, \\'Tuesday\\' = 2, \\'Wednesday\\' = 3, \\'Thursday\\' = 4, \\'Friday\\' = 5, \\'Saturday\\' = 6)') = 'Thursday' SELECT JSONExtract('{&quot;day&quot;: 5}', 'day', 'Enum8(\\'Sunday\\' = 0, \\'Monday\\' = 1, \\'Tuesday\\' = 2, \\'Wednesday\\' = 3, \\'Thursday\\' = 4, \\'Friday\\' = 5, \\'Saturday\\' = 6)') = 'Friday'  "},{"title":"JSONExtractKeysAndValues(json[, indices_or_keys…], Value_type)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#jsonextractkeysandvaluesjson-indices-or-keys-value-type","content":"Parses key-value pairs from a JSON where the values are of the given ClickHouse data type. Example: SELECT JSONExtractKeysAndValues('{&quot;x&quot;: {&quot;a&quot;: 5, &quot;b&quot;: 7, &quot;c&quot;: 11}}', 'x', 'Int8') = [('a',5),('b',7),('c',11)];  "},{"title":"JSONExtractKeys​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#jsonextractkeysjson-indices-or-keys","content":"Parses a JSON string and extracts the keys. Syntax JSONExtractKeys(json[, a, b, c...])  Arguments json — String with valid JSON.a, b, c... — Comma-separated indices or keys that specify the path to the inner field in a nested JSON object. Each argument can be either a String to get the field by the key or an Integer to get the N-th field (indexed from 1, negative integers count from the end). If not set, the whole JSON is parsed as the top-level object. Optional parameter. Returned value Array with the keys of the JSON. Type: Array(String). Example Query: SELECT JSONExtractKeys('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}');  Result: text ┌─JSONExtractKeys('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}')─┐ │ ['a','b'] │ └────────────────────────────────────────────────────────────┘  "},{"title":"JSONExtractRaw(json[, indices_or_keys]…)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#jsonextractrawjson-indices-or-keys","content":"Returns a part of JSON as unparsed string. If the part does not exist or has a wrong type, an empty string will be returned. Example: SELECT JSONExtractRaw('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, 300]}', 'b') = '[-100, 200.0, 300]';  "},{"title":"JSONExtractArrayRaw(json[, indices_or_keys…])​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#jsonextractarrayrawjson-indices-or-keys","content":"Returns an array with elements of JSON array, each represented as unparsed string. If the part does not exist or isn’t array, an empty array will be returned. Example: SELECT JSONExtractArrayRaw('{&quot;a&quot;: &quot;hello&quot;, &quot;b&quot;: [-100, 200.0, &quot;hello&quot;]}', 'b') = ['-100', '200.0', '&quot;hello&quot;'];  "},{"title":"JSONExtractKeysAndValuesRaw​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#json-extract-keys-and-values-raw","content":"Extracts raw data from a JSON object. Syntax JSONExtractKeysAndValuesRaw(json[, p, a, t, h])  Arguments json — String with valid JSON.p, a, t, h — Comma-separated indices or keys that specify the path to the inner field in a nested JSON object. Each argument can be either a string to get the field by the key or an integer to get the N-th field (indexed from 1, negative integers count from the end). If not set, the whole JSON is parsed as the top-level object. Optional parameter. Returned values Array with ('key', 'value') tuples. Both tuple members are strings.Empty array if the requested object does not exist, or input JSON is invalid. Type: Array(Tuple(String, String). Examples Query: SELECT JSONExtractKeysAndValuesRaw('{&quot;a&quot;: [-100, 200.0], &quot;b&quot;:{&quot;c&quot;: {&quot;d&quot;: &quot;hello&quot;, &quot;f&quot;: &quot;world&quot;}}}');  Result: ┌─JSONExtractKeysAndValuesRaw('{&quot;a&quot;: [-100, 200.0], &quot;b&quot;:{&quot;c&quot;: {&quot;d&quot;: &quot;hello&quot;, &quot;f&quot;: &quot;world&quot;}}}')─┐ │ [('a','[-100,200]'),('b','{&quot;c&quot;:{&quot;d&quot;:&quot;hello&quot;,&quot;f&quot;:&quot;world&quot;}}')] │ └──────────────────────────────────────────────────────────────────────────────────────────────┘  Query: SELECT JSONExtractKeysAndValuesRaw('{&quot;a&quot;: [-100, 200.0], &quot;b&quot;:{&quot;c&quot;: {&quot;d&quot;: &quot;hello&quot;, &quot;f&quot;: &quot;world&quot;}}}', 'b');  Result: ┌─JSONExtractKeysAndValuesRaw('{&quot;a&quot;: [-100, 200.0], &quot;b&quot;:{&quot;c&quot;: {&quot;d&quot;: &quot;hello&quot;, &quot;f&quot;: &quot;world&quot;}}}', 'b')─┐ │ [('c','{&quot;d&quot;:&quot;hello&quot;,&quot;f&quot;:&quot;world&quot;}')] │ └───────────────────────────────────────────────────────────────────────────────────────────────────┘  Query: SELECT JSONExtractKeysAndValuesRaw('{&quot;a&quot;: [-100, 200.0], &quot;b&quot;:{&quot;c&quot;: {&quot;d&quot;: &quot;hello&quot;, &quot;f&quot;: &quot;world&quot;}}}', -1, 'c');  Result: ┌─JSONExtractKeysAndValuesRaw('{&quot;a&quot;: [-100, 200.0], &quot;b&quot;:{&quot;c&quot;: {&quot;d&quot;: &quot;hello&quot;, &quot;f&quot;: &quot;world&quot;}}}', -1, 'c')─┐ │ [('d','&quot;hello&quot;'),('f','&quot;world&quot;')] │ └───────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"JSON_EXISTS(json, path)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#json-exists","content":"If the value exists in the JSON document, 1 will be returned. If the value does not exist, 0 will be returned. Examples: SELECT JSON_EXISTS('{&quot;hello&quot;:1}', '$.hello'); SELECT JSON_EXISTS('{&quot;hello&quot;:{&quot;world&quot;:1}}', '$.hello.world'); SELECT JSON_EXISTS('{&quot;hello&quot;:[&quot;world&quot;]}', '$.hello[*]'); SELECT JSON_EXISTS('{&quot;hello&quot;:[&quot;world&quot;]}', '$.hello[0]');  note Before version 21.11 the order of arguments was wrong, i.e. JSON_EXISTS(path, json) "},{"title":"JSON_QUERY(json, path)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#json-query","content":"Parses a JSON and extract a value as JSON array or JSON object. If the value does not exist, an empty string will be returned. Example: SELECT JSON_QUERY('{&quot;hello&quot;:&quot;world&quot;}', '$.hello'); SELECT JSON_QUERY('{&quot;array&quot;:[[0, 1, 2, 3, 4, 5], [0, -1, -2, -3, -4, -5]]}', '$.array[*][0 to 2, 4]'); SELECT JSON_QUERY('{&quot;hello&quot;:2}', '$.hello'); SELECT toTypeName(JSON_QUERY('{&quot;hello&quot;:2}', '$.hello'));  Result: [&quot;world&quot;] [0, 1, 4, 0, -1, -4] [2] String  note Before version 21.11 the order of arguments was wrong, i.e. JSON_QUERY(path, json) "},{"title":"JSON_VALUE(json, path)​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#json-value","content":"Parses a JSON and extract a value as JSON scalar. If the value does not exist, an empty string will be returned. Example: SELECT JSON_VALUE('{&quot;hello&quot;:&quot;world&quot;}', '$.hello'); SELECT JSON_VALUE('{&quot;array&quot;:[[0, 1, 2, 3, 4, 5], [0, -1, -2, -3, -4, -5]]}', '$.array[*][0 to 2, 4]'); SELECT JSON_VALUE('{&quot;hello&quot;:2}', '$.hello'); SELECT toTypeName(JSON_VALUE('{&quot;hello&quot;:2}', '$.hello'));  Result: world 0 2 String  note Before version 21.11 the order of arguments was wrong, i.e. JSON_VALUE(path, json) "},{"title":"toJSONString​","type":1,"pageTitle":"Functions for Working with JSON","url":"en/sql-reference/functions/json-functions#tojsonstring","content":"Serializes a value to its JSON representation. Various data types and nested structures are supported. 64-bit integers or bigger (like UInt64 or Int128) are enclosed in quotes by default. output_format_json_quote_64bit_integers controls this behavior. Special values NaN and inf are replaced with null. Enable output_format_json_quote_denormals setting to show them. When serializing an Enum value, the function outputs its name. Syntax toJSONString(value)  Arguments value — Value to serialize. Value may be of any data type. Returned value JSON representation of the value. Type: String. Example The first example shows serialization of a Map. The second example shows some special values wrapped into a Tuple. Query: SELECT toJSONString(map('key1', 1, 'key2', 2)); SELECT toJSONString(tuple(1.25, NULL, NaN, +inf, -inf, [])) SETTINGS output_format_json_quote_denormals = 1;  Result: {&quot;key1&quot;:1,&quot;key2&quot;:2} [1.25,null,&quot;nan&quot;,&quot;inf&quot;,&quot;-inf&quot;,[]]  See Also output_format_json_quote_64bit_integersoutput_format_json_quote_denormals "},{"title":"Sources of External Dictionaries","type":0,"sectionRef":"#","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources","content":"","keywords":""},{"title":"Local File​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-local_file","content":"Example of settings: &lt;source&gt; &lt;file&gt; &lt;path&gt;/opt/dictionaries/os.tsv&lt;/path&gt; &lt;format&gt;TabSeparated&lt;/format&gt; &lt;/file&gt; &lt;/source&gt;  or SOURCE(FILE(path './user_files/os.tsv' format 'TabSeparated'))  Setting fields: path – The absolute path to the file.format – The file format. All the formats described in Formats are supported. When dictionary with source FILE is created via DDL command (CREATE DICTIONARY ...), the source file needs to be located in user_files directory, to prevent DB users accessing arbitrary file on ClickHouse node. See Also Dictionary function "},{"title":"Executable File​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-executable","content":"Working with executable files depends on how the dictionary is stored in memory. If the dictionary is stored using cache and complex_key_cache, ClickHouse requests the necessary keys by sending a request to the executable file’s STDIN. Otherwise, ClickHouse starts executable file and treats its output as dictionary data. Example of settings: &lt;source&gt; &lt;executable&gt; &lt;command&gt;cat /opt/dictionaries/os.tsv&lt;/command&gt; &lt;format&gt;TabSeparated&lt;/format&gt; &lt;implicit_key&gt;false&lt;/implicit_key&gt; &lt;/executable&gt; &lt;/source&gt;  Setting fields: command — The absolute path to the executable file, or the file name (if the program directory is written to PATH).format — The file format. All the formats described in Formats are supported.command_termination_timeout — executable script should contain main read-write loop. After dictionary is destroyed, pipe is closed, and executable file will have command_termination_timeout seconds to shutdown, before ClickHouse will send SIGTERM signal to child process. Specified in seconds. Default value is 10. Optional parameter.command_read_timeout - timeout for reading data from command stdout in milliseconds. Default value 10000. Optional parameter.command_write_timeout - timeout for writing data to command stdin in milliseconds. Default value 10000. Optional parameter.implicit_key — The executable source file can return only values, and the correspondence to the requested keys is determined implicitly — by the order of rows in the result. Default value is false.execute_direct - If execute_direct = 1, then command will be searched inside user_scripts folder specified by user_scripts_path. Additional script arguments can be specified using whitespace separator. Example: script_name arg1 arg2. If execute_direct = 0, command is passed as argument for bin/sh -c. Default value is 0. Optional parameter.send_chunk_header - controls whether to send row count before sending a chunk of data to process. Optional. Default value is false. That dictionary source can be configured only via XML configuration. Creating dictionaries with executable source via DDL is disabled, otherwise, the DB user would be able to execute arbitrary binary on ClickHouse node. "},{"title":"Executable Pool​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-executable_pool","content":"Executable pool allows loading data from pool of processes. This source does not work with dictionary layouts that need to load all data from source. Executable pool works if the dictionary is stored using cache, complex_key_cache, ssd_cache, complex_key_ssd_cache, direct, complex_key_direct layouts. Executable pool will spawn pool of processes with specified command and keep them running until they exit. The program should read data from STDIN while it is available and output result to STDOUT, and it can wait for next block of data on STDIN. ClickHouse will not close STDIN after processing a block of data but will pipe another chunk of data when needed. The executable script should be ready for this way of data processing — it should poll STDIN and flush data to STDOUT early. Example of settings: &lt;source&gt; &lt;executable_pool&gt; &lt;command&gt;&lt;command&gt;while read key; do printf &quot;$key\\tData for key $key\\n&quot;; done&lt;/command&lt;/command&gt; &lt;format&gt;TabSeparated&lt;/format&gt; &lt;pool_size&gt;10&lt;/pool_size&gt; &lt;max_command_execution_time&gt;10&lt;max_command_execution_time&gt; &lt;implicit_key&gt;false&lt;/implicit_key&gt; &lt;/executable_pool&gt; &lt;/source&gt;  Setting fields: command — The absolute path to the executable file, or the file name (if the program directory is written to PATH).format — The file format. All the formats described in “Formats” are supported.pool_size — Size of pool. If 0 is specified as pool_size then there is no pool size restrictions. Default value is 16.command_termination_timeout — executable script should contain main read-write loop. After dictionary is destroyed, pipe is closed, and executable file will have command_termination_timeout seconds to shutdown, before ClickHouse will send SIGTERM signal to child process. Specified in seconds. Default value is 10. Optional parameter.max_command_execution_time — Maximum executable script command execution time for processing block of data. Specified in seconds. Default value is 10. Optional parameter.command_read_timeout - timeout for reading data from command stdout in milliseconds. Default value 10000. Optional parameter.command_write_timeout - timeout for writing data to command stdin in milliseconds. Default value 10000. Optional parameter.implicit_key — The executable source file can return only values, and the correspondence to the requested keys is determined implicitly — by the order of rows in the result. Default value is false. Optional parameter.execute_direct - If execute_direct = 1, then command will be searched inside user_scripts folder specified by user_scripts_path. Additional script arguments can be specified using whitespace separator. Example: script_name arg1 arg2. If execute_direct = 0, command is passed as argument for bin/sh -c. Default value is 1. Optional parameter.send_chunk_header - controls whether to send row count before sending a chunk of data to process. Optional. Default value is false. That dictionary source can be configured only via XML configuration. Creating dictionaries with executable source via DDL is disabled, otherwise, the DB user would be able to execute arbitrary binary on ClickHouse node. "},{"title":"Http(s)​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-http","content":"Working with an HTTP(s) server depends on how the dictionary is stored in memory. If the dictionary is stored using cache and complex_key_cache, ClickHouse requests the necessary keys by sending a request via the POST method. Example of settings: &lt;source&gt; &lt;http&gt; &lt;url&gt;http://[::1]/os.tsv&lt;/url&gt; &lt;format&gt;TabSeparated&lt;/format&gt; &lt;credentials&gt; &lt;user&gt;user&lt;/user&gt; &lt;password&gt;password&lt;/password&gt; &lt;/credentials&gt; &lt;headers&gt; &lt;header&gt; &lt;name&gt;API-KEY&lt;/name&gt; &lt;value&gt;key&lt;/value&gt; &lt;/header&gt; &lt;/headers&gt; &lt;/http&gt; &lt;/source&gt;  or SOURCE(HTTP( url 'http://[::1]/os.tsv' format 'TabSeparated' credentials(user 'user' password 'password') headers(header(name 'API-KEY' value 'key')) ))  In order for ClickHouse to access an HTTPS resource, you must configure openSSL in the server configuration. Setting fields: url – The source URL.format – The file format. All the formats described in “Formats” are supported.credentials – Basic HTTP authentication. Optional parameter.user – Username required for the authentication.password – Password required for the authentication.headers – All custom HTTP headers entries used for the HTTP request. Optional parameter.header – Single HTTP header entry.name – Identifiant name used for the header send on the request.value – Value set for a specific identifiant name. When creating a dictionary using the DDL command (CREATE DICTIONARY ...) remote hosts for HTTP dictionaries are checked against the contents of remote_url_allow_hosts section from config to prevent database users to access arbitrary HTTP server. "},{"title":"Known Vulnerability of the ODBC Dictionary Functionality​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#known-vulnerability-of-the-odbc-dictionary-functionality","content":"note When connecting to the database through the ODBC driver connection parameter Servername can be substituted. In this case values of USERNAME and PASSWORD from odbc.ini are sent to the remote server and can be compromised. Example of insecure use Let’s configure unixODBC for PostgreSQL. Content of /etc/odbc.ini: [gregtest] Driver = /usr/lib/psqlodbca.so Servername = localhost PORT = 5432 DATABASE = test_db #OPTION = 3 USERNAME = test PASSWORD = test  If you then make a query such as SELECT * FROM odbc('DSN=gregtest;Servername=some-server.com', 'test_db');  ODBC driver will send values of USERNAME and PASSWORD from odbc.ini to some-server.com. "},{"title":"Example of Connecting Postgresql​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#example-of-connecting-postgresql","content":"Ubuntu OS. Installing unixODBC and the ODBC driver for PostgreSQL: $ sudo apt-get install -y unixodbc odbcinst odbc-postgresql  Configuring /etc/odbc.ini (or ~/.odbc.ini if you signed in under a user that runs ClickHouse):  [DEFAULT] Driver = myconnection [myconnection] Description = PostgreSQL connection to my_db Driver = PostgreSQL Unicode Database = my_db Servername = 127.0.0.1 UserName = username Password = password Port = 5432 Protocol = 9.3 ReadOnly = No RowVersioning = No ShowSystemTables = No ConnSettings =  The dictionary configuration in ClickHouse: &lt;clickhouse&gt; &lt;dictionary&gt; &lt;name&gt;table_name&lt;/name&gt; &lt;source&gt; &lt;odbc&gt; &lt;!-- You can specify the following parameters in connection_string: --&gt; &lt;!-- DSN=myconnection;UID=username;PWD=password;HOST=127.0.0.1;PORT=5432;DATABASE=my_db --&gt; &lt;connection_string&gt;DSN=myconnection&lt;/connection_string&gt; &lt;table&gt;postgresql_table&lt;/table&gt; &lt;/odbc&gt; &lt;/source&gt; &lt;lifetime&gt; &lt;min&gt;300&lt;/min&gt; &lt;max&gt;360&lt;/max&gt; &lt;/lifetime&gt; &lt;layout&gt; &lt;hashed/&gt; &lt;/layout&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;id&lt;/name&gt; &lt;/id&gt; &lt;attribute&gt; &lt;name&gt;some_column&lt;/name&gt; &lt;type&gt;UInt64&lt;/type&gt; &lt;null_value&gt;0&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;/dictionary&gt; &lt;/clickhouse&gt;  or CREATE DICTIONARY table_name ( id UInt64, some_column UInt64 DEFAULT 0 ) PRIMARY KEY id SOURCE(ODBC(connection_string 'DSN=myconnection' table 'postgresql_table')) LAYOUT(HASHED()) LIFETIME(MIN 300 MAX 360)  You may need to edit odbc.ini to specify the full path to the library with the driver DRIVER=/usr/local/lib/psqlodbcw.so. "},{"title":"Example of Connecting MS SQL Server​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#example-of-connecting-ms-sql-server","content":"Ubuntu OS. Installing the ODBC driver for connecting to MS SQL: $ sudo apt-get install tdsodbc freetds-bin sqsh  Configuring the driver:  $ cat /etc/freetds/freetds.conf ... [MSSQL] host = 192.168.56.101 port = 1433 tds version = 7.0 client charset = UTF-8 # test TDS connection $ sqsh -S MSSQL -D database -U user -P password $ cat /etc/odbcinst.ini [FreeTDS] Description = FreeTDS Driver = /usr/lib/x86_64-linux-gnu/odbc/libtdsodbc.so Setup = /usr/lib/x86_64-linux-gnu/odbc/libtdsS.so FileUsage = 1 UsageCount = 5 $ cat /etc/odbc.ini # $ cat ~/.odbc.ini # if you signed in under a user that runs ClickHouse [MSSQL] Description = FreeTDS Driver = FreeTDS Servername = MSSQL Database = test UID = test PWD = test Port = 1433 # (optional) test ODBC connection (to use isql-tool install the [unixodbc](https://packages.debian.org/sid/unixodbc)-package) $ isql -v MSSQL &quot;user&quot; &quot;password&quot;  Remarks: to determine the earliest TDS version that is supported by a particular SQL Server version, refer to the product documentation or look at MS-TDS Product Behavior Configuring the dictionary in ClickHouse: &lt;clickhouse&gt; &lt;dictionary&gt; &lt;name&gt;test&lt;/name&gt; &lt;source&gt; &lt;odbc&gt; &lt;table&gt;dict&lt;/table&gt; &lt;connection_string&gt;DSN=MSSQL;UID=test;PWD=test&lt;/connection_string&gt; &lt;/odbc&gt; &lt;/source&gt; &lt;lifetime&gt; &lt;min&gt;300&lt;/min&gt; &lt;max&gt;360&lt;/max&gt; &lt;/lifetime&gt; &lt;layout&gt; &lt;flat /&gt; &lt;/layout&gt; &lt;structure&gt; &lt;id&gt; &lt;name&gt;k&lt;/name&gt; &lt;/id&gt; &lt;attribute&gt; &lt;name&gt;s&lt;/name&gt; &lt;type&gt;String&lt;/type&gt; &lt;null_value&gt;&lt;/null_value&gt; &lt;/attribute&gt; &lt;/structure&gt; &lt;/dictionary&gt; &lt;/clickhouse&gt;  or CREATE DICTIONARY test ( k UInt64, s String DEFAULT '' ) PRIMARY KEY k SOURCE(ODBC(table 'dict' connection_string 'DSN=MSSQL;UID=test;PWD=test')) LAYOUT(FLAT()) LIFETIME(MIN 300 MAX 360)  "},{"title":"DBMS​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dbms","content":""},{"title":"ODBC​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-odbc","content":"You can use this method to connect any database that has an ODBC driver. Example of settings: &lt;source&gt; &lt;odbc&gt; &lt;db&gt;DatabaseName&lt;/db&gt; &lt;table&gt;ShemaName.TableName&lt;/table&gt; &lt;connection_string&gt;DSN=some_parameters&lt;/connection_string&gt; &lt;invalidate_query&gt;SQL_QUERY&lt;/invalidate_query&gt; &lt;query&gt;SELECT id, value_1, value_2 FROM ShemaName.TableName&lt;/query&gt; &lt;/odbc&gt; &lt;/source&gt;  or SOURCE(ODBC( db 'DatabaseName' table 'SchemaName.TableName' connection_string 'DSN=some_parameters' invalidate_query 'SQL_QUERY' query 'SELECT id, value_1, value_2 FROM db_name.table_name' ))  Setting fields: db – Name of the database. Omit it if the database name is set in the &lt;connection_string&gt; parameters.table – Name of the table and schema if exists.connection_string – Connection string.invalidate_query – Query for checking the dictionary status. Optional parameter. Read more in the section Updating dictionaries.query – The custom query. Optional parameter. note The table and query fields cannot be used together. And either one of the table or query fields must be declared. ClickHouse receives quoting symbols from ODBC-driver and quote all settings in queries to driver, so it’s necessary to set table name accordingly to table name case in database. If you have a problems with encodings when using Oracle, see the corresponding FAQ item. "},{"title":"Mysql​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-mysql","content":"Example of settings: &lt;source&gt; &lt;mysql&gt; &lt;port&gt;3306&lt;/port&gt; &lt;user&gt;clickhouse&lt;/user&gt; &lt;password&gt;qwerty&lt;/password&gt; &lt;replica&gt; &lt;host&gt;example01-1&lt;/host&gt; &lt;priority&gt;1&lt;/priority&gt; &lt;/replica&gt; &lt;replica&gt; &lt;host&gt;example01-2&lt;/host&gt; &lt;priority&gt;1&lt;/priority&gt; &lt;/replica&gt; &lt;db&gt;db_name&lt;/db&gt; &lt;table&gt;table_name&lt;/table&gt; &lt;where&gt;id=10&lt;/where&gt; &lt;invalidate_query&gt;SQL_QUERY&lt;/invalidate_query&gt; &lt;fail_on_connection_loss&gt;true&lt;/fail_on_connection_loss&gt; &lt;query&gt;SELECT id, value_1, value_2 FROM db_name.table_name&lt;/query&gt; &lt;/mysql&gt; &lt;/source&gt;  or SOURCE(MYSQL( port 3306 user 'clickhouse' password 'qwerty' replica(host 'example01-1' priority 1) replica(host 'example01-2' priority 1) db 'db_name' table 'table_name' where 'id=10' invalidate_query 'SQL_QUERY' fail_on_connection_loss 'true' query 'SELECT id, value_1, value_2 FROM db_name.table_name' ))  Setting fields: port – The port on the MySQL server. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;). user – Name of the MySQL user. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;). password – Password of the MySQL user. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;). replica – Section of replica configurations. There can be multiple sections. - `replica/host` – The MySQL host. - `replica/priority` – The replica priority. When attempting to connect, ClickHouse traverses the replicas in order of priority. The lower the number, the higher the priority. db – Name of the database. table – Name of the table. where – The selection criteria. The syntax for conditions is the same as for WHERE clause in MySQL, for example, id &gt; 10 AND id &lt; 20. Optional parameter. invalidate_query – Query for checking the dictionary status. Optional parameter. Read more in the section Updating dictionaries. fail_on_connection_loss – The configuration parameter that controls behavior of the server on connection loss. If true, an exception is thrown immediately if the connection between client and server was lost. If false, the ClickHouse server retries to execute the query three times before throwing an exception. Note that retrying leads to increased response times. Default value: false. query – The custom query. Optional parameter. note The table or where fields cannot be used together with the query field. And either one of the table or query fields must be declared. MySQL can be connected on a local host via sockets. To do this, set host and socket. Example of settings: &lt;source&gt; &lt;mysql&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;socket&gt;/path/to/socket/file.sock&lt;/socket&gt; &lt;user&gt;clickhouse&lt;/user&gt; &lt;password&gt;qwerty&lt;/password&gt; &lt;db&gt;db_name&lt;/db&gt; &lt;table&gt;table_name&lt;/table&gt; &lt;where&gt;id=10&lt;/where&gt; &lt;invalidate_query&gt;SQL_QUERY&lt;/invalidate_query&gt; &lt;fail_on_connection_loss&gt;true&lt;/fail_on_connection_loss&gt; &lt;query&gt;SELECT id, value_1, value_2 FROM db_name.table_name&lt;/query&gt; &lt;/mysql&gt; &lt;/source&gt;  or SOURCE(MYSQL( host 'localhost' socket '/path/to/socket/file.sock' user 'clickhouse' password 'qwerty' db 'db_name' table 'table_name' where 'id=10' invalidate_query 'SQL_QUERY' fail_on_connection_loss 'true' query 'SELECT id, value_1, value_2 FROM db_name.table_name' ))  "},{"title":"ClickHouse​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-clickhouse","content":"Example of settings: &lt;source&gt; &lt;clickhouse&gt; &lt;host&gt;example01-01-1&lt;/host&gt; &lt;port&gt;9000&lt;/port&gt; &lt;user&gt;default&lt;/user&gt; &lt;password&gt;&lt;/password&gt; &lt;db&gt;default&lt;/db&gt; &lt;table&gt;ids&lt;/table&gt; &lt;where&gt;id=10&lt;/where&gt; &lt;secure&gt;1&lt;/secure&gt; &lt;query&gt;SELECT id, value_1, value_2 FROM default.ids&lt;/query&gt; &lt;/clickhouse&gt; &lt;/source&gt;  or SOURCE(CLICKHOUSE( host 'example01-01-1' port 9000 user 'default' password '' db 'default' table 'ids' where 'id=10' secure 1 query 'SELECT id, value_1, value_2 FROM default.ids' ));  Setting fields: host – The ClickHouse host. If it is a local host, the query is processed without any network activity. To improve fault tolerance, you can create a Distributed table and enter it in subsequent configurations.port – The port on the ClickHouse server.user – Name of the ClickHouse user.password – Password of the ClickHouse user.db – Name of the database.table – Name of the table.where – The selection criteria. May be omitted.invalidate_query – Query for checking the dictionary status. Optional parameter. Read more in the section Updating dictionaries.secure - Use ssl for connection.query – The custom query. Optional parameter. note The table or where fields cannot be used together with the query field. And either one of the table or query fields must be declared. "},{"title":"Mongodb​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-mongodb","content":"Example of settings: &lt;source&gt; &lt;mongodb&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;27017&lt;/port&gt; &lt;user&gt;&lt;/user&gt; &lt;password&gt;&lt;/password&gt; &lt;db&gt;test&lt;/db&gt; &lt;collection&gt;dictionary_source&lt;/collection&gt; &lt;/mongodb&gt; &lt;/source&gt;  or SOURCE(MONGODB( host 'localhost' port 27017 user '' password '' db 'test' collection 'dictionary_source' ))  Setting fields: host – The MongoDB host.port – The port on the MongoDB server.user – Name of the MongoDB user.password – Password of the MongoDB user.db – Name of the database.collection – Name of the collection. "},{"title":"Redis​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-redis","content":"Example of settings: &lt;source&gt; &lt;redis&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;6379&lt;/port&gt; &lt;storage_type&gt;simple&lt;/storage_type&gt; &lt;db_index&gt;0&lt;/db_index&gt; &lt;/redis&gt; &lt;/source&gt;  or SOURCE(REDIS( host 'localhost' port 6379 storage_type 'simple' db_index 0 ))  Setting fields: host – The Redis host.port – The port on the Redis server.storage_type – The structure of internal Redis storage using for work with keys. simple is for simple sources and for hashed single key sources, hash_map is for hashed sources with two keys. Ranged sources and cache sources with complex key are unsupported. May be omitted, default value is simple.db_index – The specific numeric index of Redis logical database. May be omitted, default value is 0. "},{"title":"Cassandra​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-cassandra","content":"Example of settings: &lt;source&gt; &lt;cassandra&gt; &lt;host&gt;localhost&lt;/host&gt; &lt;port&gt;9042&lt;/port&gt; &lt;user&gt;username&lt;/user&gt; &lt;password&gt;qwerty123&lt;/password&gt; &lt;keyspase&gt;database_name&lt;/keyspase&gt; &lt;column_family&gt;table_name&lt;/column_family&gt; &lt;allow_filering&gt;1&lt;/allow_filering&gt; &lt;partition_key_prefix&gt;1&lt;/partition_key_prefix&gt; &lt;consistency&gt;One&lt;/consistency&gt; &lt;where&gt;&quot;SomeColumn&quot; = 42&lt;/where&gt; &lt;max_threads&gt;8&lt;/max_threads&gt; &lt;query&gt;SELECT id, value_1, value_2 FROM database_name.table_name&lt;/query&gt; &lt;/cassandra&gt; &lt;/source&gt;  Setting fields: host – The Cassandra host or comma-separated list of hosts.port – The port on the Cassandra servers. If not specified, default port 9042 is used.user – Name of the Cassandra user.password – Password of the Cassandra user.keyspace – Name of the keyspace (database).column_family – Name of the column family (table).allow_filering – Flag to allow or not potentially expensive conditions on clustering key columns. Default value is 1.partition_key_prefix – Number of partition key columns in primary key of the Cassandra table. Required for compose key dictionaries. Order of key columns in the dictionary definition must be the same as in Cassandra. Default value is 1 (the first key column is a partition key and other key columns are clustering key).consistency – Consistency level. Possible values: One, Two, Three, All, EachQuorum, Quorum, LocalQuorum, LocalOne, Serial, LocalSerial. Default value is One.where – Optional selection criteria.max_threads – The maximum number of threads to use for loading data from multiple partitions in compose key dictionaries.query – The custom query. Optional parameter. note The column_family or where fields cannot be used together with the query field. And either one of the column_family or query fields must be declared. "},{"title":"PostgreSQL​","type":1,"pageTitle":"Sources of External Dictionaries","url":"en/sql-reference/dictionaries/external-dictionaries/external-dicts-dict-sources#dicts-external_dicts_dict_sources-postgresql","content":"Example of settings: &lt;source&gt; &lt;postgresql&gt; &lt;port&gt;5432&lt;/port&gt; &lt;user&gt;clickhouse&lt;/user&gt; &lt;password&gt;qwerty&lt;/password&gt; &lt;db&gt;db_name&lt;/db&gt; &lt;table&gt;table_name&lt;/table&gt; &lt;where&gt;id=10&lt;/where&gt; &lt;invalidate_query&gt;SQL_QUERY&lt;/invalidate_query&gt; &lt;query&gt;SELECT id, value_1, value_2 FROM db_name.table_name&lt;/query&gt; &lt;/postgresql&gt; &lt;/source&gt;  or SOURCE(POSTGRESQL( port 5432 host 'postgresql-hostname' user 'postgres_user' password 'postgres_password' db 'db_name' table 'table_name' replica(host 'example01-1' port 5432 priority 1) replica(host 'example01-2' port 5432 priority 2) where 'id=10' invalidate_query 'SQL_QUERY' query 'SELECT id, value_1, value_2 FROM db_name.table_name' ))  Setting fields: host – The host on the PostgreSQL server. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;).port – The port on the PostgreSQL server. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;).user – Name of the PostgreSQL user. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;).password – Password of the PostgreSQL user. You can specify it for all replicas, or for each one individually (inside &lt;replica&gt;).replica – Section of replica configurations. There can be multiple sections: replica/host – The PostgreSQL host.replica/port – The PostgreSQL port.replica/priority – The replica priority. When attempting to connect, ClickHouse traverses the replicas in order of priority. The lower the number, the higher the priority. db – Name of the database.table – Name of the table.where – The selection criteria. The syntax for conditions is the same as for WHERE clause in PostgreSQL. For example, id &gt; 10 AND id &lt; 20. Optional parameter.invalidate_query – Query for checking the dictionary status. Optional parameter. Read more in the section Updating dictionaries.query – The custom query. Optional parameter. note The table or where fields cannot be used together with the query field. And either one of the table or query fields must be declared. "},{"title":"Logical Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/logical-functions","content":"","keywords":""},{"title":"and​","type":1,"pageTitle":"Logical Functions","url":"en/sql-reference/functions/logical-functions#logical-and-function","content":"Calculates the result of the logical conjunction between two or more values. Corresponds to Logical AND Operator. Syntax and(val1, val2...)  You can use the short_circuit_function_evaluation setting to calculate the and function according to a short scheme. If this setting is enabled, vali is evaluated only on rows where (val1 AND val2 AND ... AND val{i-1}) is true. For example, an exception about division by zero is not thrown when executing the query SELECT and(number = 2, intDiv(1, number)) FROM numbers(10). Arguments val1, val2, ... — List of at least two values. Int, UInt, Float or Nullable. Returned value 0, if there is at least one zero value argument.NULL, if there are no zero values arguments and there is at least one NULL argument.1, otherwise. Type: UInt8 or Nullable(UInt8). Example Query: SELECT and(0, 1, -2);  Result: ┌─and(0, 1, -2)─┐ │ 0 │ └───────────────┘  With NULL: SELECT and(NULL, 1, 10, -2);  Result: ┌─and(NULL, 1, 10, -2)─┐ │ ᴺᵁᴸᴸ │ └──────────────────────┘  "},{"title":"or​","type":1,"pageTitle":"Logical Functions","url":"en/sql-reference/functions/logical-functions#logical-or-function","content":"Calculates the result of the logical disjunction between two or more values. Corresponds to Logical OR Operator. Syntax or(val1, val2...)  You can use the short_circuit_function_evaluation setting to calculate the or function according to a short scheme. If this setting is enabled, vali is evaluated only on rows where ((NOT val1) AND (NOT val2) AND ... AND (NOT val{i-1})) is true. For example, an exception about division by zero is not thrown when executing the query SELECT or(number = 0, intDiv(1, number) != 0) FROM numbers(10). Arguments val1, val2, ... — List of at least two values. Int, UInt, Float or Nullable. Returned value 1, if there is at least one non-zero value.0, if there are only zero values.NULL, if there are only zero values and NULL. Type: UInt8 or Nullable(UInt8). Example Query: SELECT or(1, 0, 0, 2, NULL);  Result: ┌─or(1, 0, 0, 2, NULL)─┐ │ 1 │ └──────────────────────┘  With NULL: SELECT or(0, NULL);  Result: ┌─or(0, NULL)─┐ │ ᴺᵁᴸᴸ │ └─────────────┘  "},{"title":"not​","type":1,"pageTitle":"Logical Functions","url":"en/sql-reference/functions/logical-functions#logical-not-function","content":"Calculates the result of the logical negation of the value. Corresponds to Logical Negation Operator. Syntax not(val);  Arguments val — The value. Int, UInt, Float or Nullable. Returned value 1, if the val is 0.0, if the val is a non-zero value.NULL, if the val is a NULL value. Type: UInt8 or Nullable(UInt8). Example Query: SELECT NOT(1);  Result: ┌─not(1)─┐ │ 0 │ └────────┘  "},{"title":"xor​","type":1,"pageTitle":"Logical Functions","url":"en/sql-reference/functions/logical-functions#logical-xor-function","content":"Calculates the result of the logical exclusive disjunction between two or more values. For more than two values the function works as if it calculates XOR of the first two values and then uses the result with the next value to calculate XOR and so on. Syntax xor(val1, val2...)  Arguments val1, val2, ... — List of at least two values. Int, UInt, Float or Nullable. Returned value 1, for two values: if one of the values is zero and other is not.0, for two values: if both values are zero or non-zero at the same time.NULL, if there is at least one NULL value. Type: UInt8 or Nullable(UInt8). Example Query: SELECT xor(0, 1, 1);  Result: ┌─xor(0, 1, 1)─┐ │ 0 │ └──────────────┘  "},{"title":"Machine Learning Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/machine-learning-functions","content":"","keywords":""},{"title":"evalMLMethod​","type":1,"pageTitle":"Machine Learning Functions","url":"en/sql-reference/functions/machine-learning-functions#machine_learning_methods-evalmlmethod","content":"Prediction using fitted regression models uses evalMLMethod function. See link in linearRegression. "},{"title":"stochasticLinearRegression​","type":1,"pageTitle":"Machine Learning Functions","url":"en/sql-reference/functions/machine-learning-functions#stochastic-linear-regression","content":"The stochasticLinearRegression aggregate function implements stochastic gradient descent method using linear model and MSE loss function. Uses evalMLMethod to predict on new data. "},{"title":"stochasticLogisticRegression​","type":1,"pageTitle":"Machine Learning Functions","url":"en/sql-reference/functions/machine-learning-functions#stochastic-logistic-regression","content":"The stochasticLogisticRegression aggregate function implements stochastic gradient descent method for binary classification problem. Uses evalMLMethod to predict on new data. "},{"title":"Mathematical Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/math-functions","content":"","keywords":""},{"title":"e()​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#e","content":"Returns a Float64 number that is close to the number e. "},{"title":"pi()​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#pi","content":"Returns a Float64 number that is close to the number π. "},{"title":"exp(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#expx","content":"Accepts a numeric argument and returns a Float64 number close to the exponent of the argument. "},{"title":"log(x), ln(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#logx-lnx","content":"Accepts a numeric argument and returns a Float64 number close to the natural logarithm of the argument. "},{"title":"exp2(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#exp2x","content":"Accepts a numeric argument and returns a Float64 number close to 2 to the power of x. "},{"title":"log2(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#log2x","content":"Accepts a numeric argument and returns a Float64 number close to the binary logarithm of the argument. "},{"title":"exp10(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#exp10x","content":"Accepts a numeric argument and returns a Float64 number close to 10 to the power of x. "},{"title":"log10(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#log10x","content":"Accepts a numeric argument and returns a Float64 number close to the decimal logarithm of the argument. "},{"title":"sqrt(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#sqrtx","content":"Accepts a numeric argument and returns a Float64 number close to the square root of the argument. "},{"title":"cbrt(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#cbrtx","content":"Accepts a numeric argument and returns a Float64 number close to the cubic root of the argument. "},{"title":"erf(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#erfx","content":"If ‘x’ is non-negative, then erf(x / σ√2) is the probability that a random variable having a normal distribution with standard deviation ‘σ’ takes the value that is separated from the expected value by more than ‘x’. Example (three sigma rule): SELECT erf(3 / sqrt(2));  ┌─erf(divide(3, sqrt(2)))─┐ │ 0.9973002039367398 │ └─────────────────────────┘  "},{"title":"erfc(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#erfcx","content":"Accepts a numeric argument and returns a Float64 number close to 1 - erf(x), but without loss of precision for large ‘x’ values. "},{"title":"lgamma(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#lgammax","content":"The logarithm of the gamma function. "},{"title":"tgamma(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#tgammax","content":"Gamma function. "},{"title":"sin(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#sinx","content":"The sine. "},{"title":"cos(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#cosx","content":"The cosine. "},{"title":"tan(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#tanx","content":"The tangent. "},{"title":"asin(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#asinx","content":"The arc sine. "},{"title":"acos(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#acosx","content":"The arc cosine. "},{"title":"atan(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#atanx","content":"The arc tangent. "},{"title":"pow(x, y), power(x, y)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#powx-y-powerx-y","content":"Takes two numeric arguments x and y. Returns a Float64 number close to x to the power of y. "},{"title":"intExp2​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#intexp2","content":"Accepts a numeric argument and returns a UInt64 number close to 2 to the power of x. "},{"title":"intExp10​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#intexp10","content":"Accepts a numeric argument and returns a UInt64 number close to 10 to the power of x. "},{"title":"cosh(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#coshx","content":"Hyperbolic cosine. Syntax cosh(x)  Arguments x — The angle, in radians. Values from the interval: -∞ &lt; x &lt; +∞. Float64. Returned value Values from the interval: 1 &lt;= cosh(x) &lt; +∞. Type: Float64. Example Query: SELECT cosh(0);  Result: ┌─cosh(0)──┐ │ 1 │ └──────────┘  "},{"title":"acosh(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#acoshx","content":"Inverse hyperbolic cosine. Syntax acosh(x)  Arguments x — Hyperbolic cosine of angle. Values from the interval: 1 &lt;= x &lt; +∞. Float64. Returned value The angle, in radians. Values from the interval: 0 &lt;= acosh(x) &lt; +∞. Type: Float64. Example Query: SELECT acosh(1);  Result: ┌─acosh(1)─┐ │ 0 │ └──────────┘  See Also cosh(x) "},{"title":"sinh(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#sinhx","content":"Hyperbolic sine. Syntax sinh(x)  Arguments x — The angle, in radians. Values from the interval: -∞ &lt; x &lt; +∞. Float64. Returned value Values from the interval: -∞ &lt; sinh(x) &lt; +∞. Type: Float64. Example Query: SELECT sinh(0);  Result: ┌─sinh(0)──┐ │ 0 │ └──────────┘  "},{"title":"asinh(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#asinhx","content":"Inverse hyperbolic sine. Syntax asinh(x)  Arguments x — Hyperbolic sine of angle. Values from the interval: -∞ &lt; x &lt; +∞. Float64. Returned value The angle, in radians. Values from the interval: -∞ &lt; asinh(x) &lt; +∞. Type: Float64. Example Query: SELECT asinh(0);  Result: ┌─asinh(0)─┐ │ 0 │ └──────────┘  See Also sinh(x) "},{"title":"atanh(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#atanhx","content":"Inverse hyperbolic tangent. Syntax atanh(x)  Arguments x — Hyperbolic tangent of angle. Values from the interval: –1 &lt; x &lt; 1. Float64. Returned value The angle, in radians. Values from the interval: -∞ &lt; atanh(x) &lt; +∞. Type: Float64. Example Query: SELECT atanh(0);  Result: ┌─atanh(0)─┐ │ 0 │ └──────────┘  "},{"title":"atan2(y, x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#atan2yx","content":"The function calculates the angle in the Euclidean plane, given in radians, between the positive x axis and the ray to the point (x, y) ≠ (0, 0). Syntax atan2(y, x)  Arguments y — y-coordinate of the point through which the ray passes. Float64.x — x-coordinate of the point through which the ray passes. Float64. Returned value The angle θ such that −π &lt; θ ≤ π, in radians. Type: Float64. Example Query: SELECT atan2(1, 1);  Result: ┌────────atan2(1, 1)─┐ │ 0.7853981633974483 │ └────────────────────┘  "},{"title":"hypot(x, y)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#hypotxy","content":"Calculates the length of the hypotenuse of a right-angle triangle. The function avoids problems that occur when squaring very large or very small numbers. Syntax hypot(x, y)  Arguments x — The first cathetus of a right-angle triangle. Float64.y — The second cathetus of a right-angle triangle. Float64. Returned value The length of the hypotenuse of a right-angle triangle. Type: Float64. Example Query: SELECT hypot(1, 1);  Result: ┌────────hypot(1, 1)─┐ │ 1.4142135623730951 │ └────────────────────┘  "},{"title":"log1p(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#log1px","content":"Calculates log(1+x). The function log1p(x) is more accurate than log(1+x) for small values of x. Syntax log1p(x)  Arguments x — Values from the interval: -1 &lt; x &lt; +∞. Float64. Returned value Values from the interval: -∞ &lt; log1p(x) &lt; +∞. Type: Float64. Example Query: SELECT log1p(0);  Result: ┌─log1p(0)─┐ │ 0 │ └──────────┘  See Also log(x) "},{"title":"sign(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#signx","content":"Returns the sign of a real number. Syntax sign(x)  Arguments x — Values from -∞ to +∞. Support all numeric types in ClickHouse. Returned value -1 for x &lt; 00 for x = 01 for x &gt; 0 Examples Sign for the zero value: SELECT sign(0);  Result: ┌─sign(0)─┐ │ 0 │ └─────────┘  Sign for the positive value: SELECT sign(1);  Result: ┌─sign(1)─┐ │ 1 │ └─────────┘  Sign for the negative value: SELECT sign(-1);  Result: ┌─sign(-1)─┐ │ -1 │ └──────────┘  "},{"title":"degrees(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#degreesx","content":"Converts the input value in radians to degrees. Syntax degrees(x)  Arguments x — Input in radians. Float64. Returned value Value in degrees. Type: Float64. Example Query: SELECT degrees(3.141592653589793);  Result: ┌─degrees(3.141592653589793)─┐ │ 180 │ └────────────────────────────┘  "},{"title":"radians(x)​","type":1,"pageTitle":"Mathematical Functions","url":"en/sql-reference/functions/math-functions#radiansx","content":"Converts the input value in degrees to radians. Syntax radians(x)  Arguments x — Input in degrees. Float64. Returned value Value in radians. Type: Float64. Example Query: SELECT radians(180);  Result: ┌──────radians(180)─┐ │ 3.141592653589793 │ └───────────────────┘  "},{"title":"Functions for Working with H3 Indexes","type":0,"sectionRef":"#","url":"en/sql-reference/functions/geo/h3","content":"","keywords":""},{"title":"h3IsValid​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3isvalid","content":"Verifies whether the number is a valid H3 index. Syntax h3IsValid(h3index)  Parameter h3index — Hexagon index number. Type: UInt64. Returned values 1 — The number is a valid H3 index.0 — The number is not a valid H3 index. Type: UInt8. Example Query: SELECT h3IsValid(630814730351855103) AS h3IsValid;  Result: ┌─h3IsValid─┐ │ 1 │ └───────────┘  "},{"title":"h3GetResolution​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3getresolution","content":"Defines the resolution of the given H3 index. Syntax h3GetResolution(h3index)  Parameter h3index — Hexagon index number. Type: UInt64. Returned values Index resolution. Range: [0, 15].If the index is not valid, the function returns a random value. Use h3IsValid to verify the index. Type: UInt8. Example Query: SELECT h3GetResolution(639821929606596015) AS resolution;  Result: ┌─resolution─┐ │ 14 │ └────────────┘  "},{"title":"h3EdgeAngle​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3edgeangle","content":"Calculates the average length of the H3 hexagon edge in grades. Syntax h3EdgeAngle(resolution)  Parameter resolution — Index resolution. Type: UInt8. Range: [0, 15]. Returned values The average length of the H3 hexagon edge in grades. Type: Float64. Example Query: SELECT h3EdgeAngle(10) AS edgeAngle;  Result: ┌───────h3EdgeAngle(10)─┐ │ 0.0005927224846720883 │ └───────────────────────┘  "},{"title":"h3EdgeLengthM​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3edgelengthm","content":"Calculates the average length of the H3 hexagon edge in meters. Syntax h3EdgeLengthM(resolution)  Parameter resolution — Index resolution. Type: UInt8. Range: [0, 15]. Returned values The average length of the H3 hexagon edge in meters. Type: Float64. Example Query: SELECT h3EdgeLengthM(15) AS edgeLengthM;  Result: ┌─edgeLengthM─┐ │ 0.509713273 │ └─────────────┘  "},{"title":"h3EdgeLengthKm​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3edgelengthkm","content":"Calculates the average length of the H3 hexagon edge in kilometers. Syntax h3EdgeLengthKm(resolution)  Parameter resolution — Index resolution. Type: UInt8. Range: [0, 15]. Returned values The average length of the H3 hexagon edge in kilometers. Type: Float64. Example Query: SELECT h3EdgeLengthKm(15) AS edgeLengthKm;  Result: ┌─edgeLengthKm─┐ │ 0.000509713 │ └──────────────┘  "},{"title":"geoToH3​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#geotoh3","content":"Returns H3 point index (lon, lat) with specified resolution. Syntax geoToH3(lon, lat, resolution)  Arguments lon — Longitude. Type: Float64.lat — Latitude. Type: Float64.resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned values Hexagon index number.0 in case of error. Type: UInt64. Example Query: SELECT geoToH3(37.79506683, 55.71290588, 15) AS h3Index;  Result: ┌────────────h3Index─┐ │ 644325524701193974 │ └────────────────────┘  "},{"title":"h3ToGeo​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3togeo","content":"Returns the centroid longitude and latitude corresponding to the provided H3 index. Syntax h3ToGeo(h3Index)  Arguments h3Index — H3 Index. UInt64. Returned values A tuple consisting of two values: tuple(lon,lat). lon — Longitude. Float64. lat — Latitude. Float64. Example Query: SELECT h3ToGeo(644325524701193974) AS coordinates;  Result: ┌─coordinates───────────────────────────┐ │ (37.79506616830252,55.71290243145668) │ └───────────────────────────────────────┘  "},{"title":"h3ToGeoBoundary​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3togeoboundary","content":"Returns array of pairs (lon, lat), which corresponds to the boundary of the provided H3 index. Syntax h3ToGeoBoundary(h3Index)  Arguments h3Index — H3 Index. Type: UInt64. Returned values Array of pairs '(lon, lat)'. Type: Array(Float64, Float64). Example Query: SELECT h3ToGeoBoundary(644325524701193974) AS coordinates;  Result: ┌─h3ToGeoBoundary(599686042433355775)────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ [(37.2713558667319,-121.91508032705622),(37.353926450852256,-121.8622232890249),(37.42834118609435,-121.92354999630156),(37.42012867767779,-122.03773496427027),(37.33755608435299,-122.090428929044),(37.26319797461824,-122.02910130919001)] │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"h3kRing​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3kring","content":"Lists all the H3 hexagons in the raduis of k from the given hexagon in random order. Syntax h3kRing(h3index, k)  Arguments h3index — Hexagon index number. Type: UInt64.k — Radius. Type: integer Returned values Array of H3 indexes. Type: Array(UInt64). Example Query: SELECT arrayJoin(h3kRing(644325529233966508, 1)) AS h3index;  Result: ┌────────────h3index─┐ │ 644325529233966508 │ │ 644325529233966497 │ │ 644325529233966510 │ │ 644325529233966504 │ │ 644325529233966509 │ │ 644325529233966355 │ │ 644325529233966354 │ └────────────────────┘  "},{"title":"h3GetBaseCell​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3getbasecell","content":"Returns the base cell number of the H3 index. Syntax h3GetBaseCell(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Hexagon base cell number. Type: UInt8. Example Query: SELECT h3GetBaseCell(612916788725809151) AS basecell;  Result: ┌─basecell─┐ │ 12 │ └──────────┘  "},{"title":"h3HexAreaM2​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3hexaream2","content":"Returns average hexagon area in square meters at the given resolution. Syntax h3HexAreaM2(resolution)  Parameter resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned value Area in square meters. Type: Float64. Example Query: SELECT h3HexAreaM2(13) AS area;  Result: ┌─area─┐ │ 43.9 │ └──────┘  "},{"title":"h3HexAreaKm2​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3hexareakm2","content":"Returns average hexagon area in square kilometers at the given resolution. Syntax h3HexAreaKm2(resolution)  Parameter resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned value Area in square kilometers. Type: Float64. Example Query: SELECT h3HexAreaKm2(13) AS area;  Result: ┌──────area─┐ │ 0.0000439 │ └───────────┘  "},{"title":"h3IndexesAreNeighbors​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3indexesareneighbors","content":"Returns whether or not the provided H3 indexes are neighbors. Syntax h3IndexesAreNeighbors(index1, index2)  Arguments index1 — Hexagon index number. Type: UInt64.index2 — Hexagon index number. Type: UInt64. Returned value 1 — Indexes are neighbours.0 — Indexes are not neighbours. Type: UInt8. Example Query: SELECT h3IndexesAreNeighbors(617420388351344639, 617420388352655359) AS n;  Result: ┌─n─┐ │ 1 │ └───┘  "},{"title":"h3ToChildren​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3tochildren","content":"Returns an array of child indexes for the given H3 index. Syntax h3ToChildren(index, resolution)  Arguments index — Hexagon index number. Type: UInt64.resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned values Array of the child H3-indexes. Type: Array(UInt64). Example Query: SELECT h3ToChildren(599405990164561919, 6) AS children;  Result: ┌─children───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ [603909588852408319,603909588986626047,603909589120843775,603909589255061503,603909589389279231,603909589523496959,603909589657714687] │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"h3ToParent​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3toparent","content":"Returns the parent (coarser) index containing the given H3 index. Syntax h3ToParent(index, resolution)  Arguments index — Hexagon index number. Type: UInt64.resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned value Parent H3 index. Type: UInt64. Example Query: SELECT h3ToParent(599405990164561919, 3) AS parent;  Result: ┌─────────────parent─┐ │ 590398848891879423 │ └────────────────────┘  "},{"title":"h3ToString​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3tostring","content":"Converts the H3Index representation of the index to the string representation. h3ToString(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value String representation of the H3 index. Type: String. Example Query: SELECT h3ToString(617420388352917503) AS h3_string;  Result: ┌─h3_string───────┐ │ 89184926cdbffff │ └─────────────────┘  "},{"title":"stringToH3​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#stringtoh3","content":"Converts the string representation to the H3Index (UInt64) representation. Syntax stringToH3(index_str)  Parameter index_str — String representation of the H3 index. Type: String. Returned value Hexagon index number. Returns 0 on error. Type: UInt64. Example Query: SELECT stringToH3('89184926cc3ffff') AS index;  Result: ┌──────────────index─┐ │ 617420388351344639 │ └────────────────────┘  "},{"title":"h3GetResolution​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3getresolution","content":"Returns the resolution of the H3 index. Syntax h3GetResolution(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Index resolution. Range: [0, 15]. Type: UInt8. Example Query: SELECT h3GetResolution(617420388352917503) AS res;  Result: ┌─res─┐ │ 9 │ └─────┘  "},{"title":"h3IsResClassIII​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3isresclassIII","content":"Returns whether H3 index has a resolution with Class III orientation. Syntax h3IsResClassIII(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value 1 — Index has a resolution with Class III orientation.0 — Index doesn't have a resolution with Class III orientation. Type: UInt8. Example Query: SELECT h3IsResClassIII(617420388352917503) AS res;  Result: ┌─res─┐ │ 1 │ └─────┘  "},{"title":"h3IsPentagon​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3ispentagon","content":"Returns whether this H3 index represents a pentagonal cell. Syntax h3IsPentagon(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value 1 — Index represents a pentagonal cell.0 — Index doesn't represent a pentagonal cell. Type: UInt8. Example Query: SELECT h3IsPentagon(644721767722457330) AS pentagon;  Result: ┌─pentagon─┐ │ 0 │ └──────────┘  "},{"title":"h3GetFaces​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3getfaces","content":"Returns icosahedron faces intersected by a given H3 index. Syntax h3GetFaces(index)  Parameter index — Hexagon index number. Type: UInt64. Returned values Array containing icosahedron faces intersected by a given H3 index. Type: Array(UInt64). Example Query: SELECT h3GetFaces(599686042433355775) AS faces;  Result: ┌─faces─┐ │ [7] │ └───────┘  "},{"title":"h3CellAreaM2​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3cellaream2","content":"Returns the exact area of a specific cell in square meters corresponding to the given input H3 index. Syntax h3CellAreaM2(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Cell area in square meters. Type: Float64. Example Query: SELECT h3CellAreaM2(579205133326352383) AS area;  Result: ┌───────────────area─┐ │ 4106166334463.9233 │ └────────────────────┘  "},{"title":"h3CellAreaRads2​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3cellarearads2","content":"Returns the exact area of a specific cell in square radians corresponding to the given input H3 index. Syntax h3CellAreaRads2(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Cell area in square radians. Type: Float64. Example Query: SELECT h3CellAreaRads2(579205133326352383) AS area;  Result: ┌────────────────area─┐ │ 0.10116268528089567 │ └─────────────────────┘  "},{"title":"h3ToCenterChild​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3tocenterchild","content":"Returns the center child (finer) H3 index contained by given H3 at the given resolution. Syntax h3ToCenterChild(index, resolution)  Parameter index — Hexagon index number. Type: UInt64.resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned values H3 index of the center child contained by given H3 at the given resolution. Type: UInt64. Example Query: SELECT h3ToCenterChild(577023702256844799,1) AS centerToChild;  Result: ┌──────centerToChild─┐ │ 581496515558637567 │ └────────────────────┘  "},{"title":"h3ExactEdgeLengthM​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3exactedgelengthm","content":"Returns the exact edge length of the unidirectional edge represented by the input h3 index in meters. Syntax h3ExactEdgeLengthM(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Exact edge length in meters. Type: Float64. Example Query: SELECT h3ExactEdgeLengthM(1310277011704381439) AS exactEdgeLengthM;;  Result: ┌───exactEdgeLengthM─┐ │ 195449.63163407316 │ └────────────────────┘  "},{"title":"h3ExactEdgeLengthKm​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3exactedgelengthkm","content":"Returns the exact edge length of the unidirectional edge represented by the input h3 index in kilometers. Syntax h3ExactEdgeLengthKm(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Exact edge length in kilometers. Type: Float64. Example Query: SELECT h3ExactEdgeLengthKm(1310277011704381439) AS exactEdgeLengthKm;;  Result: ┌──exactEdgeLengthKm─┐ │ 195.44963163407317 │ └────────────────────┘  "},{"title":"h3ExactEdgeLengthRads​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3exactedgelengthrads","content":"Returns the exact edge length of the unidirectional edge represented by the input h3 index in radians. Syntax h3ExactEdgeLengthRads(index)  Parameter index — Hexagon index number. Type: UInt64. Returned value Exact edge length in radians. Type: Float64. Example Query: SELECT h3ExactEdgeLengthRads(1310277011704381439) AS exactEdgeLengthRads;;  Result: ┌──exactEdgeLengthRads─┐ │ 0.030677980118976447 │ └──────────────────────┘  "},{"title":"h3NumHexagons​","type":1,"pageTitle":"Functions for Working with H3 Indexes","url":"en/sql-reference/functions/geo/h3#h3numhexagons","content":"Returns the number of unique H3 indices at the given resolution. Syntax h3NumHexagons(resolution)  Parameter resolution — Index resolution. Range: [0, 15]. Type: UInt8. Returned value Number of H3 indices. Type: Int64. Example Query: SELECT h3NumHexagons(3) AS numHexagons;  Result: ┌─numHexagons─┐ │ 41162 │ └─────────────┘  Original article "},{"title":"[experimental] Natural Language Processing functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/nlp-functions","content":"","keywords":""},{"title":"stem​","type":1,"pageTitle":"[experimental] Natural Language Processing functions","url":"en/sql-reference/functions/nlp-functions#stem","content":"Performs stemming on a given word. Syntax stem('language', word)  Arguments language — Language which rules will be applied. Must be in lowercase. String.word — word that needs to be stemmed. Must be in lowercase. String. Examples Query: SELECT arrayMap(x -&gt; stem('en', x), ['I', 'think', 'it', 'is', 'a', 'blessing', 'in', 'disguise']) as res;  Result: ┌─res────────────────────────────────────────────────┐ │ ['I','think','it','is','a','bless','in','disguis'] │ └────────────────────────────────────────────────────┘  "},{"title":"lemmatize​","type":1,"pageTitle":"[experimental] Natural Language Processing functions","url":"en/sql-reference/functions/nlp-functions#lemmatize","content":"Performs lemmatization on a given word. Needs dictionaries to operate, which can be obtained here. Syntax lemmatize('language', word)  Arguments language — Language which rules will be applied. String.word — Word that needs to be lemmatized. Must be lowercase. String. Examples Query: SELECT lemmatize('en', 'wolves');  Result: ┌─lemmatize(&quot;wolves&quot;)─┐ │ &quot;wolf&quot; │ └─────────────────────┘  Configuration: &lt;lemmatizers&gt; &lt;lemmatizer&gt; &lt;lang&gt;en&lt;/lang&gt; &lt;path&gt;en.bin&lt;/path&gt; &lt;/lemmatizer&gt; &lt;/lemmatizers&gt;  "},{"title":"synonyms​","type":1,"pageTitle":"[experimental] Natural Language Processing functions","url":"en/sql-reference/functions/nlp-functions#synonyms","content":"Finds synonyms to a given word. There are two types of synonym extensions: plain and wordnet. With the plain extension type we need to provide a path to a simple text file, where each line corresponds to a certain synonym set. Words in this line must be separated with space or tab characters. With the wordnet extension type we need to provide a path to a directory with WordNet thesaurus in it. Thesaurus must contain a WordNet sense index. Syntax synonyms('extension_name', word)  Arguments extension_name — Name of the extension in which search will be performed. String.word — Word that will be searched in extension. String. Examples Query: SELECT synonyms('list', 'important');  Result: ┌─synonyms('list', 'important')────────────┐ │ ['important','big','critical','crucial'] │ └──────────────────────────────────────────┘  Configuration: &lt;synonyms_extensions&gt; &lt;extension&gt; &lt;name&gt;en&lt;/name&gt; &lt;type&gt;plain&lt;/type&gt; &lt;path&gt;en.txt&lt;/path&gt; &lt;/extension&gt; &lt;extension&gt; &lt;name&gt;en&lt;/name&gt; &lt;type&gt;wordnet&lt;/type&gt; &lt;path&gt;en/&lt;/path&gt; &lt;/extension&gt; &lt;/synonyms_extensions&gt;  "},{"title":"Array Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/array-functions","content":"","keywords":""},{"title":"empty​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#function-empty","content":"Checks whether the input array is empty. Syntax empty([x])  An array is considered empty if it does not contain any elements. note Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only size0 subcolumn instead of reading and processing the whole array column. The query SELECT empty(arr) FROM TABLE; transforms to SELECT arr.size0 = 0 FROM TABLE;. The function also works for strings or UUID. Arguments [x] — Input array. Array. Returned value Returns 1 for an empty array or 0 for a non-empty array. Type: UInt8. Example Query: SELECT empty([]);  Result: ┌─empty(array())─┐ │ 1 │ └────────────────┘  "},{"title":"notEmpty​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#function-notempty","content":"Checks whether the input array is non-empty. Syntax notEmpty([x])  An array is considered non-empty if it contains at least one element. note Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only size0 subcolumn instead of reading and processing the whole array column. The query SELECT notEmpty(arr) FROM table transforms to SELECT arr.size0 != 0 FROM TABLE. The function also works for strings or UUID. Arguments [x] — Input array. Array. Returned value Returns 1 for a non-empty array or 0 for an empty array. Type: UInt8. Example Query: SELECT notEmpty([1,2]);  Result: ┌─notEmpty([1, 2])─┐ │ 1 │ └──────────────────┘  "},{"title":"length​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array_functions-length","content":"Returns the number of items in the array. The result type is UInt64. The function also works for strings. Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only size0 subcolumn instead of reading and processing the whole array column. The query SELECT length(arr) FROM table transforms to SELECT arr.size0 FROM TABLE. "},{"title":"emptyArrayUInt8, emptyArrayUInt16, emptyArrayUInt32, emptyArrayUInt64​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#emptyarrayuint8-emptyarrayuint16-emptyarrayuint32-emptyarrayuint64","content":""},{"title":"emptyArrayInt8, emptyArrayInt16, emptyArrayInt32, emptyArrayInt64​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#emptyarrayint8-emptyarrayint16-emptyarrayint32-emptyarrayint64","content":""},{"title":"emptyArrayFloat32, emptyArrayFloat64​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#emptyarrayfloat32-emptyarrayfloat64","content":""},{"title":"emptyArrayDate, emptyArrayDateTime​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#emptyarraydate-emptyarraydatetime","content":""},{"title":"emptyArrayString​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#emptyarraystring","content":"Accepts zero arguments and returns an empty array of the appropriate type. "},{"title":"emptyArrayToSingle​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#emptyarraytosingle","content":"Accepts an empty array and returns a one-element array that is equal to the default value. "},{"title":"range(end), range([start, ] end [, step])​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#range","content":"Returns an array of UInt numbers from start to end - 1 by step. Syntax range([start, ] end [, step])  Arguments start — The first element of the array. Optional, required if step is used. Default value: 0. UIntend — The number before which the array is constructed. Required. UIntstep — Determines the incremental step between each element in the array. Optional. Default value: 1. UInt Returned value Array of UInt numbers from start to end - 1 by step. Implementation details All arguments must be positive values: start, end, step are UInt data types, as well as elements of the returned array.An exception is thrown if query results in arrays with a total length of more than number of elements specified by the function_range_max_elements_in_block setting. Examples Query: SELECT range(5), range(1, 5), range(1, 5, 2);  Result: ┌─range(5)────┬─range(1, 5)─┬─range(1, 5, 2)─┐ │ [0,1,2,3,4] │ [1,2,3,4] │ [1,3] │ └─────────────┴─────────────┴────────────────┘  "},{"title":"array(x1, …), operator [x1, …]​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayx1-operator-x1","content":"Creates an array from the function arguments. The arguments must be constants and have types that have the smallest common type. At least one argument must be passed, because otherwise it isn’t clear which type of array to create. That is, you can’t use this function to create an empty array (to do that, use the ‘emptyArray*’ function described above). Returns an ‘Array(T)’ type result, where ‘T’ is the smallest common type out of the passed arguments. "},{"title":"arrayConcat​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayconcat","content":"Combines arrays passed as arguments. arrayConcat(arrays)  Arguments arrays – Arbitrary number of arguments of Array type.Example SELECT arrayConcat([1, 2], [3, 4], [5, 6]) AS res  ┌─res───────────┐ │ [1,2,3,4,5,6] │ └───────────────┘  "},{"title":"arrayElement(arr, n), operator arr[n]​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayelementarr-n-operator-arrn","content":"Get the element with the index n from the array arr. n must be any integer type. Indexes in an array begin from one. Negative indexes are supported. In this case, it selects the corresponding element numbered from the end. For example, arr[-1] is the last item in the array. If the index falls outside of the bounds of an array, it returns some default value (0 for numbers, an empty string for strings, etc.), except for the case with a non-constant array and a constant index 0 (in this case there will be an error Array indices are 1-based). "},{"title":"has(arr, elem)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#hasarr-elem","content":"Checks whether the ‘arr’ array has the ‘elem’ element. Returns 0 if the element is not in the array, or 1 if it is. NULL is processed as a value. SELECT has([1, 2, NULL], NULL)  ┌─has([1, 2, NULL], NULL)─┐ │ 1 │ └─────────────────────────┘  "},{"title":"hasAll​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#hasall","content":"Checks whether one array is a subset of another. hasAll(set, subset)  Arguments set – Array of any type with a set of elements.subset – Array of any type with elements that should be tested to be a subset of set. Return values 1, if set contains all of the elements from subset.0, otherwise. Peculiar properties An empty array is a subset of any array.Null processed as a value.Order of values in both of arrays does not matter. Examples SELECT hasAll([], []) returns 1. SELECT hasAll([1, Null], [Null]) returns 1. SELECT hasAll([1.0, 2, 3, 4], [1, 3]) returns 1. SELECT hasAll(['a', 'b'], ['a']) returns 1. SELECT hasAll([1], ['a']) returns 0. SELECT hasAll([[1, 2], [3, 4]], [[1, 2], [3, 5]]) returns 0. "},{"title":"hasAny​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#hasany","content":"Checks whether two arrays have intersection by some elements. hasAny(array1, array2)  Arguments array1 – Array of any type with a set of elements.array2 – Array of any type with a set of elements. Return values 1, if array1 and array2 have one similar element at least.0, otherwise. Peculiar properties Null processed as a value.Order of values in both of arrays does not matter. Examples SELECT hasAny([1], []) returns 0. SELECT hasAny([Null], [Null, 1]) returns 1. SELECT hasAny([-128, 1., 512], [1]) returns 1. SELECT hasAny([[1, 2], [3, 4]], ['a', 'c']) returns 0. SELECT hasAll([[1, 2], [3, 4]], [[1, 2], [1, 2]]) returns 1. "},{"title":"hasSubstr​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#hassubstr","content":"Checks whether all the elements of array2 appear in array1 in the same exact order. Therefore, the function will return 1, if and only if array1 = prefix + array2 + suffix. hasSubstr(array1, array2)  In other words, the functions will check whether all the elements of array2 are contained in array1 like the hasAll function. In addition, it will check that the elements are observed in the same order in both array1 and array2. For Example: hasSubstr([1,2,3,4], [2,3]) returns 1. However, hasSubstr([1,2,3,4], [3,2]) will return 0.hasSubstr([1,2,3,4], [1,2,3]) returns 1. However, hasSubstr([1,2,3,4], [1,2,4]) will return 0. Arguments array1 – Array of any type with a set of elements.array2 – Array of any type with a set of elements. Return values 1, if array1 contains array2.0, otherwise. Peculiar properties The function will return 1 if array2 is empty.Null processed as a value. In other words hasSubstr([1, 2, NULL, 3, 4], [2,3]) will return 0. However, hasSubstr([1, 2, NULL, 3, 4], [2,NULL,3]) will return 1Order of values in both of arrays does matter. Examples SELECT hasSubstr([], []) returns 1. SELECT hasSubstr([1, Null], [Null]) returns 1. SELECT hasSubstr([1.0, 2, 3, 4], [1, 3]) returns 0. SELECT hasSubstr(['a', 'b'], ['a']) returns 1. SELECT hasSubstr(['a', 'b' , 'c'], ['a', 'b']) returns 1. SELECT hasSubstr(['a', 'b' , 'c'], ['a', 'c']) returns 0. SELECT hasSubstr([[1, 2], [3, 4], [5, 6]], [[1, 2], [3, 4]]) returns 1. "},{"title":"indexOf(arr, x)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#indexofarr-x","content":"Returns the index of the first ‘x’ element (starting from 1) if it is in the array, or 0 if it is not. Example: SELECT indexOf([1, 3, NULL, NULL], NULL)  ┌─indexOf([1, 3, NULL, NULL], NULL)─┐ │ 3 │ └───────────────────────────────────┘  Elements set to NULL are handled as normal values. "},{"title":"arrayCount([func,] arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-count","content":"Returns the number of elements in the arr array for which func returns something other than 0. If ‘func’ is not specified, it returns the number of non-zero elements in the array. Note that the arrayCount is a higher-order function. You can pass a lambda function to it as the first argument. "},{"title":"countEqual(arr, x)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#countequalarr-x","content":"Returns the number of elements in the array equal to x. Equivalent to arrayCount (elem -&gt; elem = x, arr). NULL elements are handled as separate values. Example: SELECT countEqual([1, 2, NULL, NULL], NULL)  ┌─countEqual([1, 2, NULL, NULL], NULL)─┐ │ 2 │ └──────────────────────────────────────┘  "},{"title":"arrayEnumerate(arr)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array_functions-arrayenumerate","content":"Returns the array [1, 2, 3, …, length (arr) ] This function is normally used with ARRAY JOIN. It allows counting something just once for each array after applying ARRAY JOIN. Example: SELECT count() AS Reaches, countIf(num = 1) AS Hits FROM test.hits ARRAY JOIN GoalsReached, arrayEnumerate(GoalsReached) AS num WHERE CounterID = 160656 LIMIT 10  ┌─Reaches─┬──Hits─┐ │ 95606 │ 31406 │ └─────────┴───────┘  In this example, Reaches is the number of conversions (the strings received after applying ARRAY JOIN), and Hits is the number of pageviews (strings before ARRAY JOIN). In this particular case, you can get the same result in an easier way: SELECT sum(length(GoalsReached)) AS Reaches, count() AS Hits FROM test.hits WHERE (CounterID = 160656) AND notEmpty(GoalsReached)  ┌─Reaches─┬──Hits─┐ │ 95606 │ 31406 │ └─────────┴───────┘  This function can also be used in higher-order functions. For example, you can use it to get array indexes for elements that match a condition. "},{"title":"arrayEnumerateUniq(arr, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayenumerateuniqarr","content":"Returns an array the same size as the source array, indicating for each element what its position is among elements with the same value. For example: arrayEnumerateUniq([10, 20, 10, 30]) = [1, 1, 2, 1]. This function is useful when using ARRAY JOIN and aggregation of array elements. Example: SELECT Goals.ID AS GoalID, sum(Sign) AS Reaches, sumIf(Sign, num = 1) AS Visits FROM test.visits ARRAY JOIN Goals, arrayEnumerateUniq(Goals.ID) AS num WHERE CounterID = 160656 GROUP BY GoalID ORDER BY Reaches DESC LIMIT 10  ┌──GoalID─┬─Reaches─┬─Visits─┐ │ 53225 │ 3214 │ 1097 │ │ 2825062 │ 3188 │ 1097 │ │ 56600 │ 2803 │ 488 │ │ 1989037 │ 2401 │ 365 │ │ 2830064 │ 2396 │ 910 │ │ 1113562 │ 2372 │ 373 │ │ 3270895 │ 2262 │ 812 │ │ 1084657 │ 2262 │ 345 │ │ 56599 │ 2260 │ 799 │ │ 3271094 │ 2256 │ 812 │ └─────────┴─────────┴────────┘  In this example, each goal ID has a calculation of the number of conversions (each element in the Goals nested data structure is a goal that was reached, which we refer to as a conversion) and the number of sessions. Without ARRAY JOIN, we would have counted the number of sessions as sum(Sign). But in this particular case, the rows were multiplied by the nested Goals structure, so in order to count each session one time after this, we apply a condition to the value of the arrayEnumerateUniq(Goals.ID) function. The arrayEnumerateUniq function can take multiple arrays of the same size as arguments. In this case, uniqueness is considered for tuples of elements in the same positions in all the arrays. SELECT arrayEnumerateUniq([1, 1, 1, 2, 2, 2], [1, 1, 2, 1, 1, 2]) AS res  ┌─res───────────┐ │ [1,2,1,1,2,1] │ └───────────────┘  This is necessary when using ARRAY JOIN with a nested data structure and further aggregation across multiple elements in this structure. "},{"title":"arrayPopBack​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arraypopback","content":"Removes the last item from the array. arrayPopBack(array)  Arguments array – Array. Example SELECT arrayPopBack([1, 2, 3]) AS res;  ┌─res───┐ │ [1,2] │ └───────┘  "},{"title":"arrayPopFront​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arraypopfront","content":"Removes the first item from the array. arrayPopFront(array)  Arguments array – Array. Example SELECT arrayPopFront([1, 2, 3]) AS res;  ┌─res───┐ │ [2,3] │ └───────┘  "},{"title":"arrayPushBack​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arraypushback","content":"Adds one item to the end of the array. arrayPushBack(array, single_value)  Arguments array – Array.single_value – A single value. Only numbers can be added to an array with numbers, and only strings can be added to an array of strings. When adding numbers, ClickHouse automatically sets the single_value type for the data type of the array. For more information about the types of data in ClickHouse, see “Data types”. Can be NULL. The function adds a NULL element to an array, and the type of array elements converts to Nullable. Example SELECT arrayPushBack(['a'], 'b') AS res;  ┌─res───────┐ │ ['a','b'] │ └───────────┘  "},{"title":"arrayPushFront​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arraypushfront","content":"Adds one element to the beginning of the array. arrayPushFront(array, single_value)  Arguments array – Array.single_value – A single value. Only numbers can be added to an array with numbers, and only strings can be added to an array of strings. When adding numbers, ClickHouse automatically sets the single_value type for the data type of the array. For more information about the types of data in ClickHouse, see “Data types”. Can be NULL. The function adds a NULL element to an array, and the type of array elements converts to Nullable. Example SELECT arrayPushFront(['b'], 'a') AS res;  ┌─res───────┐ │ ['a','b'] │ └───────────┘  "},{"title":"arrayResize​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayresize","content":"Changes the length of the array. arrayResize(array, size[, extender])  Arguments: array — Array.size — Required length of the array. If size is less than the original size of the array, the array is truncated from the right. If size is larger than the initial size of the array, the array is extended to the right with extender values or default values for the data type of the array items.extender — Value for extending an array. Can be NULL. Returned value: An array of length size. Examples of calls SELECT arrayResize([1], 3);  ┌─arrayResize([1], 3)─┐ │ [1,0,0] │ └─────────────────────┘  SELECT arrayResize([1], 3, NULL);  ┌─arrayResize([1], 3, NULL)─┐ │ [1,NULL,NULL] │ └───────────────────────────┘  "},{"title":"arraySlice​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayslice","content":"Returns a slice of the array. arraySlice(array, offset[, length])  Arguments array – Array of data.offset – Indent from the edge of the array. A positive value indicates an offset on the left, and a negative value is an indent on the right. Numbering of the array items begins with 1.length – The length of the required slice. If you specify a negative value, the function returns an open slice [offset, array_length - length]. If you omit the value, the function returns the slice [offset, the_end_of_array]. Example SELECT arraySlice([1, 2, NULL, 4, 5], 2, 3) AS res;  ┌─res────────┐ │ [2,NULL,4] │ └────────────┘  Array elements set to NULL are handled as normal values. "},{"title":"arraySort([func,] arr, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array_functions-sort","content":"Sorts the elements of the arr array in ascending order. If the func function is specified, sorting order is determined by the result of the func function applied to the elements of the array. If func accepts multiple arguments, the arraySort function is passed several arrays that the arguments of func will correspond to. Detailed examples are shown at the end of arraySort description. Example of integer values sorting: SELECT arraySort([1, 3, 3, 0]);  ┌─arraySort([1, 3, 3, 0])─┐ │ [0,1,3,3] │ └─────────────────────────┘  Example of string values sorting: SELECT arraySort(['hello', 'world', '!']);  ┌─arraySort(['hello', 'world', '!'])─┐ │ ['!','hello','world'] │ └────────────────────────────────────┘  Consider the following sorting order for the NULL, NaN and Inf values: SELECT arraySort([1, nan, 2, NULL, 3, nan, -4, NULL, inf, -inf]);  ┌─arraySort([1, nan, 2, NULL, 3, nan, -4, NULL, inf, -inf])─┐ │ [-inf,-4,1,2,3,inf,nan,nan,NULL,NULL] │ └───────────────────────────────────────────────────────────┘  -Inf values are first in the array.NULL values are last in the array.NaN values are right before NULL.Inf values are right before NaN. Note that arraySort is a higher-order function. You can pass a lambda function to it as the first argument. In this case, sorting order is determined by the result of the lambda function applied to the elements of the array. Let’s consider the following example: SELECT arraySort((x) -&gt; -x, [1, 2, 3]) as res;  ┌─res─────┐ │ [3,2,1] │ └─────────┘  For each element of the source array, the lambda function returns the sorting key, that is, [1 –&gt; -1, 2 –&gt; -2, 3 –&gt; -3]. Since the arraySort function sorts the keys in ascending order, the result is [3, 2, 1]. Thus, the (x) –&gt; -x lambda function sets the descending order in a sorting. The lambda function can accept multiple arguments. In this case, you need to pass the arraySort function several arrays of identical length that the arguments of lambda function will correspond to. The resulting array will consist of elements from the first input array; elements from the next input array(s) specify the sorting keys. For example: SELECT arraySort((x, y) -&gt; y, ['hello', 'world'], [2, 1]) as res;  ┌─res────────────────┐ │ ['world', 'hello'] │ └────────────────────┘  Here, the elements that are passed in the second array ([2, 1]) define a sorting key for the corresponding element from the source array ([‘hello’, ‘world’]), that is, [‘hello’ –&gt; 2, ‘world’ –&gt; 1]. Since the lambda function does not use x, actual values of the source array do not affect the order in the result. So, ‘hello’ will be the second element in the result, and ‘world’ will be the first. Other examples are shown below. SELECT arraySort((x, y) -&gt; y, [0, 1, 2], ['c', 'b', 'a']) as res;  ┌─res─────┐ │ [2,1,0] │ └─────────┘  SELECT arraySort((x, y) -&gt; -y, [0, 1, 2], [1, 2, 3]) as res;  ┌─res─────┐ │ [2,1,0] │ └─────────┘  note To improve sorting efficiency, the Schwartzian transform is used. "},{"title":"arrayReverseSort([func,] arr, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array_functions-reverse-sort","content":"Sorts the elements of the arr array in descending order. If the func function is specified, arr is sorted according to the result of the func function applied to the elements of the array, and then the sorted array is reversed. If func accepts multiple arguments, the arrayReverseSort function is passed several arrays that the arguments of func will correspond to. Detailed examples are shown at the end of arrayReverseSort description. Example of integer values sorting: SELECT arrayReverseSort([1, 3, 3, 0]);  ┌─arrayReverseSort([1, 3, 3, 0])─┐ │ [3,3,1,0] │ └────────────────────────────────┘  Example of string values sorting: SELECT arrayReverseSort(['hello', 'world', '!']);  ┌─arrayReverseSort(['hello', 'world', '!'])─┐ │ ['world','hello','!'] │ └───────────────────────────────────────────┘  Consider the following sorting order for the NULL, NaN and Inf values: SELECT arrayReverseSort([1, nan, 2, NULL, 3, nan, -4, NULL, inf, -inf]) as res;  ┌─res───────────────────────────────────┐ │ [inf,3,2,1,-4,-inf,nan,nan,NULL,NULL] │ └───────────────────────────────────────┘  Inf values are first in the array.NULL values are last in the array.NaN values are right before NULL.-Inf values are right before NaN. Note that the arrayReverseSort is a higher-order function. You can pass a lambda function to it as the first argument. Example is shown below. SELECT arrayReverseSort((x) -&gt; -x, [1, 2, 3]) as res;  ┌─res─────┐ │ [1,2,3] │ └─────────┘  The array is sorted in the following way: At first, the source array ([1, 2, 3]) is sorted according to the result of the lambda function applied to the elements of the array. The result is an array [3, 2, 1].Array that is obtained on the previous step, is reversed. So, the final result is [1, 2, 3]. The lambda function can accept multiple arguments. In this case, you need to pass the arrayReverseSort function several arrays of identical length that the arguments of lambda function will correspond to. The resulting array will consist of elements from the first input array; elements from the next input array(s) specify the sorting keys. For example: SELECT arrayReverseSort((x, y) -&gt; y, ['hello', 'world'], [2, 1]) as res;  ┌─res───────────────┐ │ ['hello','world'] │ └───────────────────┘  In this example, the array is sorted in the following way: At first, the source array ([‘hello’, ‘world’]) is sorted according to the result of the lambda function applied to the elements of the arrays. The elements that are passed in the second array ([2, 1]), define the sorting keys for corresponding elements from the source array. The result is an array [‘world’, ‘hello’].Array that was sorted on the previous step, is reversed. So, the final result is [‘hello’, ‘world’]. Other examples are shown below. SELECT arrayReverseSort((x, y) -&gt; y, [4, 3, 5], ['a', 'b', 'c']) AS res;  ┌─res─────┐ │ [5,3,4] │ └─────────┘  SELECT arrayReverseSort((x, y) -&gt; -y, [4, 3, 5], [1, 2, 3]) AS res;  ┌─res─────┐ │ [4,3,5] │ └─────────┘  "},{"title":"arrayUniq(arr, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayuniqarr","content":"If one argument is passed, it counts the number of different elements in the array. If multiple arguments are passed, it counts the number of different tuples of elements at corresponding positions in multiple arrays. If you want to get a list of unique items in an array, you can use arrayReduce(‘groupUniqArray’, arr). "},{"title":"arrayJoin(arr)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-functions-join","content":"A special function. See the section “ArrayJoin function”. "},{"title":"arrayDifference​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arraydifference","content":"Calculates the difference between adjacent array elements. Returns an array where the first element will be 0, the second is the difference between a[1] - a[0], etc. The type of elements in the resulting array is determined by the type inference rules for subtraction (e.g. UInt8 - UInt8 = Int16). Syntax arrayDifference(array)  Arguments array – Array. Returned values Returns an array of differences between adjacent elements. Type: UInt*, Int*, Float*. Example Query: SELECT arrayDifference([1, 2, 3, 4]);  Result: ┌─arrayDifference([1, 2, 3, 4])─┐ │ [0,1,1,1] │ └───────────────────────────────┘  Example of the overflow due to result type Int64: Query: SELECT arrayDifference([0, 10000000000000000000]);  Result: ┌─arrayDifference([0, 10000000000000000000])─┐ │ [0,-8446744073709551616] │ └────────────────────────────────────────────┘  "},{"title":"arrayDistinct​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arraydistinct","content":"Takes an array, returns an array containing the distinct elements only. Syntax arrayDistinct(array)  Arguments array – Array. Returned values Returns an array containing the distinct elements. Example Query: SELECT arrayDistinct([1, 2, 2, 3, 1]);  Result: ┌─arrayDistinct([1, 2, 2, 3, 1])─┐ │ [1,2,3] │ └────────────────────────────────┘  "},{"title":"arrayEnumerateDense(arr)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array_functions-arrayenumeratedense","content":"Returns an array of the same size as the source array, indicating where each element first appears in the source array. Example: SELECT arrayEnumerateDense([10, 20, 10, 30])  ┌─arrayEnumerateDense([10, 20, 10, 30])─┐ │ [1,2,1,3] │ └───────────────────────────────────────┘  "},{"title":"arrayIntersect(arr)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-functions-arrayintersect","content":"Takes multiple arrays, returns an array with elements that are present in all source arrays. Example: SELECT arrayIntersect([1, 2], [1, 3], [2, 3]) AS no_intersect, arrayIntersect([1, 2], [1, 3], [1, 4]) AS intersect  ┌─no_intersect─┬─intersect─┐ │ [] │ [1] │ └──────────────┴───────────┘  "},{"title":"arrayReduce​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayreduce","content":"Applies an aggregate function to array elements and returns its result. The name of the aggregation function is passed as a string in single quotes 'max', 'sum'. When using parametric aggregate functions, the parameter is indicated after the function name in parentheses 'uniqUpTo(6)'. Syntax arrayReduce(agg_func, arr1, arr2, ..., arrN)  Arguments agg_func — The name of an aggregate function which should be a constant string.arr — Any number of array type columns as the parameters of the aggregation function. Returned value Example Query: SELECT arrayReduce('max', [1, 2, 3]);  Result: ┌─arrayReduce('max', [1, 2, 3])─┐ │ 3 │ └───────────────────────────────┘  If an aggregate function takes multiple arguments, then this function must be applied to multiple arrays of the same size. Query: SELECT arrayReduce('maxIf', [3, 5], [1, 0]);  Result: ┌─arrayReduce('maxIf', [3, 5], [1, 0])─┐ │ 3 │ └──────────────────────────────────────┘  Example with a parametric aggregate function: Query: SELECT arrayReduce('uniqUpTo(3)', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]);  Result: ┌─arrayReduce('uniqUpTo(3)', [1, 2, 3, 4, 5, 6, 7, 8, 9, 10])─┐ │ 4 │ └─────────────────────────────────────────────────────────────┘  "},{"title":"arrayReduceInRanges​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayreduceinranges","content":"Applies an aggregate function to array elements in given ranges and returns an array containing the result corresponding to each range. The function will return the same result as multiple arrayReduce(agg_func, arraySlice(arr1, index, length), ...). Syntax arrayReduceInRanges(agg_func, ranges, arr1, arr2, ..., arrN)  Arguments agg_func — The name of an aggregate function which should be a constant string.ranges — The ranges to aggretate which should be an array of tuples which containing the index and the length of each range.arr — Any number of Array type columns as the parameters of the aggregation function. Returned value Array containing results of the aggregate function over specified ranges. Type: Array. Example Query: SELECT arrayReduceInRanges( 'sum', [(1, 5), (2, 3), (3, 4), (4, 4)], [1000000, 200000, 30000, 4000, 500, 60, 7] ) AS res  Result: ┌─res─────────────────────────┐ │ [1234500,234000,34560,4567] │ └─────────────────────────────┘  "},{"title":"arrayReverse(arr)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayreverse","content":"Returns an array of the same size as the original array containing the elements in reverse order. Example: SELECT arrayReverse([1, 2, 3])  ┌─arrayReverse([1, 2, 3])─┐ │ [3,2,1] │ └─────────────────────────┘  "},{"title":"reverse(arr)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-functions-reverse","content":"Synonym for “arrayReverse” "},{"title":"arrayFlatten​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayflatten","content":"Converts an array of arrays to a flat array. Function: Applies to any depth of nested arrays.Does not change arrays that are already flat. The flattened array contains all the elements from all source arrays. Syntax flatten(array_of_arrays)  Alias: flatten. Arguments array_of_arrays — Array of arrays. For example, [[1,2,3], [4,5]]. Examples SELECT flatten([[[1]], [[2], [3]]]);  ┌─flatten(array(array([1]), array([2], [3])))─┐ │ [1,2,3] │ └─────────────────────────────────────────────┘  "},{"title":"arrayCompact​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arraycompact","content":"Removes consecutive duplicate elements from an array. The order of result values is determined by the order in the source array. Syntax arrayCompact(arr)  Arguments arr — The array to inspect. Returned value The array without duplicate. Type: Array. Example Query: SELECT arrayCompact([1, 1, nan, nan, 2, 3, 3, 3]);  Result: ┌─arrayCompact([1, 1, nan, nan, 2, 3, 3, 3])─┐ │ [1,nan,nan,2,3] │ └────────────────────────────────────────────┘  "},{"title":"arrayZip​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayzip","content":"Combines multiple arrays into a single array. The resulting array contains the corresponding elements of the source arrays grouped into tuples in the listed order of arguments. Syntax arrayZip(arr1, arr2, ..., arrN)  Arguments arrN — Array. The function can take any number of arrays of different types. All the input arrays must be of equal size. Returned value Array with elements from the source arrays grouped into tuples. Data types in the tuple are the same as types of the input arrays and in the same order as arrays are passed. Type: Array. Example Query: SELECT arrayZip(['a', 'b', 'c'], [5, 2, 1]);  Result: ┌─arrayZip(['a', 'b', 'c'], [5, 2, 1])─┐ │ [('a',5),('b',2),('c',1)] │ └──────────────────────────────────────┘  "},{"title":"arrayAUC​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayauc","content":"Calculate AUC (Area Under the Curve, which is a concept in machine learning, see more details: https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve). Syntax arrayAUC(arr_scores, arr_labels)  Arguments arr_scores — scores prediction model gives.arr_labels — labels of samples, usually 1 for positive sample and 0 for negtive sample. Returned value Returns AUC value with type Float64. Example Query: select arrayAUC([0.1, 0.4, 0.35, 0.8], [0, 0, 1, 1]);  Result: ┌─arrayAUC([0.1, 0.4, 0.35, 0.8], [0, 0, 1, 1])─┐ │ 0.75 │ └───────────────────────────────────────────────┘  "},{"title":"arrayMap(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-map","content":"Returns an array obtained from the original application of the func function to each element in the arr array. Examples: SELECT arrayMap(x -&gt; (x + 2), [1, 2, 3]) as res;  ┌─res─────┐ │ [3,4,5] │ └─────────┘  The following example shows how to create a tuple of elements from different arrays: SELECT arrayMap((x, y) -&gt; (x, y), [1, 2, 3], [4, 5, 6]) AS res  ┌─res─────────────────┐ │ [(1,4),(2,5),(3,6)] │ └─────────────────────┘  Note that the arrayMap is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayFilter(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-filter","content":"Returns an array containing only the elements in arr1 for which func returns something other than 0. Examples: SELECT arrayFilter(x -&gt; x LIKE '%World%', ['Hello', 'abc World']) AS res  ┌─res───────────┐ │ ['abc World'] │ └───────────────┘  SELECT arrayFilter( (i, x) -&gt; x LIKE '%World%', arrayEnumerate(arr), ['Hello', 'abc World'] AS arr) AS res  ┌─res─┐ │ [2] │ └─────┘  Note that the arrayFilter is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayFill(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-fill","content":"Scan through arr1 from the first element to the last element and replace arr1[i] by arr1[i - 1] if func returns 0. The first element of arr1 will not be replaced. Examples: SELECT arrayFill(x -&gt; not isNull(x), [1, null, 3, 11, 12, null, null, 5, 6, 14, null, null]) AS res  ┌─res──────────────────────────────┐ │ [1,1,3,11,12,12,12,5,6,14,14,14] │ └──────────────────────────────────┘  Note that the arrayFill is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayReverseFill(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-reverse-fill","content":"Scan through arr1 from the last element to the first element and replace arr1[i] by arr1[i + 1] if func returns 0. The last element of arr1 will not be replaced. Examples: SELECT arrayReverseFill(x -&gt; not isNull(x), [1, null, 3, 11, 12, null, null, 5, 6, 14, null, null]) AS res  ┌─res────────────────────────────────┐ │ [1,3,3,11,12,5,5,5,6,14,NULL,NULL] │ └────────────────────────────────────┘  Note that the arrayReverseFill is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arraySplit(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-split","content":"Split arr1 into multiple arrays. When func returns something other than 0, the array will be split on the left hand side of the element. The array will not be split before the first element. Examples: SELECT arraySplit((x, y) -&gt; y, [1, 2, 3, 4, 5], [1, 0, 0, 1, 0]) AS res  ┌─res─────────────┐ │ [[1,2,3],[4,5]] │ └─────────────────┘  Note that the arraySplit is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayReverseSplit(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-reverse-split","content":"Split arr1 into multiple arrays. When func returns something other than 0, the array will be split on the right hand side of the element. The array will not be split after the last element. Examples: SELECT arrayReverseSplit((x, y) -&gt; y, [1, 2, 3, 4, 5], [1, 0, 0, 1, 0]) AS res  ┌─res───────────────┐ │ [[1],[2,3,4],[5]] │ └───────────────────┘  Note that the arrayReverseSplit is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayExists([func,] arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayexistsfunc-arr1","content":"Returns 1 if there is at least one element in arr for which func returns something other than 0. Otherwise, it returns 0. Note that the arrayExists is a higher-order function. You can pass a lambda function to it as the first argument. "},{"title":"arrayAll([func,] arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayallfunc-arr1","content":"Returns 1 if func returns something other than 0 for all the elements in arr. Otherwise, it returns 0. Note that the arrayAll is a higher-order function. You can pass a lambda function to it as the first argument. "},{"title":"arrayFirst(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-first","content":"Returns the first element in the arr1 array for which func returns something other than 0. Note that the arrayFirst is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayLast(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-last","content":"Returns the last element in the arr1 array for which func returns something other than 0. Note that the arrayLast is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayFirstIndex(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-first-index","content":"Returns the index of the first element in the arr1 array for which func returns something other than 0. Note that the arrayFirstIndex is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayLastIndex(func, arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-last-index","content":"Returns the index of the last element in the arr1 array for which func returns something other than 0. Note that the arrayLastIndex is a higher-order function. You must pass a lambda function to it as the first argument, and it can’t be omitted. "},{"title":"arrayMin​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-min","content":"Returns the minimum of elements in the source array. If the func function is specified, returns the mininum of elements converted by this function. Note that the arrayMin is a higher-order function. You can pass a lambda function to it as the first argument. Syntax arrayMin([func,] arr)  Arguments func — Function. Expression.arr — Array. Array. Returned value The minimum of function values (or the array minimum). Type: if func is specified, matches func return value type, else matches the array elements type. Examples Query: SELECT arrayMin([1, 2, 4]) AS res;  Result: ┌─res─┐ │ 1 │ └─────┘  Query: SELECT arrayMin(x -&gt; (-x), [1, 2, 4]) AS res;  Result: ┌─res─┐ │ -4 │ └─────┘  "},{"title":"arrayMax​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-max","content":"Returns the maximum of elements in the source array. If the func function is specified, returns the maximum of elements converted by this function. Note that the arrayMax is a higher-order function. You can pass a lambda function to it as the first argument. Syntax arrayMax([func,] arr)  Arguments func — Function. Expression.arr — Array. Array. Returned value The maximum of function values (or the array maximum). Type: if func is specified, matches func return value type, else matches the array elements type. Examples Query: SELECT arrayMax([1, 2, 4]) AS res;  Result: ┌─res─┐ │ 4 │ └─────┘  Query: SELECT arrayMax(x -&gt; (-x), [1, 2, 4]) AS res;  Result: ┌─res─┐ │ -1 │ └─────┘  "},{"title":"arraySum​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-sum","content":"Returns the sum of elements in the source array. If the func function is specified, returns the sum of elements converted by this function. Note that the arraySum is a higher-order function. You can pass a lambda function to it as the first argument. Syntax arraySum([func,] arr)  Arguments func — Function. Expression.arr — Array. Array. Returned value The sum of the function values (or the array sum). Type: for decimal numbers in source array (or for converted values, if func is specified) — Decimal128, for floating point numbers — Float64, for numeric unsigned — UInt64, and for numeric signed — Int64. Examples Query: SELECT arraySum([2, 3]) AS res;  Result: ┌─res─┐ │ 5 │ └─────┘  Query: SELECT arraySum(x -&gt; x*x, [2, 3]) AS res;  Result: ┌─res─┐ │ 13 │ └─────┘  "},{"title":"arrayAvg​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#array-avg","content":"Returns the average of elements in the source array. If the func function is specified, returns the average of elements converted by this function. Note that the arrayAvg is a higher-order function. You can pass a lambda function to it as the first argument. Syntax arrayAvg([func,] arr)  Arguments func — Function. Expression.arr — Array. Array. Returned value The average of function values (or the array average). Type: Float64. Examples Query: SELECT arrayAvg([1, 2, 4]) AS res;  Result: ┌────────────────res─┐ │ 2.3333333333333335 │ └────────────────────┘  Query: SELECT arrayAvg(x -&gt; (x * x), [2, 4]) AS res;  Result: ┌─res─┐ │ 10 │ └─────┘  "},{"title":"arrayCumSum([func,] arr1, …)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arraycumsumfunc-arr1","content":"Returns an array of partial sums of elements in the source array (a running sum). If the func function is specified, then the values of the array elements are converted by this function before summing. Example: SELECT arrayCumSum([1, 1, 1, 1]) AS res  ┌─res──────────┐ │ [1, 2, 3, 4] │ └──────────────┘  Note that the arrayCumSum is a higher-order function. You can pass a lambda function to it as the first argument. "},{"title":"arrayCumSumNonNegative(arr)​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arraycumsumnonnegativearr","content":"Same as arrayCumSum, returns an array of partial sums of elements in the source array (a running sum). Different arrayCumSum, when then returned value contains a value less than zero, the value is replace with zero and the subsequent calculation is performed with zero parameters. For example: SELECT arrayCumSumNonNegative([1, 1, -4, 1]) AS res  ┌─res───────┐ │ [1,2,0,1] │ └───────────┘  Note that the arraySumNonNegative is a higher-order function. You can pass a lambda function to it as the first argument. "},{"title":"arrayProduct​","type":1,"pageTitle":"Array Functions","url":"en/sql-reference/functions/array-functions#arrayproduct","content":"Multiplies elements of an array. Syntax arrayProduct(arr)  Arguments arr — Array of numeric values. Returned value A product of array's elements. Type: Float64. Examples Query: SELECT arrayProduct([1,2,3,4,5,6]) as res;  Result: ┌─res───┐ │ 720 │ └───────┘  Query: SELECT arrayProduct([toDecimal64(1,8), toDecimal64(2,8), toDecimal64(3,8)]) as res, toTypeName(res);  Return value type is always Float64. Result: ┌─res─┬─toTypeName(arrayProduct(array(toDecimal64(1, 8), toDecimal64(2, 8), toDecimal64(3, 8))))─┐ │ 6 │ Float64 │ └─────┴──────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"Hash Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/hash-functions","content":"","keywords":""},{"title":"halfMD5​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#hash-functions-halfmd5","content":"Interprets all the input parameters as strings and calculates the MD5 hash value for each of them. Then combines hashes, takes the first 8 bytes of the hash of the resulting string, and interprets them as UInt64 in big-endian byte order. halfMD5(par1, ...)  The function is relatively slow (5 million short strings per second per processor core). Consider using the sipHash64 function instead. Arguments The function takes a variable number of input parameters. Arguments can be any of the supported data types. Returned Value A UInt64 data type hash value. Example SELECT halfMD5(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS halfMD5hash, toTypeName(halfMD5hash) AS type;  ┌────────halfMD5hash─┬─type───┐ │ 186182704141653334 │ UInt64 │ └────────────────────┴────────┘  "},{"title":"MD4​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#hash_functions-md4","content":"Calculates the MD4 from a string and returns the resulting set of bytes as FixedString(16). "},{"title":"MD5​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#hash_functions-md5","content":"Calculates the MD5 from a string and returns the resulting set of bytes as FixedString(16). If you do not need MD5 in particular, but you need a decent cryptographic 128-bit hash, use the ‘sipHash128’ function instead. If you want to get the same result as output by the md5sum utility, use lower(hex(MD5(s))). "},{"title":"sipHash64​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#hash_functions-siphash64","content":"Produces a 64-bit SipHash hash value. sipHash64(par1,...)  This is a cryptographic hash function. It works at least three times faster than the MD5 function. Function interprets all the input parameters as strings and calculates the hash value for each of them. Then combines hashes by the following algorithm: After hashing all the input parameters, the function gets the array of hashes.Function takes the first and the second elements and calculates a hash for the array of them.Then the function takes the hash value, calculated at the previous step, and the third element of the initial hash array, and calculates a hash for the array of them.The previous step is repeated for all the remaining elements of the initial hash array. Arguments The function takes a variable number of input parameters. Arguments can be any of the supported data types. Returned Value A UInt64 data type hash value. Example SELECT sipHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS SipHash, toTypeName(SipHash) AS type;  ┌──────────────SipHash─┬─type───┐ │ 13726873534472839665 │ UInt64 │ └──────────────────────┴────────┘  "},{"title":"sipHash128​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#hash_functions-siphash128","content":"Produces a 128-bit SipHash hash value. Differs from sipHash64 in that the final xor-folding state is done up to 128 bits. Syntax sipHash128(par1,...)  Arguments The function takes a variable number of input parameters. Arguments can be any of the supported data types. Returned value A 128-bit SipHash hash value. Type: FixedString(16). Example Query: SELECT hex(sipHash128('foo', '\\x01', 3));  Result: ┌─hex(sipHash128('foo', '', 3))────┐ │ 9DE516A64A414D4B1B609415E4523F24 │ └──────────────────────────────────┘  "},{"title":"cityHash64​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#cityhash64","content":"Produces a 64-bit CityHash hash value. cityHash64(par1,...)  This is a fast non-cryptographic hash function. It uses the CityHash algorithm for string parameters and implementation-specific fast non-cryptographic hash function for parameters with other data types. The function uses the CityHash combinator to get the final results. Arguments The function takes a variable number of input parameters. Arguments can be any of the supported data types. Returned Value A UInt64 data type hash value. Examples Call example: SELECT cityHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS CityHash, toTypeName(CityHash) AS type;  ┌─────────────CityHash─┬─type───┐ │ 12072650598913549138 │ UInt64 │ └──────────────────────┴────────┘  The following example shows how to compute the checksum of the entire table with accuracy up to the row order: SELECT groupBitXor(cityHash64(*)) FROM table  "},{"title":"intHash32​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#inthash32","content":"Calculates a 32-bit hash code from any type of integer. This is a relatively fast non-cryptographic hash function of average quality for numbers. "},{"title":"intHash64​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#inthash64","content":"Calculates a 64-bit hash code from any type of integer. It works faster than intHash32. Average quality. "},{"title":"SHA1, SHA224, SHA256, SHA512​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#sha","content":"Calculates SHA-1, SHA-224, SHA-256, SHA-512 hash from a string and returns the resulting set of bytes as FixedString. Syntax SHA1('s') ... SHA512('s')  The function works fairly slowly (SHA-1 processes about 5 million short strings per second per processor core, while SHA-224 and SHA-256 process about 2.2 million). We recommend using this function only in cases when you need a specific hash function and you can’t select it. Even in these cases, we recommend applying the function offline and pre-calculating values when inserting them into the table, instead of applying it in SELECT queries. Arguments s — Input string for SHA hash calculation. String. Returned value SHA hash as a hex-unencoded FixedString. SHA-1 returns as FixedString(20), SHA-224 as FixedString(28), SHA-256 — FixedString(32), SHA-512 — FixedString(64). Type: FixedString. Example Use the hex function to represent the result as a hex-encoded string. Query: SELECT hex(SHA1('abc'));  Result: ┌─hex(SHA1('abc'))─────────────────────────┐ │ A9993E364706816ABA3E25717850C26C9CD0D89D │ └──────────────────────────────────────────┘  "},{"title":"BLAKE3​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#blake3","content":"Calculates BLAKE3 hash string and returns the resulting set of bytes as FixedString. Syntax BLAKE3('s')  This cryptographic hash-function is integrated into ClickHouse with BLAKE3 Rust library. The function is rather fast and shows approximately two times faster performance compared to SHA-2, while generating hashes of the same length as SHA-256. Arguments s - input string for BLAKE3 hash calculation. String. Return value BLAKE3 hash as a byte array with type FixedString(32). Type: FixedString. Example Use function hex to represent the result as a hex-encoded string. Query: SELECT hex(BLAKE3('ABC'))  Result: ┌─hex(BLAKE3('ABC'))───────────────────────────────────────────────┐ │ D1717274597CF0289694F75D96D444B992A096F1AFD8E7BBFA6EBB1D360FEDFC │ └──────────────────────────────────────────────────────────────────┘  "},{"title":"URLHash(url[, N])​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#urlhashurl-n","content":"A fast, decent-quality non-cryptographic hash function for a string obtained from a URL using some type of normalization.URLHash(s) – Calculates a hash from a string without one of the trailing symbols /,? or # at the end, if present.URLHash(s, N) – Calculates a hash from a string up to the N level in the URL hierarchy, without one of the trailing symbols /,? or # at the end, if present. Levels are the same as in URLHierarchy. "},{"title":"farmFingerprint64​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#farmfingerprint64","content":""},{"title":"farmHash64​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#farmhash64","content":"Produces a 64-bit FarmHash or Fingerprint value. farmFingerprint64 is preferred for a stable and portable value. farmFingerprint64(par1, ...) farmHash64(par1, ...)  These functions use the Fingerprint64 and Hash64 methods respectively from all available methods. Arguments The function takes a variable number of input parameters. Arguments can be any of the supported data types. Returned Value A UInt64 data type hash value. Example SELECT farmHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS FarmHash, toTypeName(FarmHash) AS type;  ┌─────────────FarmHash─┬─type───┐ │ 17790458267262532859 │ UInt64 │ └──────────────────────┴────────┘  "},{"title":"javaHash​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#hash_functions-javahash","content":"Calculates JavaHash from a string. This hash function is neither fast nor having a good quality. The only reason to use it is when this algorithm is already used in another system and you have to calculate exactly the same result. Syntax SELECT javaHash('')  Returned value A Int32 data type hash value. Example Query: SELECT javaHash('Hello, world!');  Result: ┌─javaHash('Hello, world!')─┐ │ -1880044555 │ └───────────────────────────┘  "},{"title":"javaHashUTF16LE​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#javahashutf16le","content":"Calculates JavaHash from a string, assuming it contains bytes representing a string in UTF-16LE encoding. Syntax javaHashUTF16LE(stringUtf16le)  Arguments stringUtf16le — a string in UTF-16LE encoding. Returned value A Int32 data type hash value. Example Correct query with UTF-16LE encoded string. Query: SELECT javaHashUTF16LE(convertCharset('test', 'utf-8', 'utf-16le'));  Result: ┌─javaHashUTF16LE(convertCharset('test', 'utf-8', 'utf-16le'))─┐ │ 3556498 │ └──────────────────────────────────────────────────────────────┘  "},{"title":"hiveHash​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#hash-functions-hivehash","content":"Calculates HiveHash from a string. SELECT hiveHash('')  This is just JavaHash with zeroed out sign bit. This function is used in Apache Hive for versions before 3.0. This hash function is neither fast nor having a good quality. The only reason to use it is when this algorithm is already used in another system and you have to calculate exactly the same result. Returned value A Int32 data type hash value. Type: hiveHash. Example Query: SELECT hiveHash('Hello, world!');  Result: ┌─hiveHash('Hello, world!')─┐ │ 267439093 │ └───────────────────────────┘  "},{"title":"metroHash64​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#metrohash64","content":"Produces a 64-bit MetroHash hash value. metroHash64(par1, ...)  Arguments The function takes a variable number of input parameters. Arguments can be any of the supported data types. Returned Value A UInt64 data type hash value. Example SELECT metroHash64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS MetroHash, toTypeName(MetroHash) AS type;  ┌────────────MetroHash─┬─type───┐ │ 14235658766382344533 │ UInt64 │ └──────────────────────┴────────┘  "},{"title":"jumpConsistentHash​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#jumpconsistenthash","content":"Calculates JumpConsistentHash form a UInt64. Accepts two arguments: a UInt64-type key and the number of buckets. Returns Int32. For more information, see the link: JumpConsistentHash "},{"title":"murmurHash2_32, murmurHash2_64​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#murmurhash2-32-murmurhash2-64","content":"Produces a MurmurHash2 hash value. murmurHash2_32(par1, ...) murmurHash2_64(par1, ...)  Arguments Both functions take a variable number of input parameters. Arguments can be any of the supported data types. Returned Value The murmurHash2_32 function returns hash value having the UInt32 data type.The murmurHash2_64 function returns hash value having the UInt64 data type. Example SELECT murmurHash2_64(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS MurmurHash2, toTypeName(MurmurHash2) AS type;  ┌──────────MurmurHash2─┬─type───┐ │ 11832096901709403633 │ UInt64 │ └──────────────────────┴────────┘  "},{"title":"gccMurmurHash​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#gccmurmurhash","content":"Calculates a 64-bit MurmurHash2 hash value using the same hash seed as gcc. It is portable between CLang and GCC builds. Syntax gccMurmurHash(par1, ...)  Arguments par1, ... — A variable number of parameters that can be any of the supported data types. Returned value Calculated hash value. Type: UInt64. Example Query: SELECT gccMurmurHash(1, 2, 3) AS res1, gccMurmurHash(('a', [1, 2, 3], 4, (4, ['foo', 'bar'], 1, (1, 2)))) AS res2  Result: ┌─────────────────res1─┬────────────────res2─┐ │ 12384823029245979431 │ 1188926775431157506 │ └──────────────────────┴─────────────────────┘  "},{"title":"murmurHash3_32, murmurHash3_64​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#murmurhash3-32-murmurhash3-64","content":"Produces a MurmurHash3 hash value. murmurHash3_32(par1, ...) murmurHash3_64(par1, ...)  Arguments Both functions take a variable number of input parameters. Arguments can be any of the supported data types. Returned Value The murmurHash3_32 function returns a UInt32 data type hash value.The murmurHash3_64 function returns a UInt64 data type hash value. Example SELECT murmurHash3_32(array('e','x','a'), 'mple', 10, toDateTime('2019-06-15 23:00:00')) AS MurmurHash3, toTypeName(MurmurHash3) AS type;  ┌─MurmurHash3─┬─type───┐ │ 2152717 │ UInt32 │ └─────────────┴────────┘  "},{"title":"murmurHash3_128​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#murmurhash3-128","content":"Produces a 128-bit MurmurHash3 hash value. Syntax murmurHash3_128(expr)  Arguments expr — A list of expressions. String. Returned value A 128-bit MurmurHash3 hash value. Type: FixedString(16). Example Query: SELECT hex(murmurHash3_128('foo', 'foo', 'foo'));  Result: ┌─hex(murmurHash3_128('foo', 'foo', 'foo'))─┐ │ F8F7AD9B6CD4CF117A71E277E2EC2931 │ └───────────────────────────────────────────┘  "},{"title":"xxHash32, xxHash64​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#hash-functions-xxhash32","content":"Calculates xxHash from a string. It is proposed in two flavors, 32 and 64 bits. SELECT xxHash32('') OR SELECT xxHash64('')  Returned value A Uint32 or Uint64 data type hash value. Type: xxHash. Example Query: SELECT xxHash32('Hello, world!');  Result: ┌─xxHash32('Hello, world!')─┐ │ 834093149 │ └───────────────────────────┘  See Also xxHash. "},{"title":"ngramSimHash​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#ngramsimhash","content":"Splits a ASCII string into n-grams of ngramsize symbols and returns the n-gram simhash. Is case sensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax ngramSimHash(string[, ngramsize])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT ngramSimHash('ClickHouse') AS Hash;  Result: ┌───────Hash─┐ │ 1627567969 │ └────────────┘  "},{"title":"ngramSimHashCaseInsensitive​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#ngramsimhashcaseinsensitive","content":"Splits a ASCII string into n-grams of ngramsize symbols and returns the n-gram simhash. Is case insensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax ngramSimHashCaseInsensitive(string[, ngramsize])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT ngramSimHashCaseInsensitive('ClickHouse') AS Hash;  Result: ┌──────Hash─┐ │ 562180645 │ └───────────┘  "},{"title":"ngramSimHashUTF8​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#ngramsimhashutf8","content":"Splits a UTF-8 string into n-grams of ngramsize symbols and returns the n-gram simhash. Is case sensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax ngramSimHashUTF8(string[, ngramsize])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT ngramSimHashUTF8('ClickHouse') AS Hash;  Result: ┌───────Hash─┐ │ 1628157797 │ └────────────┘  "},{"title":"ngramSimHashCaseInsensitiveUTF8​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#ngramsimhashcaseinsensitiveutf8","content":"Splits a UTF-8 string into n-grams of ngramsize symbols and returns the n-gram simhash. Is case insensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax ngramSimHashCaseInsensitiveUTF8(string[, ngramsize])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT ngramSimHashCaseInsensitiveUTF8('ClickHouse') AS Hash;  Result: ┌───────Hash─┐ │ 1636742693 │ └────────────┘  "},{"title":"wordShingleSimHash​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#wordshinglesimhash","content":"Splits a ASCII string into parts (shingles) of shinglesize words and returns the word shingle simhash. Is case sensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax wordShingleSimHash(string[, shinglesize])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT wordShingleSimHash('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Hash;  Result: ┌───────Hash─┐ │ 2328277067 │ └────────────┘  "},{"title":"wordShingleSimHashCaseInsensitive​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#wordshinglesimhashcaseinsensitive","content":"Splits a ASCII string into parts (shingles) of shinglesize words and returns the word shingle simhash. Is case insensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax wordShingleSimHashCaseInsensitive(string[, shinglesize])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT wordShingleSimHashCaseInsensitive('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Hash;  Result: ┌───────Hash─┐ │ 2194812424 │ └────────────┘  "},{"title":"wordShingleSimHashUTF8​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#wordshinglesimhashutf8","content":"Splits a UTF-8 string into parts (shingles) of shinglesize words and returns the word shingle simhash. Is case sensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax wordShingleSimHashUTF8(string[, shinglesize])  Arguments string — String. String.shinglesize — The size of a word shingle. Optinal. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT wordShingleSimHashUTF8('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Hash;  Result: ┌───────Hash─┐ │ 2328277067 │ └────────────┘  "},{"title":"wordShingleSimHashCaseInsensitiveUTF8​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#wordshinglesimhashcaseinsensitiveutf8","content":"Splits a UTF-8 string into parts (shingles) of shinglesize words and returns the word shingle simhash. Is case insensitive. Can be used for detection of semi-duplicate strings with bitHammingDistance. The smaller is the Hamming Distance of the calculated simhashes of two strings, the more likely these strings are the same. Syntax wordShingleSimHashCaseInsensitiveUTF8(string[, shinglesize])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8. Returned value Hash value. Type: UInt64. Example Query: SELECT wordShingleSimHashCaseInsensitiveUTF8('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Hash;  Result: ┌───────Hash─┐ │ 2194812424 │ └────────────┘  "},{"title":"ngramMinHash​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#ngramminhash","content":"Splits a ASCII string into n-grams of ngramsize symbols and calculates hash values for each n-gram. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case sensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax ngramMinHash(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT ngramMinHash('ClickHouse') AS Tuple;  Result: ┌─Tuple──────────────────────────────────────┐ │ (18333312859352735453,9054248444481805918) │ └────────────────────────────────────────────┘  "},{"title":"ngramMinHashCaseInsensitive​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#ngramminhashcaseinsensitive","content":"Splits a ASCII string into n-grams of ngramsize symbols and calculates hash values for each n-gram. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case insensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax ngramMinHashCaseInsensitive(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT ngramMinHashCaseInsensitive('ClickHouse') AS Tuple;  Result: ┌─Tuple──────────────────────────────────────┐ │ (2106263556442004574,13203602793651726206) │ └────────────────────────────────────────────┘  "},{"title":"ngramMinHashUTF8​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#ngramminhashutf8","content":"Splits a UTF-8 string into n-grams of ngramsize symbols and calculates hash values for each n-gram. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case sensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax ngramMinHashUTF8(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT ngramMinHashUTF8('ClickHouse') AS Tuple;  Result: ┌─Tuple──────────────────────────────────────┐ │ (18333312859352735453,6742163577938632877) │ └────────────────────────────────────────────┘  "},{"title":"ngramMinHashCaseInsensitiveUTF8​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#ngramminhashcaseinsensitiveutf8","content":"Splits a UTF-8 string into n-grams of ngramsize symbols and calculates hash values for each n-gram. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case insensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax ngramMinHashCaseInsensitiveUTF8(string [, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT ngramMinHashCaseInsensitiveUTF8('ClickHouse') AS Tuple;  Result: ┌─Tuple───────────────────────────────────────┐ │ (12493625717655877135,13203602793651726206) │ └─────────────────────────────────────────────┘  "},{"title":"ngramMinHashArg​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#ngramminhasharg","content":"Splits a ASCII string into n-grams of ngramsize symbols and returns the n-grams with minimum and maximum hashes, calculated by the ngramMinHash function with the same input. Is case sensitive. Syntax ngramMinHashArg(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum n-grams each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT ngramMinHashArg('ClickHouse') AS Tuple;  Result: ┌─Tuple─────────────────────────────────────────────────────────────────────────┐ │ (('ous','ick','lic','Hou','kHo','use'),('Hou','lic','ick','ous','ckH','Cli')) │ └───────────────────────────────────────────────────────────────────────────────┘  "},{"title":"ngramMinHashArgCaseInsensitive​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#ngramminhashargcaseinsensitive","content":"Splits a ASCII string into n-grams of ngramsize symbols and returns the n-grams with minimum and maximum hashes, calculated by the ngramMinHashCaseInsensitive function with the same input. Is case insensitive. Syntax ngramMinHashArgCaseInsensitive(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum n-grams each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT ngramMinHashArgCaseInsensitive('ClickHouse') AS Tuple;  Result: ┌─Tuple─────────────────────────────────────────────────────────────────────────┐ │ (('ous','ick','lic','kHo','use','Cli'),('kHo','lic','ick','ous','ckH','Hou')) │ └───────────────────────────────────────────────────────────────────────────────┘  "},{"title":"ngramMinHashArgUTF8​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#ngramminhashargutf8","content":"Splits a UTF-8 string into n-grams of ngramsize symbols and returns the n-grams with minimum and maximum hashes, calculated by the ngramMinHashUTF8 function with the same input. Is case sensitive. Syntax ngramMinHashArgUTF8(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum n-grams each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT ngramMinHashArgUTF8('ClickHouse') AS Tuple;  Result: ┌─Tuple─────────────────────────────────────────────────────────────────────────┐ │ (('ous','ick','lic','Hou','kHo','use'),('kHo','Hou','lic','ick','ous','ckH')) │ └───────────────────────────────────────────────────────────────────────────────┘  "},{"title":"ngramMinHashArgCaseInsensitiveUTF8​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#ngramminhashargcaseinsensitiveutf8","content":"Splits a UTF-8 string into n-grams of ngramsize symbols and returns the n-grams with minimum and maximum hashes, calculated by the ngramMinHashCaseInsensitiveUTF8 function with the same input. Is case insensitive. Syntax ngramMinHashArgCaseInsensitiveUTF8(string[, ngramsize, hashnum])  Arguments string — String. String.ngramsize — The size of an n-gram. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum n-grams each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT ngramMinHashArgCaseInsensitiveUTF8('ClickHouse') AS Tuple;  Result: ┌─Tuple─────────────────────────────────────────────────────────────────────────┐ │ (('ckH','ous','ick','lic','kHo','use'),('kHo','lic','ick','ous','ckH','Hou')) │ └───────────────────────────────────────────────────────────────────────────────┘  "},{"title":"wordShingleMinHash​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#wordshingleminhash","content":"Splits a ASCII string into parts (shingles) of shinglesize words and calculates hash values for each word shingle. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case sensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax wordShingleMinHash(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT wordShingleMinHash('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Tuple;  Result: ┌─Tuple──────────────────────────────────────┐ │ (16452112859864147620,5844417301642981317) │ └────────────────────────────────────────────┘  "},{"title":"wordShingleMinHashCaseInsensitive​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#wordshingleminhashcaseinsensitive","content":"Splits a ASCII string into parts (shingles) of shinglesize words and calculates hash values for each word shingle. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case insensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax wordShingleMinHashCaseInsensitive(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT wordShingleMinHashCaseInsensitive('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Tuple;  Result: ┌─Tuple─────────────────────────────────────┐ │ (3065874883688416519,1634050779997673240) │ └───────────────────────────────────────────┘  "},{"title":"wordShingleMinHashUTF8​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#wordshingleminhashutf8","content":"Splits a UTF-8 string into parts (shingles) of shinglesize words and calculates hash values for each word shingle. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case sensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax wordShingleMinHashUTF8(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT wordShingleMinHashUTF8('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Tuple;  Result: ┌─Tuple──────────────────────────────────────┐ │ (16452112859864147620,5844417301642981317) │ └────────────────────────────────────────────┘  "},{"title":"wordShingleMinHashCaseInsensitiveUTF8​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#wordshingleminhashcaseinsensitiveutf8","content":"Splits a UTF-8 string into parts (shingles) of shinglesize words and calculates hash values for each word shingle. Uses hashnum minimum hashes to calculate the minimum hash and hashnum maximum hashes to calculate the maximum hash. Returns a tuple with these hashes. Is case insensitive. Can be used for detection of semi-duplicate strings with tupleHammingDistance. For two strings: if one of the returned hashes is the same for both strings, we think that those strings are the same. Syntax wordShingleMinHashCaseInsensitiveUTF8(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two hashes — the minimum and the maximum. Type: Tuple(UInt64, UInt64). Example Query: SELECT wordShingleMinHashCaseInsensitiveUTF8('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).') AS Tuple;  Result: ┌─Tuple─────────────────────────────────────┐ │ (3065874883688416519,1634050779997673240) │ └───────────────────────────────────────────┘  "},{"title":"wordShingleMinHashArg​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#wordshingleminhasharg","content":"Splits a ASCII string into parts (shingles) of shinglesize words each and returns the shingles with minimum and maximum word hashes, calculated by the wordshingleMinHash function with the same input. Is case sensitive. Syntax wordShingleMinHashArg(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum word shingles each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT wordShingleMinHashArg('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).', 1, 3) AS Tuple;  Result: ┌─Tuple─────────────────────────────────────────────────────────────────┐ │ (('OLAP','database','analytical'),('online','oriented','processing')) │ └───────────────────────────────────────────────────────────────────────┘  "},{"title":"wordShingleMinHashArgCaseInsensitive​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#wordshingleminhashargcaseinsensitive","content":"Splits a ASCII string into parts (shingles) of shinglesize words each and returns the shingles with minimum and maximum word hashes, calculated by the wordShingleMinHashCaseInsensitive function with the same input. Is case insensitive. Syntax wordShingleMinHashArgCaseInsensitive(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum word shingles each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT wordShingleMinHashArgCaseInsensitive('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).', 1, 3) AS Tuple;  Result: ┌─Tuple──────────────────────────────────────────────────────────────────┐ │ (('queries','database','analytical'),('oriented','processing','DBMS')) │ └────────────────────────────────────────────────────────────────────────┘  "},{"title":"wordShingleMinHashArgUTF8​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#wordshingleminhashargutf8","content":"Splits a UTF-8 string into parts (shingles) of shinglesize words each and returns the shingles with minimum and maximum word hashes, calculated by the wordShingleMinHashUTF8 function with the same input. Is case sensitive. Syntax wordShingleMinHashArgUTF8(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum word shingles each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT wordShingleMinHashArgUTF8('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).', 1, 3) AS Tuple;  Result: ┌─Tuple─────────────────────────────────────────────────────────────────┐ │ (('OLAP','database','analytical'),('online','oriented','processing')) │ └───────────────────────────────────────────────────────────────────────┘  "},{"title":"wordShingleMinHashArgCaseInsensitiveUTF8​","type":1,"pageTitle":"Hash Functions","url":"en/sql-reference/functions/hash-functions#wordshingleminhashargcaseinsensitiveutf8","content":"Splits a UTF-8 string into parts (shingles) of shinglesize words each and returns the shingles with minimum and maximum word hashes, calculated by the wordShingleMinHashCaseInsensitiveUTF8 function with the same input. Is case insensitive. Syntax wordShingleMinHashArgCaseInsensitiveUTF8(string[, shinglesize, hashnum])  Arguments string — String. String.shinglesize — The size of a word shingle. Optional. Possible values: any number from 1 to 25. Default value: 3. UInt8.hashnum — The number of minimum and maximum hashes used to calculate the result. Optional. Possible values: any number from 1 to 25. Default value: 6. UInt8. Returned value Tuple with two tuples with hashnum word shingles each. Type: Tuple(Tuple(String), Tuple(String)). Example Query: SELECT wordShingleMinHashArgCaseInsensitiveUTF8('ClickHouse® is a column-oriented database management system (DBMS) for online analytical processing of queries (OLAP).', 1, 3) AS Tuple;  Result: ┌─Tuple──────────────────────────────────────────────────────────────────┐ │ (('queries','database','analytical'),('oriented','processing','DBMS')) │ └────────────────────────────────────────────────────────────────────────┘  "},{"title":"Functions for Generating Pseudo-Random Numbers","type":0,"sectionRef":"#","url":"en/sql-reference/functions/random-functions","content":"","keywords":""},{"title":"rand, rand32​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"en/sql-reference/functions/random-functions#rand","content":"Returns a pseudo-random UInt32 number, evenly distributed among all UInt32-type numbers. Uses a linear congruential generator. "},{"title":"rand64​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"en/sql-reference/functions/random-functions#rand64","content":"Returns a pseudo-random UInt64 number, evenly distributed among all UInt64-type numbers. Uses a linear congruential generator. "},{"title":"randConstant​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"en/sql-reference/functions/random-functions#randconstant","content":"Produces a constant column with a random value. Syntax randConstant([x])  Arguments x — Expression resulting in any of the supported data types. The resulting value is discarded, but the expression itself if used for bypassing common subexpression elimination if the function is called multiple times in one query. Optional parameter. Returned value Pseudo-random number. Type: UInt32. Example Query: SELECT rand(), rand(1), rand(number), randConstant(), randConstant(1), randConstant(number) FROM numbers(3)  Result: ┌─────rand()─┬────rand(1)─┬─rand(number)─┬─randConstant()─┬─randConstant(1)─┬─randConstant(number)─┐ │ 3047369878 │ 4132449925 │ 4044508545 │ 2740811946 │ 4229401477 │ 1924032898 │ │ 2938880146 │ 1267722397 │ 4154983056 │ 2740811946 │ 4229401477 │ 1924032898 │ │ 956619638 │ 4238287282 │ 1104342490 │ 2740811946 │ 4229401477 │ 1924032898 │ └────────────┴────────────┴──────────────┴────────────────┴─────────────────┴──────────────────────┘  Random Functions for Working with Strings "},{"title":"randomString​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"en/sql-reference/functions/random-functions#random-string","content":""},{"title":"randomFixedString​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"en/sql-reference/functions/random-functions#random-fixed-string","content":""},{"title":"randomPrintableASCII​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"en/sql-reference/functions/random-functions#random-printable-ascii","content":""},{"title":"randomStringUTF8​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"en/sql-reference/functions/random-functions#random-string-utf8","content":""},{"title":"fuzzBits​","type":1,"pageTitle":"Functions for Generating Pseudo-Random Numbers","url":"en/sql-reference/functions/random-functions#fuzzbits","content":"Syntax fuzzBits([s], [prob])  Inverts bits of s, each with probability prob. Arguments s - String or FixedStringprob - constant Float32/64 Returned valueFuzzed string with same as s type. Example SELECT fuzzBits(materialize('abacaba'), 0.1) FROM numbers(3)  ``` text ┌─fuzzBits(materialize(‘abacaba’), 0.1)─┐ │ abaaaja │ │ a*cjab+ │ │ aeca2A │ └───────────────────────────────────────┘ "},{"title":"Rounding Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/rounding-functions","content":"","keywords":""},{"title":"floor(x[, N])​","type":1,"pageTitle":"Rounding Functions","url":"en/sql-reference/functions/rounding-functions#floorx-n","content":"Returns the largest round number that is less than or equal to x. A round number is a multiple of 1/10N, or the nearest number of the appropriate data type if 1 / 10N isn’t exact. ‘N’ is an integer constant, optional parameter. By default it is zero, which means to round to an integer. ‘N’ may be negative. Examples: floor(123.45, 1) = 123.4, floor(123.45, -1) = 120. x is any numeric type. The result is a number of the same type. For integer arguments, it makes sense to round with a negative N value (for non-negative N, the function does not do anything). If rounding causes overflow (for example, floor(-128, -1)), an implementation-specific result is returned. "},{"title":"ceil(x[, N]), ceiling(x[, N])​","type":1,"pageTitle":"Rounding Functions","url":"en/sql-reference/functions/rounding-functions#ceilx-n-ceilingx-n","content":"Returns the smallest round number that is greater than or equal to x. In every other way, it is the same as the floor function (see above). "},{"title":"trunc(x[, N]), truncate(x[, N])​","type":1,"pageTitle":"Rounding Functions","url":"en/sql-reference/functions/rounding-functions#truncx-n-truncatex-n","content":"Returns the round number with largest absolute value that has an absolute value less than or equal to x‘s. In every other way, it is the same as the ’floor’ function (see above). "},{"title":"round(x[, N])​","type":1,"pageTitle":"Rounding Functions","url":"en/sql-reference/functions/rounding-functions#rounding_functions-round","content":"Rounds a value to a specified number of decimal places. The function returns the nearest number of the specified order. In case when given number has equal distance to surrounding numbers, the function uses banker’s rounding for float number types and rounds away from zero for the other number types (Decimal). round(expression [, decimal_places])  Arguments expression — A number to be rounded. Can be any expression returning the numeric data type.decimal-places — An integer value. If decimal-places &gt; 0 then the function rounds the value to the right of the decimal point.If decimal-places &lt; 0 then the function rounds the value to the left of the decimal point.If decimal-places = 0 then the function rounds the value to integer. In this case the argument can be omitted. Returned value: The rounded number of the same type as the input number. "},{"title":"Examples​","type":1,"pageTitle":"Rounding Functions","url":"en/sql-reference/functions/rounding-functions#examples","content":"Example of use with Float SELECT number / 2 AS x, round(x) FROM system.numbers LIMIT 3  ┌───x─┬─round(divide(number, 2))─┐ │ 0 │ 0 │ │ 0.5 │ 0 │ │ 1 │ 1 │ └─────┴──────────────────────────┘  Example of use with Decimal SELECT cast(number / 2 AS Decimal(10,4)) AS x, round(x) FROM system.numbers LIMIT 3  ┌──────x─┬─round(CAST(divide(number, 2), 'Decimal(10, 4)'))─┐ │ 0.0000 │ 0.0000 │ │ 0.5000 │ 1.0000 │ │ 1.0000 │ 1.0000 │ └────────┴──────────────────────────────────────────────────┘  Examples of rounding Rounding to the nearest number. round(3.2, 0) = 3 round(4.1267, 2) = 4.13 round(22,-1) = 20 round(467,-2) = 500 round(-467,-2) = -500  Banker’s rounding. round(3.5) = 4 round(4.5) = 4 round(3.55, 1) = 3.6 round(3.65, 1) = 3.6  See Also roundBankers "},{"title":"roundBankers​","type":1,"pageTitle":"Rounding Functions","url":"en/sql-reference/functions/rounding-functions#roundbankers","content":"Rounds a number to a specified decimal position. If the rounding number is halfway between two numbers, the function uses banker’s rounding. Banker's rounding is a method of rounding fractional numbers. When the rounding number is halfway between two numbers, it's rounded to the nearest even digit at the specified decimal position. For example: 3.5 rounds up to 4, 2.5 rounds down to 2. It's the default rounding method for floating point numbers defined in [IEEE 754](https://en.wikipedia.org/wiki/IEEE_754#Roundings_to_nearest). The [round](#rounding_functions-round) function performs the same rounding for floating point numbers. The `roundBankers` function also rounds integers the same way, for example, `roundBankers(45, -1) = 40`. In other cases, the function rounds numbers to the nearest integer. Using banker’s rounding, you can reduce the effect that rounding numbers has on the results of summing or subtracting these numbers. For example, sum numbers 1.5, 2.5, 3.5, 4.5 with different rounding: No rounding: 1.5 + 2.5 + 3.5 + 4.5 = 12.Banker’s rounding: 2 + 2 + 4 + 4 = 12.Rounding to the nearest integer: 2 + 3 + 4 + 5 = 14. Syntax roundBankers(expression [, decimal_places])  Arguments expression — A number to be rounded. Can be any expression returning the numeric data type.decimal-places — Decimal places. An integer number. decimal-places &gt; 0 — The function rounds the number to the given position right of the decimal point. Example: roundBankers(3.55, 1) = 3.6.decimal-places &lt; 0 — The function rounds the number to the given position left of the decimal point. Example: roundBankers(24.55, -1) = 20.decimal-places = 0 — The function rounds the number to an integer. In this case the argument can be omitted. Example: roundBankers(2.5) = 2. Returned value A value rounded by the banker’s rounding method. "},{"title":"Examples​","type":1,"pageTitle":"Rounding Functions","url":"en/sql-reference/functions/rounding-functions#examples-1","content":"Example of use Query:  SELECT number / 2 AS x, roundBankers(x, 0) AS b fROM system.numbers limit 10  Result: ┌───x─┬─b─┐ │ 0 │ 0 │ │ 0.5 │ 0 │ │ 1 │ 1 │ │ 1.5 │ 2 │ │ 2 │ 2 │ │ 2.5 │ 2 │ │ 3 │ 3 │ │ 3.5 │ 4 │ │ 4 │ 4 │ │ 4.5 │ 4 │ └─────┴───┘  Examples of Banker’s rounding roundBankers(0.4) = 0 roundBankers(-3.5) = -4 roundBankers(4.5) = 4 roundBankers(3.55, 1) = 3.6 roundBankers(3.65, 1) = 3.6 roundBankers(10.35, 1) = 10.4 roundBankers(10.755, 2) = 10.76  See Also round "},{"title":"roundToExp2(num)​","type":1,"pageTitle":"Rounding Functions","url":"en/sql-reference/functions/rounding-functions#roundtoexp2num","content":"Accepts a number. If the number is less than one, it returns 0. Otherwise, it rounds the number down to the nearest (whole non-negative) degree of two. "},{"title":"roundDuration(num)​","type":1,"pageTitle":"Rounding Functions","url":"en/sql-reference/functions/rounding-functions#rounddurationnum","content":"Accepts a number. If the number is less than one, it returns 0. Otherwise, it rounds the number down to numbers from the set: 1, 10, 30, 60, 120, 180, 240, 300, 600, 1200, 1800, 3600, 7200, 18000, 36000. "},{"title":"roundAge(num)​","type":1,"pageTitle":"Rounding Functions","url":"en/sql-reference/functions/rounding-functions#roundagenum","content":"Accepts a number. If the number is less than 18, it returns 0. Otherwise, it rounds the number down to a number from the set: 18, 25, 35, 45, 55. "},{"title":"roundDown(num, arr)​","type":1,"pageTitle":"Rounding Functions","url":"en/sql-reference/functions/rounding-functions#rounddownnum-arr","content":"Accepts a number and rounds it down to an element in the specified array. If the value is less than the lowest bound, the lowest bound is returned. "},{"title":"Functions for Splitting and Merging Strings and Arrays","type":0,"sectionRef":"#","url":"en/sql-reference/functions/splitting-merging-functions","content":"","keywords":""},{"title":"splitByChar(separator, s)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"en/sql-reference/functions/splitting-merging-functions#splitbycharseparator-s","content":"Splits a string into substrings separated by a specified character. It uses a constant string separator which consisting of exactly one character. Returns an array of selected substrings. Empty substrings may be selected if the separator occurs at the beginning or end of the string, or if there are multiple consecutive separators. Syntax splitByChar(separator, s)  Arguments separator — The separator which should contain exactly one character. String.s — The string to split. String. Returned value(s) Returns an array of selected substrings. Empty substrings may be selected when: A separator occurs at the beginning or end of the string;There are multiple consecutive separators;The original string s is empty. Type: Array(String). Example SELECT splitByChar(',', '1,2,3,abcde');  ┌─splitByChar(',', '1,2,3,abcde')─┐ │ ['1','2','3','abcde'] │ └─────────────────────────────────┘  "},{"title":"splitByString(separator, s)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"en/sql-reference/functions/splitting-merging-functions#splitbystringseparator-s","content":"Splits a string into substrings separated by a string. It uses a constant string separator of multiple characters as the separator. If the string separator is empty, it will split the string s into an array of single characters. Syntax splitByString(separator, s)  Arguments separator — The separator. String.s — The string to split. String. Returned value(s) Returns an array of selected substrings. Empty substrings may be selected when: Type: Array(String). A non-empty separator occurs at the beginning or end of the string;There are multiple consecutive non-empty separators;The original string s is empty while the separator is not empty. Example SELECT splitByString(', ', '1, 2 3, 4,5, abcde');  ┌─splitByString(', ', '1, 2 3, 4,5, abcde')─┐ │ ['1','2 3','4,5','abcde'] │ └───────────────────────────────────────────┘  SELECT splitByString('', 'abcde');  ┌─splitByString('', 'abcde')─┐ │ ['a','b','c','d','e'] │ └────────────────────────────┘  "},{"title":"splitByRegexp(regexp, s)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"en/sql-reference/functions/splitting-merging-functions#splitbyregexpseparator-s","content":"Splits a string into substrings separated by a regular expression. It uses a regular expression string regexp as the separator. If the regexp is empty, it will split the string s into an array of single characters. If no match is found for this regular expression, the string s won't be split. Syntax splitByRegexp(regexp, s)  Arguments regexp — Regular expression. Constant. String or FixedString.s — The string to split. String. Returned value(s) Returns an array of selected substrings. Empty substrings may be selected when: A non-empty regular expression match occurs at the beginning or end of the string;There are multiple consecutive non-empty regular expression matches;The original string s is empty while the regular expression is not empty. Type: Array(String). Example Query: SELECT splitByRegexp('\\\\d+', 'a12bc23de345f');  Result: ┌─splitByRegexp('\\\\d+', 'a12bc23de345f')─┐ │ ['a','bc','de','f'] │ └────────────────────────────────────────┘  Query: SELECT splitByRegexp('', 'abcde');  Result: ┌─splitByRegexp('', 'abcde')─┐ │ ['a','b','c','d','e'] │ └────────────────────────────┘  "},{"title":"splitByWhitespace(s)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"en/sql-reference/functions/splitting-merging-functions#splitbywhitespaceseparator-s","content":"Splits a string into substrings separated by whitespace characters. Returns an array of selected substrings. Syntax splitByWhitespace(s)  Arguments s — The string to split. String. Returned value(s) Returns an array of selected substrings. Type: Array(String). Example SELECT splitByWhitespace(' 1! a, b. ');  ┌─splitByWhitespace(' 1! a, b. ')─┐ │ ['1!','a,','b.'] │ └─────────────────────────────────────┘  "},{"title":"splitByNonAlpha(s)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"en/sql-reference/functions/splitting-merging-functions#splitbynonalphaseparator-s","content":"Splits a string into substrings separated by whitespace and punctuation characters. Returns an array of selected substrings. Syntax splitByNonAlpha(s)  Arguments s — The string to split. String. Returned value(s) Returns an array of selected substrings. Type: Array(String). Example SELECT splitByNonAlpha(' 1! a, b. ');  ┌─splitByNonAlpha(' 1! a, b. ')─┐ │ ['1','a','b'] │ └───────────────────────────────────┘  "},{"title":"arrayStringConcat(arr[, separator])​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"en/sql-reference/functions/splitting-merging-functions#arraystringconcatarr-separator","content":"Concatenates string representations of values listed in the array with the separator. separator is an optional parameter: a constant string, set to an empty string by default. Returns the string. "},{"title":"alphaTokens(s)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"en/sql-reference/functions/splitting-merging-functions#alphatokenss","content":"Selects substrings of consecutive bytes from the ranges a-z and A-Z.Returns an array of substrings. Example SELECT alphaTokens('abca1abc');  ┌─alphaTokens('abca1abc')─┐ │ ['abca','abc'] │ └─────────────────────────┘  "},{"title":"extractAllGroups(text, regexp)​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"en/sql-reference/functions/splitting-merging-functions#extractallgroups","content":"Extracts all groups from non-overlapping substrings matched by a regular expression. Syntax extractAllGroups(text, regexp)  Arguments text — String or FixedString.regexp — Regular expression. Constant. String or FixedString. Returned values If the function finds at least one matching group, it returns Array(Array(String)) column, clustered by group_id (1 to N, where N is number of capturing groups in regexp). If there is no matching group, returns an empty array. Type: Array. Example Query: SELECT extractAllGroups('abc=123, 8=&quot;hkl&quot;', '(&quot;[^&quot;]+&quot;|\\\\w+)=(&quot;[^&quot;]+&quot;|\\\\w+)');  Result: ┌─extractAllGroups('abc=123, 8=&quot;hkl&quot;', '(&quot;[^&quot;]+&quot;|\\\\w+)=(&quot;[^&quot;]+&quot;|\\\\w+)')─┐ │ [['abc','123'],['8','&quot;hkl&quot;']] │ └───────────────────────────────────────────────────────────────────────┘  "},{"title":"ngrams​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"en/sql-reference/functions/splitting-merging-functions#ngrams","content":"Splits the UTF-8 string into n-grams of ngramsize symbols. Syntax ngrams(string, ngramsize)  Arguments string — String. String or FixedString.ngramsize — The size of an n-gram. UInt. Returned values Array with n-grams. Type: Array(String). Example Query: SELECT ngrams('ClickHouse', 3);  Result: ┌─ngrams('ClickHouse', 3)───────────────────────────┐ │ ['Cli','lic','ick','ckH','kHo','Hou','ous','use'] │ └───────────────────────────────────────────────────┘  "},{"title":"tokens​","type":1,"pageTitle":"Functions for Splitting and Merging Strings and Arrays","url":"en/sql-reference/functions/splitting-merging-functions#tokens","content":"Splits a string into tokens using non-alphanumeric ASCII characters as separators. Arguments input_string — Any set of bytes represented as the String data type object. Returned value The resulting array of tokens from input string. Type: Array. Example Query: SELECT tokens('test1,;\\\\ test2,;\\\\ test3,;\\\\ test4') AS tokens;  Result: ┌─tokens────────────────────────────┐ │ ['test1','test2','test3','test4'] │ └───────────────────────────────────┘  "},{"title":"Functions for Working with Strings","type":0,"sectionRef":"#","url":"en/sql-reference/functions/string-functions","content":"","keywords":""},{"title":"empty​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#empty","content":"Checks whether the input string is empty. Syntax empty(x)  A string is considered non-empty if it contains at least one byte, even if this is a space or a null byte. The function also works for arrays or UUID. Arguments x — Input value. String. Returned value Returns 1 for an empty string or 0 for a non-empty string. Type: UInt8. Example Query: SELECT empty('');  Result: ┌─empty('')─┐ │ 1 │ └───────────┘  "},{"title":"notEmpty​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#notempty","content":"Checks whether the input string is non-empty. Syntax notEmpty(x)  A string is considered non-empty if it contains at least one byte, even if this is a space or a null byte. The function also works for arrays or UUID. Arguments x — Input value. String. Returned value Returns 1 for a non-empty string or 0 for an empty string string. Type: UInt8. Example Query: SELECT notEmpty('text');  Result: ┌─notEmpty('text')─┐ │ 1 │ └──────────────────┘  "},{"title":"length​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#length","content":"Returns the length of a string in bytes (not in characters, and not in code points). The result type is UInt64. The function also works for arrays. "},{"title":"lengthUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#lengthutf8","content":"Returns the length of a string in Unicode code points (not in characters), assuming that the string contains a set of bytes that make up UTF-8 encoded text. If this assumption is not met, it returns some result (it does not throw an exception). The result type is UInt64. "},{"title":"char_length, CHAR_LENGTH​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#char-length","content":"Returns the length of a string in Unicode code points (not in characters), assuming that the string contains a set of bytes that make up UTF-8 encoded text. If this assumption is not met, it returns some result (it does not throw an exception). The result type is UInt64. "},{"title":"character_length, CHARACTER_LENGTH​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#character-length","content":"Returns the length of a string in Unicode code points (not in characters), assuming that the string contains a set of bytes that make up UTF-8 encoded text. If this assumption is not met, it returns some result (it does not throw an exception). The result type is UInt64. "},{"title":"leftPad​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#leftpad","content":"Pads the current string from the left with spaces or a specified string (multiple times, if needed) until the resulting string reaches the given length. Similarly to the MySQL LPAD function. Syntax leftPad('string', 'length'[, 'pad_string'])  Arguments string — Input string that needs to be padded. String.length — The length of the resulting string. UInt. If the value is less than the input string length, then the input string is returned as-is.pad_string — The string to pad the input string with. String. Optional. If not specified, then the input string is padded with spaces. Returned value The resulting string of the given length. Type: String. Example Query: SELECT leftPad('abc', 7, '*'), leftPad('def', 7);  Result: ┌─leftPad('abc', 7, '*')─┬─leftPad('def', 7)─┐ │ ****abc │ def │ └────────────────────────┴───────────────────┘  "},{"title":"leftPadUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#leftpadutf8","content":"Pads the current string from the left with spaces or a specified string (multiple times, if needed) until the resulting string reaches the given length. Similarly to the MySQL LPAD function. While in the leftPad function the length is measured in bytes, here in the leftPadUTF8 function it is measured in code points. Syntax leftPadUTF8('string','length'[, 'pad_string'])  Arguments string — Input string that needs to be padded. String.length — The length of the resulting string. UInt. If the value is less than the input string length, then the input string is returned as-is.pad_string — The string to pad the input string with. String. Optional. If not specified, then the input string is padded with spaces. Returned value The resulting string of the given length. Type: String. Example Query: SELECT leftPadUTF8('абвг', 7, '*'), leftPadUTF8('дежз', 7);  Result: ┌─leftPadUTF8('абвг', 7, '*')─┬─leftPadUTF8('дежз', 7)─┐ │ ***абвг │ дежз │ └─────────────────────────────┴────────────────────────┘  "},{"title":"rightPad​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#rightpad","content":"Pads the current string from the right with spaces or a specified string (multiple times, if needed) until the resulting string reaches the given length. Similarly to the MySQL RPAD function. Syntax rightPad('string', 'length'[, 'pad_string'])  Arguments string — Input string that needs to be padded. String.length — The length of the resulting string. UInt. If the value is less than the input string length, then the input string is returned as-is.pad_string — The string to pad the input string with. String. Optional. If not specified, then the input string is padded with spaces. Returned value The resulting string of the given length. Type: String. Example Query: SELECT rightPad('abc', 7, '*'), rightPad('abc', 7);  Result: ┌─rightPad('abc', 7, '*')─┬─rightPad('abc', 7)─┐ │ abc**** │ abc │ └─────────────────────────┴────────────────────┘  "},{"title":"rightPadUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#rightpadutf8","content":"Pads the current string from the right with spaces or a specified string (multiple times, if needed) until the resulting string reaches the given length. Similarly to the MySQL RPAD function. While in the rightPad function the length is measured in bytes, here in the rightPadUTF8 function it is measured in code points. Syntax rightPadUTF8('string','length'[, 'pad_string'])  Arguments string — Input string that needs to be padded. String.length — The length of the resulting string. UInt. If the value is less than the input string length, then the input string is returned as-is.pad_string — The string to pad the input string with. String. Optional. If not specified, then the input string is padded with spaces. Returned value The resulting string of the given length. Type: String. Example Query: SELECT rightPadUTF8('абвг', 7, '*'), rightPadUTF8('абвг', 7);  Result: ┌─rightPadUTF8('абвг', 7, '*')─┬─rightPadUTF8('абвг', 7)─┐ │ абвг*** │ абвг │ └──────────────────────────────┴─────────────────────────┘  "},{"title":"lower, lcase​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#lower","content":"Converts ASCII Latin symbols in a string to lowercase. "},{"title":"upper, ucase​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#upper","content":"Converts ASCII Latin symbols in a string to uppercase. "},{"title":"lowerUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#lowerutf8","content":"Converts a string to lowercase, assuming the string contains a set of bytes that make up a UTF-8 encoded text. It does not detect the language. So for Turkish the result might not be exactly correct. If the length of the UTF-8 byte sequence is different for upper and lower case of a code point, the result may be incorrect for this code point. If the string contains a set of bytes that is not UTF-8, then the behavior is undefined. "},{"title":"upperUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#upperutf8","content":"Converts a string to uppercase, assuming the string contains a set of bytes that make up a UTF-8 encoded text. It does not detect the language. So for Turkish the result might not be exactly correct. If the length of the UTF-8 byte sequence is different for upper and lower case of a code point, the result may be incorrect for this code point. If the string contains a set of bytes that is not UTF-8, then the behavior is undefined. "},{"title":"isValidUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#isvalidutf8","content":"Returns 1, if the set of bytes is valid UTF-8 encoded, otherwise 0. "},{"title":"toValidUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#tovalidutf8","content":"Replaces invalid UTF-8 characters by the � (U+FFFD) character. All running in a row invalid characters are collapsed into the one replacement character. toValidUTF8(input_string)  Arguments input_string — Any set of bytes represented as the String data type object. Returned value: Valid UTF-8 string. Example SELECT toValidUTF8('\\x61\\xF0\\x80\\x80\\x80b');  ┌─toValidUTF8('a����b')─┐ │ a�b │ └───────────────────────┘  "},{"title":"repeat​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#repeat","content":"Repeats a string as many times as specified and concatenates the replicated values as a single string. Alias: REPEAT. Syntax repeat(s, n)  Arguments s — The string to repeat. String.n — The number of times to repeat the string. UInt. Returned value The single string, which contains the string s repeated n times. If n \\&lt; 1, the function returns empty string. Type: String. Example Query: SELECT repeat('abc', 10);  Result: ┌─repeat('abc', 10)──────────────┐ │ abcabcabcabcabcabcabcabcabcabc │ └────────────────────────────────┘  "},{"title":"reverse​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#reverse","content":"Reverses the string (as a sequence of bytes). "},{"title":"reverseUTF8​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#reverseutf8","content":"Reverses a sequence of Unicode code points, assuming that the string contains a set of bytes representing a UTF-8 text. Otherwise, it does something else (it does not throw an exception). "},{"title":"format(pattern, s0, s1, …)​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#format","content":"Formatting constant pattern with the string listed in the arguments. pattern is a simplified Python format pattern. Format string contains “replacement fields” surrounded by curly braces {}. Anything that is not contained in braces is considered literal text, which is copied unchanged to the output. If you need to include a brace character in the literal text, it can be escaped by doubling: {{ '{{' }} and {{ '}}' }}. Field names can be numbers (starting from zero) or empty (then they are treated as consequence numbers). SELECT format('{1} {0} {1}', 'World', 'Hello')  ┌─format('{1} {0} {1}', 'World', 'Hello')─┐ │ Hello World Hello │ └─────────────────────────────────────────┘  SELECT format('{} {}', 'Hello', 'World')  ┌─format('{} {}', 'Hello', 'World')─┐ │ Hello World │ └───────────────────────────────────┘  "},{"title":"concat​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#concat","content":"Concatenates the strings listed in the arguments, without a separator. Syntax concat(s1, s2, ...)  Arguments Values of type String or FixedString. Returned values Returns the String that results from concatenating the arguments. If any of argument values is NULL, concat returns NULL. Example Query: SELECT concat('Hello, ', 'World!');  Result: ┌─concat('Hello, ', 'World!')─┐ │ Hello, World! │ └─────────────────────────────┘  "},{"title":"concatAssumeInjective​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#concatassumeinjective","content":"Same as concat, the difference is that you need to ensure that concat(s1, s2, ...) → sn is injective, it will be used for optimization of GROUP BY. The function is named “injective” if it always returns different result for different values of arguments. In other words: different arguments never yield identical result. Syntax concatAssumeInjective(s1, s2, ...)  Arguments Values of type String or FixedString. Returned values Returns the String that results from concatenating the arguments. If any of argument values is NULL, concatAssumeInjective returns NULL. Example Input table: CREATE TABLE key_val(`key1` String, `key2` String, `value` UInt32) ENGINE = TinyLog; INSERT INTO key_val VALUES ('Hello, ','World',1), ('Hello, ','World',2), ('Hello, ','World!',3), ('Hello',', World!',2); SELECT * from key_val;  ┌─key1────┬─key2─────┬─value─┐ │ Hello, │ World │ 1 │ │ Hello, │ World │ 2 │ │ Hello, │ World! │ 3 │ │ Hello │ , World! │ 2 │ └─────────┴──────────┴───────┘  Query: SELECT concat(key1, key2), sum(value) FROM key_val GROUP BY concatAssumeInjective(key1, key2);  Result: ┌─concat(key1, key2)─┬─sum(value)─┐ │ Hello, World! │ 3 │ │ Hello, World! │ 2 │ │ Hello, World │ 3 │ └────────────────────┴────────────┘  "},{"title":"substring(s, offset, length), mid(s, offset, length), substr(s, offset, length)​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#substring","content":"Returns a substring starting with the byte from the ‘offset’ index that is ‘length’ bytes long. Character indexing starts from one (as in standard SQL). "},{"title":"substringUTF8(s, offset, length)​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#substringutf8","content":"The same as ‘substring’, but for Unicode code points. Works under the assumption that the string contains a set of bytes representing a UTF-8 encoded text. If this assumption is not met, it returns some result (it does not throw an exception). "},{"title":"appendTrailingCharIfAbsent(s, c)​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#appendtrailingcharifabsent","content":"If the ‘s’ string is non-empty and does not contain the ‘c’ character at the end, it appends the ‘c’ character to the end. "},{"title":"convertCharset(s, from, to)​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#convertcharset","content":"Returns the string ‘s’ that was converted from the encoding in ‘from’ to the encoding in ‘to’. "},{"title":"base64Encode(s)​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#base64encode","content":"Encodes ‘s’ string into base64 Alias: TO_BASE64. "},{"title":"base64Decode(s)​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#base64decode","content":"Decode base64-encoded string ‘s’ into original string. In case of failure raises an exception. Alias: FROM_BASE64. "},{"title":"tryBase64Decode(s)​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#trybase64decode","content":"Similar to base64Decode, but in case of error an empty string would be returned. "},{"title":"endsWith(s, suffix)​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#endswith","content":"Returns whether to end with the specified suffix. Returns 1 if the string ends with the specified suffix, otherwise it returns 0. "},{"title":"startsWith(str, prefix)​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#startswith","content":"Returns 1 whether string starts with the specified prefix, otherwise it returns 0. SELECT startsWith('Spider-Man', 'Spi');  Returned values 1, if the string starts with the specified prefix.0, if the string does not start with the specified prefix. Example Query: SELECT startsWith('Hello, world!', 'He');  Result: ┌─startsWith('Hello, world!', 'He')─┐ │ 1 │ └───────────────────────────────────┘  "},{"title":"trim​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#trim","content":"Removes all specified characters from the start or end of a string. By default removes all consecutive occurrences of common whitespace (ASCII character 32) from both ends of a string. Syntax trim([[LEADING|TRAILING|BOTH] trim_character FROM] input_string)  Arguments trim_character — Specified characters for trim. String.input_string — String for trim. String. Returned value A string without leading and (or) trailing specified characters. Type: String. Example Query: SELECT trim(BOTH ' ()' FROM '( Hello, world! )');  Result: ┌─trim(BOTH ' ()' FROM '( Hello, world! )')─┐ │ Hello, world! │ └───────────────────────────────────────────────┘  "},{"title":"trimLeft​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#trimleft","content":"Removes all consecutive occurrences of common whitespace (ASCII character 32) from the beginning of a string. It does not remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax trimLeft(input_string)  Alias: ltrim(input_string). Arguments input_string — string to trim. String. Returned value A string without leading common whitespaces. Type: String. Example Query: SELECT trimLeft(' Hello, world! ');  Result: ┌─trimLeft(' Hello, world! ')─┐ │ Hello, world! │ └─────────────────────────────────────┘  "},{"title":"trimRight​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#trimright","content":"Removes all consecutive occurrences of common whitespace (ASCII character 32) from the end of a string. It does not remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax trimRight(input_string)  Alias: rtrim(input_string). Arguments input_string — string to trim. String. Returned value A string without trailing common whitespaces. Type: String. Example Query: SELECT trimRight(' Hello, world! ');  Result: ┌─trimRight(' Hello, world! ')─┐ │ Hello, world! │ └──────────────────────────────────────┘  "},{"title":"trimBoth​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#trimboth","content":"Removes all consecutive occurrences of common whitespace (ASCII character 32) from both ends of a string. It does not remove other kinds of whitespace characters (tab, no-break space, etc.). Syntax trimBoth(input_string)  Alias: trim(input_string). Arguments input_string — string to trim. String. Returned value A string without leading and trailing common whitespaces. Type: String. Example Query: SELECT trimBoth(' Hello, world! ');  Result: ┌─trimBoth(' Hello, world! ')─┐ │ Hello, world! │ └─────────────────────────────────────┘  "},{"title":"CRC32(s)​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#crc32","content":"Returns the CRC32 checksum of a string, using CRC-32-IEEE 802.3 polynomial and initial value 0xffffffff (zlib implementation). The result type is UInt32. "},{"title":"CRC32IEEE(s)​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#crc32ieee","content":"Returns the CRC32 checksum of a string, using CRC-32-IEEE 802.3 polynomial. The result type is UInt32. "},{"title":"CRC64(s)​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#crc64","content":"Returns the CRC64 checksum of a string, using CRC-64-ECMA polynomial. The result type is UInt64. "},{"title":"normalizeQuery​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#normalized-query","content":"Replaces literals, sequences of literals and complex aliases with placeholders. Syntax normalizeQuery(x)  Arguments x — Sequence of characters. String. Returned value Sequence of characters with placeholders. Type: String. Example Query: SELECT normalizeQuery('[1, 2, 3, x]') AS query;  Result: ┌─query────┐ │ [?.., x] │ └──────────┘  "},{"title":"normalizedQueryHash​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#normalized-query-hash","content":"Returns identical 64bit hash values without the values of literals for similar queries. It helps to analyze query log. Syntax normalizedQueryHash(x)  Arguments x — Sequence of characters. String. Returned value Hash value. Type: UInt64. Example Query: SELECT normalizedQueryHash('SELECT 1 AS `xyz`') != normalizedQueryHash('SELECT 1 AS `abc`') AS res;  Result: ┌─res─┐ │ 1 │ └─────┘  "},{"title":"normalizeUTF8NFC​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#normalizeutf8nfc","content":"Converts a string to NFC normalized form, assuming the string contains a set of bytes that make up a UTF-8 encoded text. Syntax normalizeUTF8NFC(words)  Arguments words — Input string that contains UTF-8 encoded text. String. Returned value String transformed to NFC normalization form. Type: String. Example Query: SELECT length('â'), normalizeUTF8NFC('â') AS nfc, length(nfc) AS nfc_len;  Result: ┌─length('â')─┬─nfc─┬─nfc_len─┐ │ 2 │ â │ 2 │ └─────────────┴─────┴─────────┘  "},{"title":"normalizeUTF8NFD​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#normalizeutf8nfd","content":"Converts a string to NFD normalized form, assuming the string contains a set of bytes that make up a UTF-8 encoded text. Syntax normalizeUTF8NFD(words)  Arguments words — Input string that contains UTF-8 encoded text. String. Returned value String transformed to NFD normalization form. Type: String. Example Query: SELECT length('â'), normalizeUTF8NFD('â') AS nfd, length(nfd) AS nfd_len;  Result: ┌─length('â')─┬─nfd─┬─nfd_len─┐ │ 2 │ â │ 3 │ └─────────────┴─────┴─────────┘  "},{"title":"normalizeUTF8NFKC​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#normalizeutf8nfkc","content":"Converts a string to NFKC normalized form, assuming the string contains a set of bytes that make up a UTF-8 encoded text. Syntax normalizeUTF8NFKC(words)  Arguments words — Input string that contains UTF-8 encoded text. String. Returned value String transformed to NFKC normalization form. Type: String. Example Query: SELECT length('â'), normalizeUTF8NFKC('â') AS nfkc, length(nfkc) AS nfkc_len;  Result: ┌─length('â')─┬─nfkc─┬─nfkc_len─┐ │ 2 │ â │ 2 │ └─────────────┴──────┴──────────┘  "},{"title":"normalizeUTF8NFKD​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#normalizeutf8nfkd","content":"Converts a string to NFKD normalized form, assuming the string contains a set of bytes that make up a UTF-8 encoded text. Syntax normalizeUTF8NFKD(words)  Arguments words — Input string that contains UTF-8 encoded text. String. Returned value String transformed to NFKD normalization form. Type: String. Example Query: SELECT length('â'), normalizeUTF8NFKD('â') AS nfkd, length(nfkd) AS nfkd_len;  Result: ┌─length('â')─┬─nfkd─┬─nfkd_len─┐ │ 2 │ â │ 3 │ └─────────────┴──────┴──────────┘  "},{"title":"encodeXMLComponent​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#encode-xml-component","content":"Escapes characters to place string into XML text node or attribute. The following five XML predefined entities will be replaced: &lt;, &amp;, &gt;, &quot;, '. Syntax encodeXMLComponent(x)  Arguments x — The sequence of characters. String. Returned value The sequence of characters with escape characters. Type: String. Example Query: SELECT encodeXMLComponent('Hello, &quot;world&quot;!'); SELECT encodeXMLComponent('&lt;123&gt;'); SELECT encodeXMLComponent('&amp;clickhouse'); SELECT encodeXMLComponent('\\'foo\\'');  Result: Hello, &amp;quot;world&amp;quot;! &amp;lt;123&amp;gt; &amp;amp;clickhouse &amp;apos;foo&amp;apos;  "},{"title":"decodeXMLComponent​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#decode-xml-component","content":"Replaces XML predefined entities with characters. Predefined entities are &amp;quot; &amp;amp; &amp;apos; &amp;gt; &amp;lt;This function also replaces numeric character references with Unicode characters. Both decimal (like &amp;#10003;) and hexadecimal (&amp;#x2713;) forms are supported. Syntax decodeXMLComponent(x)  Arguments x — A sequence of characters. String. Returned value The sequence of characters after replacement. Type: String. Example Query: SELECT decodeXMLComponent('&amp;apos;foo&amp;apos;'); SELECT decodeXMLComponent('&amp;lt; &amp;#x3A3; &amp;gt;');  Result: 'foo' &lt; Σ &gt;  See Also List of XML and HTML character entity references "},{"title":"extractTextFromHTML​","type":1,"pageTitle":"Functions for Working with Strings","url":"en/sql-reference/functions/string-functions#extracttextfromhtml","content":"A function to extract text from HTML or XHTML. It does not necessarily 100% conform to any of the HTML, XML or XHTML standards, but the implementation is reasonably accurate and it is fast. The rules are the following: Comments are skipped. Example: &lt;!-- test --&gt;. Comment must end with --&gt;. Nested comments are not possible. Note: constructions like &lt;!--&gt; and &lt;!---&gt; are not valid comments in HTML but they are skipped by other rules.CDATA is pasted verbatim. Note: CDATA is XML/XHTML specific. But it is processed for &quot;best-effort&quot; approach.script and style elements are removed with all their content. Note: it is assumed that closing tag cannot appear inside content. For example, in JS string literal has to be escaped like &quot;&lt;\\/script&gt;&quot;. Note: comments and CDATA are possible inside script or style - then closing tags are not searched inside CDATA. Example: &lt;script&gt;&lt;![CDATA[&lt;/script&gt;]]&gt;&lt;/script&gt;. But they are still searched inside comments. Sometimes it becomes complicated: &lt;script&gt;var x = &quot;&lt;!--&quot;; &lt;/script&gt; var y = &quot;--&gt;&quot;; alert(x + y);&lt;/script&gt;Note: script and style can be the names of XML namespaces - then they are not treated like usual script or style elements. Example: &lt;script:a&gt;Hello&lt;/script:a&gt;. Note: whitespaces are possible after closing tag name: &lt;/script &gt; but not before: &lt; / script&gt;.Other tags or tag-like elements are skipped without inner content. Example: &lt;a&gt;.&lt;/a&gt;Note: it is expected that this HTML is illegal: &lt;a test=&quot;&gt;&quot;&gt;&lt;/a&gt;Note: it also skips something like tags: &lt;&gt;, &lt;!&gt;, etc. Note: tag without end is skipped to the end of input: &lt;hello HTML and XML entities are not decoded. They must be processed by separate function.Whitespaces in the text are collapsed or inserted by specific rules. Whitespaces at the beginning and at the end are removed.Consecutive whitespaces are collapsed.But if the text is separated by other elements and there is no whitespace, it is inserted.It may cause unnatural examples: Hello&lt;b&gt;world&lt;/b&gt;, Hello&lt;!-- --&gt;world - there is no whitespace in HTML, but the function inserts it. Also consider: Hello&lt;p&gt;world&lt;/p&gt;, Hello&lt;br&gt;world. This behavior is reasonable for data analysis, e.g. to convert HTML to a bag of words. Also note that correct handling of whitespaces requires the support of &lt;pre&gt;&lt;/pre&gt; and CSS display and white-space properties. Syntax extractTextFromHTML(x)  Arguments x — input text. String. Returned value Extracted text. Type: String. Example The first example contains several tags and a comment and also shows whitespace processing. The second example shows CDATA and script tag processing. In the third example text is extracted from the full HTML response received by the url function. Query: SELECT extractTextFromHTML(' &lt;p&gt; A text &lt;i&gt;with&lt;/i&gt;&lt;b&gt;tags&lt;/b&gt;. &lt;!-- comments --&gt; &lt;/p&gt; '); SELECT extractTextFromHTML('&lt;![CDATA[The content within &lt;b&gt;CDATA&lt;/b&gt;]]&gt; &lt;script&gt;alert(&quot;Script&quot;);&lt;/script&gt;'); SELECT extractTextFromHTML(html) FROM url('http://www.donothingfor2minutes.com/', RawBLOB, 'html String');  Result: A text with tags . The content within &lt;b&gt;CDATA&lt;/b&gt; Do Nothing for 2 Minutes 2:00 &amp;nbsp;  "},{"title":"Functions for Searching and Replacing in Strings","type":0,"sectionRef":"#","url":"en/sql-reference/functions/string-replace-functions","content":"","keywords":""},{"title":"replaceOne(haystack, pattern, replacement)​","type":1,"pageTitle":"Functions for Searching and Replacing in Strings","url":"en/sql-reference/functions/string-replace-functions#replaceonehaystack-pattern-replacement","content":"Replaces the first occurrence, if it exists, of the ‘pattern’ substring in ‘haystack’ with the ‘replacement’ substring. Hereafter, ‘pattern’ and ‘replacement’ must be constants. "},{"title":"replaceAll(haystack, pattern, replacement), replace(haystack, pattern, replacement)​","type":1,"pageTitle":"Functions for Searching and Replacing in Strings","url":"en/sql-reference/functions/string-replace-functions#replaceallhaystack-pattern-replacement-replacehaystack-pattern-replacement","content":"Replaces all occurrences of the ‘pattern’ substring in ‘haystack’ with the ‘replacement’ substring. "},{"title":"replaceRegexpOne(haystack, pattern, replacement)​","type":1,"pageTitle":"Functions for Searching and Replacing in Strings","url":"en/sql-reference/functions/string-replace-functions#replaceregexponehaystack-pattern-replacement","content":"Replacement using the ‘pattern’ regular expression. A re2 regular expression. Replaces only the first occurrence, if it exists. A pattern can be specified as ‘replacement’. This pattern can include substitutions \\0-\\9. The substitution \\0 includes the entire regular expression. Substitutions \\1-\\9 correspond to the subpattern numbers.To use the \\ character in a template, escape it using \\. Also keep in mind that a string literal requires an extra escape. Example 1. Converting the date to American format: SELECT DISTINCT EventDate, replaceRegexpOne(toString(EventDate), '(\\\\d{4})-(\\\\d{2})-(\\\\d{2})', '\\\\2/\\\\3/\\\\1') AS res FROM test.hits LIMIT 7 FORMAT TabSeparated  2014-03-17 03/17/2014 2014-03-18 03/18/2014 2014-03-19 03/19/2014 2014-03-20 03/20/2014 2014-03-21 03/21/2014 2014-03-22 03/22/2014 2014-03-23 03/23/2014  Example 2. Copying a string ten times: SELECT replaceRegexpOne('Hello, World!', '.*', '\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0\\\\0') AS res  ┌─res────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World!Hello, World! │ └────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘  "},{"title":"replaceRegexpAll(haystack, pattern, replacement)​","type":1,"pageTitle":"Functions for Searching and Replacing in Strings","url":"en/sql-reference/functions/string-replace-functions#replaceregexpallhaystack-pattern-replacement","content":"This does the same thing, but replaces all the occurrences. Example: SELECT replaceRegexpAll('Hello, World!', '.', '\\\\0\\\\0') AS res  ┌─res────────────────────────┐ │ HHeelllloo,, WWoorrlldd!! │ └────────────────────────────┘  As an exception, if a regular expression worked on an empty substring, the replacement is not made more than once. Example: SELECT replaceRegexpAll('Hello, World!', '^', 'here: ') AS res  ┌─res─────────────────┐ │ here: Hello, World! │ └─────────────────────┘  "},{"title":"regexpQuoteMeta(s)​","type":1,"pageTitle":"Functions for Searching and Replacing in Strings","url":"en/sql-reference/functions/string-replace-functions#regexpquotemetas","content":"The function adds a backslash before some predefined characters in the string. Predefined characters: \\0, \\\\, |, (, ), ^, $, ., [, ], ?, *, +, {, :, -. This implementation slightly differs from re2::RE2::QuoteMeta. It escapes zero byte as \\0 instead of \\x00 and it escapes only required characters. For more information, see the link: RE2 "},{"title":"Functions for Searching in Strings","type":0,"sectionRef":"#","url":"en/sql-reference/functions/string-search-functions","content":"","keywords":""},{"title":"position(haystack, needle), locate(haystack, needle)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#position","content":"Searches for the substring needle in the string haystack. Returns the position (in bytes) of the found substring in the string, starting from 1. For a case-insensitive search, use the function positionCaseInsensitive. Syntax position(haystack, needle[, start_pos])  position(needle IN haystack)  Alias: locate(haystack, needle[, start_pos]). note Syntax of position(needle IN haystack) provides SQL-compatibility, the function works the same way as to position(haystack, needle). Arguments haystack — String, in which substring will to be searched. String.needle — Substring to be searched. String.start_pos – Position of the first character in the string to start search. UInt. Optional. Returned values Starting position in bytes (counting from 1), if substring was found.0, if the substring was not found. Type: Integer. Examples The phrase “Hello, world!” contains a set of bytes representing a single-byte encoded text. The function returns some expected result: Query: SELECT position('Hello, world!', '!');  Result: ┌─position('Hello, world!', '!')─┐ │ 13 │ └────────────────────────────────┘  SELECT position('Hello, world!', 'o', 1), position('Hello, world!', 'o', 7)  ┌─position('Hello, world!', 'o', 1)─┬─position('Hello, world!', 'o', 7)─┐ │ 5 │ 9 │ └───────────────────────────────────┴───────────────────────────────────┘  The same phrase in Russian contains characters which can’t be represented using a single byte. The function returns some unexpected result (use positionUTF8 function for multi-byte encoded text): Query: SELECT position('Привет, мир!', '!');  Result: ┌─position('Привет, мир!', '!')─┐ │ 21 │ └───────────────────────────────┘  Examples for POSITION(needle IN haystack) syntax Query: SELECT 3 = position('c' IN 'abc');  Result: ┌─equals(3, position('abc', 'c'))─┐ │ 1 │ └─────────────────────────────────┘  Query: SELECT 6 = position('/' IN s) FROM (SELECT 'Hello/World' AS s);  Result: ┌─equals(6, position(s, '/'))─┐ │ 1 │ └─────────────────────────────┘  "},{"title":"positionCaseInsensitive​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#positioncaseinsensitive","content":"The same as position returns the position (in bytes) of the found substring in the string, starting from 1. Use the function for a case-insensitive search. Works under the assumption that the string contains a set of bytes representing a single-byte encoded text. If this assumption is not met and a character can’t be represented using a single byte, the function does not throw an exception and returns some unexpected result. If character can be represented using two bytes, it will use two bytes and so on. Syntax positionCaseInsensitive(haystack, needle[, start_pos])  Arguments haystack — String, in which substring will to be searched. String.needle — Substring to be searched. String.start_pos — Optional parameter, position of the first character in the string to start search. UInt. Returned values Starting position in bytes (counting from 1), if substring was found.0, if the substring was not found. Type: Integer. Example Query: SELECT positionCaseInsensitive('Hello, world!', 'hello');  Result: ┌─positionCaseInsensitive('Hello, world!', 'hello')─┐ │ 1 │ └───────────────────────────────────────────────────┘  "},{"title":"positionUTF8​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#positionutf8","content":"Returns the position (in Unicode points) of the found substring in the string, starting from 1. Works under the assumption that the string contains a set of bytes representing a UTF-8 encoded text. If this assumption is not met, the function does not throw an exception and returns some unexpected result. If character can be represented using two Unicode points, it will use two and so on. For a case-insensitive search, use the function positionCaseInsensitiveUTF8. Syntax positionUTF8(haystack, needle[, start_pos])  Arguments haystack — String, in which substring will to be searched. String.needle — Substring to be searched. String.start_pos — Optional parameter, position of the first character in the string to start search. UInt Returned values Starting position in Unicode points (counting from 1), if substring was found.0, if the substring was not found. Type: Integer. Examples The phrase “Hello, world!” in Russian contains a set of Unicode points representing a single-point encoded text. The function returns some expected result: Query: SELECT positionUTF8('Привет, мир!', '!');  Result: ┌─positionUTF8('Привет, мир!', '!')─┐ │ 12 │ └───────────────────────────────────┘  The phrase “Salut, étudiante!”, where character é can be represented using a one point (U+00E9) or two points (U+0065U+0301) the function can be returned some unexpected result: Query for the letter é, which is represented one Unicode point U+00E9: SELECT positionUTF8('Salut, étudiante!', '!');  Result: ┌─positionUTF8('Salut, étudiante!', '!')─┐ │ 17 │ └────────────────────────────────────────┘  Query for the letter é, which is represented two Unicode points U+0065U+0301: SELECT positionUTF8('Salut, étudiante!', '!');  Result: ┌─positionUTF8('Salut, étudiante!', '!')─┐ │ 18 │ └────────────────────────────────────────┘  "},{"title":"positionCaseInsensitiveUTF8​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#positioncaseinsensitiveutf8","content":"The same as positionUTF8, but is case-insensitive. Returns the position (in Unicode points) of the found substring in the string, starting from 1. Works under the assumption that the string contains a set of bytes representing a UTF-8 encoded text. If this assumption is not met, the function does not throw an exception and returns some unexpected result. If character can be represented using two Unicode points, it will use two and so on. Syntax positionCaseInsensitiveUTF8(haystack, needle[, start_pos])  Arguments haystack — String, in which substring will to be searched. String.needle — Substring to be searched. String.start_pos — Optional parameter, position of the first character in the string to start search. UInt Returned value Starting position in Unicode points (counting from 1), if substring was found.0, if the substring was not found. Type: Integer. Example Query: SELECT positionCaseInsensitiveUTF8('Привет, мир!', 'Мир');  Result: ┌─positionCaseInsensitiveUTF8('Привет, мир!', 'Мир')─┐ │ 9 │ └────────────────────────────────────────────────────┘  "},{"title":"multiSearchAllPositions​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#multisearchallpositions","content":"The same as position but returns Array of positions (in bytes) of the found corresponding substrings in the string. Positions are indexed starting from 1. The search is performed on sequences of bytes without respect to string encoding and collation. For case-insensitive ASCII search, use the function multiSearchAllPositionsCaseInsensitive.For search in UTF-8, use the function multiSearchAllPositionsUTF8.For case-insensitive UTF-8 search, use the function multiSearchAllPositionsCaseInsensitiveUTF8. Syntax multiSearchAllPositions(haystack, [needle1, needle2, ..., needlen])  Arguments haystack — String, in which substring will to be searched. String.needle — Substring to be searched. String. Returned values Array of starting positions in bytes (counting from 1), if the corresponding substring was found and 0 if not found. Example Query: SELECT multiSearchAllPositions('Hello, World!', ['hello', '!', 'world']);  Result: ┌─multiSearchAllPositions('Hello, World!', ['hello', '!', 'world'])─┐ │ [0,13,0] │ └───────────────────────────────────────────────────────────────────┘  "},{"title":"multiSearchAllPositionsUTF8​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#multiSearchAllPositionsUTF8","content":"See multiSearchAllPositions. "},{"title":"multiSearchFirstPosition(haystack, [needle1, needle2, …, needlen])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#multisearchfirstposition","content":"The same as position but returns the leftmost offset of the string haystack that is matched to some of the needles. For a case-insensitive search or/and in UTF-8 format use functions multiSearchFirstPositionCaseInsensitive, multiSearchFirstPositionUTF8, multiSearchFirstPositionCaseInsensitiveUTF8. "},{"title":"multiSearchFirstIndex(haystack, [needle1, needle2, …, needlen])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#multisearchfirstindexhaystack-needle1-needle2-needlen","content":"Returns the index i (starting from 1) of the leftmost found needlei in the string haystack and 0 otherwise. For a case-insensitive search or/and in UTF-8 format use functions multiSearchFirstIndexCaseInsensitive, multiSearchFirstIndexUTF8, multiSearchFirstIndexCaseInsensitiveUTF8. "},{"title":"multiSearchAny(haystack, [needle1, needle2, …, needlen])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#function-multisearchany","content":"Returns 1, if at least one string needlei matches the string haystack and 0 otherwise. For a case-insensitive search or/and in UTF-8 format use functions multiSearchAnyCaseInsensitive, multiSearchAnyUTF8, multiSearchAnyCaseInsensitiveUTF8. note In all multiSearch* functions the number of needles should be less than 28 because of implementation specification. "},{"title":"match(haystack, pattern)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#matchhaystack-pattern","content":"Checks whether the string matches the pattern regular expression. A re2 regular expression. The syntax of the re2 regular expressions is more limited than the syntax of the Perl regular expressions. Returns 0 if it does not match, or 1 if it matches. The regular expression works with the string as if it is a set of bytes. The regular expression can’t contain null bytes. For patterns to search for substrings in a string, it is better to use LIKE or ‘position’, since they work much faster. "},{"title":"multiMatchAny(haystack, [pattern1, pattern2, …, patternn])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#multimatchanyhaystack-pattern1-pattern2-patternn","content":"The same as match, but returns 0 if none of the regular expressions are matched and 1 if any of the patterns matches. It uses hyperscan library. For patterns to search substrings in a string, it is better to use multiSearchAny since it works much faster. note The length of any of the haystack string must be less than 232 bytes otherwise the exception is thrown. This restriction takes place because of hyperscan API. "},{"title":"multiMatchAnyIndex(haystack, [pattern1, pattern2, …, patternn])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#multimatchanyindexhaystack-pattern1-pattern2-patternn","content":"The same as multiMatchAny, but returns any index that matches the haystack. "},{"title":"multiMatchAllIndices(haystack, [pattern1, pattern2, …, patternn])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#multimatchallindiceshaystack-pattern1-pattern2-patternn","content":"The same as multiMatchAny, but returns the array of all indicies that match the haystack in any order. "},{"title":"multiFuzzyMatchAny(haystack, distance, [pattern1, pattern2, …, patternn])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#multifuzzymatchanyhaystack-distance-pattern1-pattern2-patternn","content":"The same as multiMatchAny, but returns 1 if any pattern matches the haystack within a constant edit distance. This function relies on the experimental feature of hyperscan library, and can be slow for some corner cases. The performance depends on the edit distance value and patterns used, but it's always more expensive compared to a non-fuzzy variants. "},{"title":"multiFuzzyMatchAnyIndex(haystack, distance, [pattern1, pattern2, …, patternn])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#multifuzzymatchanyindexhaystack-distance-pattern1-pattern2-patternn","content":"The same as multiFuzzyMatchAny, but returns any index that matches the haystack within a constant edit distance. "},{"title":"multiFuzzyMatchAllIndices(haystack, distance, [pattern1, pattern2, …, patternn])​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#multifuzzymatchallindiceshaystack-distance-pattern1-pattern2-patternn","content":"The same as multiFuzzyMatchAny, but returns the array of all indices in any order that match the haystack within a constant edit distance. note multiFuzzyMatch* functions do not support UTF-8 regular expressions, and such expressions are treated as bytes because of hyperscan restriction. note To turn off all functions that use hyperscan, use setting SET allow_hyperscan = 0;. "},{"title":"extract(haystack, pattern)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#extracthaystack-pattern","content":"Extracts a fragment of a string using a regular expression. If ‘haystack’ does not match the ‘pattern’ regex, an empty string is returned. If the regex does not contain subpatterns, it takes the fragment that matches the entire regex. Otherwise, it takes the fragment that matches the first subpattern. "},{"title":"extractAll(haystack, pattern)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#extractallhaystack-pattern","content":"Extracts all the fragments of a string using a regular expression. If ‘haystack’ does not match the ‘pattern’ regex, an empty string is returned. Returns an array of strings consisting of all matches to the regex. In general, the behavior is the same as the ‘extract’ function (it takes the first subpattern, or the entire expression if there isn’t a subpattern). "},{"title":"extractAllGroupsHorizontal​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#extractallgroups-horizontal","content":"Matches all groups of the haystack string using the pattern regular expression. Returns an array of arrays, where the first array includes all fragments matching the first group, the second array - matching the second group, etc. note extractAllGroupsHorizontal function is slower than extractAllGroupsVertical. Syntax extractAllGroupsHorizontal(haystack, pattern)  Arguments haystack — Input string. Type: String.pattern — Regular expression with re2 syntax. Must contain groups, each group enclosed in parentheses. If pattern contains no groups, an exception is thrown. Type: String. Returned value Type: Array. If haystack does not match the pattern regex, an array of empty arrays is returned. Example Query: SELECT extractAllGroupsHorizontal('abc=111, def=222, ghi=333', '(&quot;[^&quot;]+&quot;|\\\\w+)=(&quot;[^&quot;]+&quot;|\\\\w+)');  Result: ┌─extractAllGroupsHorizontal('abc=111, def=222, ghi=333', '(&quot;[^&quot;]+&quot;|\\\\w+)=(&quot;[^&quot;]+&quot;|\\\\w+)')─┐ │ [['abc','def','ghi'],['111','222','333']] │ └──────────────────────────────────────────────────────────────────────────────────────────┘  See Also extractAllGroupsVertical "},{"title":"extractAllGroupsVertical​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#extractallgroups-vertical","content":"Matches all groups of the haystack string using the pattern regular expression. Returns an array of arrays, where each array includes matching fragments from every group. Fragments are grouped in order of appearance in the haystack. Syntax extractAllGroupsVertical(haystack, pattern)  Arguments haystack — Input string. Type: String.pattern — Regular expression with re2 syntax. Must contain groups, each group enclosed in parentheses. If pattern contains no groups, an exception is thrown. Type: String. Returned value Type: Array. If haystack does not match the pattern regex, an empty array is returned. Example Query: SELECT extractAllGroupsVertical('abc=111, def=222, ghi=333', '(&quot;[^&quot;]+&quot;|\\\\w+)=(&quot;[^&quot;]+&quot;|\\\\w+)');  Result: ┌─extractAllGroupsVertical('abc=111, def=222, ghi=333', '(&quot;[^&quot;]+&quot;|\\\\w+)=(&quot;[^&quot;]+&quot;|\\\\w+)')─┐ │ [['abc','111'],['def','222'],['ghi','333']] │ └────────────────────────────────────────────────────────────────────────────────────────┘  See Also extractAllGroupsHorizontal "},{"title":"like(haystack, pattern), haystack LIKE pattern operator​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#function-like","content":"Checks whether a string matches a simple regular expression. The regular expression can contain the metasymbols % and _. % indicates any quantity of any bytes (including zero characters). _ indicates any one byte. Use the backslash (\\) for escaping metasymbols. See the note on escaping in the description of the ‘match’ function. For regular expressions like %needle%, the code is more optimal and works as fast as the position function. For other regular expressions, the code is the same as for the ‘match’ function. "},{"title":"notLike(haystack, pattern), haystack NOT LIKE pattern operator​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#function-notlike","content":"The same thing as ‘like’, but negative. "},{"title":"ilike​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#ilike","content":"Case insensitive variant of like function. You can use ILIKE operator instead of the ilike function. Syntax ilike(haystack, pattern)  Arguments haystack — Input string. String.pattern — If pattern does not contain percent signs or underscores, then the pattern only represents the string itself. An underscore (_) in pattern stands for (matches) any single character. A percent sign (%) matches any sequence of zero or more characters. Some pattern examples: 'abc' ILIKE 'abc' true 'abc' ILIKE 'a%' true 'abc' ILIKE '_b_' true 'abc' ILIKE 'c' false  Returned values True, if the string matches pattern.False, if the string does not match pattern. Example Input table: ┌─id─┬─name─────┬─days─┐ │ 1 │ January │ 31 │ │ 2 │ February │ 29 │ │ 3 │ March │ 31 │ │ 4 │ April │ 30 │ └────┴──────────┴──────┘  Query: SELECT * FROM Months WHERE ilike(name, '%j%');  Result: ┌─id─┬─name────┬─days─┐ │ 1 │ January │ 31 │ └────┴─────────┴──────┘  See Also like  "},{"title":"ngramDistance(haystack, needle)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#ngramdistancehaystack-needle","content":"Calculates the 4-gram distance between haystack and needle: counts the symmetric difference between two multisets of 4-grams and normalizes it by the sum of their cardinalities. Returns float number from 0 to 1 – the closer to zero, the more strings are similar to each other. If the constant needle or haystack is more than 32Kb, throws an exception. If some of the non-constant haystack or needle strings are more than 32Kb, the distance is always one. For case-insensitive search or/and in UTF-8 format use functions ngramDistanceCaseInsensitive, ngramDistanceUTF8, ngramDistanceCaseInsensitiveUTF8. "},{"title":"ngramSearch(haystack, needle)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#ngramsearchhaystack-needle","content":"Same as ngramDistance but calculates the non-symmetric difference between needle and haystack – the number of n-grams from needle minus the common number of n-grams normalized by the number of needle n-grams. The closer to one, the more likely needle is in the haystack. Can be useful for fuzzy string search. For case-insensitive search or/and in UTF-8 format use functions ngramSearchCaseInsensitive, ngramSearchUTF8, ngramSearchCaseInsensitiveUTF8. note For UTF-8 case we use 3-gram distance. All these are not perfectly fair n-gram distances. We use 2-byte hashes to hash n-grams and then calculate the (non-)symmetric difference between these hash tables – collisions may occur. With UTF-8 case-insensitive format we do not use fair tolower function – we zero the 5-th bit (starting from zero) of each codepoint byte and first bit of zeroth byte if bytes more than one – this works for Latin and mostly for all Cyrillic letters. "},{"title":"countSubstrings​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#countSubstrings","content":"Returns the number of substring occurrences. For a case-insensitive search, use countSubstringsCaseInsensitive or countSubstringsCaseInsensitiveUTF8 functions. Syntax countSubstrings(haystack, needle[, start_pos])  Arguments haystack — The string to search in. String.needle — The substring to search for. String.start_pos – Position of the first character in the string to start search. Optional. UInt. Returned values Number of occurrences. Type: UInt64. Examples Query: SELECT countSubstrings('foobar.com', '.');  Result: ┌─countSubstrings('foobar.com', '.')─┐ │ 1 │ └────────────────────────────────────┘  Query: SELECT countSubstrings('aaaa', 'aa');  Result: ┌─countSubstrings('aaaa', 'aa')─┐ │ 2 │ └───────────────────────────────┘  Query: SELECT countSubstrings('abc___abc', 'abc', 4);  Result: ┌─countSubstrings('abc___abc', 'abc', 4)─┐ │ 1 │ └────────────────────────────────────────┘  "},{"title":"countSubstringsCaseInsensitive​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#countSubstringsCaseInsensitive","content":"Returns the number of substring occurrences case-insensitive. Syntax countSubstringsCaseInsensitive(haystack, needle[, start_pos])  Arguments haystack — The string to search in. String.needle — The substring to search for. String.start_pos — Position of the first character in the string to start search. Optional. UInt. Returned values Number of occurrences. Type: UInt64. Examples Query: SELECT countSubstringsCaseInsensitive('aba', 'B');  Result: ┌─countSubstringsCaseInsensitive('aba', 'B')─┐ │ 1 │ └────────────────────────────────────────────┘  Query: SELECT countSubstringsCaseInsensitive('foobar.com', 'CoM');  Result: ┌─countSubstringsCaseInsensitive('foobar.com', 'CoM')─┐ │ 1 │ └─────────────────────────────────────────────────────┘  Query: SELECT countSubstringsCaseInsensitive('abC___abC', 'aBc', 2);  Result: ┌─countSubstringsCaseInsensitive('abC___abC', 'aBc', 2)─┐ │ 1 │ └───────────────────────────────────────────────────────┘  "},{"title":"countSubstringsCaseInsensitiveUTF8​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#countSubstringsCaseInsensitiveUTF8","content":"Returns the number of substring occurrences in UTF-8 case-insensitive. Syntax SELECT countSubstringsCaseInsensitiveUTF8(haystack, needle[, start_pos])  Arguments haystack — The string to search in. String.needle — The substring to search for. String.start_pos — Position of the first character in the string to start search. Optional. UInt. Returned values Number of occurrences. Type: UInt64. Examples Query: SELECT countSubstringsCaseInsensitiveUTF8('абв', 'A');  Result: ┌─countSubstringsCaseInsensitiveUTF8('абв', 'A')─┐ │ 1 │ └────────────────────────────────────────────────┘  Query: SELECT countSubstringsCaseInsensitiveUTF8('аБв__АбВ__абв', 'Абв');  Result: ┌─countSubstringsCaseInsensitiveUTF8('аБв__АбВ__абв', 'Абв')─┐ │ 3 │ └────────────────────────────────────────────────────────────┘  "},{"title":"countMatches(haystack, pattern)​","type":1,"pageTitle":"Functions for Searching in Strings","url":"en/sql-reference/functions/string-search-functions#countmatcheshaystack-pattern","content":"Returns the number of regular expression matches for a pattern in a haystack. Syntax countMatches(haystack, pattern)  Arguments haystack — The string to search in. String.pattern — The regular expression with re2 syntax. String. Returned value The number of matches. Type: UInt64. Examples Query: SELECT countMatches('foobar.com', 'o+');  Result: ┌─countMatches('foobar.com', 'o+')─┐ │ 2 │ └──────────────────────────────────┘  Query: SELECT countMatches('aaaa', 'aa');  Result: ┌─countMatches('aaaa', 'aa')────┐ │ 2 │ └───────────────────────────────┘  "},{"title":"Time Window Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/time-window-functions","content":"","keywords":""},{"title":"tumble​","type":1,"pageTitle":"Time Window Functions","url":"en/sql-reference/functions/time-window-functions#time-window-functions-tumble","content":"A tumbling time window assigns records to non-overlapping, continuous windows with a fixed duration (interval). tumble(time_attr, interval [, timezone])  Arguments time_attr - Date and time. DateTime data type.interval - Window interval in Interval data type.timezone — Timezone name (optional).  Returned values The inclusive lower and exclusive upper bound of the corresponding tumbling window. Type: Tuple(DateTime, DateTime) Example Query: SELECT tumble(now(), toIntervalDay('1'))  Result: ┌─tumble(now(), toIntervalDay('1'))─────────────┐ │ ['2020-01-01 00:00:00','2020-01-02 00:00:00'] │ └───────────────────────────────────────────────┘  "},{"title":"hop​","type":1,"pageTitle":"Time Window Functions","url":"en/sql-reference/functions/time-window-functions#time-window-functions-hop","content":"A hopping time window has a fixed duration (window_interval) and hops by a specified hop interval (hop_interval). If the hop_interval is smaller than the window_interval, hopping windows are overlapping. Thus, records can be assigned to multiple windows. hop(time_attr, hop_interval, window_interval [, timezone])  Arguments time_attr - Date and time. DateTime data type.hop_interval - Hop interval in Interval data type. Should be a positive number.window_interval - Window interval in Interval data type. Should be a positive number.timezone — Timezone name (optional).  Returned values The inclusive lower and exclusive upper bound of the corresponding hopping window. Since one record can be assigned to multiple hop windows, the function only returns the bound of the first window when hop function is used without WINDOW VIEW. Type: Tuple(DateTime, DateTime) Example Query: SELECT hop(now(), INTERVAL '1' SECOND, INTERVAL '2' SECOND)  Result: ┌─hop(now(), toIntervalSecond('1'), toIntervalSecond('2'))──┐ │ ('2020-01-14 16:58:22','2020-01-14 16:58:24') │ └───────────────────────────────────────────────────────────┘  "},{"title":"tumbleStart​","type":1,"pageTitle":"Time Window Functions","url":"en/sql-reference/functions/time-window-functions#time-window-functions-tumblestart","content":"Returns the inclusive lower bound of the corresponding tumbling window. tumbleStart(bounds_tuple); tumbleStart(time_attr, interval [, timezone]);  "},{"title":"tumbleEnd​","type":1,"pageTitle":"Time Window Functions","url":"en/sql-reference/functions/time-window-functions#time-window-functions-tumbleend","content":"Returns the exclusive upper bound of the corresponding tumbling window. tumbleEnd(bounds_tuple); tumbleEnd(time_attr, interval [, timezone]);  "},{"title":"hopStart​","type":1,"pageTitle":"Time Window Functions","url":"en/sql-reference/functions/time-window-functions#time-window-functions-hopstart","content":"Returns the inclusive lower bound of the corresponding hopping window. hopStart(bounds_tuple); hopStart(time_attr, hop_interval, window_interval [, timezone]);  "},{"title":"hopEnd​","type":1,"pageTitle":"Time Window Functions","url":"en/sql-reference/functions/time-window-functions#time-window-functions-hopend","content":"Returns the exclusive upper bound of the corresponding hopping window. hopEnd(bounds_tuple); hopEnd(time_attr, hop_interval, window_interval [, timezone]);  "},{"title":"Functions for Working with Tuples","type":0,"sectionRef":"#","url":"en/sql-reference/functions/tuple-functions","content":"","keywords":""},{"title":"tuple​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#tuple","content":"A function that allows grouping multiple columns. For columns with the types T1, T2, …, it returns a Tuple(T1, T2, …) type tuple containing these columns. There is no cost to execute the function. Tuples are normally used as intermediate values for an argument of IN operators, or for creating a list of formal parameters of lambda functions. Tuples can’t be written to a table. The function implements the operator (x, y, …). Syntax tuple(x, y, …)  "},{"title":"tupleElement​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#tupleelement","content":"A function that allows getting a column from a tuple. ‘N’ is the column index, starting from 1. ‘N’ must be a constant. ‘N’ must be a strict postive integer no greater than the size of the tuple. There is no cost to execute the function. The function implements the operator x.N. Syntax tupleElement(tuple, n)  "},{"title":"untuple​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#untuple","content":"Performs syntactic substitution of tuple elements in the call location. Syntax untuple(x)  You can use the EXCEPT expression to skip columns as a result of the query. Arguments x — A tuple function, column, or tuple of elements. Tuple. Returned value None. Examples Input table: ┌─key─┬─v1─┬─v2─┬─v3─┬─v4─┬─v5─┬─v6────────┐ │ 1 │ 10 │ 20 │ 40 │ 30 │ 15 │ (33,'ab') │ │ 2 │ 25 │ 65 │ 70 │ 40 │ 6 │ (44,'cd') │ │ 3 │ 57 │ 30 │ 20 │ 10 │ 5 │ (55,'ef') │ │ 4 │ 55 │ 12 │ 7 │ 80 │ 90 │ (66,'gh') │ │ 5 │ 30 │ 50 │ 70 │ 25 │ 55 │ (77,'kl') │ └─────┴────┴────┴────┴────┴────┴───────────┘  Example of using a Tuple-type column as the untuple function parameter: Query: SELECT untuple(v6) FROM kv;  Result: ┌─_ut_1─┬─_ut_2─┐ │ 33 │ ab │ │ 44 │ cd │ │ 55 │ ef │ │ 66 │ gh │ │ 77 │ kl │ └───────┴───────┘  Note: the names are implementation specific and are subject to change. You should not assume specific names of the columns after application of the untuple. Example of using an EXCEPT expression: Query: SELECT untuple((* EXCEPT (v2, v3),)) FROM kv;  Result: ┌─key─┬─v1─┬─v4─┬─v5─┬─v6────────┐ │ 1 │ 10 │ 30 │ 15 │ (33,'ab') │ │ 2 │ 25 │ 40 │ 6 │ (44,'cd') │ │ 3 │ 57 │ 10 │ 5 │ (55,'ef') │ │ 4 │ 55 │ 80 │ 90 │ (66,'gh') │ │ 5 │ 30 │ 25 │ 55 │ (77,'kl') │ └─────┴────┴────┴────┴───────────┘  See Also Tuple "},{"title":"tupleHammingDistance​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#tuplehammingdistance","content":"Returns the Hamming Distance between two tuples of the same size. Syntax tupleHammingDistance(tuple1, tuple2)  Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Tuples should have the same type of the elements. Returned value The Hamming distance. Type: The result type is calculed the same way it is for Arithmetic functions, based on the number of elements in the input tuples. SELECT toTypeName(tupleHammingDistance(tuple(0), tuple(0))) AS t1, toTypeName(tupleHammingDistance((0, 0), (0, 0))) AS t2, toTypeName(tupleHammingDistance((0, 0, 0), (0, 0, 0))) AS t3, toTypeName(tupleHammingDistance((0, 0, 0, 0), (0, 0, 0, 0))) AS t4, toTypeName(tupleHammingDistance((0, 0, 0, 0, 0), (0, 0, 0, 0, 0))) AS t5  ┌─t1────┬─t2─────┬─t3─────┬─t4─────┬─t5─────┐ │ UInt8 │ UInt16 │ UInt32 │ UInt64 │ UInt64 │ └───────┴────────┴────────┴────────┴────────┘  Examples Query: SELECT tupleHammingDistance((1, 2, 3), (3, 2, 1)) AS HammingDistance;  Result: ┌─HammingDistance─┐ │ 2 │ └─────────────────┘  Can be used with MinHash functions for detection of semi-duplicate strings: SELECT tupleHammingDistance(wordShingleMinHash(string), wordShingleMinHashCaseInsensitive(string)) as HammingDistance FROM (SELECT 'ClickHouse is a column-oriented database management system for online analytical processing of queries.' AS string);  Result: ┌─HammingDistance─┐ │ 2 │ └─────────────────┘  "},{"title":"tupleToNameValuePairs​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#tupletonamevaluepairs","content":"Turns a named tuple into an array of (name, value) pairs. For a Tuple(a T, b T, ..., c T) returns Array(Tuple(String, T), ...)in which the Strings represents the named fields of the tuple and T are the values associated with those names. All values in the tuple should be of the same type. Syntax tupleToNameValuePairs(tuple)  Arguments tuple — Named tuple. Tuple with any types of values. Returned value An array with (name, value) pairs. Type: Array(Tuple(String, ...)). Example Query: CREATE TABLE tupletest (`col` Tuple(user_ID UInt64, session_ID UInt64) ENGINE = Memory; INSERT INTO tupletest VALUES (tuple( 100, 2502)), (tuple(1,100)); SELECT tupleToNameValuePairs(col) FROM tupletest;  Result: ┌─tupleToNameValuePairs(col)────────────┐ │ [('user_ID',100),('session_ID',2502)] │ │ [('user_ID',1),('session_ID',100)] │ └───────────────────────────────────────┘  It is possible to transform colums to rows using this function: CREATE TABLE tupletest (`col` Tuple(CPU Float64, Memory Float64, Disk Float64)) ENGINE = Memory; INSERT INTO tupletest VALUES(tuple(3.3, 5.5, 6.6)); SELECT arrayJoin(tupleToNameValuePairs(col))FROM tupletest;  Result: ┌─arrayJoin(tupleToNameValuePairs(col))─┐ │ ('CPU',3.3) │ │ ('Memory',5.5) │ │ ('Disk',6.6) │ └───────────────────────────────────────┘  If you pass a simple tuple to the function, ClickHouse uses the indexes of the values as their names: SELECT tupleToNameValuePairs(tuple(3, 2, 1));  Result: ┌─tupleToNameValuePairs(tuple(3, 2, 1))─┐ │ [('1',3),('2',2),('3',1)] │ └───────────────────────────────────────┘  "},{"title":"tuplePlus​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#tupleplus","content":"Calculates the sum of corresponding values of two tuples of the same size. Syntax tuplePlus(tuple1, tuple2)  Alias: vectorSum. Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Returned value Tuple with the sum. Type: Tuple. Example Query: SELECT tuplePlus((1, 2), (2, 3));  Result: ┌─tuplePlus((1, 2), (2, 3))─┐ │ (3,5) │ └───────────────────────────┘  "},{"title":"tupleMinus​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#tupleminus","content":"Calculates the subtraction of corresponding values of two tuples of the same size. Syntax tupleMinus(tuple1, tuple2)  Alias: vectorDifference. Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Returned value Tuple with the result of subtraction. Type: Tuple. Example Query: SELECT tupleMinus((1, 2), (2, 3));  Result: ┌─tupleMinus((1, 2), (2, 3))─┐ │ (-1,-1) │ └────────────────────────────┘  "},{"title":"tupleMultiply​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#tuplemultiply","content":"Calculates the multiplication of corresponding values of two tuples of the same size. Syntax tupleMultiply(tuple1, tuple2)  Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Returned value Tuple with the multiplication. Type: Tuple. Example Query: SELECT tupleMultiply((1, 2), (2, 3));  Result: ┌─tupleMultiply((1, 2), (2, 3))─┐ │ (2,6) │ └───────────────────────────────┘  "},{"title":"tupleDivide​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#tupledivide","content":"Calculates the division of corresponding values of two tuples of the same size. Note that division by zero will return inf. Syntax tupleDivide(tuple1, tuple2)  Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Returned value Tuple with the result of division. Type: Tuple. Example Query: SELECT tupleDivide((1, 2), (2, 3));  Result: ┌─tupleDivide((1, 2), (2, 3))─┐ │ (0.5,0.6666666666666666) │ └─────────────────────────────┘  "},{"title":"tupleNegate​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#tuplenegate","content":"Calculates the negation of the tuple values. Syntax tupleNegate(tuple)  Arguments tuple — Tuple. Returned value Tuple with the result of negation. Type: Tuple. Example Query: SELECT tupleNegate((1, 2));  Result: ┌─tupleNegate((1, 2))─┐ │ (-1,-2) │ └─────────────────────┘  "},{"title":"tupleMultiplyByNumber​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#tuplemultiplybynumber","content":"Returns a tuple with all values multiplied by a number. Syntax tupleMultiplyByNumber(tuple, number)  Arguments tuple — Tuple.number — Multiplier. Int/UInt, Float or Decimal. Returned value Tuple with multiplied values. Type: Tuple. Example Query: SELECT tupleMultiplyByNumber((1, 2), -2.1);  Result: ┌─tupleMultiplyByNumber((1, 2), -2.1)─┐ │ (-2.1,-4.2) │ └─────────────────────────────────────┘  "},{"title":"tupleDivideByNumber​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#tupledividebynumber","content":"Returns a tuple with all values divided by a number. Note that division by zero will return inf. Syntax tupleDivideByNumber(tuple, number)  Arguments tuple — Tuple.number — Divider. Int/UInt, Float or Decimal. Returned value Tuple with divided values. Type: Tuple. Example Query: SELECT tupleDivideByNumber((1, 2), 0.5);  Result: ┌─tupleDivideByNumber((1, 2), 0.5)─┐ │ (2,4) │ └──────────────────────────────────┘  "},{"title":"dotProduct​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#dotproduct","content":"Calculates the scalar product of two tuples of the same size. Syntax dotProduct(tuple1, tuple2)  Alias: scalarProduct. Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Returned value Scalar product. Type: Int/UInt, Float or Decimal. Example Query: SELECT dotProduct((1, 2), (2, 3));  Result: ┌─dotProduct((1, 2), (2, 3))─┐ │ 8 │ └────────────────────────────┘  "},{"title":"L1Norm​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#l1norm","content":"Calculates the sum of absolute values of a tuple. Syntax L1Norm(tuple)  Alias: normL1. Arguments tuple — Tuple. Returned value L1-norm or taxicab geometry distance. Type: UInt, Float or Decimal. Example Query: SELECT L1Norm((1, 2));  Result: ┌─L1Norm((1, 2))─┐ │ 3 │ └────────────────┘  "},{"title":"L2Norm​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#l2norm","content":"Calculates the square root of the sum of the squares of the tuple values. Syntax L2Norm(tuple)  Alias: normL2. Arguments tuple — Tuple. Returned value L2-norm or Euclidean distance. Type: Float. Example Query: SELECT L2Norm((1, 2));  Result: ┌───L2Norm((1, 2))─┐ │ 2.23606797749979 │ └──────────────────┘  "},{"title":"LinfNorm​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#linfnorm","content":"Calculates the maximum of absolute values of a tuple. Syntax LinfNorm(tuple)  Alias: normLinf. Arguments tuple — Tuple. Returned value Linf-norm or the maximum absolute value. Type: Float. Example Query: SELECT LinfNorm((1, -2));  Result: ┌─LinfNorm((1, -2))─┐ │ 2 │ └───────────────────┘  "},{"title":"LpNorm​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#lpnorm","content":"Calculates the root of p-th power of the sum of the absolute values of a tuple in the power of p. Syntax LpNorm(tuple, p)  Alias: normLp. Arguments tuple — Tuple.p — The power. Possible values: real number in [1; inf). UInt or Float. Returned value Lp-norm Type: Float. Example Query: SELECT LpNorm((1, -2), 2);  Result: ┌─LpNorm((1, -2), 2)─┐ │ 2.23606797749979 │ └────────────────────┘  "},{"title":"L1Distance​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#l1distance","content":"Calculates the distance between two points (the values of the tuples are the coordinates) in L1 space (1-norm (taxicab geometry distance)). Syntax L1Distance(tuple1, tuple2)  Alias: distanceL1. Arguments tuple1 — First tuple. Tuple.tuple1 — Second tuple. Tuple. Returned value 1-norm distance. Type: Float. Example Query: SELECT L1Distance((1, 2), (2, 3));  Result: ┌─L1Distance((1, 2), (2, 3))─┐ │ 2 │ └────────────────────────────┘  "},{"title":"L2Distance​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#l2distance","content":"Calculates the distance between two points (the values of the tuples are the coordinates) in Euclidean space (Euclidean distance). Syntax L2Distance(tuple1, tuple2)  Alias: distanceL2. Arguments tuple1 — First tuple. Tuple.tuple1 — Second tuple. Tuple. Returned value 2-norm distance. Type: Float. Example Query: SELECT L2Distance((1, 2), (2, 3));  Result: ┌─L2Distance((1, 2), (2, 3))─┐ │ 1.4142135623730951 │ └────────────────────────────┘  "},{"title":"LinfDistance​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#linfdistance","content":"Calculates the distance between two points (the values of the tuples are the coordinates) in L_{inf} space (maximum norm). Syntax LinfDistance(tuple1, tuple2)  Alias: distanceLinf. Arguments tuple1 — First tuple. Tuple.tuple1 — Second tuple. Tuple. Returned value Infinity-norm distance. Type: Float. Example Query: SELECT LinfDistance((1, 2), (2, 3));  Result: ┌─LinfDistance((1, 2), (2, 3))─┐ │ 1 │ └──────────────────────────────┘  "},{"title":"LpDistance​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#lpdistance","content":"Calculates the distance between two points (the values of the tuples are the coordinates) in Lp space (p-norm distance). Syntax LpDistance(tuple1, tuple2, p)  Alias: distanceLp. Arguments tuple1 — First tuple. Tuple.tuple1 — Second tuple. Tuple.p — The power. Possible values: real number from [1; inf). UInt or Float. Returned value p-norm distance. Type: Float. Example Query: SELECT LpDistance((1, 2), (2, 3), 3);  Result: ┌─LpDistance((1, 2), (2, 3), 3)─┐ │ 1.2599210498948732 │ └───────────────────────────────┘  "},{"title":"L1Normalize​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#l1normalize","content":"Calculates the unit vector of a given vector (the values of the tuple are the coordinates) in L1 space (taxicab geometry). Syntax L1Normalize(tuple)  Alias: normalizeL1. Arguments tuple — Tuple. Returned value Unit vector. Type: Tuple of Float. Example Query: SELECT L1Normalize((1, 2));  Result: ┌─L1Normalize((1, 2))─────────────────────┐ │ (0.3333333333333333,0.6666666666666666) │ └─────────────────────────────────────────┘  "},{"title":"L2Normalize​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#l2normalize","content":"Calculates the unit vector of a given vector (the values of the tuple are the coordinates) in Euclidean space (using Euclidean distance). Syntax L2Normalize(tuple)  Alias: normalizeL1. Arguments tuple — Tuple. Returned value Unit vector. Type: Tuple of Float. Example Query: SELECT L2Normalize((3, 4));  Result: ┌─L2Normalize((3, 4))─┐ │ (0.6,0.8) │ └─────────────────────┘  "},{"title":"LinfNormalize​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#linfnormalize","content":"Calculates the unit vector of a given vector (the values of the tuple are the coordinates) in L_{inf} space (using maximum norm). Syntax LinfNormalize(tuple)  Alias: normalizeLinf . Arguments tuple — Tuple. Returned value Unit vector. Type: Tuple of Float. Example Query: SELECT LinfNormalize((3, 4));  Result: ┌─LinfNormalize((3, 4))─┐ │ (0.75,1) │ └───────────────────────┘  "},{"title":"LpNormalize​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#lpnormalize","content":"Calculates the unit vector of a given vector (the values of the tuple are the coordinates) in Lp space (using p-norm). Syntax LpNormalize(tuple, p)  Alias: normalizeLp . Arguments tuple — Tuple.p — The power. Possible values: any number from [1;inf). UInt or Float. Returned value Unit vector. Type: Tuple of Float. Example Query: SELECT LpNormalize((3, 4),5);  Result: ┌─LpNormalize((3, 4), 5)──────────────────┐ │ (0.7187302630182624,0.9583070173576831) │ └─────────────────────────────────────────┘  "},{"title":"cosineDistance​","type":1,"pageTitle":"Functions for Working with Tuples","url":"en/sql-reference/functions/tuple-functions#cosinedistance","content":"Calculates the cosine distance between two vectors (the values of the tuples are the coordinates). The less the returned value is, the more similar are the vectors. Syntax cosineDistance(tuple1, tuple2)  Arguments tuple1 — First tuple. Tuple.tuple2 — Second tuple. Tuple. Returned value Cosine of the angle between two vectors substracted from one. Type: Float. Example Query: SELECT cosineDistance((1, 2), (2, 3));  Result: ┌─cosineDistance((1, 2), (2, 3))─┐ │ 0.007722123286332261 │ └────────────────────────────────┘  "},{"title":"Functions for maps","type":0,"sectionRef":"#","url":"en/sql-reference/functions/tuple-map-functions","content":"","keywords":""},{"title":"map​","type":1,"pageTitle":"Functions for maps","url":"en/sql-reference/functions/tuple-map-functions#function-map","content":"Arranges key:value pairs into Map(key, value) data type. Syntax map(key1, value1[, key2, value2, ...])  Arguments key — The key part of the pair. String or Integer.value — The value part of the pair. String, Integer or Array. Returned value Data structure as key:value pairs. Type: Map(key, value). Examples Query: SELECT map('key1', number, 'key2', number * 2) FROM numbers(3);  Result: ┌─map('key1', number, 'key2', multiply(number, 2))─┐ │ {'key1':0,'key2':0} │ │ {'key1':1,'key2':2} │ │ {'key1':2,'key2':4} │ └──────────────────────────────────────────────────┘  Query: CREATE TABLE table_map (a Map(String, UInt64)) ENGINE = MergeTree() ORDER BY a; INSERT INTO table_map SELECT map('key1', number, 'key2', number * 2) FROM numbers(3); SELECT a['key2'] FROM table_map;  Result: ┌─arrayElement(a, 'key2')─┐ │ 0 │ │ 2 │ │ 4 │ └─────────────────────────┘  See Also Map(key, value) data type "},{"title":"mapAdd​","type":1,"pageTitle":"Functions for maps","url":"en/sql-reference/functions/tuple-map-functions#function-mapadd","content":"Collect all the keys and sum corresponding values. Syntax mapAdd(arg1, arg2 [, ...])  Arguments Arguments are maps or tuples of two arrays, where items in the first array represent keys, and the second array contains values for the each key. All key arrays should have same type, and all value arrays should contain items which are promoted to the one type (Int64, UInt64 or Float64). The common promoted type is used as a type for the result array. Returned value Depending on the arguments returns one map or tuple, where the first array contains the sorted keys and the second array contains values. Example Query with a tuple: SELECT mapAdd(([toUInt8(1), 2], [1, 1]), ([toUInt8(1), 2], [1, 1])) as res, toTypeName(res) as type;  Result: ┌─res───────────┬─type───────────────────────────────┐ │ ([1,2],[2,2]) │ Tuple(Array(UInt8), Array(UInt64)) │ └───────────────┴────────────────────────────────────┘  Query with Map type: SELECT mapAdd(map(1,1), map(1,1));  Result: ┌─mapAdd(map(1, 1), map(1, 1))─┐ │ {1:2} │ └──────────────────────────────┘  "},{"title":"mapSubtract​","type":1,"pageTitle":"Functions for maps","url":"en/sql-reference/functions/tuple-map-functions#function-mapsubtract","content":"Collect all the keys and subtract corresponding values. Syntax mapSubtract(Tuple(Array, Array), Tuple(Array, Array) [, ...])  Arguments Arguments are maps or tuples of two arrays, where items in the first array represent keys, and the second array contains values for the each key. All key arrays should have same type, and all value arrays should contain items which are promote to the one type (Int64, UInt64 or Float64). The common promoted type is used as a type for the result array. Returned value Depending on the arguments returns one map or tuple, where the first array contains the sorted keys and the second array contains values. Example Query with a tuple map: SELECT mapSubtract(([toUInt8(1), 2], [toInt32(1), 1]), ([toUInt8(1), 2], [toInt32(2), 1])) as res, toTypeName(res) as type;  Result: ┌─res────────────┬─type──────────────────────────────┐ │ ([1,2],[-1,0]) │ Tuple(Array(UInt8), Array(Int64)) │ └────────────────┴───────────────────────────────────┘  Query with Map type: SELECT mapSubtract(map(1,1), map(1,1));  Result: ┌─mapSubtract(map(1, 1), map(1, 1))─┐ │ {1:0} │ └───────────────────────────────────┘  "},{"title":"mapPopulateSeries​","type":1,"pageTitle":"Functions for maps","url":"en/sql-reference/functions/tuple-map-functions#function-mappopulateseries","content":"Fills missing keys in the maps (key and value array pair), where keys are integers. Also, it supports specifying the max key, which is used to extend the keys array. Syntax mapPopulateSeries(keys, values[, max]) mapPopulateSeries(map[, max])  Generates a map (a tuple with two arrays or a value of Map type, depending on the arguments), where keys are a series of numbers, from minimum to maximum keys (or max argument if it specified) taken from the map with a step size of one, and corresponding values. If the value is not specified for the key, then it uses the default value in the resulting map. For repeated keys, only the first value (in order of appearing) gets associated with the key. For array arguments the number of elements in keys and values must be the same for each row. Arguments Arguments are maps or two arrays, where the first array represent keys, and the second array contains values for the each key. Mapped arrays: keys — Array of keys. Array(Int).values — Array of values. Array(Int).max — Maximum key value. Optional. Int8, Int16, Int32, Int64, Int128, Int256. or map — Map with integer keys. Map. Returned value Depending on the arguments returns a map or a tuple of two arrays: keys in sorted order, and values the corresponding keys. Example Query with mapped arrays: SELECT mapPopulateSeries([1,2,4], [11,22,44], 5) AS res, toTypeName(res) AS type;  Result: ┌─res──────────────────────────┬─type──────────────────────────────┐ │ ([1,2,3,4,5],[11,22,0,44,0]) │ Tuple(Array(UInt8), Array(UInt8)) │ └──────────────────────────────┴───────────────────────────────────┘  Query with Map type: SELECT mapPopulateSeries(map(1, 10, 5, 20), 6);  Result: ┌─mapPopulateSeries(map(1, 10, 5, 20), 6)─┐ │ {1:10,2:0,3:0,4:0,5:20,6:0} │ └─────────────────────────────────────────┘  "},{"title":"mapContains​","type":1,"pageTitle":"Functions for maps","url":"en/sql-reference/functions/tuple-map-functions#mapcontains","content":"Determines whether the map contains the key parameter. Syntax mapContains(map, key)  Parameters map — Map. Map.key — Key. Type matches the type of keys of map parameter. Returned value 1 if map contains key, 0 if not. Type: UInt8. Example Query: CREATE TABLE test (a Map(String,String)) ENGINE = Memory; INSERT INTO test VALUES ({'name':'eleven','age':'11'}), ({'number':'twelve','position':'6.0'}); SELECT mapContains(a, 'name') FROM test;  Result: ┌─mapContains(a, 'name')─┐ │ 1 │ │ 0 │ └────────────────────────┘  "},{"title":"mapKeys​","type":1,"pageTitle":"Functions for maps","url":"en/sql-reference/functions/tuple-map-functions#mapkeys","content":"Returns all keys from the map parameter. Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only keys subcolumn instead of reading and processing the whole column data. The query SELECT mapKeys(m) FROM table transforms to SELECT m.keys FROM table. Syntax mapKeys(map)  Parameters map — Map. Map. Returned value Array containing all keys from the map. Type: Array. Example Query: CREATE TABLE test (a Map(String,String)) ENGINE = Memory; INSERT INTO test VALUES ({'name':'eleven','age':'11'}), ({'number':'twelve','position':'6.0'}); SELECT mapKeys(a) FROM test;  Result: ┌─mapKeys(a)────────────┐ │ ['name','age'] │ │ ['number','position'] │ └───────────────────────┘  "},{"title":"mapValues​","type":1,"pageTitle":"Functions for maps","url":"en/sql-reference/functions/tuple-map-functions#mapvalues","content":"Returns all values from the map parameter. Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only values subcolumn instead of reading and processing the whole column data. The query SELECT mapValues(m) FROM table transforms to SELECT m.values FROM table. Syntax mapValues(map)  Parameters map — Map. Map. Returned value Array containing all the values from map. Type: Array. Example Query: CREATE TABLE test (a Map(String,String)) ENGINE = Memory; INSERT INTO test VALUES ({'name':'eleven','age':'11'}), ({'number':'twelve','position':'6.0'}); SELECT mapValues(a) FROM test;  Result: ┌─mapValues(a)─────┐ │ ['eleven','11'] │ │ ['twelve','6.0'] │ └──────────────────┘  "},{"title":"mapContainsKeyLike​","type":1,"pageTitle":"Functions for maps","url":"en/sql-reference/functions/tuple-map-functions#mapContainsKeyLike","content":"Syntax mapContainsKeyLike(map, pattern)  Parameters map — Map. Map. pattern - String pattern to match.  Returned value 1 if map contains key like specified pattern, 0 if not.  Example Query: CREATE TABLE test (a Map(String,String)) ENGINE = Memory; INSERT INTO test VALUES ({'abc':'abc','def':'def'}), ({'hij':'hij','klm':'klm'}); SELECT mapContainsKeyLike(a, 'a%') FROM test;  Result: ┌─mapContainsKeyLike(a, 'a%')─┐ │ 1 │ │ 0 │ └─────────────────────────────┘  "},{"title":"mapExtractKeyLike​","type":1,"pageTitle":"Functions for maps","url":"en/sql-reference/functions/tuple-map-functions#mapExtractKeyLike","content":"Syntax mapExtractKeyLike(map, pattern)  Parameters map — Map. Map. pattern - String pattern to match.  Returned value A map contained elements the key of which matchs the specified pattern. If there are no elements matched the pattern, it will return an empty map. Example Query: CREATE TABLE test (a Map(String,String)) ENGINE = Memory; INSERT INTO test VALUES ({'abc':'abc','def':'def'}), ({'hij':'hij','klm':'klm'}); SELECT mapExtractKeyLike(a, 'a%') FROM test;  Result: ┌─mapExtractKeyLike(a, 'a%')─┐ │ {'abc':'abc'} │ │ {} │ └────────────────────────────┘  Original article "},{"title":"Type Conversion Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/type-conversion-functions","content":"","keywords":""},{"title":"Common Issues of Numeric Conversions​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#numeric-conversion-issues","content":"When you convert a value from one to another data type, you should remember that in common case, it is an unsafe operation that can lead to a data loss. A data loss can occur if you try to fit value from a larger data type to a smaller data type, or if you convert values between different data types. ClickHouse has the same behavior as C++ programs. "},{"title":"toInt(8|16|32|64|128|256)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#toint8163264128256","content":"Converts an input value to the Int data type. This function family includes: toInt8(expr) — Results in the Int8 data type.toInt16(expr) — Results in the Int16 data type.toInt32(expr) — Results in the Int32 data type.toInt64(expr) — Results in the Int64 data type.toInt128(expr) — Results in the Int128 data type.toInt256(expr) — Results in the Int256 data type. Arguments expr — Expression returning a number or a string with the decimal representation of a number. Binary, octal, and hexadecimal representations of numbers are not supported. Leading zeroes are stripped. Returned value Integer value in the Int8, Int16, Int32, Int64, Int128 or Int256 data type. Functions use rounding towards zero, meaning they truncate fractional digits of numbers. The behavior of functions for the NaN and Inf arguments is undefined. Remember about numeric convertions issues, when using the functions. Example Query: SELECT toInt64(nan), toInt32(32), toInt16('16'), toInt8(8.8);  Result: ┌─────────toInt64(nan)─┬─toInt32(32)─┬─toInt16('16')─┬─toInt8(8.8)─┐ │ -9223372036854775808 │ 32 │ 16 │ 8 │ └──────────────────────┴─────────────┴───────────────┴─────────────┘  "},{"title":"toInt(8|16|32|64|128|256)OrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#toint8163264orzero","content":"It takes an argument of type String and tries to parse it into Int (8 | 16 | 32 | 64 | 128 | 256). If failed, returns 0. Example Query: SELECT toInt64OrZero('123123'), toInt8OrZero('123qwe123');  Result: ┌─toInt64OrZero('123123')─┬─toInt8OrZero('123qwe123')─┐ │ 123123 │ 0 │ └─────────────────────────┴───────────────────────────┘  "},{"title":"toInt(8|16|32|64|128|256)OrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#toint8163264128256ornull","content":"It takes an argument of type String and tries to parse it into Int (8 | 16 | 32 | 64 | 128 | 256). If failed, returns NULL. Example Query: SELECT toInt64OrNull('123123'), toInt8OrNull('123qwe123');  Result: ┌─toInt64OrNull('123123')─┬─toInt8OrNull('123qwe123')─┐ │ 123123 │ ᴺᵁᴸᴸ │ └─────────────────────────┴───────────────────────────┘  "},{"title":"toInt(8|16|32|64|128|256)OrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#toint8163264128256orDefault","content":"It takes an argument of type String and tries to parse it into Int (8 | 16 | 32 | 64 | 128 | 256). If failed, returns the default type value. Example Query: SELECT toInt64OrDefault('123123', cast('-1' as Int64)), toInt8OrDefault('123qwe123', cast('-1' as Int8));  Result: ┌─toInt64OrDefault('123123', CAST('-1', 'Int64'))─┬─toInt8OrDefault('123qwe123', CAST('-1', 'Int8'))─┐ │ 123123 │ -1 │ └─────────────────────────────────────────────────┴──────────────────────────────────────────────────┘  "},{"title":"toUInt(8|16|32|64|256)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#touint8163264256","content":"Converts an input value to the UInt data type. This function family includes: toUInt8(expr) — Results in the UInt8 data type.toUInt16(expr) — Results in the UInt16 data type.toUInt32(expr) — Results in the UInt32 data type.toUInt64(expr) — Results in the UInt64 data type.toUInt256(expr) — Results in the UInt256 data type. Arguments expr — Expression returning a number or a string with the decimal representation of a number. Binary, octal, and hexadecimal representations of numbers are not supported. Leading zeroes are stripped. Returned value Integer value in the UInt8, UInt16, UInt32, UInt64 or UInt256 data type. Functions use rounding towards zero, meaning they truncate fractional digits of numbers. The behavior of functions for negative agruments and for the NaN and Inf arguments is undefined. If you pass a string with a negative number, for example '-32', ClickHouse raises an exception. Remember about numeric convertions issues, when using the functions. Example Query: SELECT toUInt64(nan), toUInt32(-32), toUInt16('16'), toUInt8(8.8);  Result: ┌───────toUInt64(nan)─┬─toUInt32(-32)─┬─toUInt16('16')─┬─toUInt8(8.8)─┐ │ 9223372036854775808 │ 4294967264 │ 16 │ 8 │ └─────────────────────┴───────────────┴────────────────┴──────────────┘  "},{"title":"toUInt(8|16|32|64|256)OrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#touint8163264256orzero","content":""},{"title":"toUInt(8|16|32|64|256)OrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#touint8163264256ornull","content":""},{"title":"toUInt(8|16|32|64|256)OrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#touint8163264256ordefault","content":""},{"title":"toFloat(32|64)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#tofloat3264","content":""},{"title":"toFloat(32|64)OrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#tofloat3264orzero","content":""},{"title":"toFloat(32|64)OrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#tofloat3264ornull","content":""},{"title":"toFloat(32|64)OrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#tofloat3264ordefault","content":""},{"title":"toDate​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todate","content":"Alias: DATE. "},{"title":"toDateOrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todateorzero","content":""},{"title":"toDateOrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todateornull","content":""},{"title":"toDateOrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todateordefault","content":""},{"title":"toDateTime​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todatetime","content":""},{"title":"toDateTimeOrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todatetimeorzero","content":""},{"title":"toDateTimeOrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todatetimeornull","content":""},{"title":"toDateTimeOrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todatetimeordefault","content":""},{"title":"toDate32​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todate32","content":"Converts the argument to the Date32 data type. If the value is outside the range returns the border values supported by Date32. If the argument has Date type, borders of Date are taken into account. Syntax toDate32(expr)  Arguments expr — The value. String, UInt32 or Date. Returned value A calendar date. Type: Date32. Example The value is within the range: SELECT toDate32('1955-01-01') AS value, toTypeName(value);  ┌──────value─┬─toTypeName(toDate32('1925-01-01'))─┐ │ 1955-01-01 │ Date32 │ └────────────┴────────────────────────────────────┘  The value is outside the range: SELECT toDate32('1924-01-01') AS value, toTypeName(value);  ┌──────value─┬─toTypeName(toDate32('1925-01-01'))─┐ │ 1925-01-01 │ Date32 │ └────────────┴────────────────────────────────────┘  With Date-type argument: SELECT toDate32(toDate('1924-01-01')) AS value, toTypeName(value);  ┌──────value─┬─toTypeName(toDate32(toDate('1924-01-01')))─┐ │ 1970-01-01 │ Date32 │ └────────────┴────────────────────────────────────────────┘  "},{"title":"toDate32OrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todate32-or-zero","content":"The same as toDate32 but returns the min value of Date32 if invalid argument is received. Example Query: SELECT toDate32OrZero('1924-01-01'), toDate32OrZero('');  Result: ┌─toDate32OrZero('1924-01-01')─┬─toDate32OrZero('')─┐ │ 1925-01-01 │ 1925-01-01 │ └──────────────────────────────┴────────────────────┘  "},{"title":"toDate32OrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todate32-or-null","content":"The same as toDate32 but returns NULL if invalid argument is received. Example Query: SELECT toDate32OrNull('1955-01-01'), toDate32OrNull('');  Result: ┌─toDate32OrNull('1955-01-01')─┬─toDate32OrNull('')─┐ │ 1955-01-01 │ ᴺᵁᴸᴸ │ └──────────────────────────────┴────────────────────┘  "},{"title":"toDate32OrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todate32-or-default","content":"Converts the argument to the Date32 data type. If the value is outside the range returns the lower border value supported by Date32. If the argument has Date type, borders of Date are taken into account. Returns default value if invalid argument is received. Example Query: SELECT toDate32OrDefault('1930-01-01', toDate32('2020-01-01')), toDate32OrDefault('xx1930-01-01', toDate32('2020-01-01'));  Result: ┌─toDate32OrDefault('1930-01-01', toDate32('2020-01-01'))─┬─toDate32OrDefault('xx1930-01-01', toDate32('2020-01-01'))─┐ │ 1930-01-01 │ 2020-01-01 │ └─────────────────────────────────────────────────────────┴───────────────────────────────────────────────────────────┘  "},{"title":"toDecimal(32|64|128|256)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todecimal3264128256","content":"Converts value to the Decimal data type with precision of S. The value can be a number or a string. The S (scale) parameter specifies the number of decimal places. toDecimal32(value, S)toDecimal64(value, S)toDecimal128(value, S)toDecimal256(value, S) "},{"title":"toDecimal(32|64|128|256)OrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todecimal3264128256ornull","content":"Converts an input string to a Nullable(Decimal(P,S)) data type value. This family of functions include: toDecimal32OrNull(expr, S) — Results in Nullable(Decimal32(S)) data type.toDecimal64OrNull(expr, S) — Results in Nullable(Decimal64(S)) data type.toDecimal128OrNull(expr, S) — Results in Nullable(Decimal128(S)) data type.toDecimal256OrNull(expr, S) — Results in Nullable(Decimal256(S)) data type. These functions should be used instead of toDecimal*() functions, if you prefer to get a NULL value instead of an exception in the event of an input value parsing error. Arguments expr — Expression, returns a value in the String data type. ClickHouse expects the textual representation of the decimal number. For example, '1.111'.S — Scale, the number of decimal places in the resulting value. Returned value A value in the Nullable(Decimal(P,S)) data type. The value contains: Number with S decimal places, if ClickHouse interprets the input string as a number.NULL, if ClickHouse can’t interpret the input string as a number or if the input number contains more than S decimal places. Examples Query: SELECT toDecimal32OrNull(toString(-1.111), 5) AS val, toTypeName(val);  Result: ┌────val─┬─toTypeName(toDecimal32OrNull(toString(-1.111), 5))─┐ │ -1.111 │ Nullable(Decimal(9, 5)) │ └────────┴────────────────────────────────────────────────────┘  Query: SELECT toDecimal32OrNull(toString(-1.111), 2) AS val, toTypeName(val);  Result: ┌──val─┬─toTypeName(toDecimal32OrNull(toString(-1.111), 2))─┐ │ ᴺᵁᴸᴸ │ Nullable(Decimal(9, 2)) │ └──────┴────────────────────────────────────────────────────┘  "},{"title":"toDecimal(32|64|128|256)OrDefault​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todecimal3264128256ordefault","content":"Converts an input string to a Decimal(P,S) data type value. This family of functions include: toDecimal32OrDefault(expr, S) — Results in Decimal32(S) data type.toDecimal64OrDefault(expr, S) — Results in Decimal64(S) data type.toDecimal128OrDefault(expr, S) — Results in Decimal128(S) data type.toDecimal256OrDefault(expr, S) — Results in Decimal256(S) data type. These functions should be used instead of toDecimal*() functions, if you prefer to get a default value instead of an exception in the event of an input value parsing error. Arguments expr — Expression, returns a value in the String data type. ClickHouse expects the textual representation of the decimal number. For example, '1.111'.S — Scale, the number of decimal places in the resulting value. Returned value A value in the Decimal(P,S) data type. The value contains: Number with S decimal places, if ClickHouse interprets the input string as a number.Default Decimal(P,S) data type value, if ClickHouse can’t interpret the input string as a number or if the input number contains more than S decimal places. Examples Query: SELECT toDecimal32OrDefault(toString(-1.111), 5) AS val, toTypeName(val);  Result: ┌────val─┬─toTypeName(toDecimal32OrDefault(toString(-1.111), 5))─┐ │ -1.111 │ Decimal(9, 5) │ └────────┴───────────────────────────────────────────────────────┘  Query: SELECT toDecimal32OrDefault(toString(-1.111), 2) AS val, toTypeName(val);  Result: ┌─val─┬─toTypeName(toDecimal32OrDefault(toString(-1.111), 2))─┐ │ 0 │ Decimal(9, 2) │ └─────┴───────────────────────────────────────────────────────┘  "},{"title":"toDecimal(32|64|128|256)OrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#todecimal3264128256orzero","content":"Converts an input value to the Decimal(P,S) data type. This family of functions include: toDecimal32OrZero( expr, S) — Results in Decimal32(S) data type.toDecimal64OrZero( expr, S) — Results in Decimal64(S) data type.toDecimal128OrZero( expr, S) — Results in Decimal128(S) data type.toDecimal256OrZero( expr, S) — Results in Decimal256(S) data type. These functions should be used instead of toDecimal*() functions, if you prefer to get a 0 value instead of an exception in the event of an input value parsing error. Arguments expr — Expression, returns a value in the String data type. ClickHouse expects the textual representation of the decimal number. For example, '1.111'.S — Scale, the number of decimal places in the resulting value. Returned value A value in the Nullable(Decimal(P,S)) data type. The value contains: Number with S decimal places, if ClickHouse interprets the input string as a number.0 with S decimal places, if ClickHouse can’t interpret the input string as a number or if the input number contains more than S decimal places. Example Query: SELECT toDecimal32OrZero(toString(-1.111), 5) AS val, toTypeName(val);  Result: ┌────val─┬─toTypeName(toDecimal32OrZero(toString(-1.111), 5))─┐ │ -1.111 │ Decimal(9, 5) │ └────────┴────────────────────────────────────────────────────┘  Query: SELECT toDecimal32OrZero(toString(-1.111), 2) AS val, toTypeName(val);  Result: ┌──val─┬─toTypeName(toDecimal32OrZero(toString(-1.111), 2))─┐ │ 0.00 │ Decimal(9, 2) │ └──────┴────────────────────────────────────────────────────┘  "},{"title":"toString​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#tostring","content":"Functions for converting between numbers, strings (but not fixed strings), dates, and dates with times. All these functions accept one argument. When converting to or from a string, the value is formatted or parsed using the same rules as for the TabSeparated format (and almost all other text formats). If the string can’t be parsed, an exception is thrown and the request is canceled. When converting dates to numbers or vice versa, the date corresponds to the number of days since the beginning of the Unix epoch. When converting dates with times to numbers or vice versa, the date with time corresponds to the number of seconds since the beginning of the Unix epoch. The date and date-with-time formats for the toDate/toDateTime functions are defined as follows: YYYY-MM-DD YYYY-MM-DD hh:mm:ss  As an exception, if converting from UInt32, Int32, UInt64, or Int64 numeric types to Date, and if the number is greater than or equal to 65536, the number is interpreted as a Unix timestamp (and not as the number of days) and is rounded to the date. This allows support for the common occurrence of writing ‘toDate(unix_timestamp)’, which otherwise would be an error and would require writing the more cumbersome ‘toDate(toDateTime(unix_timestamp))’. Conversion between a date and date with time is performed the natural way: by adding a null time or dropping the time. Conversion between numeric types uses the same rules as assignments between different numeric types in C++. Additionally, the toString function of the DateTime argument can take a second String argument containing the name of the time zone. Example: Asia/Yekaterinburg In this case, the time is formatted according to the specified time zone. Example Query: SELECT now() AS now_local, toString(now(), 'Asia/Yekaterinburg') AS now_yekat;  Result: ┌───────────now_local─┬─now_yekat───────────┐ │ 2016-06-15 00:11:21 │ 2016-06-15 02:11:21 │ └─────────────────────┴─────────────────────┘  Also see the toUnixTimestamp function. "},{"title":"toFixedString(s, N)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#tofixedstrings-n","content":"Converts a String type argument to a FixedString(N) type (a string with fixed length N). N must be a constant. If the string has fewer bytes than N, it is padded with null bytes to the right. If the string has more bytes than N, an exception is thrown. "},{"title":"toStringCutToZero(s)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#tostringcuttozeros","content":"Accepts a String or FixedString argument. Returns the String with the content truncated at the first zero byte found. Example Query: SELECT toFixedString('foo', 8) AS s, toStringCutToZero(s) AS s_cut;  Result: ┌─s─────────────┬─s_cut─┐ │ foo\\0\\0\\0\\0\\0 │ foo │ └───────────────┴───────┘  Query: SELECT toFixedString('foo\\0bar', 8) AS s, toStringCutToZero(s) AS s_cut;  Result: ┌─s──────────┬─s_cut─┐ │ foo\\0bar\\0 │ foo │ └────────────┴───────┘  "},{"title":"reinterpretAsUInt(8|16|32|64)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#reinterpretasuint8163264","content":""},{"title":"reinterpretAsInt(8|16|32|64)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#reinterpretasint8163264","content":""},{"title":"reinterpretAsFloat(32|64)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#reinterpretasfloat3264","content":""},{"title":"reinterpretAsDate​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#reinterpretasdate","content":""},{"title":"reinterpretAsDateTime​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#reinterpretasdatetime","content":"These functions accept a string and interpret the bytes placed at the beginning of the string as a number in host order (little endian). If the string isn’t long enough, the functions work as if the string is padded with the necessary number of null bytes. If the string is longer than needed, the extra bytes are ignored. A date is interpreted as the number of days since the beginning of the Unix Epoch, and a date with time is interpreted as the number of seconds since the beginning of the Unix Epoch. "},{"title":"reinterpretAsString​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#type_conversion_functions-reinterpretAsString","content":"This function accepts a number or date or date with time, and returns a string containing bytes representing the corresponding value in host order (little endian). Null bytes are dropped from the end. For example, a UInt32 type value of 255 is a string that is one byte long. "},{"title":"reinterpretAsFixedString​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#reinterpretasfixedstring","content":"This function accepts a number or date or date with time, and returns a FixedString containing bytes representing the corresponding value in host order (little endian). Null bytes are dropped from the end. For example, a UInt32 type value of 255 is a FixedString that is one byte long. "},{"title":"reinterpretAsUUID​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#reinterpretasuuid","content":"Accepts 16 bytes string and returns UUID containing bytes representing the corresponding value in network byte order (big-endian). If the string isn't long enough, the function works as if the string is padded with the necessary number of null bytes to the end. If the string longer than 16 bytes, the extra bytes at the end are ignored. Syntax reinterpretAsUUID(fixed_string)  Arguments fixed_string — Big-endian byte string. FixedString. Returned value The UUID type value. UUID. Examples String to UUID. Query: SELECT reinterpretAsUUID(reverse(unhex('000102030405060708090a0b0c0d0e0f')));  Result: ┌─reinterpretAsUUID(reverse(unhex('000102030405060708090a0b0c0d0e0f')))─┐ │ 08090a0b-0c0d-0e0f-0001-020304050607 │ └───────────────────────────────────────────────────────────────────────┘  Going back and forth from String to UUID. Query: WITH generateUUIDv4() AS uuid, identity(lower(hex(reverse(reinterpretAsString(uuid))))) AS str, reinterpretAsUUID(reverse(unhex(str))) AS uuid2 SELECT uuid = uuid2;  Result: ┌─equals(uuid, uuid2)─┐ │ 1 │ └─────────────────────┘  "},{"title":"reinterpret(x, T)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#type_conversion_function-reinterpret","content":"Uses the same source in-memory bytes sequence for x value and reinterprets it to destination type. Syntax reinterpret(x, type)  Arguments x — Any type.type — Destination type. String. Returned value Destination type value. Examples Query: SELECT reinterpret(toInt8(-1), 'UInt8') as int_to_uint, reinterpret(toInt8(1), 'Float32') as int_to_float, reinterpret('1', 'UInt32') as string_to_int;  Result: ┌─int_to_uint─┬─int_to_float─┬─string_to_int─┐ │ 255 │ 1e-45 │ 49 │ └─────────────┴──────────────┴───────────────┘  "},{"title":"CAST(x, T)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#type_conversion_function-cast","content":"Converts an input value to the specified data type. Unlike the reinterpret function, CAST tries to present the same value using the new data type. If the conversion can not be done then an exception is raised. Several syntax variants are supported. Syntax CAST(x, T) CAST(x AS t) x::t  Arguments x — A value to convert. May be of any type.T — The name of the target data type. String.t — The target data type. Returned value Converted value. note If the input value does not fit the bounds of the target type, the result overflows. For example, CAST(-1, 'UInt8') returns 255. Examples Query: SELECT CAST(toInt8(-1), 'UInt8') AS cast_int_to_uint, CAST(1.5 AS Decimal(3,2)) AS cast_float_to_decimal, '1'::Int32 AS cast_string_to_int;  Result: ┌─cast_int_to_uint─┬─cast_float_to_decimal─┬─cast_string_to_int─┐ │ 255 │ 1.50 │ 1 │ └──────────────────┴───────────────────────┴────────────────────┘  Query: SELECT '2016-06-15 23:00:00' AS timestamp, CAST(timestamp AS DateTime) AS datetime, CAST(timestamp AS Date) AS date, CAST(timestamp, 'String') AS string, CAST(timestamp, 'FixedString(22)') AS fixed_string;  Result: ┌─timestamp───────────┬────────────datetime─┬───────date─┬─string──────────────┬─fixed_string──────────────┐ │ 2016-06-15 23:00:00 │ 2016-06-15 23:00:00 │ 2016-06-15 │ 2016-06-15 23:00:00 │ 2016-06-15 23:00:00\\0\\0\\0 │ └─────────────────────┴─────────────────────┴────────────┴─────────────────────┴───────────────────────────┘  Conversion to FixedString(N) only works for arguments of type String or FixedString. Type conversion to Nullable and back is supported. Example Query: SELECT toTypeName(x) FROM t_null;  Result: ┌─toTypeName(x)─┐ │ Int8 │ │ Int8 │ └───────────────┘  Query: SELECT toTypeName(CAST(x, 'Nullable(UInt16)')) FROM t_null;  Result: ┌─toTypeName(CAST(x, 'Nullable(UInt16)'))─┐ │ Nullable(UInt16) │ │ Nullable(UInt16) │ └─────────────────────────────────────────┘  See also cast_keep_nullable setting "},{"title":"accurateCast(x, T)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#type_conversion_function-accurate-cast","content":"Converts x to the T data type. The difference from cast(x, T) is that accurateCast does not allow overflow of numeric types during cast if type value x does not fit the bounds of type T. For example, accurateCast(-1, 'UInt8') throws an exception. Example Query: SELECT cast(-1, 'UInt8') as uint8;  Result: ┌─uint8─┐ │ 255 │ └───────┘  Query: SELECT accurateCast(-1, 'UInt8') as uint8;  Result: Code: 70. DB::Exception: Received from localhost:9000. DB::Exception: Value in column Int8 cannot be safely converted into type UInt8: While processing accurateCast(-1, 'UInt8') AS uint8.  "},{"title":"accurateCastOrNull(x, T)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#type_conversion_function-accurate-cast_or_null","content":"Converts input value x to the specified data type T. Always returns Nullable type and returns NULL if the casted value is not representable in the target type. Syntax accurateCastOrNull(x, T)  Parameters x — Input value.T — The name of the returned data type. Returned value The value, converted to the specified data type T. Example Query: SELECT toTypeName(accurateCastOrNull(5, 'UInt8'));  Result: ┌─toTypeName(accurateCastOrNull(5, 'UInt8'))─┐ │ Nullable(UInt8) │ └────────────────────────────────────────────┘  Query: SELECT accurateCastOrNull(-1, 'UInt8') as uint8, accurateCastOrNull(128, 'Int8') as int8, accurateCastOrNull('Test', 'FixedString(2)') as fixed_string;  Result: ┌─uint8─┬─int8─┬─fixed_string─┐ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ ᴺᵁᴸᴸ │ └───────┴──────┴──────────────┘  "},{"title":"accurateCastOrDefault(x, T[, default_value])​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#type_conversion_function-accurate-cast_or_default","content":"Converts input value x to the specified data type T. Returns default type value or default_value if specified if the casted value is not representable in the target type. Syntax accurateCastOrDefault(x, T)  Parameters x — Input value.T — The name of the returned data type.default_value — Default value of returned data type. Returned value The value converted to the specified data type T. Example Query: SELECT toTypeName(accurateCastOrDefault(5, 'UInt8'));  Result: ┌─toTypeName(accurateCastOrDefault(5, 'UInt8'))─┐ │ UInt8 │ └───────────────────────────────────────────────┘  Query: SELECT accurateCastOrDefault(-1, 'UInt8') as uint8, accurateCastOrDefault(-1, 'UInt8', 5) as uint8_default, accurateCastOrDefault(128, 'Int8') as int8, accurateCastOrDefault(128, 'Int8', 5) as int8_default, accurateCastOrDefault('Test', 'FixedString(2)') as fixed_string, accurateCastOrDefault('Test', 'FixedString(2)', 'Te') as fixed_string_default;  Result: ┌─uint8─┬─uint8_default─┬─int8─┬─int8_default─┬─fixed_string─┬─fixed_string_default─┐ │ 0 │ 5 │ 0 │ 5 │ │ Te │ └───────┴───────────────┴──────┴──────────────┴──────────────┴──────────────────────┘  "},{"title":"toInterval(Year|Quarter|Month|Week|Day|Hour|Minute|Second)​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#function-tointerval","content":"Converts a Number type argument to an Interval data type. Syntax toIntervalSecond(number) toIntervalMinute(number) toIntervalHour(number) toIntervalDay(number) toIntervalWeek(number) toIntervalMonth(number) toIntervalQuarter(number) toIntervalYear(number)  Arguments number — Duration of interval. Positive integer number. Returned values The value in Interval data type. Example Query: WITH toDate('2019-01-01') AS date, INTERVAL 1 WEEK AS interval_week, toIntervalWeek(1) AS interval_to_week SELECT date + interval_week, date + interval_to_week;  Result: ┌─plus(date, interval_week)─┬─plus(date, interval_to_week)─┐ │ 2019-01-08 │ 2019-01-08 │ └───────────────────────────┴──────────────────────────────┘  "},{"title":"parseDateTimeBestEffort​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#parsedatetimebesteffort","content":""},{"title":"parseDateTime32BestEffort​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#parsedatetime32besteffort","content":"Converts a date and time in the String representation to DateTime data type. The function parses ISO 8601, RFC 1123 - 5.2.14 RFC-822 Date and Time Specification, ClickHouse’s and some other date and time formats. Syntax parseDateTimeBestEffort(time_string [, time_zone])  Arguments time_string — String containing a date and time to convert. String.time_zone — Time zone. The function parses time_string according to the time zone. String. Supported non-standard formats A string containing 9..10 digit unix timestamp.A string with a date and a time component: YYYYMMDDhhmmss, DD/MM/YYYY hh:mm:ss, DD-MM-YY hh:mm, YYYY-MM-DD hh:mm:ss, etc.A string with a date, but no time component: YYYY, YYYYMM, YYYY*MM, DD/MM/YYYY, DD-MM-YY etc.A string with a day and time: DD, DD hh, DD hh:mm. In this case YYYY-MM are substituted as 2000-01.A string that includes the date and time along with time zone offset information: YYYY-MM-DD hh:mm:ss ±h:mm, etc. For example, 2020-12-12 17:36:00 -5:00. For all of the formats with separator the function parses months names expressed by their full name or by the first three letters of a month name. Examples: 24/DEC/18, 24-Dec-18, 01-September-2018. Returned value time_string converted to the DateTime data type. Examples Query: SELECT parseDateTimeBestEffort('12/12/2020 12:12:57') AS parseDateTimeBestEffort;  Result: ┌─parseDateTimeBestEffort─┐ │ 2020-12-12 12:12:57 │ └─────────────────────────┘  Query: SELECT parseDateTimeBestEffort('Sat, 18 Aug 2018 07:22:16 GMT', 'Asia/Istanbul') AS parseDateTimeBestEffort;  Result: ┌─parseDateTimeBestEffort─┐ │ 2018-08-18 10:22:16 │ └─────────────────────────┘  Query: SELECT parseDateTimeBestEffort('1284101485') AS parseDateTimeBestEffort;  Result: ┌─parseDateTimeBestEffort─┐ │ 2015-07-07 12:04:41 │ └─────────────────────────┘  Query: SELECT parseDateTimeBestEffort('2018-12-12 10:12:12') AS parseDateTimeBestEffort;  Result: ┌─parseDateTimeBestEffort─┐ │ 2018-12-12 10:12:12 │ └─────────────────────────┘  Query: SELECT parseDateTimeBestEffort('10 20:19');  Result: ┌─parseDateTimeBestEffort('10 20:19')─┐ │ 2000-01-10 20:19:00 │ └─────────────────────────────────────┘  See Also ISO 8601 announcement by @xkcdRFC 1123toDatetoDateTime "},{"title":"parseDateTimeBestEffortUS​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#parsedatetimebesteffortUS","content":"This function is similar to parseDateTimeBestEffort, the only difference is that this function prefers US date format (MM/DD/YYYY etc.) in case of ambiguity. Syntax parseDateTimeBestEffortUS(time_string [, time_zone])  Arguments time_string — String containing a date and time to convert. String.time_zone — Time zone. The function parses time_string according to the time zone. String. Supported non-standard formats A string containing 9..10 digit unix timestamp.A string with a date and a time component: YYYYMMDDhhmmss, MM/DD/YYYY hh:mm:ss, MM-DD-YY hh:mm, YYYY-MM-DD hh:mm:ss, etc.A string with a date, but no time component: YYYY, YYYYMM, YYYY*MM, MM/DD/YYYY, MM-DD-YY etc.A string with a day and time: DD, DD hh, DD hh:mm. In this case, YYYY-MM are substituted as 2000-01.A string that includes the date and time along with time zone offset information: YYYY-MM-DD hh:mm:ss ±h:mm, etc. For example, 2020-12-12 17:36:00 -5:00. Returned value time_string converted to the DateTime data type. Examples Query: SELECT parseDateTimeBestEffortUS('09/12/2020 12:12:57') AS parseDateTimeBestEffortUS;  Result: ┌─parseDateTimeBestEffortUS─┐ │ 2020-09-12 12:12:57 │ └─────────────────────────——┘  Query: SELECT parseDateTimeBestEffortUS('09-12-2020 12:12:57') AS parseDateTimeBestEffortUS;  Result: ┌─parseDateTimeBestEffortUS─┐ │ 2020-09-12 12:12:57 │ └─────────────────────────——┘  Query: SELECT parseDateTimeBestEffortUS('09.12.2020 12:12:57') AS parseDateTimeBestEffortUS;  Result: ┌─parseDateTimeBestEffortUS─┐ │ 2020-09-12 12:12:57 │ └─────────────────────────——┘  "},{"title":"parseDateTimeBestEffortOrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#parsedatetimebesteffortornull","content":""},{"title":"parseDateTime32BestEffortOrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#parsedatetime32besteffortornull","content":"Same as for parseDateTimeBestEffort except that it returns NULL when it encounters a date format that cannot be processed. "},{"title":"parseDateTimeBestEffortOrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#parsedatetimebesteffortorzero","content":""},{"title":"parseDateTime32BestEffortOrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#parsedatetime32besteffortorzero","content":"Same as for parseDateTimeBestEffort except that it returns zero date or zero date time when it encounters a date format that cannot be processed. "},{"title":"parseDateTimeBestEffortUSOrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#parsedatetimebesteffortusornull","content":"Same as parseDateTimeBestEffortUS function except that it returns NULL when it encounters a date format that cannot be processed. Syntax parseDateTimeBestEffortUSOrNull(time_string[, time_zone])  Parameters time_string — String containing a date or date with time to convert. The date must be in the US date format (MM/DD/YYYY, etc). String.time_zone — Timezone. The function parses time_string according to the timezone. Optional. String. Supported non-standard formats A string containing 9..10 digit unix timestamp.A string with a date and a time components: YYYYMMDDhhmmss, MM/DD/YYYY hh:mm:ss, MM-DD-YY hh:mm, YYYY-MM-DD hh:mm:ss, etc.A string with a date, but no time component: YYYY, YYYYMM, YYYY*MM, MM/DD/YYYY, MM-DD-YY, etc.A string with a day and time: DD, DD hh, DD hh:mm. In this case, YYYY-MM are substituted with 2000-01.A string that includes date and time along with timezone offset information: YYYY-MM-DD hh:mm:ss ±h:mm, etc. For example, 2020-12-12 17:36:00 -5:00. Returned values time_string converted to the DateTime data type.NULL if the input string cannot be converted to the DateTime data type. Examples Query: SELECT parseDateTimeBestEffortUSOrNull('02/10/2021 21:12:57') AS parseDateTimeBestEffortUSOrNull;  Result: ┌─parseDateTimeBestEffortUSOrNull─┐ │ 2021-02-10 21:12:57 │ └─────────────────────────────────┘  Query: SELECT parseDateTimeBestEffortUSOrNull('02-10-2021 21:12:57 GMT', 'Asia/Istanbul') AS parseDateTimeBestEffortUSOrNull;  Result: ┌─parseDateTimeBestEffortUSOrNull─┐ │ 2021-02-11 00:12:57 │ └─────────────────────────────────┘  Query: SELECT parseDateTimeBestEffortUSOrNull('02.10.2021') AS parseDateTimeBestEffortUSOrNull;  Result: ┌─parseDateTimeBestEffortUSOrNull─┐ │ 2021-02-10 00:00:00 │ └─────────────────────────────────┘  Query: SELECT parseDateTimeBestEffortUSOrNull('10.2021') AS parseDateTimeBestEffortUSOrNull;  Result: ┌─parseDateTimeBestEffortUSOrNull─┐ │ ᴺᵁᴸᴸ │ └─────────────────────────────────┘  "},{"title":"parseDateTimeBestEffortUSOrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#parsedatetimebesteffortusorzero","content":"Same as parseDateTimeBestEffortUS function except that it returns zero date (1970-01-01) or zero date with time (1970-01-01 00:00:00) when it encounters a date format that cannot be processed. Syntax parseDateTimeBestEffortUSOrZero(time_string[, time_zone])  Parameters time_string — String containing a date or date with time to convert. The date must be in the US date format (MM/DD/YYYY, etc). String.time_zone — Timezone. The function parses time_string according to the timezone. Optional. String. Supported non-standard formats A string containing 9..10 digit unix timestamp.A string with a date and a time components: YYYYMMDDhhmmss, MM/DD/YYYY hh:mm:ss, MM-DD-YY hh:mm, YYYY-MM-DD hh:mm:ss, etc.A string with a date, but no time component: YYYY, YYYYMM, YYYY*MM, MM/DD/YYYY, MM-DD-YY, etc.A string with a day and time: DD, DD hh, DD hh:mm. In this case, YYYY-MM are substituted with 2000-01.A string that includes date and time along with timezone offset information: YYYY-MM-DD hh:mm:ss ±h:mm, etc. For example, 2020-12-12 17:36:00 -5:00. Returned values time_string converted to the DateTime data type.Zero date or zero date with time if the input string cannot be converted to the DateTime data type. Examples Query: SELECT parseDateTimeBestEffortUSOrZero('02/10/2021 21:12:57') AS parseDateTimeBestEffortUSOrZero;  Result: ┌─parseDateTimeBestEffortUSOrZero─┐ │ 2021-02-10 21:12:57 │ └─────────────────────────────────┘  Query: SELECT parseDateTimeBestEffortUSOrZero('02-10-2021 21:12:57 GMT', 'Asia/Istanbul') AS parseDateTimeBestEffortUSOrZero;  Result: ┌─parseDateTimeBestEffortUSOrZero─┐ │ 2021-02-11 00:12:57 │ └─────────────────────────────────┘  Query: SELECT parseDateTimeBestEffortUSOrZero('02.10.2021') AS parseDateTimeBestEffortUSOrZero;  Result: ┌─parseDateTimeBestEffortUSOrZero─┐ │ 2021-02-10 00:00:00 │ └─────────────────────────────────┘  Query: SELECT parseDateTimeBestEffortUSOrZero('02.2021') AS parseDateTimeBestEffortUSOrZero;  Result: ┌─parseDateTimeBestEffortUSOrZero─┐ │ 1970-01-01 00:00:00 │ └─────────────────────────────────┘  "},{"title":"parseDateTime64BestEffort​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#parsedatetime64besteffort","content":"Same as parseDateTimeBestEffort function but also parse milliseconds and microseconds and returns DateTime data type. Syntax parseDateTime64BestEffort(time_string [, precision [, time_zone]])  Parameters time_string — String containing a date or date with time to convert. String.precision — Required precision. 3 — for milliseconds, 6 — for microseconds. Default — 3. Optional. UInt8.time_zone — Timezone. The function parses time_string according to the timezone. Optional. String. Returned value time_string converted to the DateTime data type. Examples Query: SELECT parseDateTime64BestEffort('2021-01-01') AS a, toTypeName(a) AS t UNION ALL SELECT parseDateTime64BestEffort('2021-01-01 01:01:00.12346') AS a, toTypeName(a) AS t UNION ALL SELECT parseDateTime64BestEffort('2021-01-01 01:01:00.12346',6) AS a, toTypeName(a) AS t UNION ALL SELECT parseDateTime64BestEffort('2021-01-01 01:01:00.12346',3,'Asia/Istanbul') AS a, toTypeName(a) AS t FORMAT PrettyCompactMonoBlock;  Result: ┌──────────────────────────a─┬─t──────────────────────────────┐ │ 2021-01-01 01:01:00.123000 │ DateTime64(3) │ │ 2021-01-01 00:00:00.000000 │ DateTime64(3) │ │ 2021-01-01 01:01:00.123460 │ DateTime64(6) │ │ 2020-12-31 22:01:00.123000 │ DateTime64(3, 'Asia/Istanbul') │ └────────────────────────────┴────────────────────────────────┘  "},{"title":"parseDateTime64BestEffortOrNull​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#parsedatetime32besteffortornull","content":"Same as for parseDateTime64BestEffort except that it returns NULL when it encounters a date format that cannot be processed. "},{"title":"parseDateTime64BestEffortOrZero​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#parsedatetime64besteffortorzero","content":"Same as for parseDateTime64BestEffort except that it returns zero date or zero date time when it encounters a date format that cannot be processed. "},{"title":"toLowCardinality​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#tolowcardinality","content":"Converts input parameter to the LowCardianlity version of same data type. To convert data from the LowCardinality data type use the CAST function. For example, CAST(x as String). Syntax toLowCardinality(expr)  Arguments expr — Expression resulting in one of the supported data types. Returned values Result of expr. Type: LowCardinality(expr_result_type) Example Query: SELECT toLowCardinality('1');  Result: ┌─toLowCardinality('1')─┐ │ 1 │ └───────────────────────┘  "},{"title":"toUnixTimestamp64Milli​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#tounixtimestamp64milli","content":""},{"title":"toUnixTimestamp64Micro​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#tounixtimestamp64micro","content":""},{"title":"toUnixTimestamp64Nano​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#tounixtimestamp64nano","content":"Converts a DateTime64 to a Int64 value with fixed sub-second precision. Input value is scaled up or down appropriately depending on it precision. note The output value is a timestamp in UTC, not in the timezone of DateTime64. Syntax toUnixTimestamp64Milli(value)  Arguments value — DateTime64 value with any precision. Returned value value converted to the Int64 data type. Examples Query: WITH toDateTime64('2019-09-16 19:20:12.345678910', 6) AS dt64 SELECT toUnixTimestamp64Milli(dt64);  Result: ┌─toUnixTimestamp64Milli(dt64)─┐ │ 1568650812345 │ └──────────────────────────────┘  Query: WITH toDateTime64('2019-09-16 19:20:12.345678910', 6) AS dt64 SELECT toUnixTimestamp64Nano(dt64);  Result: ┌─toUnixTimestamp64Nano(dt64)─┐ │ 1568650812345678000 │ └─────────────────────────────┘  "},{"title":"fromUnixTimestamp64Milli​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#fromunixtimestamp64milli","content":""},{"title":"fromUnixTimestamp64Micro​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#fromunixtimestamp64micro","content":""},{"title":"fromUnixTimestamp64Nano​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#fromunixtimestamp64nano","content":"Converts an Int64 to a DateTime64 value with fixed sub-second precision and optional timezone. Input value is scaled up or down appropriately depending on it’s precision. Please note that input value is treated as UTC timestamp, not timestamp at given (or implicit) timezone. Syntax fromUnixTimestamp64Milli(value [, ti])  Arguments value — Int64 value with any precision.timezone — String (optional) timezone name of the result. Returned value value converted to the DateTime64 data type. Example Query: WITH CAST(1234567891011, 'Int64') AS i64 SELECT fromUnixTimestamp64Milli(i64, 'UTC');  Result: ┌─fromUnixTimestamp64Milli(i64, 'UTC')─┐ │ 2009-02-13 23:31:31.011 │ └──────────────────────────────────────┘  "},{"title":"formatRow​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#formatrow","content":"Converts arbitrary expressions into a string via given format. Syntax formatRow(format, x, y, ...)  Arguments format — Text format. For example, CSV, TSV.x,y, ... — Expressions. Returned value A formatted string (for text formats it's usually terminated with the new line character). Example Query: SELECT formatRow('CSV', number, 'good') FROM numbers(3);  Result: ┌─formatRow('CSV', number, 'good')─┐ │ 0,&quot;good&quot; │ │ 1,&quot;good&quot; │ │ 2,&quot;good&quot; │ └──────────────────────────────────┘  "},{"title":"formatRowNoNewline​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#formatrownonewline","content":"Converts arbitrary expressions into a string via given format. The function trims the last \\n if any. Syntax formatRowNoNewline(format, x, y, ...)  Arguments format — Text format. For example, CSV, TSV.x,y, ... — Expressions. Returned value A formatted string. Example Query: SELECT formatRowNoNewline('CSV', number, 'good') FROM numbers(3);  Result: ┌─formatRowNoNewline('CSV', number, 'good')─┐ │ 0,&quot;good&quot; │ │ 1,&quot;good&quot; │ │ 2,&quot;good&quot; │ └───────────────────────────────────────────┘  "},{"title":"snowflakeToDateTime​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#snowflaketodatetime","content":"Extracts time from Snowflake ID as DateTime format. Syntax snowflakeToDateTime(value [, time_zone])  Parameters value — Snowflake ID. Int64.time_zone — Timezone. The function parses time_string according to the timezone. Optional. String. Returned value Input value converted to the DateTime data type. Example Query: SELECT snowflakeToDateTime(CAST('1426860702823350272', 'Int64'), 'UTC');  Result:  ┌─snowflakeToDateTime(CAST('1426860702823350272', 'Int64'), 'UTC')─┐ │ 2021-08-15 10:57:56 │ └──────────────────────────────────────────────────────────────────┘  "},{"title":"snowflakeToDateTime64​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#snowflaketodatetime64","content":"Extracts time from Snowflake ID as DateTime64 format. Syntax snowflakeToDateTime64(value [, time_zone])  Parameters value — Snowflake ID. Int64.time_zone — Timezone. The function parses time_string according to the timezone. Optional. String. Returned value Input value converted to the DateTime64 data type. Example Query: SELECT snowflakeToDateTime64(CAST('1426860802823350272', 'Int64'), 'UTC');  Result:  ┌─snowflakeToDateTime64(CAST('1426860802823350272', 'Int64'), 'UTC')─┐ │ 2021-08-15 10:58:19.841 │ └────────────────────────────────────────────────────────────────────┘  "},{"title":"dateTimeToSnowflake​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#datetimetosnowflake","content":"Converts DateTime value to the first Snowflake ID at the giving time. Syntax dateTimeToSnowflake(value)  Parameters value — Date and time. DateTime. Returned value Input value converted to the Int64 data type as the first Snowflake ID at that time. Example Query: WITH toDateTime('2021-08-15 18:57:56', 'Asia/Shanghai') AS dt SELECT dateTimeToSnowflake(dt);  Result: ┌─dateTimeToSnowflake(dt)─┐ │ 1426860702823350272 │ └─────────────────────────┘  "},{"title":"dateTime64ToSnowflake​","type":1,"pageTitle":"Type Conversion Functions","url":"en/sql-reference/functions/type-conversion-functions#datetime64tosnowflake","content":"Convert DateTime64 to the first Snowflake ID at the giving time. Syntax dateTime64ToSnowflake(value)  Parameters value — Date and time. DateTime64. Returned value Input value converted to the Int64 data type as the first Snowflake ID at that time. Example Query: WITH toDateTime64('2021-08-15 18:57:56.492', 3, 'Asia/Shanghai') AS dt64 SELECT dateTime64ToSnowflake(dt64);  Result: ┌─dateTime64ToSnowflake(dt64)─┐ │ 1426860704886947840 │ └─────────────────────────────┘  "},{"title":"Functions for Working with URLs","type":0,"sectionRef":"#","url":"en/sql-reference/functions/url-functions","content":"","keywords":""},{"title":"Functions that Extract Parts of a URL​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#functions-that-extract-parts-of-a-url","content":"If the relevant part isn’t present in a URL, an empty string is returned. "},{"title":"protocol​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#protocol","content":"Extracts the protocol from a URL. Examples of typical returned values: http, https, ftp, mailto, tel, magnet… "},{"title":"domain​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#domain","content":"Extracts the hostname from a URL. domain(url)  Arguments url — URL. Type: String. The URL can be specified with or without a scheme. Examples: svn+ssh://some.svn-hosting.com:80/repo/trunk some.svn-hosting.com:80/repo/trunk https://clickhouse.com/time/  For these examples, the domain function returns the following results: some.svn-hosting.com some.svn-hosting.com clickhouse.com  Returned values Host name. If ClickHouse can parse the input string as a URL.Empty string. If ClickHouse can’t parse the input string as a URL. Type: String. Example SELECT domain('svn+ssh://some.svn-hosting.com:80/repo/trunk');  ┌─domain('svn+ssh://some.svn-hosting.com:80/repo/trunk')─┐ │ some.svn-hosting.com │ └────────────────────────────────────────────────────────┘  "},{"title":"domainWithoutWWW​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#domainwithoutwww","content":"Returns the domain and removes no more than one ‘www.’ from the beginning of it, if present. "},{"title":"topLevelDomain​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#topleveldomain","content":"Extracts the the top-level domain from a URL. topLevelDomain(url)  Arguments url — URL. Type: String. The URL can be specified with or without a scheme. Examples: svn+ssh://some.svn-hosting.com:80/repo/trunk some.svn-hosting.com:80/repo/trunk https://clickhouse.com/time/  Returned values Domain name. If ClickHouse can parse the input string as a URL.Empty string. If ClickHouse cannot parse the input string as a URL. Type: String. Example SELECT topLevelDomain('svn+ssh://www.some.svn-hosting.com:80/repo/trunk');  ┌─topLevelDomain('svn+ssh://www.some.svn-hosting.com:80/repo/trunk')─┐ │ com │ └────────────────────────────────────────────────────────────────────┘  "},{"title":"firstSignificantSubdomain​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#firstsignificantsubdomain","content":"Returns the “first significant subdomain”. The first significant subdomain is a second-level domain if it is ‘com’, ‘net’, ‘org’, or ‘co’. Otherwise, it is a third-level domain. For example, firstSignificantSubdomain (‘https://news.clickhouse.com/’) = ‘clickhouse’, firstSignificantSubdomain (‘https://news.clickhouse.com.tr/’) = ‘clickhouse’. The list of “insignificant” second-level domains and other implementation details may change in the future. "},{"title":"cutToFirstSignificantSubdomain​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#cuttofirstsignificantsubdomain","content":"Returns the part of the domain that includes top-level subdomains up to the “first significant subdomain” (see the explanation above). For example: cutToFirstSignificantSubdomain('https://news.clickhouse.com.tr/') = 'clickhouse.com.tr'.cutToFirstSignificantSubdomain('www.tr') = 'tr'.cutToFirstSignificantSubdomain('tr') = ''. "},{"title":"cutToFirstSignificantSubdomainWithWWW​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#cuttofirstsignificantsubdomainwithwww","content":"Returns the part of the domain that includes top-level subdomains up to the “first significant subdomain”, without stripping &quot;www&quot;. For example: cutToFirstSignificantSubdomain('https://news.clickhouse.com.tr/') = 'clickhouse.com.tr'.cutToFirstSignificantSubdomain('www.tr') = 'www.tr'.cutToFirstSignificantSubdomain('tr') = ''. "},{"title":"cutToFirstSignificantSubdomainCustom​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#cuttofirstsignificantsubdomaincustom","content":"Returns the part of the domain that includes top-level subdomains up to the first significant subdomain. Accepts custom TLD list name. Can be useful if you need fresh TLD list or you have custom. Configuration example: &lt;!-- &lt;top_level_domains_path&gt;/var/lib/clickhouse/top_level_domains/&lt;/top_level_domains_path&gt; --&gt; &lt;top_level_domains_lists&gt; &lt;!-- https://publicsuffix.org/list/public_suffix_list.dat --&gt; &lt;public_suffix_list&gt;public_suffix_list.dat&lt;/public_suffix_list&gt; &lt;!-- NOTE: path is under top_level_domains_path --&gt; &lt;/top_level_domains_lists&gt;  Syntax cutToFirstSignificantSubdomain(URL, TLD)  Parameters URL — URL. String.TLD — Custom TLD list name. String. Returned value Part of the domain that includes top-level subdomains up to the first significant subdomain. Type: String. Example Query: SELECT cutToFirstSignificantSubdomainCustom('bar.foo.there-is-no-such-domain', 'public_suffix_list');  Result: ┌─cutToFirstSignificantSubdomainCustom('bar.foo.there-is-no-such-domain', 'public_suffix_list')─┐ │ foo.there-is-no-such-domain │ └───────────────────────────────────────────────────────────────────────────────────────────────┘  See Also firstSignificantSubdomain. "},{"title":"cutToFirstSignificantSubdomainCustomWithWWW​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#cuttofirstsignificantsubdomaincustomwithwww","content":"Returns the part of the domain that includes top-level subdomains up to the first significant subdomain without stripping www. Accepts custom TLD list name. Can be useful if you need fresh TLD list or you have custom. Configuration example: &lt;!-- &lt;top_level_domains_path&gt;/var/lib/clickhouse/top_level_domains/&lt;/top_level_domains_path&gt; --&gt; &lt;top_level_domains_lists&gt; &lt;!-- https://publicsuffix.org/list/public_suffix_list.dat --&gt; &lt;public_suffix_list&gt;public_suffix_list.dat&lt;/public_suffix_list&gt; &lt;!-- NOTE: path is under top_level_domains_path --&gt; &lt;/top_level_domains_lists&gt;  Syntax cutToFirstSignificantSubdomainCustomWithWWW(URL, TLD)  Parameters URL — URL. String.TLD — Custom TLD list name. String. Returned value Part of the domain that includes top-level subdomains up to the first significant subdomain without stripping www. Type: String. Example Query: SELECT cutToFirstSignificantSubdomainCustomWithWWW('www.foo', 'public_suffix_list');  Result: ┌─cutToFirstSignificantSubdomainCustomWithWWW('www.foo', 'public_suffix_list')─┐ │ www.foo │ └──────────────────────────────────────────────────────────────────────────────┘  See Also firstSignificantSubdomain. "},{"title":"firstSignificantSubdomainCustom​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#firstsignificantsubdomaincustom","content":"Returns the first significant subdomain. Accepts customs TLD list name. Can be useful if you need fresh TLD list or you have custom. Configuration example: &lt;!-- &lt;top_level_domains_path&gt;/var/lib/clickhouse/top_level_domains/&lt;/top_level_domains_path&gt; --&gt; &lt;top_level_domains_lists&gt; &lt;!-- https://publicsuffix.org/list/public_suffix_list.dat --&gt; &lt;public_suffix_list&gt;public_suffix_list.dat&lt;/public_suffix_list&gt; &lt;!-- NOTE: path is under top_level_domains_path --&gt; &lt;/top_level_domains_lists&gt;  Syntax firstSignificantSubdomainCustom(URL, TLD)  Parameters URL — URL. String.TLD — Custom TLD list name. String. Returned value First significant subdomain. Type: String. Example Query: SELECT firstSignificantSubdomainCustom('bar.foo.there-is-no-such-domain', 'public_suffix_list');  Result: ┌─firstSignificantSubdomainCustom('bar.foo.there-is-no-such-domain', 'public_suffix_list')─┐ │ foo │ └──────────────────────────────────────────────────────────────────────────────────────────┘  See Also firstSignificantSubdomain. "},{"title":"port(URL[, default_port = 0])​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#port","content":"Returns the port or default_port if there is no port in the URL (or in case of validation error). "},{"title":"path​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#path","content":"Returns the path. Example: /top/news.html The path does not include the query string. "},{"title":"pathFull​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#pathfull","content":"The same as above, but including query string and fragment. Example: /top/news.html?page=2#comments "},{"title":"queryString​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#querystring","content":"Returns the query string. Example: page=1&amp;lr=213. query-string does not include the initial question mark, as well as # and everything after #. "},{"title":"fragment​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#fragment","content":"Returns the fragment identifier. fragment does not include the initial hash symbol. "},{"title":"queryStringAndFragment​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#querystringandfragment","content":"Returns the query string and fragment identifier. Example: page=1#29390. "},{"title":"extractURLParameter(URL, name)​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#extracturlparameterurl-name","content":"Returns the value of the ‘name’ parameter in the URL, if present. Otherwise, an empty string. If there are many parameters with this name, it returns the first occurrence. This function works under the assumption that the parameter name is encoded in the URL exactly the same way as in the passed argument. "},{"title":"extractURLParameters(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#extracturlparametersurl","content":"Returns an array of name=value strings corresponding to the URL parameters. The values are not decoded in any way. "},{"title":"extractURLParameterNames(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#extracturlparameternamesurl","content":"Returns an array of name strings corresponding to the names of URL parameters. The values are not decoded in any way. "},{"title":"URLHierarchy(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#urlhierarchyurl","content":"Returns an array containing the URL, truncated at the end by the symbols /,? in the path and query-string. Consecutive separator characters are counted as one. The cut is made in the position after all the consecutive separator characters. "},{"title":"URLPathHierarchy(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#urlpathhierarchyurl","content":"The same as above, but without the protocol and host in the result. The / element (root) is not included. URLPathHierarchy('https://example.com/browse/CONV-6788') = [ '/browse/', '/browse/CONV-6788' ]  "},{"title":"encodeURLComponent(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#encodeurlcomponenturl","content":"Returns the encoded URL. Example: SELECT encodeURLComponent('http://127.0.0.1:8123/?query=SELECT 1;') AS EncodedURL;  ┌─EncodedURL───────────────────────────────────────────────┐ │ http%3A%2F%2F127.0.0.1%3A8123%2F%3Fquery%3DSELECT%201%3B │ └──────────────────────────────────────────────────────────┘  "},{"title":"decodeURLComponent(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#decodeurlcomponenturl","content":"Returns the decoded URL. Example: SELECT decodeURLComponent('http://127.0.0.1:8123/?query=SELECT%201%3B') AS DecodedURL;  ┌─DecodedURL─────────────────────────────┐ │ http://127.0.0.1:8123/?query=SELECT 1; │ └────────────────────────────────────────┘  "},{"title":"encodeURLFormComponent(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#encodeurlformcomponenturl","content":"Returns the encoded URL. Follows rfc-1866, space( ) is encoded as plus(+). Example: SELECT encodeURLFormComponent('http://127.0.0.1:8123/?query=SELECT 1 2+3') AS EncodedURL;  ┌─EncodedURL────────────────────────────────────────────────┐ │ http%3A%2F%2F127.0.0.1%3A8123%2F%3Fquery%3DSELECT+1+2%2B3 │ └───────────────────────────────────────────────────────────┘  "},{"title":"decodeURLFormComponent(URL)​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#decodeurlformcomponenturl","content":"Returns the decoded URL. Follows rfc-1866, plain plus(+) is decoded as space( ). Example: SELECT decodeURLFormComponent('http://127.0.0.1:8123/?query=SELECT%201+2%2B3') AS DecodedURL;  ┌─DecodedURL────────────────────────────────┐ │ http://127.0.0.1:8123/?query=SELECT 1 2+3 │ └───────────────────────────────────────────┘  "},{"title":"netloc​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#netloc","content":"Extracts network locality (username:password@host:port) from a URL. Syntax netloc(URL)  Arguments url — URL. String. Returned value username:password@host:port. Type: String. Example Query: SELECT netloc('http://paul@www.example.com:80/');  Result: ┌─netloc('http://paul@www.example.com:80/')─┐ │ paul@www.example.com:80 │ └───────────────────────────────────────────┘  "},{"title":"Functions that Remove Part of a URL​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#functions-that-remove-part-of-a-url","content":"If the URL does not have anything similar, the URL remains unchanged. "},{"title":"cutWWW​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#cutwww","content":"Removes no more than one ‘www.’ from the beginning of the URL’s domain, if present. "},{"title":"cutQueryString​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#cutquerystring","content":"Removes query string. The question mark is also removed. "},{"title":"cutFragment​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#cutfragment","content":"Removes the fragment identifier. The number sign is also removed. "},{"title":"cutQueryStringAndFragment​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#cutquerystringandfragment","content":"Removes the query string and fragment identifier. The question mark and number sign are also removed. "},{"title":"cutURLParameter(URL, name)​","type":1,"pageTitle":"Functions for Working with URLs","url":"en/sql-reference/functions/url-functions#cuturlparameterurl-name","content":"Removes the ‘name’ URL parameter, if present. This function works under the assumption that the parameter name is encoded in the URL exactly the same way as in the passed argument. "},{"title":"Functions for Working with UUID","type":0,"sectionRef":"#","url":"en/sql-reference/functions/uuid-functions","content":"","keywords":""},{"title":"generateUUIDv4​","type":1,"pageTitle":"Functions for Working with UUID","url":"en/sql-reference/functions/uuid-functions#uuid-function-generate","content":"Generates the UUID of version 4. generateUUIDv4()  Returned value The UUID type value. Usage example This example demonstrates creating a table with the UUID type column and inserting a value into the table. CREATE TABLE t_uuid (x UUID) ENGINE=TinyLog INSERT INTO t_uuid SELECT generateUUIDv4() SELECT * FROM t_uuid  ┌────────────────────────────────────x─┐ │ f4bf890f-f9dc-4332-ad5c-0c18e73f28e9 │ └──────────────────────────────────────┘  "},{"title":"empty​","type":1,"pageTitle":"Functions for Working with UUID","url":"en/sql-reference/functions/uuid-functions#empty","content":"Checks whether the input UUID is empty. Syntax empty(UUID)  The UUID is considered empty if it contains all zeros (zero UUID). The function also works for arrays or strings. Arguments x — Input UUID. UUID. Returned value Returns 1 for an empty UUID or 0 for a non-empty UUID.  Type: UInt8. Example To generate the UUID value, ClickHouse provides the generateUUIDv4 function. Query: SELECT empty(generateUUIDv4());  Result: ┌─empty(generateUUIDv4())─┐ │ 0 │ └─────────────────────────┘  "},{"title":"notEmpty​","type":1,"pageTitle":"Functions for Working with UUID","url":"en/sql-reference/functions/uuid-functions#notempty","content":"Checks whether the input UUID is non-empty. Syntax notEmpty(UUID)  The UUID is considered empty if it contains all zeros (zero UUID). The function also works for arrays or strings. Arguments x — Input UUID. UUID. Returned value Returns 1 for a non-empty UUID or 0 for an empty UUID.  Type: UInt8. Example To generate the UUID value, ClickHouse provides the generateUUIDv4 function. Query: SELECT notEmpty(generateUUIDv4());  Result: ┌─notEmpty(generateUUIDv4())─┐ │ 1 │ └────────────────────────────┘  "},{"title":"toUUID (x)​","type":1,"pageTitle":"Functions for Working with UUID","url":"en/sql-reference/functions/uuid-functions#touuid-x","content":"Converts String type value to UUID type. toUUID(String)  Returned value The UUID type value. Usage example SELECT toUUID('61f0c404-5cb3-11e7-907b-a6006ad3dba0') AS uuid  ┌─────────────────────────────────uuid─┐ │ 61f0c404-5cb3-11e7-907b-a6006ad3dba0 │ └──────────────────────────────────────┘  "},{"title":"toUUIDOrNull (x)​","type":1,"pageTitle":"Functions for Working with UUID","url":"en/sql-reference/functions/uuid-functions#touuidornull-x","content":"It takes an argument of type String and tries to parse it into UUID. If failed, returns NULL. toUUIDOrNull(String)  Returned value The Nullable(UUID) type value. Usage example SELECT toUUIDOrNull('61f0c404-5cb3-11e7-907b-a6006ad3dba0T') AS uuid  ┌─uuid─┐ │ ᴺᵁᴸᴸ │ └──────┘  "},{"title":"toUUIDOrZero (x)​","type":1,"pageTitle":"Functions for Working with UUID","url":"en/sql-reference/functions/uuid-functions#touuidorzero-x","content":"It takes an argument of type String and tries to parse it into UUID. If failed, returns zero UUID. toUUIDOrZero(String)  Returned value The UUID type value. Usage example SELECT toUUIDOrZero('61f0c404-5cb3-11e7-907b-a6006ad3dba0T') AS uuid  ┌─────────────────────────────────uuid─┐ │ 00000000-0000-0000-0000-000000000000 │ └──────────────────────────────────────┘  "},{"title":"UUIDStringToNum​","type":1,"pageTitle":"Functions for Working with UUID","url":"en/sql-reference/functions/uuid-functions#uuidstringtonum","content":"Accepts a string containing 36 characters in the format xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx, and returns it as a set of bytes in a FixedString(16). UUIDStringToNum(String)  Returned value FixedString(16) Usage examples SELECT '612f3c40-5d3b-217e-707b-6a546a3d7b29' AS uuid, UUIDStringToNum(uuid) AS bytes  ┌─uuid─────────────────────────────────┬─bytes────────────┐ │ 612f3c40-5d3b-217e-707b-6a546a3d7b29 │ a/&lt;@];!~p{jTj={) │ └──────────────────────────────────────┴──────────────────┘  "},{"title":"UUIDNumToString​","type":1,"pageTitle":"Functions for Working with UUID","url":"en/sql-reference/functions/uuid-functions#uuidnumtostring","content":"Accepts a FixedString(16) value, and returns a string containing 36 characters in text format. UUIDNumToString(FixedString(16))  Returned value String. Usage example SELECT 'a/&lt;@];!~p{jTj={)' AS bytes, UUIDNumToString(toFixedString(bytes, 16)) AS uuid  ┌─bytes────────────┬─uuid─────────────────────────────────┐ │ a/&lt;@];!~p{jTj={) │ 612f3c40-5d3b-217e-707b-6a546a3d7b29 │ └──────────────────┴──────────────────────────────────────┘  "},{"title":"serverUUID()​","type":1,"pageTitle":"Functions for Working with UUID","url":"en/sql-reference/functions/uuid-functions#server-uuid","content":"Returns the random and unique UUID, which is generated when the server is first started and stored forever. The result writes to the file uuid created in the ClickHouse server directory /var/lib/clickhouse/. Syntax serverUUID()  Returned value The UUID of the server.  Type: UUID. "},{"title":"See Also​","type":1,"pageTitle":"Functions for Working with UUID","url":"en/sql-reference/functions/uuid-functions#see-also","content":"dictGetUUID "},{"title":"Functions for Working with Embedded Dictionaries","type":0,"sectionRef":"#","url":"en/sql-reference/functions/ym-dict-functions","content":"","keywords":""},{"title":"Multiple Geobases​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"en/sql-reference/functions/ym-dict-functions#multiple-geobases","content":"ClickHouse supports working with multiple alternative geobases (regional hierarchies) simultaneously, in order to support various perspectives on which countries certain regions belong to. The ‘clickhouse-server’ config specifies the file with the regional hierarchy::&lt;path_to_regions_hierarchy_file&gt;/opt/geo/regions_hierarchy.txt&lt;/path_to_regions_hierarchy_file&gt; Besides this file, it also searches for files nearby that have the _ symbol and any suffix appended to the name (before the file extension). For example, it will also find the file /opt/geo/regions_hierarchy_ua.txt, if present. ua is called the dictionary key. For a dictionary without a suffix, the key is an empty string. All the dictionaries are re-loaded in runtime (once every certain number of seconds, as defined in the builtin_dictionaries_reload_interval config parameter, or once an hour by default). However, the list of available dictionaries is defined one time, when the server starts. All functions for working with regions have an optional argument at the end – the dictionary key. It is referred to as the geobase. Example: regionToCountry(RegionID) – Uses the default dictionary: /opt/geo/regions_hierarchy.txt regionToCountry(RegionID, '') – Uses the default dictionary: /opt/geo/regions_hierarchy.txt regionToCountry(RegionID, 'ua') – Uses the dictionary for the 'ua' key: /opt/geo/regions_hierarchy_ua.txt  "},{"title":"regionToCity(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"en/sql-reference/functions/ym-dict-functions#regiontocityid-geobase","content":"Accepts a UInt32 number – the region ID from the geobase. If this region is a city or part of a city, it returns the region ID for the appropriate city. Otherwise, returns 0. "},{"title":"regionToArea(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"en/sql-reference/functions/ym-dict-functions#regiontoareaid-geobase","content":"Converts a region to an area (type 5 in the geobase). In every other way, this function is the same as ‘regionToCity’. SELECT DISTINCT regionToName(regionToArea(toUInt32(number), 'ua')) FROM system.numbers LIMIT 15  ┌─regionToName(regionToArea(toUInt32(number), \\'ua\\'))─┐ │ │ │ Moscow and Moscow region │ │ St. Petersburg and Leningrad region │ │ Belgorod region │ │ Ivanovsk region │ │ Kaluga region │ │ Kostroma region │ │ Kursk region │ │ Lipetsk region │ │ Orlov region │ │ Ryazan region │ │ Smolensk region │ │ Tambov region │ │ Tver region │ │ Tula region │ └──────────────────────────────────────────────────────┘  "},{"title":"regionToDistrict(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"en/sql-reference/functions/ym-dict-functions#regiontodistrictid-geobase","content":"Converts a region to a federal district (type 4 in the geobase). In every other way, this function is the same as ‘regionToCity’. SELECT DISTINCT regionToName(regionToDistrict(toUInt32(number), 'ua')) FROM system.numbers LIMIT 15  ┌─regionToName(regionToDistrict(toUInt32(number), \\'ua\\'))─┐ │ │ │ Central federal district │ │ Northwest federal district │ │ South federal district │ │ North Caucases federal district │ │ Privolga federal district │ │ Ural federal district │ │ Siberian federal district │ │ Far East federal district │ │ Scotland │ │ Faroe Islands │ │ Flemish region │ │ Brussels capital region │ │ Wallonia │ │ Federation of Bosnia and Herzegovina │ └──────────────────────────────────────────────────────────┘  "},{"title":"regionToCountry(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"en/sql-reference/functions/ym-dict-functions#regiontocountryid-geobase","content":"Converts a region to a country. In every other way, this function is the same as ‘regionToCity’. Example: regionToCountry(toUInt32(213)) = 225 converts Moscow (213) to Russia (225). "},{"title":"regionToContinent(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"en/sql-reference/functions/ym-dict-functions#regiontocontinentid-geobase","content":"Converts a region to a continent. In every other way, this function is the same as ‘regionToCity’. Example: regionToContinent(toUInt32(213)) = 10001 converts Moscow (213) to Eurasia (10001). "},{"title":"regionToTopContinent (#regiontotopcontinent)​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"en/sql-reference/functions/ym-dict-functions#regiontotopcontinent-regiontotopcontinent","content":"Finds the highest continent in the hierarchy for the region. Syntax regionToTopContinent(id[, geobase])  Arguments id — Region ID from the geobase. UInt32.geobase — Dictionary key. See Multiple Geobases. String. Optional. Returned value Identifier of the top level continent (the latter when you climb the hierarchy of regions).0, if there is none. Type: UInt32. "},{"title":"regionToPopulation(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"en/sql-reference/functions/ym-dict-functions#regiontopopulationid-geobase","content":"Gets the population for a region. The population can be recorded in files with the geobase. See the section “External dictionaries”. If the population is not recorded for the region, it returns 0. In the geobase, the population might be recorded for child regions, but not for parent regions. "},{"title":"regionIn(lhs, rhs[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"en/sql-reference/functions/ym-dict-functions#regioninlhs-rhs-geobase","content":"Checks whether a ‘lhs’ region belongs to a ‘rhs’ region. Returns a UInt8 number equal to 1 if it belongs, or 0 if it does not belong. The relationship is reflexive – any region also belongs to itself. "},{"title":"regionHierarchy(id[, geobase])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"en/sql-reference/functions/ym-dict-functions#regionhierarchyid-geobase","content":"Accepts a UInt32 number – the region ID from the geobase. Returns an array of region IDs consisting of the passed region and all parents along the chain. Example: regionHierarchy(toUInt32(213)) = [213,1,3,225,10001,10000]. "},{"title":"regionToName(id[, lang])​","type":1,"pageTitle":"Functions for Working with Embedded Dictionaries","url":"en/sql-reference/functions/ym-dict-functions#regiontonameid-lang","content":"Accepts a UInt32 number – the region ID from the geobase. A string with the name of the language can be passed as a second argument. Supported languages are: ru, en, ua, uk, by, kz, tr. If the second argument is omitted, the language ‘ru’ is used. If the language is not supported, an exception is thrown. Returns a string – the name of the region in the corresponding language. If the region with the specified ID does not exist, an empty string is returned. ua and uk both mean Ukrainian. "},{"title":"Operators","type":0,"sectionRef":"#","url":"en/sql-reference/operators/","content":"","keywords":""},{"title":"Access Operators​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#access-operators","content":"a[N] – Access to an element of an array. The arrayElement(a, N) function. a.N – Access to a tuple element. The tupleElement(a, N) function. "},{"title":"Numeric Negation Operator​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#numeric-negation-operator","content":"-a – The negate (a) function. For tuple negation: tupleNegate. "},{"title":"Multiplication and Division Operators​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#multiplication-and-division-operators","content":"a * b – The multiply (a, b) function. For multiplying tuple by number: tupleMultiplyByNumber, for scalar profuct: dotProduct. a / b – The divide(a, b) function. For dividing tuple by number: tupleDivideByNumber. a % b – The modulo(a, b) function. "},{"title":"Addition and Subtraction Operators​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#addition-and-subtraction-operators","content":"a + b – The plus(a, b) function. For tuple addiction: tuplePlus. a - b – The minus(a, b) function. For tuple subtraction: tupleMinus. "},{"title":"Comparison Operators​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#comparison-operators","content":"a = b – The equals(a, b) function. a == b – The equals(a, b) function. a != b – The notEquals(a, b) function. a &lt;&gt; b – The notEquals(a, b) function. a &lt;= b – The lessOrEquals(a, b) function. a &gt;= b – The greaterOrEquals(a, b) function. a &lt; b – The less(a, b) function. a &gt; b – The greater(a, b) function. a LIKE s – The like(a, b) function. a NOT LIKE s – The notLike(a, b) function. a ILIKE s – The ilike(a, b) function. a BETWEEN b AND c – The same as a &gt;= b AND a &lt;= c. a NOT BETWEEN b AND c – The same as a &lt; b OR a &gt; c. "},{"title":"Operators for Working with Data Sets​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#operators-for-working-with-data-sets","content":"See IN operators and EXISTS operator. a IN ... – The in(a, b) function. a NOT IN ... – The notIn(a, b) function. a GLOBAL IN ... – The globalIn(a, b) function. a GLOBAL NOT IN ... – The globalNotIn(a, b) function. a = ANY (subquery) – The in(a, subquery) function. a != ANY (subquery) – The same as a NOT IN (SELECT singleValueOrNull(*) FROM subquery). a = ALL (subquery) – The same as a IN (SELECT singleValueOrNull(*) FROM subquery). a != ALL (subquery) – The notIn(a, subquery) function. Examples Query with ALL: SELECT number AS a FROM numbers(10) WHERE a &gt; ALL (SELECT number FROM numbers(3, 3));  Result: ┌─a─┐ │ 6 │ │ 7 │ │ 8 │ │ 9 │ └───┘  Query with ANY: SELECT number AS a FROM numbers(10) WHERE a &gt; ANY (SELECT number FROM numbers(3, 3));  Result: ┌─a─┐ │ 4 │ │ 5 │ │ 6 │ │ 7 │ │ 8 │ │ 9 │ └───┘  "},{"title":"Operators for Working with Dates and Times​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#operators-datetime","content":""},{"title":"EXTRACT​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#operator-extract","content":"EXTRACT(part FROM date);  Extract parts from a given date. For example, you can retrieve a month from a given date, or a second from a time. The part parameter specifies which part of the date to retrieve. The following values are available: DAY — The day of the month. Possible values: 1–31.MONTH — The number of a month. Possible values: 1–12.YEAR — The year.SECOND — The second. Possible values: 0–59.MINUTE — The minute. Possible values: 0–59.HOUR — The hour. Possible values: 0–23. The part parameter is case-insensitive. The date parameter specifies the date or the time to process. Either Date or DateTime type is supported. Examples: SELECT EXTRACT(DAY FROM toDate('2017-06-15')); SELECT EXTRACT(MONTH FROM toDate('2017-06-15')); SELECT EXTRACT(YEAR FROM toDate('2017-06-15'));  In the following example we create a table and insert into it a value with the DateTime type. CREATE TABLE test.Orders ( OrderId UInt64, OrderName String, OrderDate DateTime ) ENGINE = Log;  INSERT INTO test.Orders VALUES (1, 'Jarlsberg Cheese', toDateTime('2008-10-11 13:23:44'));  SELECT toYear(OrderDate) AS OrderYear, toMonth(OrderDate) AS OrderMonth, toDayOfMonth(OrderDate) AS OrderDay, toHour(OrderDate) AS OrderHour, toMinute(OrderDate) AS OrderMinute, toSecond(OrderDate) AS OrderSecond FROM test.Orders;  ┌─OrderYear─┬─OrderMonth─┬─OrderDay─┬─OrderHour─┬─OrderMinute─┬─OrderSecond─┐ │ 2008 │ 10 │ 11 │ 13 │ 23 │ 44 │ └───────────┴────────────┴──────────┴───────────┴─────────────┴─────────────┘  You can see more examples in tests. "},{"title":"INTERVAL​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#operator-interval","content":"Creates an Interval-type value that should be used in arithmetical operations with Date and DateTime-type values. Types of intervals: SECONDMINUTEHOURDAYWEEKMONTHQUARTERYEAR You can also use a string literal when setting the INTERVAL value. For example, INTERVAL 1 HOUR is identical to the INTERVAL '1 hour' or INTERVAL '1' hour. warning Intervals with different types can’t be combined. You can’t use expressions like INTERVAL 4 DAY 1 HOUR. Specify intervals in units that are smaller or equal to the smallest unit of the interval, for example, INTERVAL 25 HOUR. You can use consecutive operations, like in the example below. Examples: SELECT now() AS current_date_time, current_date_time + INTERVAL 4 DAY + INTERVAL 3 HOUR;  ┌───current_date_time─┬─plus(plus(now(), toIntervalDay(4)), toIntervalHour(3))─┐ │ 2020-11-03 22:09:50 │ 2020-11-08 01:09:50 │ └─────────────────────┴────────────────────────────────────────────────────────┘  SELECT now() AS current_date_time, current_date_time + INTERVAL '4 day' + INTERVAL '3 hour';  ┌───current_date_time─┬─plus(plus(now(), toIntervalDay(4)), toIntervalHour(3))─┐ │ 2020-11-03 22:12:10 │ 2020-11-08 01:12:10 │ └─────────────────────┴────────────────────────────────────────────────────────┘  SELECT now() AS current_date_time, current_date_time + INTERVAL '4' day + INTERVAL '3' hour;  ┌───current_date_time─┬─plus(plus(now(), toIntervalDay('4')), toIntervalHour('3'))─┐ │ 2020-11-03 22:33:19 │ 2020-11-08 01:33:19 │ └─────────────────────┴────────────────────────────────────────────────────────────┘  You can work with dates without using INTERVAL, just by adding or subtracting seconds, minutes, and hours. For example, an interval of one day can be set by adding 60*60*24. note The INTERVAL syntax or addDays function are always preferred. Simple addition or subtraction (syntax like now() + ...) doesn't consider time settings. For example, daylight saving time. Examples: SELECT toDateTime('2014-10-26 00:00:00', 'Asia/Istanbul') AS time, time + 60 * 60 * 24 AS time_plus_24_hours, time + toIntervalDay(1) AS time_plus_1_day;  ┌────────────────time─┬──time_plus_24_hours─┬─────time_plus_1_day─┐ │ 2014-10-26 00:00:00 │ 2014-10-26 23:00:00 │ 2014-10-27 00:00:00 │ └─────────────────────┴─────────────────────┴─────────────────────┘  See Also Interval data typetoInterval type conversion functions "},{"title":"Logical AND Operator​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#logical-and-operator","content":"Syntax SELECT a AND b — calculates logical conjunction of a and b with the function and. "},{"title":"Logical OR Operator​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#logical-or-operator","content":"Syntax SELECT a OR b — calculates logical disjunction of a and b with the function or. "},{"title":"Logical Negation Operator​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#logical-negation-operator","content":"Syntax SELECT NOT a — calculates logical negation of a with the function not. "},{"title":"Conditional Operator​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#conditional-operator","content":"a ? b : c – The if(a, b, c) function. Note: The conditional operator calculates the values of b and c, then checks whether condition a is met, and then returns the corresponding value. If b or C is an arrayJoin() function, each row will be replicated regardless of the “a” condition. "},{"title":"Conditional Expression​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#operator_case","content":"CASE [x] WHEN a THEN b [WHEN ... THEN ...] [ELSE c] END  If x is specified, then transform(x, [a, ...], [b, ...], c) function is used. Otherwise – multiIf(a, b, ..., c). If there is no ELSE c clause in the expression, the default value is NULL. The transform function does not work with NULL. "},{"title":"Concatenation Operator​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#concatenation-operator","content":"s1 || s2 – The concat(s1, s2) function. "},{"title":"Lambda Creation Operator​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#lambda-creation-operator","content":"x -&gt; expr – The lambda(x, expr) function. The following operators do not have a priority since they are brackets: "},{"title":"Array Creation Operator​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#array-creation-operator","content":"[x1, ...] – The array(x1, ...) function. "},{"title":"Tuple Creation Operator​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#tuple-creation-operator","content":"(x1, x2, ...) – The tuple(x2, x2, ...) function. "},{"title":"Associativity​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#associativity","content":"All binary operators have left associativity. For example, 1 + 2 + 3 is transformed to plus(plus(1, 2), 3). Sometimes this does not work the way you expect. For example, SELECT 4 &gt; 2 &gt; 3 will result in 0. For efficiency, the and and or functions accept any number of arguments. The corresponding chains of AND and OR operators are transformed into a single call of these functions. "},{"title":"Checking for NULL​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#checking-for-null","content":"ClickHouse supports the IS NULL and IS NOT NULL operators. "},{"title":"IS NULL​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#operator-is-null","content":"For Nullable type values, the IS NULL operator returns: 1, if the value is NULL.0 otherwise. For other values, the IS NULL operator always returns 0. Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only null subcolumn instead of reading and processing the whole column data. The query SELECT n IS NULL FROM table transforms to SELECT n.null FROM TABLE. SELECT x+100 FROM t_null WHERE y IS NULL  ┌─plus(x, 100)─┐ │ 101 │ └──────────────┘  "},{"title":"IS NOT NULL​","type":1,"pageTitle":"Operators","url":"en/sql-reference/operators/#is-not-null","content":"For Nullable type values, the IS NOT NULL operator returns: 0, if the value is NULL.1 otherwise. For other values, the IS NOT NULL operator always returns 1. SELECT * FROM t_null WHERE y IS NOT NULL  ┌─x─┬─y─┐ │ 2 │ 3 │ └───┴───┘  Can be optimized by enabling the optimize_functions_to_subcolumns setting. With optimize_functions_to_subcolumns = 1 the function reads only null subcolumn instead of reading and processing the whole column data. The query SELECT n IS NOT NULL FROM table transforms to SELECT NOT n.null FROM TABLE. "},{"title":"Other Functions","type":0,"sectionRef":"#","url":"en/sql-reference/functions/other-functions","content":"","keywords":""},{"title":"hostName()​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#hostname","content":"Returns a string with the name of the host that this function was performed on. For distributed processing, this is the name of the remote server host, if the function is performed on a remote server. If it is executed in the context of a distributed table, then it generates a normal column with values relevant to each shard. Otherwise it produces a constant value. "},{"title":"getMacro​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#getmacro","content":"Gets a named value from the macros section of the server configuration. Syntax getMacro(name);  Arguments name — Name to retrieve from the macros section. String. Returned value Value of the specified macro. Type: String. Example The example macros section in the server configuration file: &lt;macros&gt; &lt;test&gt;Value&lt;/test&gt; &lt;/macros&gt;  Query: SELECT getMacro('test');  Result: ┌─getMacro('test')─┐ │ Value │ └──────────────────┘  An alternative way to get the same value: SELECT * FROM system.macros WHERE macro = 'test';  ┌─macro─┬─substitution─┐ │ test │ Value │ └───────┴──────────────┘  "},{"title":"FQDN​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#fqdn","content":"Returns the fully qualified domain name. Syntax fqdn();  This function is case-insensitive. Returned value String with the fully qualified domain name. Type: String. Example Query: SELECT FQDN();  Result: ┌─FQDN()──────────────────────────┐ │ clickhouse.ru-central1.internal │ └─────────────────────────────────┘  "},{"title":"basename​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#basename","content":"Extracts the trailing part of a string after the last slash or backslash. This function if often used to extract the filename from a path. basename( expr )  Arguments expr — Expression resulting in a String type value. All the backslashes must be escaped in the resulting value. Returned Value A string that contains: The trailing part of a string after the last slash or backslash. If the input string contains a path ending with slash or backslash, for example, `/` or `c:\\`, the function returns an empty string. The original string if there are no slashes or backslashes. Example SELECT 'some/long/path/to/file' AS a, basename(a)  ┌─a──────────────────────┬─basename('some\\\\long\\\\path\\\\to\\\\file')─┐ │ some\\long\\path\\to\\file │ file │ └────────────────────────┴────────────────────────────────────────┘  SELECT 'some\\\\long\\\\path\\\\to\\\\file' AS a, basename(a)  ┌─a──────────────────────┬─basename('some\\\\long\\\\path\\\\to\\\\file')─┐ │ some\\long\\path\\to\\file │ file │ └────────────────────────┴────────────────────────────────────────┘  SELECT 'some-file-name' AS a, basename(a)  ┌─a──────────────┬─basename('some-file-name')─┐ │ some-file-name │ some-file-name │ └────────────────┴────────────────────────────┘  "},{"title":"visibleWidth(x)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#visiblewidthx","content":"Calculates the approximate width when outputting values to the console in text format (tab-separated). This function is used by the system for implementing Pretty formats. NULL is represented as a string corresponding to NULL in Pretty formats. SELECT visibleWidth(NULL)  ┌─visibleWidth(NULL)─┐ │ 4 │ └────────────────────┘  "},{"title":"toTypeName(x)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#totypenamex","content":"Returns a string containing the type name of the passed argument. If NULL is passed to the function as input, then it returns the Nullable(Nothing) type, which corresponds to an internal NULL representation in ClickHouse. "},{"title":"blockSize()​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#function-blocksize","content":"Gets the size of the block. In ClickHouse, queries are always run on blocks (sets of column parts). This function allows getting the size of the block that you called it for. "},{"title":"byteSize​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#function-bytesize","content":"Returns estimation of uncompressed byte size of its arguments in memory. Syntax byteSize(argument [, ...])  Arguments argument — Value. Returned value Estimation of byte size of the arguments in memory. Type: UInt64. Examples For String arguments the funtion returns the string length + 9 (terminating zero + length). Query: SELECT byteSize('string');  Result: ┌─byteSize('string')─┐ │ 15 │ └────────────────────┘  Query: CREATE TABLE test ( `key` Int32, `u8` UInt8, `u16` UInt16, `u32` UInt32, `u64` UInt64, `i8` Int8, `i16` Int16, `i32` Int32, `i64` Int64, `f32` Float32, `f64` Float64 ) ENGINE = MergeTree ORDER BY key; INSERT INTO test VALUES(1, 8, 16, 32, 64, -8, -16, -32, -64, 32.32, 64.64); SELECT key, byteSize(u8) AS `byteSize(UInt8)`, byteSize(u16) AS `byteSize(UInt16)`, byteSize(u32) AS `byteSize(UInt32)`, byteSize(u64) AS `byteSize(UInt64)`, byteSize(i8) AS `byteSize(Int8)`, byteSize(i16) AS `byteSize(Int16)`, byteSize(i32) AS `byteSize(Int32)`, byteSize(i64) AS `byteSize(Int64)`, byteSize(f32) AS `byteSize(Float32)`, byteSize(f64) AS `byteSize(Float64)` FROM test ORDER BY key ASC FORMAT Vertical;  Result: Row 1: ────── key: 1 byteSize(UInt8): 1 byteSize(UInt16): 2 byteSize(UInt32): 4 byteSize(UInt64): 8 byteSize(Int8): 1 byteSize(Int16): 2 byteSize(Int32): 4 byteSize(Int64): 8 byteSize(Float32): 4 byteSize(Float64): 8  If the function takes multiple arguments, it returns their combined byte size. Query: SELECT byteSize(NULL, 1, 0.3, '');  Result: ┌─byteSize(NULL, 1, 0.3, '')─┐ │ 19 │ └────────────────────────────┘  "},{"title":"materialize(x)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#materializex","content":"Turns a constant into a full column containing just one value. In ClickHouse, full columns and constants are represented differently in memory. Functions work differently for constant arguments and normal arguments (different code is executed), although the result is almost always the same. This function is for debugging this behavior. "},{"title":"ignore(…)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#ignore","content":"Accepts any arguments, including NULL. Always returns 0. However, the argument is still evaluated. This can be used for benchmarks. "},{"title":"sleep(seconds)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#sleepseconds","content":"Sleeps ‘seconds’ seconds on each data block. You can specify an integer or a floating-point number. "},{"title":"sleepEachRow(seconds)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#sleepeachrowseconds","content":"Sleeps ‘seconds’ seconds on each row. You can specify an integer or a floating-point number. "},{"title":"currentDatabase()​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#currentdatabase","content":"Returns the name of the current database. You can use this function in table engine parameters in a CREATE TABLE query where you need to specify the database. "},{"title":"currentUser()​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#other-function-currentuser","content":"Returns the login of current user. Login of user, that initiated query, will be returned in case distibuted query. SELECT currentUser();  Alias: user(), USER(). Returned values Login of current user.Login of user that initiated query in case of disributed query. Type: String. Example Query: SELECT currentUser();  Result: ┌─currentUser()─┐ │ default │ └───────────────┘  "},{"title":"isConstant​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#is-constant","content":"Checks whether the argument is a constant expression. A constant expression means an expression whose resulting value is known at the query analysis (i.e. before execution). For example, expressions over literals are constant expressions. The function is intended for development, debugging and demonstration. Syntax isConstant(x)  Arguments x — Expression to check. Returned values 1 — x is constant.0 — x is non-constant. Type: UInt8. Examples Query: SELECT isConstant(x + 1) FROM (SELECT 43 AS x)  Result: ┌─isConstant(plus(x, 1))─┐ │ 1 │ └────────────────────────┘  Query: WITH 3.14 AS pi SELECT isConstant(cos(pi))  Result: ┌─isConstant(cos(pi))─┐ │ 1 │ └─────────────────────┘  Query: SELECT isConstant(number) FROM numbers(1)  Result: ┌─isConstant(number)─┐ │ 0 │ └────────────────────┘  "},{"title":"isFinite(x)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#isfinitex","content":"Accepts Float32 and Float64 and returns UInt8 equal to 1 if the argument is not infinite and not a NaN, otherwise 0. "},{"title":"isInfinite(x)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#isinfinitex","content":"Accepts Float32 and Float64 and returns UInt8 equal to 1 if the argument is infinite, otherwise 0. Note that 0 is returned for a NaN. "},{"title":"ifNotFinite​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#ifnotfinite","content":"Checks whether floating point value is finite. Syntax ifNotFinite(x,y)  Arguments x — Value to be checked for infinity. Type: Float*.y — Fallback value. Type: Float*. Returned value x if x is finite.y if x is not finite. Example Query: SELECT 1/0 as infimum, ifNotFinite(infimum,42)  Result: ┌─infimum─┬─ifNotFinite(divide(1, 0), 42)─┐ │ inf │ 42 │ └─────────┴───────────────────────────────┘  You can get similar result by using ternary operator: isFinite(x) ? x : y. "},{"title":"isNaN(x)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#isnanx","content":"Accepts Float32 and Float64 and returns UInt8 equal to 1 if the argument is a NaN, otherwise 0. "},{"title":"hasColumnInTable([‘hostname’[, ‘username’[, ‘password’]],] ‘database’, ‘table’, ‘column’)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#hascolumnintablehostname-username-password-database-table-column","content":"Accepts constant strings: database name, table name, and column name. Returns a UInt8 constant expression equal to 1 if there is a column, otherwise 0. If the hostname parameter is set, the test will run on a remote server. The function throws an exception if the table does not exist. For elements in a nested data structure, the function checks for the existence of a column. For the nested data structure itself, the function returns 0. "},{"title":"bar​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#function-bar","content":"Allows building a unicode-art diagram. bar(x, min, max, width) draws a band with a width proportional to (x - min) and equal to width characters when x = max. Arguments x — Size to display.min, max — Integer constants. The value must fit in Int64.width — Constant, positive integer, can be fractional. The band is drawn with accuracy to one eighth of a symbol. Example: SELECT toHour(EventTime) AS h, count() AS c, bar(c, 0, 600000, 20) AS bar FROM test.hits GROUP BY h ORDER BY h ASC  ┌──h─┬──────c─┬─bar────────────────┐ │ 0 │ 292907 │ █████████▋ │ │ 1 │ 180563 │ ██████ │ │ 2 │ 114861 │ ███▋ │ │ 3 │ 85069 │ ██▋ │ │ 4 │ 68543 │ ██▎ │ │ 5 │ 78116 │ ██▌ │ │ 6 │ 113474 │ ███▋ │ │ 7 │ 170678 │ █████▋ │ │ 8 │ 278380 │ █████████▎ │ │ 9 │ 391053 │ █████████████ │ │ 10 │ 457681 │ ███████████████▎ │ │ 11 │ 493667 │ ████████████████▍ │ │ 12 │ 509641 │ ████████████████▊ │ │ 13 │ 522947 │ █████████████████▍ │ │ 14 │ 539954 │ █████████████████▊ │ │ 15 │ 528460 │ █████████████████▌ │ │ 16 │ 539201 │ █████████████████▊ │ │ 17 │ 523539 │ █████████████████▍ │ │ 18 │ 506467 │ ████████████████▊ │ │ 19 │ 520915 │ █████████████████▎ │ │ 20 │ 521665 │ █████████████████▍ │ │ 21 │ 542078 │ ██████████████████ │ │ 22 │ 493642 │ ████████████████▍ │ │ 23 │ 400397 │ █████████████▎ │ └────┴────────┴────────────────────┘  "},{"title":"transform​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#transform","content":"Transforms a value according to the explicitly defined mapping of some elements to other ones. There are two variations of this function: "},{"title":"transform(x, array_from, array_to, default)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#transformx-array-from-array-to-default","content":"x – What to transform. array_from – Constant array of values for converting. array_to – Constant array of values to convert the values in ‘from’ to. default – Which value to use if ‘x’ is not equal to any of the values in ‘from’. array_from and array_to – Arrays of the same size. Types: transform(T, Array(T), Array(U), U) -&gt; U T and U can be numeric, string, or Date or DateTime types. Where the same letter is indicated (T or U), for numeric types these might not be matching types, but types that have a common type. For example, the first argument can have the Int64 type, while the second has the Array(UInt16) type. If the ‘x’ value is equal to one of the elements in the ‘array_from’ array, it returns the existing element (that is numbered the same) from the ‘array_to’ array. Otherwise, it returns ‘default’. If there are multiple matching elements in ‘array_from’, it returns one of the matches. Example: SELECT transform(SearchEngineID, [2, 3], ['Yandex', 'Google'], 'Other') AS title, count() AS c FROM test.hits WHERE SearchEngineID != 0 GROUP BY title ORDER BY c DESC  ┌─title─────┬──────c─┐ │ Yandex │ 498635 │ │ Google │ 229872 │ │ Other │ 104472 │ └───────────┴────────┘  "},{"title":"transform(x, array_from, array_to)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#transformx-array-from-array-to","content":"Differs from the first variation in that the ‘default’ argument is omitted. If the ‘x’ value is equal to one of the elements in the ‘array_from’ array, it returns the matching element (that is numbered the same) from the ‘array_to’ array. Otherwise, it returns ‘x’. Types: transform(T, Array(T), Array(T)) -&gt; T Example: SELECT transform(domain(Referer), ['yandex.ru', 'google.ru', 'vk.com'], ['www.yandex', 'example.com']) AS s, count() AS c FROM test.hits GROUP BY domain(Referer) ORDER BY count() DESC LIMIT 10  ┌─s──────────────┬───────c─┐ │ │ 2906259 │ │ www.yandex │ 867767 │ │ ███████.ru │ 313599 │ │ mail.yandex.ru │ 107147 │ │ ██████.ru │ 100355 │ │ █████████.ru │ 65040 │ │ news.yandex.ru │ 64515 │ │ ██████.net │ 59141 │ │ example.com │ 57316 │ └────────────────┴─────────┘  "},{"title":"formatReadableSize(x)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#formatreadablesizex","content":"Accepts the size (number of bytes). Returns a rounded size with a suffix (KiB, MiB, etc.) as a string. Example: SELECT arrayJoin([1, 1024, 1024*1024, 192851925]) AS filesize_bytes, formatReadableSize(filesize_bytes) AS filesize  ┌─filesize_bytes─┬─filesize───┐ │ 1 │ 1.00 B │ │ 1024 │ 1.00 KiB │ │ 1048576 │ 1.00 MiB │ │ 192851925 │ 183.92 MiB │ └────────────────┴────────────┘  "},{"title":"formatReadableQuantity(x)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#formatreadablequantityx","content":"Accepts the number. Returns a rounded number with a suffix (thousand, million, billion, etc.) as a string. It is useful for reading big numbers by human. Example: SELECT arrayJoin([1024, 1234 * 1000, (4567 * 1000) * 1000, 98765432101234]) AS number, formatReadableQuantity(number) AS number_for_humans  ┌─────────number─┬─number_for_humans─┐ │ 1024 │ 1.02 thousand │ │ 1234000 │ 1.23 million │ │ 4567000000 │ 4.57 billion │ │ 98765432101234 │ 98.77 trillion │ └────────────────┴───────────────────┘  "},{"title":"formatReadableTimeDelta​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#formatreadabletimedelta","content":"Accepts the time delta in seconds. Returns a time delta with (year, month, day, hour, minute, second) as a string. Syntax formatReadableTimeDelta(column[, maximum_unit])  Arguments column — A column with numeric time delta.maximum_unit — Optional. Maximum unit to show. Acceptable values seconds, minutes, hours, days, months, years. Example: SELECT arrayJoin([100, 12345, 432546534]) AS elapsed, formatReadableTimeDelta(elapsed) AS time_delta  ┌────elapsed─┬─time_delta ─────────────────────────────────────────────────────┐ │ 100 │ 1 minute and 40 seconds │ │ 12345 │ 3 hours, 25 minutes and 45 seconds │ │ 432546534 │ 13 years, 8 months, 17 days, 7 hours, 48 minutes and 54 seconds │ └────────────┴─────────────────────────────────────────────────────────────────┘  SELECT arrayJoin([100, 12345, 432546534]) AS elapsed, formatReadableTimeDelta(elapsed, 'minutes') AS time_delta  ┌────elapsed─┬─time_delta ─────────────────────────────────────────────────────┐ │ 100 │ 1 minute and 40 seconds │ │ 12345 │ 205 minutes and 45 seconds │ │ 432546534 │ 7209108 minutes and 54 seconds │ └────────────┴─────────────────────────────────────────────────────────────────┘  "},{"title":"least(a, b)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#leasta-b","content":"Returns the smallest value from a and b. "},{"title":"greatest(a, b)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#greatesta-b","content":"Returns the largest value of a and b. "},{"title":"uptime()​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#uptime","content":"Returns the server’s uptime in seconds. If it is executed in the context of a distributed table, then it generates a normal column with values relevant to each shard. Otherwise it produces a constant value. "},{"title":"version()​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#version","content":"Returns the version of the server as a string. If it is executed in the context of a distributed table, then it generates a normal column with values relevant to each shard. Otherwise it produces a constant value. "},{"title":"buildId()​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#buildid","content":"Returns the build ID generated by a compiler for the running ClickHouse server binary. If it is executed in the context of a distributed table, then it generates a normal column with values relevant to each shard. Otherwise it produces a constant value. "},{"title":"blockNumber​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#blocknumber","content":"Returns the sequence number of the data block where the row is located. "},{"title":"rowNumberInBlock​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#function-rownumberinblock","content":"Returns the ordinal number of the row in the data block. Different data blocks are always recalculated. "},{"title":"rowNumberInAllBlocks()​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#rownumberinallblocks","content":"Returns the ordinal number of the row in the data block. This function only considers the affected data blocks. "},{"title":"neighbor​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#neighbor","content":"The window function that provides access to a row at a specified offset which comes before or after the current row of a given column. Syntax neighbor(column, offset[, default_value])  The result of the function depends on the affected data blocks and the order of data in the block. warning It can reach the neighbor rows only inside the currently processed data block. The rows order used during the calculation of neighbor can differ from the order of rows returned to the user. To prevent that you can make a subquery with ORDER BY and call the function from outside the subquery. Arguments column — A column name or scalar expression.offset — The number of rows forwards or backwards from the current row of column. Int64.default_value — Optional. The value to be returned if offset goes beyond the scope of the block. Type of data blocks affected. Returned values Value for column in offset distance from current row if offset value is not outside block bounds.Default value for column if offset value is outside block bounds. If default_value is given, then it will be used. Type: type of data blocks affected or default value type. Example Query: SELECT number, neighbor(number, 2) FROM system.numbers LIMIT 10;  Result: ┌─number─┬─neighbor(number, 2)─┐ │ 0 │ 2 │ │ 1 │ 3 │ │ 2 │ 4 │ │ 3 │ 5 │ │ 4 │ 6 │ │ 5 │ 7 │ │ 6 │ 8 │ │ 7 │ 9 │ │ 8 │ 0 │ │ 9 │ 0 │ └────────┴─────────────────────┘  Query: SELECT number, neighbor(number, 2, 999) FROM system.numbers LIMIT 10;  Result: ┌─number─┬─neighbor(number, 2, 999)─┐ │ 0 │ 2 │ │ 1 │ 3 │ │ 2 │ 4 │ │ 3 │ 5 │ │ 4 │ 6 │ │ 5 │ 7 │ │ 6 │ 8 │ │ 7 │ 9 │ │ 8 │ 999 │ │ 9 │ 999 │ └────────┴──────────────────────────┘  This function can be used to compute year-over-year metric value: Query: WITH toDate('2018-01-01') AS start_date SELECT toStartOfMonth(start_date + (number * 32)) AS month, toInt32(month) % 100 AS money, neighbor(money, -12) AS prev_year, round(prev_year / money, 2) AS year_over_year FROM numbers(16)  Result: ┌──────month─┬─money─┬─prev_year─┬─year_over_year─┐ │ 2018-01-01 │ 32 │ 0 │ 0 │ │ 2018-02-01 │ 63 │ 0 │ 0 │ │ 2018-03-01 │ 91 │ 0 │ 0 │ │ 2018-04-01 │ 22 │ 0 │ 0 │ │ 2018-05-01 │ 52 │ 0 │ 0 │ │ 2018-06-01 │ 83 │ 0 │ 0 │ │ 2018-07-01 │ 13 │ 0 │ 0 │ │ 2018-08-01 │ 44 │ 0 │ 0 │ │ 2018-09-01 │ 75 │ 0 │ 0 │ │ 2018-10-01 │ 5 │ 0 │ 0 │ │ 2018-11-01 │ 36 │ 0 │ 0 │ │ 2018-12-01 │ 66 │ 0 │ 0 │ │ 2019-01-01 │ 97 │ 32 │ 0.33 │ │ 2019-02-01 │ 28 │ 63 │ 2.25 │ │ 2019-03-01 │ 56 │ 91 │ 1.62 │ │ 2019-04-01 │ 87 │ 22 │ 0.25 │ └────────────┴───────┴───────────┴────────────────┘  "},{"title":"runningDifference(x)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#other_functions-runningdifference","content":"Calculates the difference between successive row values ​​in the data block. Returns 0 for the first row and the difference from the previous row for each subsequent row. warning It can reach the previous row only inside the currently processed data block. The result of the function depends on the affected data blocks and the order of data in the block. The rows order used during the calculation of runningDifference can differ from the order of rows returned to the user. To prevent that you can make a subquery with ORDER BY and call the function from outside the subquery. Example: SELECT EventID, EventTime, runningDifference(EventTime) AS delta FROM ( SELECT EventID, EventTime FROM events WHERE EventDate = '2016-11-24' ORDER BY EventTime ASC LIMIT 5 )  ┌─EventID─┬───────────EventTime─┬─delta─┐ │ 1106 │ 2016-11-24 00:00:04 │ 0 │ │ 1107 │ 2016-11-24 00:00:05 │ 1 │ │ 1108 │ 2016-11-24 00:00:05 │ 0 │ │ 1109 │ 2016-11-24 00:00:09 │ 4 │ │ 1110 │ 2016-11-24 00:00:10 │ 1 │ └─────────┴─────────────────────┴───────┘  Please note - block size affects the result. With each new block, the runningDifference state is reset. SELECT number, runningDifference(number + 1) AS diff FROM numbers(100000) WHERE diff != 1  ┌─number─┬─diff─┐ │ 0 │ 0 │ └────────┴──────┘ ┌─number─┬─diff─┐ │ 65536 │ 0 │ └────────┴──────┘  set max_block_size=100000 -- default value is 65536! SELECT number, runningDifference(number + 1) AS diff FROM numbers(100000) WHERE diff != 1  ┌─number─┬─diff─┐ │ 0 │ 0 │ └────────┴──────┘  "},{"title":"runningDifferenceStartingWithFirstValue​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#runningdifferencestartingwithfirstvalue","content":"Same as for runningDifference, the difference is the value of the first row, returned the value of the first row, and each subsequent row returns the difference from the previous row. "},{"title":"runningConcurrency​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#runningconcurrency","content":"Calculates the number of concurrent events. Each event has a start time and an end time. The start time is included in the event, while the end time is excluded. Columns with a start time and an end time must be of the same data type. The function calculates the total number of active (concurrent) events for each event start time. warning Events must be ordered by the start time in ascending order. If this requirement is violated the function raises an exception. Every data block is processed separately. If events from different data blocks overlap then they can not be processed correctly. Syntax runningConcurrency(start, end)  Arguments start — A column with the start time of events. Date, DateTime, or DateTime64.end — A column with the end time of events. Date, DateTime, or DateTime64. Returned values The number of concurrent events at each event start time. Type: UInt32 Example Consider the table: ┌──────start─┬────────end─┐ │ 2021-03-03 │ 2021-03-11 │ │ 2021-03-06 │ 2021-03-12 │ │ 2021-03-07 │ 2021-03-08 │ │ 2021-03-11 │ 2021-03-12 │ └────────────┴────────────┘  Query: SELECT start, runningConcurrency(start, end) FROM example_table;  Result: ┌──────start─┬─runningConcurrency(start, end)─┐ │ 2021-03-03 │ 1 │ │ 2021-03-06 │ 2 │ │ 2021-03-07 │ 3 │ │ 2021-03-11 │ 2 │ └────────────┴────────────────────────────────┘  "},{"title":"MACNumToString(num)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#macnumtostringnum","content":"Accepts a UInt64 number. Interprets it as a MAC address in big endian. Returns a string containing the corresponding MAC address in the format AA:BB:CC:DD:EE:FF (colon-separated numbers in hexadecimal form). "},{"title":"MACStringToNum(s)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#macstringtonums","content":"The inverse function of MACNumToString. If the MAC address has an invalid format, it returns 0. "},{"title":"MACStringToOUI(s)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#macstringtoouis","content":"Accepts a MAC address in the format AA:BB:CC:DD:EE:FF (colon-separated numbers in hexadecimal form). Returns the first three octets as a UInt64 number. If the MAC address has an invalid format, it returns 0. "},{"title":"getSizeOfEnumType​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#getsizeofenumtype","content":"Returns the number of fields in Enum. getSizeOfEnumType(value)  Arguments: value — Value of type Enum. Returned values The number of fields with Enum input values.An exception is thrown if the type is not Enum. Example SELECT getSizeOfEnumType( CAST('a' AS Enum8('a' = 1, 'b' = 2) ) ) AS x  ┌─x─┐ │ 2 │ └───┘  "},{"title":"blockSerializedSize​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#blockserializedsize","content":"Returns size on disk (without taking into account compression). blockSerializedSize(value[, value[, ...]])  Arguments value — Any value. Returned values The number of bytes that will be written to disk for block of values (without compression). Example Query: SELECT blockSerializedSize(maxState(1)) as x  Result: ┌─x─┐ │ 2 │ └───┘  "},{"title":"toColumnTypeName​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#tocolumntypename","content":"Returns the name of the class that represents the data type of the column in RAM. toColumnTypeName(value)  Arguments: value — Any type of value. Returned values A string with the name of the class that is used for representing the value data type in RAM. Example of the difference betweentoTypeName ' and ' toColumnTypeName SELECT toTypeName(CAST('2018-01-01 01:02:03' AS DateTime))  ┌─toTypeName(CAST('2018-01-01 01:02:03', 'DateTime'))─┐ │ DateTime │ └─────────────────────────────────────────────────────┘  SELECT toColumnTypeName(CAST('2018-01-01 01:02:03' AS DateTime))  ┌─toColumnTypeName(CAST('2018-01-01 01:02:03', 'DateTime'))─┐ │ Const(UInt32) │ └───────────────────────────────────────────────────────────┘  The example shows that the DateTime data type is stored in memory as Const(UInt32). "},{"title":"dumpColumnStructure​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#dumpcolumnstructure","content":"Outputs a detailed description of data structures in RAM dumpColumnStructure(value)  Arguments: value — Any type of value. Returned values A string describing the structure that is used for representing the value data type in RAM. Example SELECT dumpColumnStructure(CAST('2018-01-01 01:02:03', 'DateTime'))  ┌─dumpColumnStructure(CAST('2018-01-01 01:02:03', 'DateTime'))─┐ │ DateTime, Const(size = 1, UInt32(size = 1)) │ └──────────────────────────────────────────────────────────────┘  "},{"title":"defaultValueOfArgumentType​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#defaultvalueofargumenttype","content":"Outputs the default value for the data type. Does not include default values for custom columns set by the user. defaultValueOfArgumentType(expression)  Arguments: expression — Arbitrary type of value or an expression that results in a value of an arbitrary type. Returned values 0 for numbers.Empty string for strings.ᴺᵁᴸᴸ for Nullable. Example SELECT defaultValueOfArgumentType( CAST(1 AS Int8) )  ┌─defaultValueOfArgumentType(CAST(1, 'Int8'))─┐ │ 0 │ └─────────────────────────────────────────────┘  SELECT defaultValueOfArgumentType( CAST(1 AS Nullable(Int8) ) )  ┌─defaultValueOfArgumentType(CAST(1, 'Nullable(Int8)'))─┐ │ ᴺᵁᴸᴸ │ └───────────────────────────────────────────────────────┘  "},{"title":"defaultValueOfTypeName​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#defaultvalueoftypename","content":"Outputs the default value for given type name. Does not include default values for custom columns set by the user. defaultValueOfTypeName(type)  Arguments: type — A string representing a type name. Returned values 0 for numbers.Empty string for strings.ᴺᵁᴸᴸ for Nullable. Example SELECT defaultValueOfTypeName('Int8')  ┌─defaultValueOfTypeName('Int8')─┐ │ 0 │ └────────────────────────────────┘  SELECT defaultValueOfTypeName('Nullable(Int8)')  ┌─defaultValueOfTypeName('Nullable(Int8)')─┐ │ ᴺᵁᴸᴸ │ └──────────────────────────────────────────┘  "},{"title":"indexHint​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#indexhint","content":"The function is intended for debugging and introspection purposes. The function ignores it's argument and always returns 1. Arguments are not even evaluated. But for the purpose of index analysis, the argument of this function is analyzed as if it was present directly without being wrapped inside indexHint function. This allows to select data in index ranges by the corresponding condition but without further filtering by this condition. The index in ClickHouse is sparse and using indexHint will yield more data than specifying the same condition directly. Syntax SELECT * FROM table WHERE indexHint(&lt;expression&gt;)  Returned value Type: Uint8. Example Here is the example of test data from the table ontime. Input table: SELECT count() FROM ontime  ┌─count()─┐ │ 4276457 │ └─────────┘  The table has indexes on the fields (FlightDate, (Year, FlightDate)). Create a query, where the index is not used. Query: SELECT FlightDate AS k, count() FROM ontime GROUP BY k ORDER BY k  ClickHouse processed the entire table (Processed 4.28 million rows). Result: ┌──────────k─┬─count()─┐ │ 2017-01-01 │ 13970 │ │ 2017-01-02 │ 15882 │ ........................ │ 2017-09-28 │ 16411 │ │ 2017-09-29 │ 16384 │ │ 2017-09-30 │ 12520 │ └────────────┴─────────┘  To apply the index, select a specific date. Query: SELECT FlightDate AS k, count() FROM ontime WHERE k = '2017-09-15' GROUP BY k ORDER BY k  By using the index, ClickHouse processed a significantly smaller number of rows (Processed 32.74 thousand rows). Result: ┌──────────k─┬─count()─┐ │ 2017-09-15 │ 16428 │ └────────────┴─────────┘  Now wrap the expression k = '2017-09-15' into indexHint function. Query: SELECT FlightDate AS k, count() FROM ontime WHERE indexHint(k = '2017-09-15') GROUP BY k ORDER BY k ASC  ClickHouse used the index in the same way as the previous time (Processed 32.74 thousand rows). The expression k = '2017-09-15' was not used when generating the result. In examle the indexHint function allows to see adjacent dates. Result: ┌──────────k─┬─count()─┐ │ 2017-09-14 │ 7071 │ │ 2017-09-15 │ 16428 │ │ 2017-09-16 │ 1077 │ │ 2017-09-30 │ 8167 │ └────────────┴─────────┘  "},{"title":"replicate​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#other-functions-replicate","content":"Creates an array with a single value. Used for internal implementation of arrayJoin. SELECT replicate(x, arr);  Arguments: arr — Original array. ClickHouse creates a new array of the same length as the original and fills it with the value x.x — The value that the resulting array will be filled with. Returned value An array filled with the value x. Type: Array. Example Query: SELECT replicate(1, ['a', 'b', 'c'])  Result: ┌─replicate(1, ['a', 'b', 'c'])─┐ │ [1,1,1] │ └───────────────────────────────┘  "},{"title":"filesystemAvailable​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#filesystemavailable","content":"Returns amount of remaining space on the filesystem where the files of the databases located. It is always smaller than total free space (filesystemFree) because some space is reserved for OS. Syntax filesystemAvailable()  Returned value The amount of remaining space available in bytes. Type: UInt64. Example Query: SELECT formatReadableSize(filesystemAvailable()) AS &quot;Available space&quot;, toTypeName(filesystemAvailable()) AS &quot;Type&quot;;  Result: ┌─Available space─┬─Type───┐ │ 30.75 GiB │ UInt64 │ └─────────────────┴────────┘  "},{"title":"filesystemFree​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#filesystemfree","content":"Returns total amount of the free space on the filesystem where the files of the databases located. See also filesystemAvailable Syntax filesystemFree()  Returned value Amount of free space in bytes. Type: UInt64. Example Query: SELECT formatReadableSize(filesystemFree()) AS &quot;Free space&quot;, toTypeName(filesystemFree()) AS &quot;Type&quot;;  Result: ┌─Free space─┬─Type───┐ │ 32.39 GiB │ UInt64 │ └────────────┴────────┘  "},{"title":"filesystemCapacity​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#filesystemcapacity","content":"Returns the capacity of the filesystem in bytes. For evaluation, the path to the data directory must be configured. Syntax filesystemCapacity()  Returned value Capacity information of the filesystem in bytes. Type: UInt64. Example Query: SELECT formatReadableSize(filesystemCapacity()) AS &quot;Capacity&quot;, toTypeName(filesystemCapacity()) AS &quot;Type&quot;  Result: ┌─Capacity──┬─Type───┐ │ 39.32 GiB │ UInt64 │ └───────────┴────────┘  "},{"title":"initializeAggregation​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#initializeaggregation","content":"Calculates result of aggregate function based on single value. It is intended to use this function to initialize aggregate functions with combinator -State. You can create states of aggregate functions and insert them to columns of type AggregateFunction or use initialized aggregates as default values. Syntax initializeAggregation (aggregate_function, arg1, arg2, ..., argN)  Arguments aggregate_function — Name of the aggregation function to initialize. String.arg — Arguments of aggregate function. Returned value(s) Result of aggregation for every row passed to the function. The return type is the same as the return type of function, that initializeAgregation takes as first argument. Example Query: SELECT uniqMerge(state) FROM (SELECT initializeAggregation('uniqState', number % 3) AS state FROM numbers(10000));  Result: ┌─uniqMerge(state)─┐ │ 3 │ └──────────────────┘  Query: SELECT finalizeAggregation(state), toTypeName(state) FROM (SELECT initializeAggregation('sumState', number % 3) AS state FROM numbers(5));  Result: ┌─finalizeAggregation(state)─┬─toTypeName(state)─────────────┐ │ 0 │ AggregateFunction(sum, UInt8) │ │ 1 │ AggregateFunction(sum, UInt8) │ │ 2 │ AggregateFunction(sum, UInt8) │ │ 0 │ AggregateFunction(sum, UInt8) │ │ 1 │ AggregateFunction(sum, UInt8) │ └────────────────────────────┴───────────────────────────────┘  Example with AggregatingMergeTree table engine and AggregateFunction column: CREATE TABLE metrics ( key UInt64, value AggregateFunction(sum, UInt64) DEFAULT initializeAggregation('sumState', toUInt64(0)) ) ENGINE = AggregatingMergeTree ORDER BY key  INSERT INTO metrics VALUES (0, initializeAggregation('sumState', toUInt64(42)))  See Also arrayReduce "},{"title":"finalizeAggregation​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#function-finalizeaggregation","content":"Takes state of aggregate function. Returns result of aggregation (or finalized state when using-State combinator). Syntax finalizeAggregation(state)  Arguments state — State of aggregation. AggregateFunction. Returned value(s) Value/values that was aggregated. Type: Value of any types that was aggregated. Examples Query: SELECT finalizeAggregation(( SELECT countState(number) FROM numbers(10)));  Result: ┌─finalizeAggregation(_subquery16)─┐ │ 10 │ └──────────────────────────────────┘  Query: SELECT finalizeAggregation(( SELECT sumState(number) FROM numbers(10)));  Result: ┌─finalizeAggregation(_subquery20)─┐ │ 45 │ └──────────────────────────────────┘  Note that NULL values are ignored. Query: SELECT finalizeAggregation(arrayReduce('anyState', [NULL, 2, 3]));  Result: ┌─finalizeAggregation(arrayReduce('anyState', [NULL, 2, 3]))─┐ │ 2 │ └────────────────────────────────────────────────────────────┘  Combined example: Query: WITH initializeAggregation('sumState', number) AS one_row_sum_state SELECT number, finalizeAggregation(one_row_sum_state) AS one_row_sum, runningAccumulate(one_row_sum_state) AS cumulative_sum FROM numbers(10);  Result: ┌─number─┬─one_row_sum─┬─cumulative_sum─┐ │ 0 │ 0 │ 0 │ │ 1 │ 1 │ 1 │ │ 2 │ 2 │ 3 │ │ 3 │ 3 │ 6 │ │ 4 │ 4 │ 10 │ │ 5 │ 5 │ 15 │ │ 6 │ 6 │ 21 │ │ 7 │ 7 │ 28 │ │ 8 │ 8 │ 36 │ │ 9 │ 9 │ 45 │ └────────┴─────────────┴────────────────┘  See Also arrayReduceinitializeAggregation "},{"title":"runningAccumulate​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#runningaccumulate","content":"Accumulates states of an aggregate function for each row of a data block. warning The state is reset for each new data block. Syntax runningAccumulate(agg_state[, grouping]);  Arguments agg_state — State of the aggregate function. AggregateFunction.grouping — Grouping key. Optional. The state of the function is reset if the grouping value is changed. It can be any of the supported data types for which the equality operator is defined. Returned value Each resulting row contains a result of the aggregate function, accumulated for all the input rows from 0 to the current position. runningAccumulate resets states for each new data block or when the grouping value changes. Type depends on the aggregate function used. Examples Consider how you can use runningAccumulate to find the cumulative sum of numbers without and with grouping. Query: SELECT k, runningAccumulate(sum_k) AS res FROM (SELECT number as k, sumState(k) AS sum_k FROM numbers(10) GROUP BY k ORDER BY k);  Result: ┌─k─┬─res─┐ │ 0 │ 0 │ │ 1 │ 1 │ │ 2 │ 3 │ │ 3 │ 6 │ │ 4 │ 10 │ │ 5 │ 15 │ │ 6 │ 21 │ │ 7 │ 28 │ │ 8 │ 36 │ │ 9 │ 45 │ └───┴─────┘  The subquery generates sumState for every number from 0 to 9. sumState returns the state of the sum function that contains the sum of a single number. The whole query does the following: For the first row, runningAccumulate takes sumState(0) and returns 0.For the second row, the function merges sumState(0) and sumState(1) resulting in sumState(0 + 1), and returns 1 as a result.For the third row, the function merges sumState(0 + 1) and sumState(2) resulting in sumState(0 + 1 + 2), and returns 3 as a result.The actions are repeated until the block ends. The following example shows the groupping parameter usage: Query: SELECT grouping, item, runningAccumulate(state, grouping) AS res FROM ( SELECT toInt8(number / 4) AS grouping, number AS item, sumState(number) AS state FROM numbers(15) GROUP BY item ORDER BY item ASC );  Result: ┌─grouping─┬─item─┬─res─┐ │ 0 │ 0 │ 0 │ │ 0 │ 1 │ 1 │ │ 0 │ 2 │ 3 │ │ 0 │ 3 │ 6 │ │ 1 │ 4 │ 4 │ │ 1 │ 5 │ 9 │ │ 1 │ 6 │ 15 │ │ 1 │ 7 │ 22 │ │ 2 │ 8 │ 8 │ │ 2 │ 9 │ 17 │ │ 2 │ 10 │ 27 │ │ 2 │ 11 │ 38 │ │ 3 │ 12 │ 12 │ │ 3 │ 13 │ 25 │ │ 3 │ 14 │ 39 │ └──────────┴──────┴─────┘  As you can see, runningAccumulate merges states for each group of rows separately. "},{"title":"joinGet​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#joinget","content":"The function lets you extract data from the table the same way as from a dictionary. Gets data from Join tables using the specified join key. Only supports tables created with the ENGINE = Join(ANY, LEFT, &lt;join_keys&gt;) statement. Syntax joinGet(join_storage_table_name, `value_column`, join_keys)  Arguments join_storage_table_name — an identifier indicates where search is performed. The identifier is searched in the default database (see parameter default_database in the config file). To override the default database, use the USE db_name or specify the database and the table through the separator db_name.db_table, see the example.value_column — name of the column of the table that contains required data.join_keys — list of keys. Returned value Returns list of values corresponded to list of keys. If certain does not exist in source table then 0 or null will be returned based on join_use_nulls setting. More info about join_use_nulls in Join operation. Example Input table: CREATE DATABASE db_test CREATE TABLE db_test.id_val(`id` UInt32, `val` UInt32) ENGINE = Join(ANY, LEFT, id) SETTINGS join_use_nulls = 1 INSERT INTO db_test.id_val VALUES (1,11)(2,12)(4,13)  ┌─id─┬─val─┐ │ 4 │ 13 │ │ 2 │ 12 │ │ 1 │ 11 │ └────┴─────┘  Query: SELECT joinGet(db_test.id_val,'val',toUInt32(number)) from numbers(4) SETTINGS join_use_nulls = 1  Result: ┌─joinGet(db_test.id_val, 'val', toUInt32(number))─┐ │ 0 │ │ 11 │ │ 12 │ │ 0 │ └──────────────────────────────────────────────────┘  "},{"title":"modelEvaluate(model_name, …)​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#function-modelevaluate","content":"Evaluate external model. Accepts a model name and model arguments. Returns Float64. "},{"title":"throwIf(x[, custom_message])​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#throwifx-custom-message","content":"Throw an exception if the argument is non zero. custom_message - is an optional parameter: a constant string, provides an error message SELECT throwIf(number = 3, 'Too many') FROM numbers(10);  ↙ Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.) Received exception from server (version 19.14.1): Code: 395. DB::Exception: Received from localhost:9000. DB::Exception: Too many.  "},{"title":"identity​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#identity","content":"Returns the same value that was used as its argument. Used for debugging and testing, allows to cancel using index, and get the query performance of a full scan. When query is analyzed for possible use of index, the analyzer does not look inside identity functions. Also constant folding is not applied too. Syntax identity(x)  Example Query: SELECT identity(42)  Result: ┌─identity(42)─┐ │ 42 │ └──────────────┘  "},{"title":"randomPrintableASCII​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#randomascii","content":"Generates a string with a random set of ASCII printable characters. Syntax randomPrintableASCII(length)  Arguments length — Resulting string length. Positive integer. If you pass `length &lt; 0`, behavior of the function is undefined.  Returned value String with a random set of ASCII printable characters. Type: String Example SELECT number, randomPrintableASCII(30) as str, length(str) FROM system.numbers LIMIT 3  ┌─number─┬─str────────────────────────────┬─length(randomPrintableASCII(30))─┐ │ 0 │ SuiCOSTvC0csfABSw=UcSzp2.`rv8x │ 30 │ │ 1 │ 1Ag NlJ &amp;RCN:*&gt;HVPG;PE-nO&quot;SUFD │ 30 │ │ 2 │ /&quot;+&lt;&quot;wUTh:=LjJ Vm!c&amp;hI*m#XTfzz │ 30 │ └────────┴────────────────────────────────┴──────────────────────────────────┘  "},{"title":"randomString​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#randomstring","content":"Generates a binary string of the specified length filled with random bytes (including zero bytes). Syntax randomString(length)  Arguments length — String length. Positive integer. Returned value String filled with random bytes. Type: String. Example Query: SELECT randomString(30) AS str, length(str) AS len FROM numbers(2) FORMAT Vertical;  Result: Row 1: ────── str: 3 G : pT ?w тi k aV f6 len: 30 Row 2: ────── str: 9 ,] ^ ) ]?? 8 len: 30  See Also generateRandomrandomPrintableASCII "},{"title":"randomFixedString​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#randomfixedstring","content":"Generates a binary string of the specified length filled with random bytes (including zero bytes). Syntax randomFixedString(length);  Arguments length — String length in bytes. UInt64. Returned value(s) String filled with random bytes. Type: FixedString. Example Query: SELECT randomFixedString(13) as rnd, toTypeName(rnd)  Result: ┌─rnd──────┬─toTypeName(randomFixedString(13))─┐ │ j▒h㋖HɨZ'▒ │ FixedString(13) │ └──────────┴───────────────────────────────────┘  "},{"title":"randomStringUTF8​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#randomstringutf8","content":"Generates a random string of a specified length. Result string contains valid UTF-8 code points. The value of code points may be outside of the range of assigned Unicode. Syntax randomStringUTF8(length);  Arguments length — Required length of the resulting string in code points. UInt64. Returned value(s) UTF-8 random string. Type: String. Example Query: SELECT randomStringUTF8(13)  Result: ┌─randomStringUTF8(13)─┐ │ 𘤗𙉝д兠庇󡅴󱱎󦐪􂕌𔊹𓰛 │ └──────────────────────┘  "},{"title":"getSetting​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#getSetting","content":"Returns the current value of a custom setting. Syntax getSetting('custom_setting');  Parameter custom_setting — The setting name. String. Returned value The setting current value. Example SET custom_a = 123; SELECT getSetting('custom_a');  Result 123  See Also Custom Settings "},{"title":"isDecimalOverflow​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#is-decimal-overflow","content":"Checks whether the Decimal value is out of its (or specified) precision. Syntax isDecimalOverflow(d, [p])  Arguments d — value. Decimal.p — precision. Optional. If omitted, the initial precision of the first argument is used. Using of this paratemer could be helpful for data extraction to another DBMS or file. UInt8. Returned values 1 — Decimal value has more digits then it's precision allow,0 — Decimal value satisfies the specified precision. Example Query: SELECT isDecimalOverflow(toDecimal32(1000000000, 0), 9), isDecimalOverflow(toDecimal32(1000000000, 0)), isDecimalOverflow(toDecimal32(-1000000000, 0), 9), isDecimalOverflow(toDecimal32(-1000000000, 0));  Result: 1 1 1 1  "},{"title":"countDigits​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#count-digits","content":"Returns number of decimal digits you need to represent the value. Syntax countDigits(x)  Arguments x — Int or Decimal value. Returned value Number of digits. Type: UInt8. note For Decimal values takes into account their scales: calculates result over underlying integer type which is (value * scale). For example: countDigits(42) = 2, countDigits(42.000) = 5, countDigits(0.04200) = 4. I.e. you may check decimal overflow for Decimal64 with countDecimal(x) &gt; 18. It's a slow variant of isDecimalOverflow. Example Query: SELECT countDigits(toDecimal32(1, 9)), countDigits(toDecimal32(-1, 9)), countDigits(toDecimal64(1, 18)), countDigits(toDecimal64(-1, 18)), countDigits(toDecimal128(1, 38)), countDigits(toDecimal128(-1, 38));  Result: 10 10 19 19 39 39  "},{"title":"errorCodeToName​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#error-code-to-name","content":"Returned value Variable name for the error code. Type: LowCardinality(String). Syntax errorCodeToName(1)  Result: UNSUPPORTED_METHOD  "},{"title":"tcpPort​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#tcpPort","content":"Returns native interface TCP port number listened by this server. If it is executed in the context of a distributed table, then it generates a normal column, otherwise it produces a constant value. Syntax tcpPort()  Arguments None. Returned value The TCP port number. Type: UInt16. Example Query: SELECT tcpPort();  Result: ┌─tcpPort()─┐ │ 9000 │ └───────────┘  See Also tcp_port "},{"title":"currentProfiles​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#current-profiles","content":"Returns a list of the current settings profiles for the current user. The command SET PROFILE could be used to change the current setting profile. If the command SET PROFILE was not used the function returns the profiles specified at the current user's definition (see CREATE USER). Syntax currentProfiles()  Returned value List of the current user settings profiles.  Type: Array(String). "},{"title":"enabledProfiles​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#enabled-profiles","content":"Returns settings profiles, assigned to the current user both explicitly and implicitly. Explicitly assigned profiles are the same as returned by the currentProfiles function. Implicitly assigned profiles include parent profiles of other assigned profiles, profiles assigned via granted roles, profiles assigned via their own settings, and the main default profile (see the default_profile section in the main server configuration file). Syntax enabledProfiles()  Returned value List of the enabled settings profiles.  Type: Array(String). "},{"title":"defaultProfiles​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#default-profiles","content":"Returns all the profiles specified at the current user's definition (see CREATE USER statement). Syntax defaultProfiles()  Returned value List of the default settings profiles.  Type: Array(String). "},{"title":"currentRoles​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#current-roles","content":"Returns the names of the roles which are current for the current user. The current roles can be changed by the SET ROLE statement. If the SET ROLE statement was not used, the function currentRoles returns the same as defaultRoles. Syntax currentRoles()  Returned value List of the current roles for the current user.  Type: Array(String). "},{"title":"enabledRoles​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#enabled-roles","content":"Returns the names of the current roles and the roles, granted to some of the current roles. Syntax enabledRoles()  Returned value List of the enabled roles for the current user.  Type: Array(String). "},{"title":"defaultRoles​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#default-roles","content":"Returns the names of the roles which are enabled by default for the current user when he logins. Initially these are all roles granted to the current user (see GRANT), but that can be changed with the SET DEFAULT ROLE statement. Syntax defaultRoles()  Returned value List of the default roles for the current user.  Type: Array(String). "},{"title":"getServerPort​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#getserverport","content":"Returns the number of the server port. When the port is not used by the server, throws an exception. Syntax getServerPort(port_name)  Arguments port_name — The name of the server port. String. Possible values: 'tcp_port''tcp_port_secure''http_port''https_port''interserver_http_port''interserver_https_port''mysql_port''postgresql_port''grpc_port''prometheus.port' Returned value The number of the server port. Type: UInt16. Example Query: SELECT getServerPort('tcp_port');  Result: ┌─getServerPort('tcp_port')─┐ │ 9000 │ └───────────────────────────┘  "},{"title":"queryID​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#query-id","content":"Returns the ID of the current query. Other parameters of a query can be extracted from the system.query_log table via query_id. In contrast to initialQueryID function, queryID can return different results on different shards (see example). Syntax queryID()  Returned value The ID of the current query. Type: String Example Query: CREATE TABLE tmp (str String) ENGINE = Log; INSERT INTO tmp (*) VALUES ('a'); SELECT count(DISTINCT t) FROM (SELECT queryID() AS t FROM remote('127.0.0.{1..3}', currentDatabase(), 'tmp') GROUP BY queryID());  Result: ┌─count()─┐ │ 3 │ └─────────┘  "},{"title":"initialQueryID​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#initial-query-id","content":"Returns the ID of the initial current query. Other parameters of a query can be extracted from the system.query_log table via initial_query_id. In contrast to queryID function, initialQueryID returns the same results on different shards (see example). Syntax initialQueryID()  Returned value The ID of the initial current query. Type: String Example Query: CREATE TABLE tmp (str String) ENGINE = Log; INSERT INTO tmp (*) VALUES ('a'); SELECT count(DISTINCT t) FROM (SELECT initialQueryID() AS t FROM remote('127.0.0.{1..3}', currentDatabase(), 'tmp') GROUP BY queryID());  Result: ┌─count()─┐ │ 1 │ └─────────┘  "},{"title":"shardNum​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#shard-num","content":"Returns the index of a shard which processes a part of data for a distributed query. Indices are started from 1. If a query is not distributed then constant value 0 is returned. Syntax shardNum()  Returned value Shard index or constant 0. Type: UInt32. Example In the following example a configuration with two shards is used. The query is executed on the system.one table on every shard. Query: CREATE TABLE shard_num_example (dummy UInt8) ENGINE=Distributed(test_cluster_two_shards_localhost, system, one, dummy); SELECT dummy, shardNum(), shardCount() FROM shard_num_example;  Result: ┌─dummy─┬─shardNum()─┬─shardCount()─┐ │ 0 │ 2 │ 2 │ │ 0 │ 1 │ 2 │ └───────┴────────────┴──────────────┘  See Also Distributed Table Engine "},{"title":"shardCount​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#shard-count","content":"Returns the total number of shards for a distributed query. If a query is not distributed then constant value 0 is returned. Syntax shardCount()  Returned value Total number of shards or 0. Type: UInt32. See Also shardNum() function example also contains shardCount() function call. "},{"title":"getOSKernelVersion​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#getoskernelversion","content":"Returns a string with the current OS kernel version. Syntax getOSKernelVersion()  Arguments None. Returned value The current OS kernel version. Type: String. Example Query: SELECT getOSKernelVersion();  Result: ┌─getOSKernelVersion()────┐ │ Linux 4.15.0-55-generic │ └─────────────────────────┘  "},{"title":"zookeeperSessionUptime​","type":1,"pageTitle":"Other Functions","url":"en/sql-reference/functions/other-functions#zookeepersessionuptime","content":"Returns the uptime of the current ZooKeeper session in seconds. Syntax zookeeperSessionUptime()  Arguments None. Returned value Uptime of the current ZooKeeper session in seconds. Type: UInt32. Example Query: SELECT zookeeperSessionUptime();  Result: ┌─zookeeperSessionUptime()─┐ │ 286 │ └──────────────────────────┘  "},{"title":"EXISTS","type":0,"sectionRef":"#","url":"en/sql-reference/operators/exists","content":"EXISTS The EXISTS operator checks how many records are in the result of a subquery. If it is empty, then the operator returns 0. Otherwise, it returns 1. EXISTS can be used in a WHERE clause. warning References to main query tables and columns are not supported in a subquery. Syntax WHERE EXISTS(subquery) Example Query with a subquery returning several rows: SELECT count() FROM numbers(10) WHERE EXISTS(SELECT number FROM numbers(10) WHERE number &gt; 8); Result: ┌─count()─┐ │ 10 │ └─────────┘ Query with a subquery that returns an empty result: SELECT count() FROM numbers(10) WHERE EXISTS(SELECT number FROM numbers(10) WHERE number &gt; 11); Result: ┌─count()─┐ │ 0 │ └─────────┘ ","keywords":""},{"title":"IN Operators","type":0,"sectionRef":"#","url":"en/sql-reference/operators/in","content":"","keywords":""},{"title":"NULL Processing​","type":1,"pageTitle":"IN Operators","url":"en/sql-reference/operators/in#in-null-processing","content":"During request processing, the IN operator assumes that the result of an operation with NULL always equals 0, regardless of whether NULL is on the right or left side of the operator. NULL values are not included in any dataset, do not correspond to each other and cannot be compared if transform_null_in = 0. Here is an example with the t_null table: ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ │ 2 │ 3 │ └───┴──────┘  Running the query SELECT x FROM t_null WHERE y IN (NULL,3) gives you the following result: ┌─x─┐ │ 2 │ └───┘  You can see that the row in which y = NULL is thrown out of the query results. This is because ClickHouse can’t decide whether NULL is included in the (NULL,3) set, returns 0 as the result of the operation, and SELECT excludes this row from the final output. SELECT y IN (NULL, 3) FROM t_null  ┌─in(y, tuple(NULL, 3))─┐ │ 0 │ │ 1 │ └───────────────────────┘  "},{"title":"Distributed Subqueries​","type":1,"pageTitle":"IN Operators","url":"en/sql-reference/operators/in#select-distributed-subqueries","content":"There are two options for IN-s with subqueries (similar to JOINs): normal IN / JOIN and GLOBAL IN / GLOBAL JOIN. They differ in how they are run for distributed query processing. note Remember that the algorithms described below may work differently depending on the settings distributed_product_mode setting. When using the regular IN, the query is sent to remote servers, and each of them runs the subqueries in the IN or JOIN clause. When using GLOBAL IN / GLOBAL JOINs, first all the subqueries are run for GLOBAL IN / GLOBAL JOINs, and the results are collected in temporary tables. Then the temporary tables are sent to each remote server, where the queries are run using this temporary data. For a non-distributed query, use the regular IN / JOIN. Be careful when using subqueries in the IN / JOIN clauses for distributed query processing. Let’s look at some examples. Assume that each server in the cluster has a normal local_table. Each server also has a distributed_table table with the Distributed type, which looks at all the servers in the cluster. For a query to the distributed_table, the query will be sent to all the remote servers and run on them using the local_table. For example, the query SELECT uniq(UserID) FROM distributed_table  will be sent to all remote servers as SELECT uniq(UserID) FROM local_table  and run on each of them in parallel, until it reaches the stage where intermediate results can be combined. Then the intermediate results will be returned to the requestor server and merged on it, and the final result will be sent to the client. Now let’s examine a query with IN: SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM local_table WHERE CounterID = 34)  Calculation of the intersection of audiences of two sites. This query will be sent to all remote servers as SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM local_table WHERE CounterID = 34)  In other words, the data set in the IN clause will be collected on each server independently, only across the data that is stored locally on each of the servers. This will work correctly and optimally if you are prepared for this case and have spread data across the cluster servers such that the data for a single UserID resides entirely on a single server. In this case, all the necessary data will be available locally on each server. Otherwise, the result will be inaccurate. We refer to this variation of the query as “local IN”. To correct how the query works when data is spread randomly across the cluster servers, you could specify distributed_table inside a subquery. The query would look like this: SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)  This query will be sent to all remote servers as SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)  The subquery will begin running on each remote server. Since the subquery uses a distributed table, the subquery that is on each remote server will be resent to every remote server as SELECT UserID FROM local_table WHERE CounterID = 34  For example, if you have a cluster of 100 servers, executing the entire query will require 10,000 elementary requests, which is generally considered unacceptable. In such cases, you should always use GLOBAL IN instead of IN. Let’s look at how it works for the query SELECT uniq(UserID) FROM distributed_table WHERE CounterID = 101500 AND UserID GLOBAL IN (SELECT UserID FROM distributed_table WHERE CounterID = 34)  The requestor server will run the subquery SELECT UserID FROM distributed_table WHERE CounterID = 34  and the result will be put in a temporary table in RAM. Then the request will be sent to each remote server as SELECT uniq(UserID) FROM local_table WHERE CounterID = 101500 AND UserID GLOBAL IN _data1  and the temporary table _data1 will be sent to every remote server with the query (the name of the temporary table is implementation-defined). This is more optimal than using the normal IN. However, keep the following points in mind: When creating a temporary table, data is not made unique. To reduce the volume of data transmitted over the network, specify DISTINCT in the subquery. (You do not need to do this for a normal IN.)The temporary table will be sent to all the remote servers. Transmission does not account for network topology. For example, if 10 remote servers reside in a datacenter that is very remote in relation to the requestor server, the data will be sent 10 times over the channel to the remote datacenter. Try to avoid large data sets when using GLOBAL IN.When transmitting data to remote servers, restrictions on network bandwidth are not configurable. You might overload the network.Try to distribute data across servers so that you do not need to use GLOBAL IN on a regular basis.If you need to use GLOBAL IN often, plan the location of the ClickHouse cluster so that a single group of replicas resides in no more than one data center with a fast network between them, so that a query can be processed entirely within a single data center. It also makes sense to specify a local table in the GLOBAL IN clause, in case this local table is only available on the requestor server and you want to use data from it on remote servers. "},{"title":"Distributed Subqueries and max_rows_in_set​","type":1,"pageTitle":"IN Operators","url":"en/sql-reference/operators/in#distributed-subqueries-and-max_rows_in_set","content":"You can use max_rows_in_set and max_bytes_in_set to control how much data is tranferred during distributed queries. This is specially important if the global in query returns a large amount of data. Consider the following sql - select * from table1 where col1 global in (select col1 from table2 where &lt;some_predicate&gt;)  If some_predicate is not selective enough, it will return large amount of data and cause performance issues. In such cases, it is wise to limit the data transfer over the network. Also, note that set_overflow_mode is set to throw (by default) meaning that an exception is raised when these thresholds are met. "},{"title":"Distributed Subqueries and max_parallel_replicas​","type":1,"pageTitle":"IN Operators","url":"en/sql-reference/operators/in#max_parallel_replica-subqueries","content":"When max_parallel_replicas is greater than 1, distributed queries are further transformed. For example, the following: SELECT CounterID, count() FROM distributed_table_1 WHERE UserID IN (SELECT UserID FROM local_table_2 WHERE CounterID &lt; 100) SETTINGS max_parallel_replicas=3  is transformed on each server into SELECT CounterID, count() FROM local_table_1 WHERE UserID IN (SELECT UserID FROM local_table_2 WHERE CounterID &lt; 100) SETTINGS parallel_replicas_count=3, parallel_replicas_offset=M  where M is between 1 and 3 depending on which replica the local query is executing on. These settings affect every MergeTree-family table in the query and have the same effect as applying SAMPLE 1/3 OFFSET (M-1)/3 on each table. Therefore adding the max_parallel_replicas setting will only produce correct results if both tables have the same replication scheme and are sampled by UserID or a subkey of it. In particular, if local_table_2 does not have a sampling key, incorrect results will be produced. The same rule applies to JOIN. One workaround if local_table_2 does not meet the requirements, is to use GLOBAL IN or GLOBAL JOIN. "},{"title":"ClickHouse SQL Statements","type":0,"sectionRef":"#","url":"en/sql-reference/statements/","content":"ClickHouse SQL Statements Statements represent various kinds of action you can perform using SQL queries. Each kind of statement has it’s own syntax and usage details that are described separately: SELECTINSERT INTOCREATEALTERSYSTEMSHOWGRANTREVOKEATTACHCHECK TABLEDESCRIBE TABLEDETACHDROPEXISTSKILLOPTIMIZERENAMESETSET ROLETRUNCATEUSEEXPLAIN","keywords":""},{"title":"ALTER","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/","content":"","keywords":""},{"title":"Mutations​","type":1,"pageTitle":"ALTER","url":"en/sql-reference/statements/alter/#mutations","content":"ALTER queries that are intended to manipulate table data are implemented with a mechanism called “mutations”, most notably ALTER TABLE … DELETE and ALTER TABLE … UPDATE. They are asynchronous background processes similar to merges in MergeTree tables that to produce new “mutated” versions of parts. For *MergeTree tables mutations execute by rewriting whole data parts. There is no atomicity - parts are substituted for mutated parts as soon as they are ready and a SELECT query that started executing during a mutation will see data from parts that have already been mutated along with data from parts that have not been mutated yet. Mutations are totally ordered by their creation order and are applied to each part in that order. Mutations are also partially ordered with INSERT INTO queries: data that was inserted into the table before the mutation was submitted will be mutated and data that was inserted after that will not be mutated. Note that mutations do not block inserts in any way. A mutation query returns immediately after the mutation entry is added (in case of replicated tables to ZooKeeper, for non-replicated tables - to the filesystem). The mutation itself executes asynchronously using the system profile settings. To track the progress of mutations you can use the system.mutations table. A mutation that was successfully submitted will continue to execute even if ClickHouse servers are restarted. There is no way to roll back the mutation once it is submitted, but if the mutation is stuck for some reason it can be cancelled with the KILL MUTATION query. Entries for finished mutations are not deleted right away (the number of preserved entries is determined by the finished_mutations_to_keep storage engine parameter). Older mutation entries are deleted. "},{"title":"Synchronicity of ALTER Queries​","type":1,"pageTitle":"ALTER","url":"en/sql-reference/statements/alter/#synchronicity-of-alter-queries","content":"For non-replicated tables, all ALTER queries are performed synchronously. For replicated tables, the query just adds instructions for the appropriate actions to ZooKeeper, and the actions themselves are performed as soon as possible. However, the query can wait for these actions to be completed on all the replicas. For all ALTER queries, you can use the replication_alter_partitions_sync setting to set up waiting. You can specify how long (in seconds) to wait for inactive replicas to execute all ALTER queries with the replication_wait_for_inactive_replica_timeout setting. note For all ALTER queries, if replication_alter_partitions_sync = 2 and some replicas are not active for more than the time, specified in the replication_wait_for_inactive_replica_timeout setting, then an exception UNFINISHED is thrown. For ALTER TABLE ... UPDATE|DELETE queries the synchronicity is defined by the mutations_sync setting. "},{"title":"Column Manipulations","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/column","content":"","keywords":""},{"title":"ADD COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"en/sql-reference/statements/alter/column#alter_add-column","content":"ADD COLUMN [IF NOT EXISTS] name [type] [default_expr] [codec] [AFTER name_after | FIRST]  Adds a new column to the table with the specified name, type, codec and default_expr (see the section Default expressions). If the IF NOT EXISTS clause is included, the query won’t return an error if the column already exists. If you specify AFTER name_after (the name of another column), the column is added after the specified one in the list of table columns. If you want to add a column to the beginning of the table use the FIRST clause. Otherwise, the column is added to the end of the table. For a chain of actions, name_after can be the name of a column that is added in one of the previous actions. Adding a column just changes the table structure, without performing any actions with data. The data does not appear on the disk after ALTER. If the data is missing for a column when reading from the table, it is filled in with default values (by performing the default expression if there is one, or using zeros or empty strings). The column appears on the disk after merging data parts (see MergeTree). This approach allows us to complete the ALTER query instantly, without increasing the volume of old data. Example: ALTER TABLE alter_test ADD COLUMN Added1 UInt32 FIRST; ALTER TABLE alter_test ADD COLUMN Added2 UInt32 AFTER NestedColumn; ALTER TABLE alter_test ADD COLUMN Added3 UInt32 AFTER ToDrop; DESC alter_test FORMAT TSV;  Added1 UInt32 CounterID UInt32 StartDate Date UserID UInt32 VisitID UInt32 NestedColumn.A Array(UInt8) NestedColumn.S Array(String) Added2 UInt32 ToDrop UInt32 Added3 UInt32  "},{"title":"DROP COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"en/sql-reference/statements/alter/column#alter_drop-column","content":"DROP COLUMN [IF EXISTS] name  Deletes the column with the name name. If the IF EXISTS clause is specified, the query won’t return an error if the column does not exist. Deletes data from the file system. Since this deletes entire files, the query is completed almost instantly. warning You can’t delete a column if it is referenced by materialized view. Otherwise, it returns an error. Example: ALTER TABLE visits DROP COLUMN browser  "},{"title":"RENAME COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"en/sql-reference/statements/alter/column#alter_rename-column","content":"RENAME COLUMN [IF EXISTS] name to new_name  Renames the column name to new_name. If the IF EXISTS clause is specified, the query won’t return an error if the column does not exist. Since renaming does not involve the underlying data, the query is completed almost instantly. NOTE: Columns specified in the key expression of the table (either with ORDER BY or PRIMARY KEY) cannot be renamed. Trying to change these columns will produce SQL Error [524]. Example: ALTER TABLE visits RENAME COLUMN webBrowser TO browser  "},{"title":"CLEAR COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"en/sql-reference/statements/alter/column#alter_clear-column","content":"CLEAR COLUMN [IF EXISTS] name IN PARTITION partition_name  Resets all data in a column for a specified partition. Read more about setting the partition name in the section How to specify the partition expression. If the IF EXISTS clause is specified, the query won’t return an error if the column does not exist. Example: ALTER TABLE visits CLEAR COLUMN browser IN PARTITION tuple()  "},{"title":"COMMENT COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"en/sql-reference/statements/alter/column#alter_comment-column","content":"COMMENT COLUMN [IF EXISTS] name 'Text comment'  Adds a comment to the column. If the IF EXISTS clause is specified, the query won’t return an error if the column does not exist. Each column can have one comment. If a comment already exists for the column, a new comment overwrites the previous comment. Comments are stored in the comment_expression column returned by the DESCRIBE TABLE query. Example: ALTER TABLE visits COMMENT COLUMN browser 'The table shows the browser used for accessing the site.'  "},{"title":"MODIFY COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"en/sql-reference/statements/alter/column#alter_modify-column","content":"MODIFY COLUMN [IF EXISTS] name [type] [default_expr] [codec] [TTL] [AFTER name_after | FIRST] ALTER COLUMN [IF EXISTS] name TYPE [type] [default_expr] [codec] [TTL] [AFTER name_after | FIRST]  This query changes the name column properties: Type Default expression Compression Codec TTL For examples of columns compression CODECS modifying, see Column Compression Codecs. For examples of columns TTL modifying, see Column TTL. If the IF EXISTS clause is specified, the query won’t return an error if the column does not exist. The query also can change the order of the columns using FIRST | AFTER clause, see ADD COLUMN description. When changing the type, values are converted as if the toType functions were applied to them. If only the default expression is changed, the query does not do anything complex, and is completed almost instantly. Example: ALTER TABLE visits MODIFY COLUMN browser Array(String)  Changing the column type is the only complex action – it changes the contents of files with data. For large tables, this may take a long time. The ALTER query is atomic. For MergeTree tables it is also lock-free. The ALTER query for changing columns is replicated. The instructions are saved in ZooKeeper, then each replica applies them. All ALTER queries are run in the same order. The query waits for the appropriate actions to be completed on the other replicas. However, a query to change columns in a replicated table can be interrupted, and all actions will be performed asynchronously. "},{"title":"MODIFY COLUMN REMOVE​","type":1,"pageTitle":"Column Manipulations","url":"en/sql-reference/statements/alter/column#modify-remove","content":"Removes one of the column properties: DEFAULT, ALIAS, MATERIALIZED, CODEC, COMMENT, TTL. Syntax: ALTER TABLE table_name MODIFY column_name REMOVE property;  Example Remove TTL: ALTER TABLE table_with_ttl MODIFY COLUMN column_ttl REMOVE TTL;  See Also REMOVE TTL. "},{"title":"MATERIALIZE COLUMN​","type":1,"pageTitle":"Column Manipulations","url":"en/sql-reference/statements/alter/column#materialize-column","content":"Materializes or updates a column with an expression for a default value (DEFAULT or MATERIALIZED). It is used if it is necessary to add or update a column with a complicated expression, because evaluating such an expression directly on SELECT executing turns out to be expensive. Syntax: ALTER TABLE table MATERIALIZE COLUMN col;  Example DROP TABLE IF EXISTS tmp; SET mutations_sync = 2; CREATE TABLE tmp (x Int64) ENGINE = MergeTree() ORDER BY tuple() PARTITION BY tuple(); INSERT INTO tmp SELECT * FROM system.numbers LIMIT 5; ALTER TABLE tmp ADD COLUMN s String MATERIALIZED toString(x); ALTER TABLE tmp MATERIALIZE COLUMN s; SELECT groupArray(x), groupArray(s) FROM (select x,s from tmp order by x); ┌─groupArray(x)─┬─groupArray(s)─────────┐ │ [0,1,2,3,4] │ ['0','1','2','3','4'] │ └───────────────┴───────────────────────┘ ALTER TABLE tmp MODIFY COLUMN s String MATERIALIZED toString(round(100/x)); INSERT INTO tmp SELECT * FROM system.numbers LIMIT 5,5; SELECT groupArray(x), groupArray(s) FROM tmp; ┌─groupArray(x)─────────┬─groupArray(s)──────────────────────────────────┐ │ [0,1,2,3,4,5,6,7,8,9] │ ['0','1','2','3','4','20','17','14','12','11'] │ └───────────────────────┴────────────────────────────────────────────────┘ ALTER TABLE tmp MATERIALIZE COLUMN s; SELECT groupArray(x), groupArray(s) FROM tmp; ┌─groupArray(x)─────────┬─groupArray(s)─────────────────────────────────────────┐ │ [0,1,2,3,4,5,6,7,8,9] │ ['inf','100','50','33','25','20','17','14','12','11'] │ └───────────────────────┴───────────────────────────────────────────────────────┘  See Also MATERIALIZED. "},{"title":"Limitations​","type":1,"pageTitle":"Column Manipulations","url":"en/sql-reference/statements/alter/column#alter-query-limitations","content":"The ALTER query lets you create and delete separate elements (columns) in nested data structures, but not whole nested data structures. To add a nested data structure, you can add columns with a name like name.nested_name and the type Array(T). A nested data structure is equivalent to multiple array columns with a name that has the same prefix before the dot. There is no support for deleting columns in the primary key or the sampling key (columns that are used in the ENGINE expression). Changing the type for columns that are included in the primary key is only possible if this change does not cause the data to be modified (for example, you are allowed to add values to an Enum or to change a type from DateTime to UInt32). If the ALTER query is not sufficient to make the table changes you need, you can create a new table, copy the data to it using the INSERT SELECT query, then switch the tables using the RENAME query and delete the old table. You can use the clickhouse-copier as an alternative to the INSERT SELECT query. The ALTER query blocks all reads and writes for the table. In other words, if a long SELECT is running at the time of the ALTER query, the ALTER query will wait for it to complete. At the same time, all new queries to the same table will wait while this ALTER is running. For tables that do not store data themselves (such as Merge and Distributed), ALTER just changes the table structure, and does not change the structure of subordinate tables. For example, when running ALTER for a Distributed table, you will also need to run ALTER for the tables on all remote servers. "},{"title":"ALTER TABLE … MODIFY COMMENT","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/comment","content":"ALTER TABLE … MODIFY COMMENT Adds, modifies, or removes comment to the table, regardless if it was set before or not. Comment change is reflected in both system.tables and SHOW CREATE TABLE query. Syntax ALTER TABLE [db].name [ON CLUSTER cluster] MODIFY COMMENT 'Comment' Examples Creating a table with comment (for more information, see the [COMMENT] clause(../../../sql-reference/statements/create/table.md#comment-table)): CREATE TABLE table_with_comment ( `k` UInt64, `s` String ) ENGINE = Memory() COMMENT 'The temporary table'; Modifying the table comment: ALTER TABLE table_with_comment MODIFY COMMENT 'new comment on a table'; SELECT comment FROM system.tables WHERE database = currentDatabase() AND name = 'table_with_comment'; Output of a new comment: ┌─comment────────────────┐ │ new comment on a table │ └────────────────────────┘ Removing the table comment: ALTER TABLE table_with_comment MODIFY COMMENT ''; SELECT comment FROM system.tables WHERE database = currentDatabase() AND name = 'table_with_comment'; Output of a removed comment: ┌─comment─┐ │ │ └─────────┘ ","keywords":""},{"title":"Manipulating Constraints","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/constraint","content":"Manipulating Constraints Constraints could be added or deleted using following syntax: ALTER TABLE [db].name ADD CONSTRAINT constraint_name CHECK expression; ALTER TABLE [db].name DROP CONSTRAINT constraint_name; See more on constraints. Queries will add or remove metadata about constraints from table so they are processed immediately. warning Constraint check will not be executed on existing data if it was added. All changes on replicated tables are broadcasted to ZooKeeper and will be applied on other replicas as well.","keywords":""},{"title":"ALTER TABLE … DELETE Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/delete","content":"ALTER TABLE … DELETE Statement ALTER TABLE [db.]table [ON CLUSTER cluster] DELETE WHERE filter_expr Deletes data matching the specified filtering expression. Implemented as a mutation. note The ALTER TABLE prefix makes this syntax different from most other systems supporting SQL. It is intended to signify that unlike similar queries in OLTP databases this is a heavy operation not designed for frequent use. The filter_expr must be of type UInt8. The query deletes rows in the table for which this expression takes a non-zero value. One query can contain several commands separated by commas. The synchronicity of the query processing is defined by the mutations_sync setting. By default, it is asynchronous. See also MutationsSynchronicity of ALTER Queriesmutations_sync setting","keywords":""},{"title":"Manipulating Data Skipping Indices","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/index/","content":"Manipulating Data Skipping Indices The following operations are available: ALTER TABLE [db].name ADD INDEX name expression TYPE type GRANULARITY value [FIRST|AFTER name] - Adds index description to tables metadata. ALTER TABLE [db].name DROP INDEX name - Removes index description from tables metadata and deletes index files from disk. ALTER TABLE [db.]table MATERIALIZE INDEX name IN PARTITION partition_name - The query rebuilds the secondary index name in the partition partition_name. Implemented as a mutation. To rebuild index over the whole data in the table you need to remove IN PARTITION from query. The first two commands are lightweight in a sense that they only change metadata or remove files. Also, they are replicated, syncing indices metadata via ZooKeeper. note Index manipulation is supported only for tables with *MergeTree engine (including replicated variants).","keywords":""},{"title":"Manipulating Key Expressions","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/order-by","content":"Manipulating Key Expressions ALTER TABLE [db].name [ON CLUSTER cluster] MODIFY ORDER BY new_expression The command changes the sorting key of the table to new_expression (an expression or a tuple of expressions). Primary key remains the same. The command is lightweight in a sense that it only changes metadata. To keep the property that data part rows are ordered by the sorting key expression you cannot add expressions containing existing columns to the sorting key (only columns added by the ADD COLUMN command in the same ALTER query, without default column value). note It only works for tables in the MergeTree family (including replicated tables).","keywords":""},{"title":"Manipulating Partitions and Parts","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/partition","content":"","keywords":""},{"title":"DETACH PARTITION|PART​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter_detach-partition","content":"ALTER TABLE table_name DETACH PARTITION|PART partition_expr  Moves all data for the specified partition to the detached directory. The server forgets about the detached data partition as if it does not exist. The server will not know about this data until you make the ATTACH query. Example: ALTER TABLE mt DETACH PARTITION '2020-11-21'; ALTER TABLE mt DETACH PART 'all_2_2_0';  Read about setting the partition expression in a section How to specify the partition expression. After the query is executed, you can do whatever you want with the data in the detached directory — delete it from the file system, or just leave it. This query is replicated – it moves the data to the detached directory on all replicas. Note that you can execute this query only on a leader replica. To find out if a replica is a leader, perform the SELECT query to the system.replicas table. Alternatively, it is easier to make a DETACH query on all replicas - all the replicas throw an exception, except the leader replicas (as multiple leaders are allowed). "},{"title":"DROP PARTITION|PART​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter_drop-partition","content":"ALTER TABLE table_name DROP PARTITION|PART partition_expr  Deletes the specified partition from the table. This query tags the partition as inactive and deletes data completely, approximately in 10 minutes. Read about setting the partition expression in a section How to specify the partition expression. The query is replicated – it deletes data on all replicas. Example: ALTER TABLE mt DROP PARTITION '2020-11-21'; ALTER TABLE mt DROP PART 'all_4_4_0';  "},{"title":"DROP DETACHED PARTITION|PART​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter_drop-detached","content":"ALTER TABLE table_name DROP DETACHED PARTITION|PART partition_expr  Removes the specified part or all parts of the specified partition from detached. Read more about setting the partition expression in a section How to specify the partition expression. "},{"title":"ATTACH PARTITION|PART​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter_attach-partition","content":"ALTER TABLE table_name ATTACH PARTITION|PART partition_expr  Adds data to the table from the detached directory. It is possible to add data for an entire partition or for a separate part. Examples: ALTER TABLE visits ATTACH PARTITION 201901; ALTER TABLE visits ATTACH PART 201901_2_2_0;  Read more about setting the partition expression in a section How to specify the partition expression. This query is replicated. The replica-initiator checks whether there is data in the detached directory. If data exists, the query checks its integrity. If everything is correct, the query adds the data to the table. If the non-initiator replica, receiving the attach command, finds the part with the correct checksums in its own detached folder, it attaches the data without fetching it from other replicas. If there is no part with the correct checksums, the data is downloaded from any replica having the part. You can put data to the detached directory on one replica and use the ALTER ... ATTACH query to add it to the table on all replicas. "},{"title":"ATTACH PARTITION FROM​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter_attach-partition-from","content":"ALTER TABLE table2 ATTACH PARTITION partition_expr FROM table1  This query copies the data partition from table1 to table2. Note that data will be deleted neither from table1 nor from table2. For the query to run successfully, the following conditions must be met: Both tables must have the same structure.Both tables must have the same partition key. "},{"title":"REPLACE PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter_replace-partition","content":"ALTER TABLE table2 REPLACE PARTITION partition_expr FROM table1  This query copies the data partition from the table1 to table2 and replaces existing partition in the table2. Note that data won’t be deleted from table1. For the query to run successfully, the following conditions must be met: Both tables must have the same structure.Both tables must have the same partition key. "},{"title":"MOVE PARTITION TO TABLE​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter_move_to_table-partition","content":"ALTER TABLE table_source MOVE PARTITION partition_expr TO TABLE table_dest  This query moves the data partition from the table_source to table_dest with deleting the data from table_source. For the query to run successfully, the following conditions must be met: Both tables must have the same structure.Both tables must have the same partition key.Both tables must be the same engine family (replicated or non-replicated).Both tables must have the same storage policy. "},{"title":"CLEAR COLUMN IN PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter_clear-column-partition","content":"ALTER TABLE table_name CLEAR COLUMN column_name IN PARTITION partition_expr  Resets all values in the specified column in a partition. If the DEFAULT clause was determined when creating a table, this query sets the column value to a specified default value. Example: ALTER TABLE visits CLEAR COLUMN hour in PARTITION 201902  "},{"title":"FREEZE PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter_freeze-partition","content":"ALTER TABLE table_name FREEZE [PARTITION partition_expr] [WITH NAME 'backup_name']  This query creates a local backup of a specified partition. If the PARTITION clause is omitted, the query creates the backup of all partitions at once. note The entire backup process is performed without stopping the server. Note that for old-styled tables you can specify the prefix of the partition name (for example, 2019) - then the query creates the backup for all the corresponding partitions. Read about setting the partition expression in a section How to specify the partition expression. At the time of execution, for a data snapshot, the query creates hardlinks to a table data. Hardlinks are placed in the directory /var/lib/clickhouse/shadow/N/..., where: /var/lib/clickhouse/ is the working ClickHouse directory specified in the config.N is the incremental number of the backup.if the WITH NAME parameter is specified, then the value of the 'backup_name' parameter is used instead of the incremental number.  note If you use a set of disks for data storage in a table, the shadow/N directory appears on every disk, storing data parts that matched by the PARTITION expression. The same structure of directories is created inside the backup as inside /var/lib/clickhouse/. The query performs chmod for all files, forbidding writing into them. After creating the backup, you can copy the data from /var/lib/clickhouse/shadow/ to the remote server and then delete it from the local server. Note that the ALTER t FREEZE PARTITION query is not replicated. It creates a local backup only on the local server. The query creates backup almost instantly (but first it waits for the current queries to the corresponding table to finish running). ALTER TABLE t FREEZE PARTITION copies only the data, not table metadata. To make a backup of table metadata, copy the file /var/lib/clickhouse/metadata/database/table.sql To restore data from a backup, do the following: Create the table if it does not exist. To view the query, use the .sql file (replace ATTACH in it with CREATE).Copy the data from the data/database/table/ directory inside the backup to the /var/lib/clickhouse/data/database/table/detached/ directory.Run ALTER TABLE t ATTACH PARTITION queries to add the data to a table. Restoring from a backup does not require stopping the server. For more information about backups and restoring data, see the Data Backup section. "},{"title":"UNFREEZE PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter_unfreeze-partition","content":"ALTER TABLE 'table_name' UNFREEZE [PARTITION 'part_expr'] WITH NAME 'backup_name'  Removes freezed partitions with the specified name from the disk. If the PARTITION clause is omitted, the query removes the backup of all partitions at once. "},{"title":"CLEAR INDEX IN PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter_clear-index-partition","content":"ALTER TABLE table_name CLEAR INDEX index_name IN PARTITION partition_expr  The query works similar to CLEAR COLUMN, but it resets an index instead of a column data. "},{"title":"FETCH PARTITION|PART​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter_fetch-partition","content":"ALTER TABLE table_name FETCH PARTITION|PART partition_expr FROM 'path-in-zookeeper'  Downloads a partition from another server. This query only works for the replicated tables. The query does the following: Downloads the partition|part from the specified shard. In ‘path-in-zookeeper’ you must specify a path to the shard in ZooKeeper.Then the query puts the downloaded data to the detached directory of the table_name table. Use the ATTACH PARTITION|PART query to add the data to the table. For example: FETCH PARTITION ALTER TABLE users FETCH PARTITION 201902 FROM '/clickhouse/tables/01-01/visits'; ALTER TABLE users ATTACH PARTITION 201902;  FETCH PART ALTER TABLE users FETCH PART 201901_2_2_0 FROM '/clickhouse/tables/01-01/visits'; ALTER TABLE users ATTACH PART 201901_2_2_0;  Note that: The ALTER ... FETCH PARTITION|PART query isn’t replicated. It places the part or partition to the detached directory only on the local server.The ALTER TABLE ... ATTACH query is replicated. It adds the data to all replicas. The data is added to one of the replicas from the detached directory, and to the others - from neighboring replicas. Before downloading, the system checks if the partition exists and the table structure matches. The most appropriate replica is selected automatically from the healthy replicas. Although the query is called ALTER TABLE, it does not change the table structure and does not immediately change the data available in the table. "},{"title":"MOVE PARTITION|PART​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter_move-partition","content":"Moves partitions or data parts to another volume or disk for MergeTree-engine tables. See Using Multiple Block Devices for Data Storage. ALTER TABLE table_name MOVE PARTITION|PART partition_expr TO DISK|VOLUME 'disk_name'  The ALTER TABLE t MOVE query: Not replicated, because different replicas can have different storage policies.Returns an error if the specified disk or volume is not configured. Query also returns an error if conditions of data moving, that specified in the storage policy, can’t be applied.Can return an error in the case, when data to be moved is already moved by a background process, concurrent ALTER TABLE t MOVE query or as a result of background data merging. A user shouldn’t perform any additional actions in this case. Example: ALTER TABLE hits MOVE PART '20190301_14343_16206_438' TO VOLUME 'slow' ALTER TABLE hits MOVE PARTITION '2019-09-01' TO DISK 'fast_ssd'  "},{"title":"UPDATE IN PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#update-in-partition","content":"Manipulates data in the specifies partition matching the specified filtering expression. Implemented as a mutation. Syntax: ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] [IN PARTITION partition_id] WHERE filter_expr  "},{"title":"Example​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#example","content":"ALTER TABLE mt UPDATE x = x + 1 IN PARTITION 2 WHERE p = 2;  "},{"title":"See Also​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#see-also","content":"UPDATE "},{"title":"DELETE IN PARTITION​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#delete-in-partition","content":"Deletes data in the specifies partition matching the specified filtering expression. Implemented as a mutation. Syntax: ALTER TABLE [db.]table DELETE [IN PARTITION partition_id] WHERE filter_expr  "},{"title":"Example​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#example-1","content":"ALTER TABLE mt DELETE IN PARTITION 2 WHERE p = 2;  "},{"title":"See Also​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#see-also-1","content":"DELETE "},{"title":"How to Set Partition Expression​","type":1,"pageTitle":"Manipulating Partitions and Parts","url":"en/sql-reference/statements/alter/partition#alter-how-to-specify-part-expr","content":"You can specify the partition expression in ALTER ... PARTITION queries in different ways: As a value from the partition column of the system.parts table. For example, ALTER TABLE visits DETACH PARTITION 201901.As a tuple of expressions or constants that matches (in types) the table partitioning keys tuple. In the case of a single element partitioning key, the expression should be wrapped in the tuple (...) function. For example, ALTER TABLE visits DETACH PARTITION tuple(toYYYYMM(toDate('2019-01-25'))).Using the partition ID. Partition ID is a string identifier of the partition (human-readable, if possible) that is used as the names of partitions in the file system and in ZooKeeper. The partition ID must be specified in the PARTITION ID clause, in a single quotes. For example, ALTER TABLE visits DETACH PARTITION ID '201901'.In the ALTER ATTACH PART and DROP DETACHED PART query, to specify the name of a part, use string literal with a value from the name column of the system.detached_parts table. For example, ALTER TABLE visits ATTACH PART '201901_1_1_0'. Usage of quotes when specifying the partition depends on the type of partition expression. For example, for the String type, you have to specify its name in quotes ('). For the Date and Int* types no quotes are needed. All the rules above are also true for the OPTIMIZE query. If you need to specify the only partition when optimizing a non-partitioned table, set the expression PARTITION tuple(). For example: OPTIMIZE TABLE table_not_partitioned PARTITION tuple() FINAL;  IN PARTITION specifies the partition to which the UPDATE or DELETE expressions are applied as a result of the ALTER TABLE query. New parts are created only from the specified partition. In this way, IN PARTITION helps to reduce the load when the table is divided into many partitions, and you only need to update the data point-by-point. The examples of ALTER ... PARTITION queries are demonstrated in the tests 00502_custom_partitioning_local and 00502_custom_partitioning_replicated_zookeeper. "},{"title":"Manipulating Projections","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/projection","content":"Manipulating Projections The following operations with projections are available: ALTER TABLE [db].name ADD PROJECTION name ( SELECT &lt;COLUMN LIST EXPR&gt; [GROUP BY] [ORDER BY] ) - Adds projection description to tables metadata. ALTER TABLE [db].name DROP PROJECTION name - Removes projection description from tables metadata and deletes projection files from disk. Implemented as a mutation. ALTER TABLE [db.]table MATERIALIZE PROJECTION name IN PARTITION partition_name - The query rebuilds the projection name in the partition partition_name. Implemented as a mutation. ALTER TABLE [db.]table CLEAR PROJECTION name IN PARTITION partition_name - Deletes projection files from disk without removing description. Implemented as a mutation. The commands ADD, DROP and CLEAR are lightweight in a sense that they only change metadata or remove files. Also, they are replicated, syncing projections metadata via ZooKeeper. note Projection manipulation is supported only for tables with *MergeTree engine (including replicated variants).","keywords":""},{"title":"ALTER QUOTA","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/quota","content":"ALTER QUOTA Changes quotas. Syntax: ALTER QUOTA [IF EXISTS] name [ON CLUSTER cluster_name] [RENAME TO new_name] [KEYED BY {user_name | ip_address | client_key | client_key,user_name | client_key,ip_address} | NOT KEYED] [FOR [RANDOMIZED] INTERVAL number {second | minute | hour | day | week | month | quarter | year} {MAX { {queries | query_selects | query_inserts | errors | result_rows | result_bytes | read_rows | read_bytes | execution_time} = number } [,...] | NO LIMITS | TRACKING ONLY} [,...]] [TO {role [,...] | ALL | ALL EXCEPT role [,...]}] Keys user_name, ip_address, client_key, client_key, user_name and client_key, ip_address correspond to the fields in the system.quotas table. Parameters queries, query_selects, 'query_inserts', errors, result_rows, result_bytes, read_rows, read_bytes, execution_time` correspond to the fields in the system.quotas_usage table. ON CLUSTER clause allows creating quotas on a cluster, see Distributed DDL. Examples Limit the maximum number of queries for the current user with 123 queries in 15 months constraint: ALTER QUOTA IF EXISTS qA FOR INTERVAL 15 month MAX queries = 123 TO CURRENT_USER; For the default user limit the maximum execution time with half a second in 30 minutes, and limit the maximum number of queries with 321 and the maximum number of errors with 10 in 5 quaters: ALTER QUOTA IF EXISTS qB FOR INTERVAL 30 minute MAX execution_time = 0.5, FOR INTERVAL 5 quarter MAX queries = 321, errors = 10 TO default; ","keywords":""},{"title":"role","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/role","content":"","keywords":""},{"title":"ALTER ROLE​","type":1,"pageTitle":"role","url":"en/sql-reference/statements/alter/role#alter-role-statement","content":"Changes roles. Syntax: ALTER ROLE [IF EXISTS] name1 [ON CLUSTER cluster_name1] [RENAME TO new_name1] [, name2 [ON CLUSTER cluster_name2] [RENAME TO new_name2] ...] [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | PROFILE 'profile_name'] [,...]  "},{"title":"ALTER ROW POLICY","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/row-policy","content":"ALTER ROW POLICY Changes row policy. Syntax: ALTER [ROW] POLICY [IF EXISTS] name1 [ON CLUSTER cluster_name1] ON [database1.]table1 [RENAME TO new_name1] [, name2 [ON CLUSTER cluster_name2] ON [database2.]table2 [RENAME TO new_name2] ...] [AS {PERMISSIVE | RESTRICTIVE}] [FOR SELECT] [USING {condition | NONE}][,...] [TO {role [,...] | ALL | ALL EXCEPT role [,...]}] ","keywords":""},{"title":"Manipulating Sampling-Key Expressions","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/sample-by","content":"Manipulating Sampling-Key Expressions Syntax: ALTER TABLE [db].name [ON CLUSTER cluster] MODIFY SAMPLE BY new_expression The command changes the sampling key of the table to new_expression (an expression or a tuple of expressions). The command is lightweight in the sense that it only changes metadata. The primary key must contain the new sample key. note It only works for tables in the MergeTree family (including replicated tables).","keywords":""},{"title":"Table Settings Manipulations","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/setting","content":"","keywords":""},{"title":"MODIFY SETTING​","type":1,"pageTitle":"Table Settings Manipulations","url":"en/sql-reference/statements/alter/setting#alter_modify_setting","content":"Changes table settings. Syntax MODIFY SETTING setting_name=value [, ...]  Example CREATE TABLE example_table (id UInt32, data String) ENGINE=MergeTree() ORDER BY id; ALTER TABLE example_table MODIFY SETTING max_part_loading_threads=8, max_parts_in_total=50000;  "},{"title":"RESET SETTING​","type":1,"pageTitle":"Table Settings Manipulations","url":"en/sql-reference/statements/alter/setting#alter_reset_setting","content":"Resets table settings to their default values. If a setting is in a default state, then no action is taken. Syntax RESET SETTING setting_name [, ...]  Example CREATE TABLE example_table (id UInt32, data String) ENGINE=MergeTree() ORDER BY id SETTINGS max_part_loading_threads=8; ALTER TABLE example_table RESET SETTING max_part_loading_threads;  See Also MergeTree settings "},{"title":"settings-profile","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/settings-profile","content":"","keywords":""},{"title":"ALTER SETTINGS PROFILE​","type":1,"pageTitle":"settings-profile","url":"en/sql-reference/statements/alter/settings-profile#alter-settings-profile-statement","content":"Changes settings profiles. Syntax: ALTER SETTINGS PROFILE [IF EXISTS] TO name1 [ON CLUSTER cluster_name1] [RENAME TO new_name1] [, name2 [ON CLUSTER cluster_name2] [RENAME TO new_name2] ...] [SETTINGS variable [= value] [MIN [=] min_value] [MAX [=] max_value] [READONLY|WRITABLE] | INHERIT 'profile_name'] [,...]  "},{"title":"Manipulations with Table TTL","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/ttl","content":"","keywords":""},{"title":"MODIFY TTL​","type":1,"pageTitle":"Manipulations with Table TTL","url":"en/sql-reference/statements/alter/ttl#modify-ttl","content":"You can change table TTL with a request of the following form: ALTER TABLE table_name MODIFY TTL ttl_expression;  "},{"title":"REMOVE TTL​","type":1,"pageTitle":"Manipulations with Table TTL","url":"en/sql-reference/statements/alter/ttl#remove-ttl","content":"TTL-property can be removed from table with the following query: ALTER TABLE table_name REMOVE TTL  Example Consider the table with table TTL: CREATE TABLE table_with_ttl ( event_time DateTime, UserID UInt64, Comment String ) ENGINE MergeTree() ORDER BY tuple() TTL event_time + INTERVAL 3 MONTH; SETTINGS min_bytes_for_wide_part = 0; INSERT INTO table_with_ttl VALUES (now(), 1, 'username1'); INSERT INTO table_with_ttl VALUES (now() - INTERVAL 4 MONTH, 2, 'username2');  Run OPTIMIZE to force TTL cleanup: OPTIMIZE TABLE table_with_ttl FINAL; SELECT * FROM table_with_ttl FORMAT PrettyCompact;  Second row was deleted from table. ┌─────────event_time────┬──UserID─┬─────Comment──┐ │ 2020-12-11 12:44:57 │ 1 │ username1 │ └───────────────────────┴─────────┴──────────────┘  Now remove table TTL with the following query: ALTER TABLE table_with_ttl REMOVE TTL;  Re-insert the deleted row and force the TTL cleanup again with OPTIMIZE: INSERT INTO table_with_ttl VALUES (now() - INTERVAL 4 MONTH, 2, 'username2'); OPTIMIZE TABLE table_with_ttl FINAL; SELECT * FROM table_with_ttl FORMAT PrettyCompact;  The TTL is no longer there, so the second row is not deleted: ┌─────────event_time────┬──UserID─┬─────Comment──┐ │ 2020-12-11 12:44:57 │ 1 │ username1 │ │ 2020-08-11 12:44:57 │ 2 │ username2 │ └───────────────────────┴─────────┴──────────────┘  See Also More about the TTL-expression.Modify column with TTL. "},{"title":"ALTER TABLE … UPDATE Statements","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/update","content":"ALTER TABLE … UPDATE Statements ALTER TABLE [db.]table UPDATE column1 = expr1 [, ...] WHERE filter_expr Manipulates data matching the specified filtering expression. Implemented as a mutation. note The ALTER TABLE prefix makes this syntax different from most other systems supporting SQL. It is intended to signify that unlike similar queries in OLTP databases this is a heavy operation not designed for frequent use. The filter_expr must be of type UInt8. This query updates values of specified columns to the values of corresponding expressions in rows for which the filter_expr takes a non-zero value. Values are casted to the column type using the CAST operator. Updating columns that are used in the calculation of the primary or the partition key is not supported. One query can contain several commands separated by commas. The synchronicity of the query processing is defined by the mutations_sync setting. By default, it is asynchronous. See also MutationsSynchronicity of ALTER Queriesmutations_sync setting","keywords":""},{"title":"ALTER USER","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/user","content":"","keywords":""},{"title":"GRANTEES Clause​","type":1,"pageTitle":"ALTER USER","url":"en/sql-reference/statements/alter/user#grantees","content":"Specifies users or roles which are allowed to receive privileges from this user on the condition this user has also all required access granted with GRANT OPTION. Options of the GRANTEES clause: user — Specifies a user this user can grant privileges to.role — Specifies a role this user can grant privileges to.ANY — This user can grant privileges to anyone. It's the default setting.NONE — This user can grant privileges to none. You can exclude any user or role by using the EXCEPT expression. For example, ALTER USER user1 GRANTEES ANY EXCEPT user2. It means if user1 has some privileges granted with GRANT OPTION it will be able to grant those privileges to anyone except user2. "},{"title":"Examples​","type":1,"pageTitle":"ALTER USER","url":"en/sql-reference/statements/alter/user#alter-user-examples","content":"Set assigned roles as default: ALTER USER user DEFAULT ROLE role1, role2  If roles aren’t previously assigned to a user, ClickHouse throws an exception. Set all the assigned roles to default: ALTER USER user DEFAULT ROLE ALL  If a role is assigned to a user in the future, it will become default automatically. Set all the assigned roles to default, excepting role1 and role2: ALTER USER user DEFAULT ROLE ALL EXCEPT role1, role2  Allows the user with john account to grant his privileges to the user with jack account: ALTER USER john GRANTEES jack;  "},{"title":"ALTER TABLE … MODIFY QUERY Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/alter/view","content":"","keywords":""},{"title":"ALTER LIVE VIEW Statement​","type":1,"pageTitle":"ALTER TABLE … MODIFY QUERY Statement","url":"en/sql-reference/statements/alter/view#alter-live-view","content":"ALTER LIVE VIEW ... REFRESH statement refreshes a Live view. See Force Live View Refresh. "},{"title":"ATTACH Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/attach","content":"","keywords":""},{"title":"Attach Existing Table​","type":1,"pageTitle":"ATTACH Statement","url":"en/sql-reference/statements/attach#attach-existing-table","content":"Syntax ATTACH TABLE [IF NOT EXISTS] [db.]name [ON CLUSTER cluster]  This query is used when starting the server. The server stores table metadata as files with ATTACH queries, which it simply runs at launch (with the exception of some system tables, which are explicitly created on the server). If the table was detached permanently, it won't be reattached at the server start, so you need to use ATTACH query explicitly. "},{"title":"Create New Table And Attach Data​","type":1,"pageTitle":"ATTACH Statement","url":"en/sql-reference/statements/attach#create-new-table-and-attach-data","content":""},{"title":"With Specified Path to Table Data​","type":1,"pageTitle":"ATTACH Statement","url":"en/sql-reference/statements/attach#attach-with-specified-path","content":"The query creates a new table with provided structure and attaches table data from the provided directory in user_files. Syntax ATTACH TABLE name FROM 'path/to/data/' (col1 Type1, ...)  Example Query: DROP TABLE IF EXISTS test; INSERT INTO TABLE FUNCTION file('01188_attach/test/data.TSV', 'TSV', 's String, n UInt8') VALUES ('test', 42); ATTACH TABLE test FROM '01188_attach/test' (s String, n UInt8) ENGINE = File(TSV); SELECT * FROM test;  Result: ┌─s────┬──n─┐ │ test │ 42 │ └──────┴────┘  "},{"title":"With Specified Table UUID​","type":1,"pageTitle":"ATTACH Statement","url":"en/sql-reference/statements/attach#attach-with-specified-uuid","content":"This query creates a new table with provided structure and attaches data from the table with the specified UUID. It is supported by the Atomic database engine. Syntax ATTACH TABLE name UUID '&lt;uuid&gt;' (col1 Type1, ...)  "},{"title":"Attach Existing Dictionary​","type":1,"pageTitle":"ATTACH Statement","url":"en/sql-reference/statements/attach#attach-existing-dictionary","content":"Attaches a previously detached dictionary. Syntax ATTACH DICTIONARY [IF NOT EXISTS] [db.]name [ON CLUSTER cluster]  "},{"title":"CHECK TABLE Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/check-table","content":"","keywords":""},{"title":"Checking the MergeTree Family Tables​","type":1,"pageTitle":"CHECK TABLE Statement","url":"en/sql-reference/statements/check-table#checking-mergetree-tables","content":"For MergeTree family engines, if check_query_single_value_result = 0, the CHECK TABLE query shows a check status for every individual data part of a table on the local server. SET check_query_single_value_result = 0; CHECK TABLE test_table;  ┌─part_path─┬─is_passed─┬─message─┐ │ all_1_4_1 │ 1 │ │ │ all_1_4_2 │ 1 │ │ └───────────┴───────────┴─────────┘  If check_query_single_value_result = 1, the CHECK TABLE query shows the general table check status. SET check_query_single_value_result = 1; CHECK TABLE test_table;  ┌─result─┐ │ 1 │ └────────┘  "},{"title":"If the Data Is Corrupted​","type":1,"pageTitle":"CHECK TABLE Statement","url":"en/sql-reference/statements/check-table#if-data-is-corrupted","content":"If the table is corrupted, you can copy the non-corrupted data to another table. To do this: Create a new table with the same structure as damaged table. To do this execute the query CREATE TABLE &lt;new_table_name&gt; AS &lt;damaged_table_name&gt;.Set the max_threads value to 1 to process the next query in a single thread. To do this run the query SET max_threads = 1.Execute the query INSERT INTO &lt;new_table_name&gt; SELECT * FROM &lt;damaged_table_name&gt;. This request copies the non-corrupted data from the damaged table to another table. Only the data before the corrupted part will be copied.Restart the clickhouse-client to reset the max_threads value. "},{"title":"CREATE Queries","type":0,"sectionRef":"#","url":"en/sql-reference/statements/create/","content":"CREATE Queries Create queries make a new entity of one of the following kinds: DATABASETABLEVIEWDICTIONARYFUNCTIONUSERROLEROW POLICYQUOTASETTINGS PROFILE Original article","keywords":""},{"title":"CREATE DATABASE","type":0,"sectionRef":"#","url":"en/sql-reference/statements/create/database","content":"","keywords":""},{"title":"Clauses​","type":1,"pageTitle":"CREATE DATABASE","url":"en/sql-reference/statements/create/database#clauses","content":""},{"title":"IF NOT EXISTS​","type":1,"pageTitle":"CREATE DATABASE","url":"en/sql-reference/statements/create/database#if-not-exists","content":"If the db_name database already exists, then ClickHouse does not create a new database and: Doesn’t throw an exception if clause is specified.Throws an exception if clause isn’t specified. "},{"title":"ON CLUSTER​","type":1,"pageTitle":"CREATE DATABASE","url":"en/sql-reference/statements/create/database#on-cluster","content":"ClickHouse creates the db_name database on all the servers of a specified cluster. More details in a Distributed DDL article. "},{"title":"ENGINE​","type":1,"pageTitle":"CREATE DATABASE","url":"en/sql-reference/statements/create/database#engine","content":"By default, ClickHouse uses its own Atomic database engine. There are also Lazy, MySQL, PostgresSQL, MaterializedMySQL, MaterializedPostgreSQL, Replicated, SQLite. "},{"title":"COMMENT​","type":1,"pageTitle":"CREATE DATABASE","url":"en/sql-reference/statements/create/database#comment","content":"You can add a comment to the database when you creating it. The comment is supported for all database engines. Syntax CREATE DATABASE db_name ENGINE = engine(...) COMMENT 'Comment'  Example Query: CREATE DATABASE db_comment ENGINE = Memory COMMENT 'The temporary database'; SELECT name, comment FROM system.databases WHERE name = 'db_comment';  Result: ┌─name───────┬─comment────────────────┐ │ db_comment │ The temporary database │ └────────────┴────────────────────────┘  "},{"title":"CREATE DICTIONARY","type":0,"sectionRef":"#","url":"en/sql-reference/statements/create/dictionary","content":"CREATE DICTIONARY Creates a new external dictionary with given structure, source, layout and lifetime. Syntax CREATE [OR REPLACE] DICTIONARY [IF NOT EXISTS] [db.]dictionary_name [ON CLUSTER cluster] ( key1 type1 [DEFAULT|EXPRESSION expr1] [IS_OBJECT_ID], key2 type2 [DEFAULT|EXPRESSION expr2], attr1 type2 [DEFAULT|EXPRESSION expr3] [HIERARCHICAL|INJECTIVE], attr2 type2 [DEFAULT|EXPRESSION expr4] [HIERARCHICAL|INJECTIVE] ) PRIMARY KEY key1, key2 SOURCE(SOURCE_NAME([param1 value1 ... paramN valueN])) LAYOUT(LAYOUT_NAME([param_name param_value])) LIFETIME({MIN min_val MAX max_val | max_val}) SETTINGS(setting_name = setting_value, setting_name = setting_value, ...) COMMENT 'Comment' External dictionary structure consists of attributes. Dictionary attributes are specified similarly to table columns. The only required attribute property is its type, all other properties may have default values. ON CLUSTER clause allows creating dictionary on a cluster, see Distributed DDL. Depending on dictionary layout one or more attributes can be specified as dictionary keys. For more information, see External Dictionaries section. You can add a comment to the dictionary when you creating it using COMMENT clause. Example Input table source_table: ┌─id─┬─value──┐ │ 1 │ First │ │ 2 │ Second │ └────┴────────┘ Creating the dictionary: CREATE DICTIONARY dictionary_with_comment ( id UInt64, value String ) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() TABLE 'source_table')) LAYOUT(FLAT()) LIFETIME(MIN 0 MAX 1000) COMMENT 'The temporary dictionary'; Output the dictionary: SHOW CREATE DICTIONARY dictionary_with_comment; ┌─statement───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐ │ CREATE DICTIONARY default.dictionary_with_comment ( `id` UInt64, `value` String ) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() TABLE 'source_table')) LIFETIME(MIN 0 MAX 1000) LAYOUT(FLAT()) COMMENT 'The temporary dictionary' │ └─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘ Output the comment to dictionary: SELECT comment FROM system.dictionaries WHERE name == 'dictionary_with_comment' AND database == currentDatabase(); ┌─comment──────────────────┐ │ The temporary dictionary │ └──────────────────────────┘ See Also system.dictionaries — This table contains information about external dictionaries.","keywords":""},{"title":"CREATE FUNCTION","type":0,"sectionRef":"#","url":"en/sql-reference/statements/create/function","content":"CREATE FUNCTION Creates a user defined function from a lambda expression. The expression must consist of function parameters, constants, operators, or other function calls. Syntax CREATE FUNCTION name AS (parameter0, ...) -&gt; expression A function can have an arbitrary number of parameters. There are a few restrictions: The name of a function must be unique among user defined and system functions.Recursive functions are not allowed.All variables used by a function must be specified in its parameter list. If any restriction is violated then an exception is raised. Example Query: CREATE FUNCTION linear_equation AS (x, k, b) -&gt; k*x + b; SELECT number, linear_equation(number, 2, 1) FROM numbers(3); Result: ┌─number─┬─plus(multiply(2, number), 1)─┐ │ 0 │ 1 │ │ 1 │ 3 │ │ 2 │ 5 │ └────────┴──────────────────────────────┘ A conditional function is called in a user defined function in the following query: CREATE FUNCTION parity_str AS (n) -&gt; if(n % 2, 'odd', 'even'); SELECT number, parity_str(number) FROM numbers(3); Result: ┌─number─┬─if(modulo(number, 2), 'odd', 'even')─┐ │ 0 │ even │ │ 1 │ odd │ │ 2 │ even │ └────────┴──────────────────────────────────────┘ ","keywords":""},{"title":"CREATE QUOTA","type":0,"sectionRef":"#","url":"en/sql-reference/statements/create/quota","content":"CREATE QUOTA Creates a quota that can be assigned to a user or a role. Syntax: CREATE QUOTA [IF NOT EXISTS | OR REPLACE] name [ON CLUSTER cluster_name] [KEYED BY {user_name | ip_address | client_key | client_key,user_name | client_key,ip_address} | NOT KEYED] [FOR [RANDOMIZED] INTERVAL number {second | minute | hour | day | week | month | quarter | year} {MAX { {queries | query_selects | query_inserts | errors | result_rows | result_bytes | read_rows | read_bytes | execution_time} = number } [,...] | NO LIMITS | TRACKING ONLY} [,...]] [TO {role [,...] | ALL | ALL EXCEPT role [,...]}] Keys user_name, ip_address, client_key, client_key, user_name and client_key, ip_address correspond to the fields in the system.quotas table. Parameters queries, query_selects, query_inserts, errors, result_rows, result_bytes, read_rows, read_bytes, execution_time correspond to the fields in the system.quotas_usage table. ON CLUSTER clause allows creating quotas on a cluster, see Distributed DDL. Examples Limit the maximum number of queries for the current user with 123 queries in 15 months constraint: CREATE QUOTA qA FOR INTERVAL 15 month MAX queries = 123 TO CURRENT_USER; For the default user limit the maximum execution time with half a second in 30 minutes, and limit the maximum number of queries with 321 and the maximum number of errors with 10 in 5 quaters: CREATE QUOTA qB FOR INTERVAL 30 minute MAX execution_time = 0.5, FOR INTERVAL 5 quarter MAX queries = 321, errors = 10 TO default; ","keywords":""},{"title":"CREATE ROLE","type":0,"sectionRef":"#","url":"en/sql-reference/statements/create/role","content":"","keywords":""},{"title":"Managing Roles​","type":1,"pageTitle":"CREATE ROLE","url":"en/sql-reference/statements/create/role#managing-roles","content":"A user can be assigned multiple roles. Users can apply their assigned roles in arbitrary combinations by the SET ROLE statement. The final scope of privileges is a combined set of all the privileges of all the applied roles. If a user has privileges granted directly to it’s user account, they are also combined with the privileges granted by roles. User can have default roles which apply at user login. To set default roles, use the SET DEFAULT ROLE statement or the ALTER USER statement. To revoke a role, use the REVOKE statement. To delete role, use the DROP ROLE statement. The deleted role is being automatically revoked from all the users and roles to which it was assigned. "},{"title":"Examples​","type":1,"pageTitle":"CREATE ROLE","url":"en/sql-reference/statements/create/role#create-role-examples","content":"CREATE ROLE accountant; GRANT SELECT ON db.* TO accountant;  This sequence of queries creates the role accountant that has the privilege of reading data from the db database. Assigning the role to the user mira: GRANT accountant TO mira;  After the role is assigned, the user can apply it and execute the allowed queries. For example: SET ROLE accountant; SELECT * FROM db.*;  "},{"title":"CREATE ROW POLICY","type":0,"sectionRef":"#","url":"en/sql-reference/statements/create/row-policy","content":"","keywords":""},{"title":"USING Clause​","type":1,"pageTitle":"CREATE ROW POLICY","url":"en/sql-reference/statements/create/row-policy#create-row-policy-using","content":"Allows to specify a condition to filter rows. An user will see a row if the condition is calculated to non-zero for the row. "},{"title":"TO Clause​","type":1,"pageTitle":"CREATE ROW POLICY","url":"en/sql-reference/statements/create/row-policy#create-row-policy-to","content":"In the section TO you can provide a list of users and roles this policy should work for. For example, CREATE ROW POLICY ... TO accountant, john@localhost. Keyword ALL means all the ClickHouse users including current user. Keyword ALL EXCEPT allow to exclude some users from the all users list, for example, CREATE ROW POLICY ... TO ALL EXCEPT accountant, john@localhost note If there are no row policies defined for a table then any user can SELECT all the row from the table. Defining one or more row policies for the table makes the access to the table depending on the row policies no matter if those row policies are defined for the current user or not. For example, the following policy CREATE ROW POLICY pol1 ON mydb.table1 USING b=1 TO mira, peter forbids the users mira and peter to see the rows with b != 1, and any non-mentioned user (e.g., the user paul) will see no rows from mydb.table1 at all. If that's not desirable it can't be fixed by adding one more row policy, like the following: CREATE ROW POLICY pol2 ON mydb.table1 USING 1 TO ALL EXCEPT mira, peter "},{"title":"AS Clause​","type":1,"pageTitle":"CREATE ROW POLICY","url":"en/sql-reference/statements/create/row-policy#create-row-policy-as","content":"It's allowed to have more than one policy enabled on the same table for the same user at the one time. So we need a way to combine the conditions from multiple policies. By default policies are combined using the boolean OR operator. For example, the following policies CREATE ROW POLICY pol1 ON mydb.table1 USING b=1 TO mira, peter CREATE ROW POLICY pol2 ON mydb.table1 USING c=2 TO peter, antonio  enables the user peter to see rows with either b=1 or c=2. The AS clause specifies how policies should be combined with other policies. Policies can be either permissive or restrictive. By default policies are permissive, which means they are combined using the boolean OR operator. A policy can be defined as restrictive as an alternative. Restrictive policies are combined using the boolean AND operator. Here is the general formula: row_is_visible = (one or more of the permissive policies' conditions are non-zero) AND (all of the restrictive policies's conditions are non-zero)  For example, the following policies CREATE ROW POLICY pol1 ON mydb.table1 USING b=1 TO mira, peter CREATE ROW POLICY pol2 ON mydb.table1 USING c=2 AS RESTRICTIVE TO peter, antonio  enables the user peter to see rows only if both b=1 AND c=2. "},{"title":"ON CLUSTER Clause​","type":1,"pageTitle":"CREATE ROW POLICY","url":"en/sql-reference/statements/create/row-policy#create-row-policy-on-cluster","content":"Allows creating row policies on a cluster, see Distributed DDL. "},{"title":"Examples​","type":1,"pageTitle":"CREATE ROW POLICY","url":"en/sql-reference/statements/create/row-policy#examples","content":"CREATE ROW POLICY filter1 ON mydb.mytable USING a&lt;1000 TO accountant, john@localhost CREATE ROW POLICY filter2 ON mydb.mytable USING a&lt;1000 AND b=5 TO ALL EXCEPT mira CREATE ROW POLICY filter3 ON mydb.mytable USING 1 TO admin "},{"title":"CREATE SETTINGS PROFILE","type":0,"sectionRef":"#","url":"en/sql-reference/statements/create/settings-profile","content":"","keywords":""},{"title":"Example​","type":1,"pageTitle":"CREATE SETTINGS PROFILE","url":"en/sql-reference/statements/create/settings-profile#create-settings-profile-syntax","content":"Create the max_memory_usage_profile settings profile with value and constraints for the max_memory_usage setting and assign it to user robin: CREATE SETTINGS PROFILE max_memory_usage_profile SETTINGS max_memory_usage = 100000001 MIN 90000000 MAX 110000000 TO robin  "},{"title":"DETACH Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/detach","content":"DETACH Statement Makes the server &quot;forget&quot; about the existence of a table, a materialized view, or a dictionary. Syntax DETACH TABLE|VIEW|DICTIONARY [IF EXISTS] [db.]name [ON CLUSTER cluster] [PERMANENTLY] Detaching does not delete the data or metadata of a table, a materialized view or a dictionary. If an entity was not detached PERMANENTLY, on the next server launch the server will read the metadata and recall the table/view/dictionary again. If an entity was detached PERMANENTLY, there will be no automatic recall. Whether a table or a dictionary was detached permanently or not, in both cases you can reattach them using the ATTACH query. System log tables can be also attached back (e.g. query_log, text_log, etc). Other system tables can't be reattached. On the next server launch the server will recall those tables again. ATTACH MATERIALIZED VIEW does not work with short syntax (without SELECT), but you can attach it using the ATTACH TABLE query. Note that you can not detach permanently the table which is already detached (temporary). But you can attach it back and then detach permanently again. Also you can not DROP the detached table, or CREATE TABLE with the same name as detached permanently, or replace it with the other table with RENAME TABLE query. Example Creating a table: Query: CREATE TABLE test ENGINE = Log AS SELECT * FROM numbers(10); SELECT * FROM test; Result: ┌─number─┐ │ 0 │ │ 1 │ │ 2 │ │ 3 │ │ 4 │ │ 5 │ │ 6 │ │ 7 │ │ 8 │ │ 9 │ └────────┘ Detaching the table: Query: DETACH TABLE test; SELECT * FROM test; Result: Received exception from server (version 21.4.1): Code: 60. DB::Exception: Received from localhost:9000. DB::Exception: Table default.test does not exist. See Also Materialized ViewDictionaries","keywords":""},{"title":"EXCHANGE Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/exchange","content":"","keywords":""},{"title":"EXCHANGE TABLES​","type":1,"pageTitle":"EXCHANGE Statement","url":"en/sql-reference/statements/exchange#exchange_tables","content":"Exchanges the names of two tables. Syntax EXCHANGE TABLES [db0.]table_A AND [db1.]table_B  "},{"title":"EXCHANGE DICTIONARIES​","type":1,"pageTitle":"EXCHANGE Statement","url":"en/sql-reference/statements/exchange#exchange_dictionaries","content":"Exchanges the names of two dictionaries. Syntax EXCHANGE DICTIONARIES [db0.]dict_A AND [db1.]dict_B  See Also Dictionaries "},{"title":"DESCRIBE TABLE","type":0,"sectionRef":"#","url":"en/sql-reference/statements/describe-table","content":"DESCRIBE TABLE Returns information about table columns. Syntax DESC|DESCRIBE TABLE [db.]table [INTO OUTFILE filename] [FORMAT format] The DESCRIBE statement returns a row for each table column with the following String values: name — A column name.type — A column type.default_type — A clause that is used in the column default expression: DEFAULT, MATERIALIZED or ALIAS. If there is no default expression, then empty string is returned.default_expression — An expression specified after the DEFAULT clause.comment — A column comment.codec_expression — A codec that is applied to the column.ttl_expression — A TTL expression.is_subcolumn — A flag that equals 1 for internal subcolumns. It is included into the result only if subcolumn description is enabled by the describe_include_subcolumns setting. All columns in Nested data structures are described separately. The name of each column is prefixed with a parent column name and a dot. To show internal subcolumns of other data types, use the describe_include_subcolumns setting. Example Query: CREATE TABLE describe_example ( id UInt64, text String DEFAULT 'unknown' CODEC(ZSTD), user Tuple (name String, age UInt8) ) ENGINE = MergeTree() ORDER BY id; DESCRIBE TABLE describe_example; DESCRIBE TABLE describe_example SETTINGS describe_include_subcolumns=1; Result: ┌─name─┬─type──────────────────────────┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┬─ttl_expression─┐ │ id │ UInt64 │ │ │ │ │ │ │ text │ String │ DEFAULT │ 'unknown' │ │ ZSTD(1) │ │ │ user │ Tuple(name String, age UInt8) │ │ │ │ │ │ └──────┴───────────────────────────────┴──────────────┴────────────────────┴─────────┴──────────────────┴────────────────┘ The second query additionally shows subcolumns: ┌─name──────┬─type──────────────────────────┬─default_type─┬─default_expression─┬─comment─┬─codec_expression─┬─ttl_expression─┬─is_subcolumn─┐ │ id │ UInt64 │ │ │ │ │ │ 0 │ │ text │ String │ DEFAULT │ 'unknown' │ │ ZSTD(1) │ │ 0 │ │ user │ Tuple(name String, age UInt8) │ │ │ │ │ │ 0 │ │ user.name │ String │ │ │ │ │ │ 1 │ │ user.age │ UInt8 │ │ │ │ │ │ 1 │ └───────────┴───────────────────────────────┴──────────────┴────────────────────┴─────────┴──────────────────┴────────────────┴──────────────┘ See Also describe_include_subcolumns setting.","keywords":""},{"title":"EXISTS Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/exists","content":"EXISTS Statement EXISTS [TEMPORARY] [TABLE|DICTIONARY] [db.]name [INTO OUTFILE filename] [FORMAT format] Returns a single UInt8-type column, which contains the single value 0 if the table or database does not exist, or 1 if the table exists in the specified database.","keywords":""},{"title":"Miscellaneous Statements","type":0,"sectionRef":"#","url":"en/sql-reference/statements/misc","content":"Miscellaneous Statements ATTACHCHECK TABLEDESCRIBE TABLEDETACHDROPEXISTSKILLOPTIMIZERENAMESETSET ROLETRUNCATEUSE","keywords":""},{"title":"CREATE USER","type":0,"sectionRef":"#","url":"en/sql-reference/statements/create/user","content":"","keywords":""},{"title":"Identification​","type":1,"pageTitle":"CREATE USER","url":"en/sql-reference/statements/create/user#identification","content":"There are multiple ways of user identification: IDENTIFIED WITH no_passwordIDENTIFIED WITH plaintext_password BY 'qwerty'IDENTIFIED WITH sha256_password BY 'qwerty' or IDENTIFIED BY 'password'IDENTIFIED WITH sha256_hash BY 'hash'IDENTIFIED WITH double_sha1_password BY 'qwerty'IDENTIFIED WITH double_sha1_hash BY 'hash'IDENTIFIED WITH ldap SERVER 'server_name'IDENTIFIED WITH kerberos or IDENTIFIED WITH kerberos REALM 'realm' "},{"title":"User Host​","type":1,"pageTitle":"CREATE USER","url":"en/sql-reference/statements/create/user#user-host","content":"User host is a host from which a connection to ClickHouse server could be established. The host can be specified in the HOST query section in the following ways: HOST IP 'ip_address_or_subnetwork' — User can connect to ClickHouse server only from the specified IP address or a subnetwork. Examples: HOST IP '192.168.0.0/16', HOST IP '2001:DB8::/32'. For use in production, only specify HOST IP elements (IP addresses and their masks), since using host and host_regexp might cause extra latency.HOST ANY — User can connect from any location. This is a default option.HOST LOCAL — User can connect only locally.HOST NAME 'fqdn' — User host can be specified as FQDN. For example, HOST NAME 'mysite.com'.HOST REGEXP 'regexp' — You can use pcre regular expressions when specifying user hosts. For example, HOST REGEXP '.*\\.mysite\\.com'.HOST LIKE 'template' — Allows you to use the LIKE operator to filter the user hosts. For example, HOST LIKE '%' is equivalent to HOST ANY, HOST LIKE '%.mysite.com' filters all the hosts in the mysite.com domain. Another way of specifying host is to use @ syntax following the username. Examples: CREATE USER mira@'127.0.0.1' — Equivalent to the HOST IP syntax.CREATE USER mira@'localhost' — Equivalent to the HOST LOCAL syntax.CREATE USER mira@'192.168.%.%' — Equivalent to the HOST LIKE syntax. warning ClickHouse treats user_name@'address' as a username as a whole. Thus, technically you can create multiple users with the same user_name and different constructions after @. However, we do not recommend to do so. "},{"title":"GRANTEES Clause​","type":1,"pageTitle":"CREATE USER","url":"en/sql-reference/statements/create/user#grantees","content":"Specifies users or roles which are allowed to receive privileges from this user on the condition this user has also all required access granted with GRANT OPTION. Options of the GRANTEES clause: user — Specifies a user this user can grant privileges to.role — Specifies a role this user can grant privileges to.ANY — This user can grant privileges to anyone. It's the default setting.NONE — This user can grant privileges to none. You can exclude any user or role by using the EXCEPT expression. For example, CREATE USER user1 GRANTEES ANY EXCEPT user2. It means if user1 has some privileges granted with GRANT OPTION it will be able to grant those privileges to anyone except user2. "},{"title":"Examples​","type":1,"pageTitle":"CREATE USER","url":"en/sql-reference/statements/create/user#create-user-examples","content":"Create the user account mira protected by the password qwerty: CREATE USER mira HOST IP '127.0.0.1' IDENTIFIED WITH sha256_password BY 'qwerty';  mira should start client app at the host where the ClickHouse server runs. Create the user account john, assign roles to it and make this roles default: CREATE USER john DEFAULT ROLE role1, role2;  Create the user account john and make all his future roles default: CREATE USER john DEFAULT ROLE ALL;  When some role is assigned to john in the future, it will become default automatically. Create the user account john and make all his future roles default excepting role1 and role2: CREATE USER john DEFAULT ROLE ALL EXCEPT role1, role2;  Create the user account john and allow him to grant his privileges to the user with jack account: CREATE USER john GRANTEES jack;  "},{"title":"DROP Statements","type":0,"sectionRef":"#","url":"en/sql-reference/statements/drop","content":"","keywords":""},{"title":"DROP DATABASE​","type":1,"pageTitle":"DROP Statements","url":"en/sql-reference/statements/drop#drop-database","content":"Deletes all tables inside the db database, then deletes the db database itself. Syntax: DROP DATABASE [IF EXISTS] db [ON CLUSTER cluster]  "},{"title":"DROP TABLE​","type":1,"pageTitle":"DROP Statements","url":"en/sql-reference/statements/drop#drop-table","content":"Deletes the table. Syntax: DROP [TEMPORARY] TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster]  "},{"title":"DROP DICTIONARY​","type":1,"pageTitle":"DROP Statements","url":"en/sql-reference/statements/drop#drop-dictionary","content":"Deletes the dictionary. Syntax: DROP DICTIONARY [IF EXISTS] [db.]name  "},{"title":"DROP USER​","type":1,"pageTitle":"DROP Statements","url":"en/sql-reference/statements/drop#drop-user-statement","content":"Deletes a user. Syntax: DROP USER [IF EXISTS] name [,...] [ON CLUSTER cluster_name]  "},{"title":"DROP ROLE​","type":1,"pageTitle":"DROP Statements","url":"en/sql-reference/statements/drop#drop-role-statement","content":"Deletes a role. The deleted role is revoked from all the entities where it was assigned. Syntax: DROP ROLE [IF EXISTS] name [,...] [ON CLUSTER cluster_name]  "},{"title":"DROP ROW POLICY​","type":1,"pageTitle":"DROP Statements","url":"en/sql-reference/statements/drop#drop-row-policy-statement","content":"Deletes a row policy. Deleted row policy is revoked from all the entities where it was assigned. Syntax: DROP [ROW] POLICY [IF EXISTS] name [,...] ON [database.]table [,...] [ON CLUSTER cluster_name]  "},{"title":"DROP QUOTA​","type":1,"pageTitle":"DROP Statements","url":"en/sql-reference/statements/drop#drop-quota-statement","content":"Deletes a quota. The deleted quota is revoked from all the entities where it was assigned. Syntax: DROP QUOTA [IF EXISTS] name [,...] [ON CLUSTER cluster_name]  "},{"title":"DROP SETTINGS PROFILE​","type":1,"pageTitle":"DROP Statements","url":"en/sql-reference/statements/drop#drop-settings-profile-statement","content":"Deletes a settings profile. The deleted settings profile is revoked from all the entities where it was assigned. Syntax: DROP [SETTINGS] PROFILE [IF EXISTS] name [,...] [ON CLUSTER cluster_name]  "},{"title":"DROP VIEW​","type":1,"pageTitle":"DROP Statements","url":"en/sql-reference/statements/drop#drop-view","content":"Deletes a view. Views can be deleted by a DROP TABLE command as well but DROP VIEW checks that [db.]name is a view. Syntax: DROP VIEW [IF EXISTS] [db.]name [ON CLUSTER cluster]  "},{"title":"DROP FUNCTION​","type":1,"pageTitle":"DROP Statements","url":"en/sql-reference/statements/drop#drop-function","content":"Deletes a user defined function created by CREATE FUNCTION. System functions can not be dropped. Syntax DROP FUNCTION [IF EXISTS] function_name  Example CREATE FUNCTION linear_equation AS (x, k, b) -&gt; k*x + b; DROP FUNCTION linear_equation;  "},{"title":"KILL Statements","type":0,"sectionRef":"#","url":"en/sql-reference/statements/kill","content":"","keywords":""},{"title":"KILL QUERY​","type":1,"pageTitle":"KILL Statements","url":"en/sql-reference/statements/kill#kill-query-statement","content":"KILL QUERY [ON CLUSTER cluster] WHERE &lt;where expression to SELECT FROM system.processes query&gt; [SYNC|ASYNC|TEST] [FORMAT format]  Attempts to forcibly terminate the currently running queries. The queries to terminate are selected from the system.processes table using the criteria defined in the WHERE clause of the KILL query. Examples: -- Forcibly terminates all queries with the specified query_id: KILL QUERY WHERE query_id='2-857d-4a57-9ee0-327da5d60a90' -- Synchronously terminates all queries run by 'username': KILL QUERY WHERE user='username' SYNC  Read-only users can only stop their own queries. By default, the asynchronous version of queries is used (ASYNC), which does not wait for confirmation that queries have stopped. The synchronous version (SYNC) waits for all queries to stop and displays information about each process as it stops. The response contains the kill_status column, which can take the following values: finished – The query was terminated successfully.waiting – Waiting for the query to end after sending it a signal to terminate.The other values ​​explain why the query can’t be stopped. A test query (TEST) only checks the user’s rights and displays a list of queries to stop. "},{"title":"KILL MUTATION​","type":1,"pageTitle":"KILL Statements","url":"en/sql-reference/statements/kill#kill-mutation","content":"KILL MUTATION [ON CLUSTER cluster] WHERE &lt;where expression to SELECT FROM system.mutations query&gt; [TEST] [FORMAT format]  Tries to cancel and remove mutations that are currently executing. Mutations to cancel are selected from the system.mutations table using the filter specified by the WHERE clause of the KILL query. A test query (TEST) only checks the user’s rights and displays a list of mutations to stop. Examples: -- Cancel and remove all mutations of the single table: KILL MUTATION WHERE database = 'default' AND table = 'table' -- Cancel the specific mutation: KILL MUTATION WHERE database = 'default' AND table = 'table' AND mutation_id = 'mutation_3.txt'  The query is useful when a mutation is stuck and cannot finish (e.g. if some function in the mutation query throws an exception when applied to the data contained in the table). Changes already made by the mutation are not rolled back. "},{"title":"RENAME Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/rename","content":"","keywords":""},{"title":"RENAME DATABASE​","type":1,"pageTitle":"RENAME Statement","url":"en/sql-reference/statements/rename#misc_operations-rename_database","content":"Renames databases. Syntax RENAME DATABASE atomic_database1 TO atomic_database2 [,...] [ON CLUSTER cluster]  "},{"title":"RENAME TABLE​","type":1,"pageTitle":"RENAME Statement","url":"en/sql-reference/statements/rename#misc_operations-rename_table","content":"Renames one or more tables. Renaming tables is a light operation. If you pass a different database after TO, the table will be moved to this database. However, the directories with databases must reside in the same file system. Otherwise, an error is returned. If you rename multiple tables in one query, the operation is not atomic. It may be partially executed, and queries in other sessions may get Table ... does not exist ... error. Syntax RENAME TABLE [db1.]name1 TO [db2.]name2 [,...] [ON CLUSTER cluster]  Example RENAME TABLE table_A TO table_A_bak, table_B TO table_B_bak;  "},{"title":"RENAME DICTIONARY​","type":1,"pageTitle":"RENAME Statement","url":"en/sql-reference/statements/rename#rename_dictionary","content":"Renames one or several dictionaries. This query can be used to move dictionaries between databases. Syntax RENAME DICTIONARY [db0.]dict_A TO [db1.]dict_B [,...] [ON CLUSTER cluster]  See Also Dictionaries "},{"title":"ALL Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/all","content":"ALL Clause If there are multiple matching rows in the table, then ALL returns all of them. SELECT ALL is identical to SELECT without DISTINCT. If both ALL and DISTINCT specified, exception will be thrown. ALL can also be specified inside aggregate function with the same effect(noop), for instance: SELECT sum(ALL number) FROM numbers(10); equals to SELECT sum(number) FROM numbers(10); Original article","keywords":""},{"title":"REVOKE Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/revoke","content":"","keywords":""},{"title":"Syntax​","type":1,"pageTitle":"REVOKE Statement","url":"en/sql-reference/statements/revoke#revoke-syntax","content":"Revoking privileges from users REVOKE [ON CLUSTER cluster_name] privilege[(column_name [,...])] [,...] ON {db.table|db.*|*.*|table|*} FROM {user | CURRENT_USER} [,...] | ALL | ALL EXCEPT {user | CURRENT_USER} [,...]  Revoking roles from users REVOKE [ON CLUSTER cluster_name] [ADMIN OPTION FOR] role [,...] FROM {user | role | CURRENT_USER} [,...] | ALL | ALL EXCEPT {user_name | role_name | CURRENT_USER} [,...]  "},{"title":"Description​","type":1,"pageTitle":"REVOKE Statement","url":"en/sql-reference/statements/revoke#revoke-description","content":"To revoke some privilege you can use a privilege of a wider scope than you plan to revoke. For example, if a user has the SELECT (x,y) privilege, administrator can execute REVOKE SELECT(x,y) ..., or REVOKE SELECT * ..., or even REVOKE ALL PRIVILEGES ... query to revoke this privilege. "},{"title":"Partial Revokes​","type":1,"pageTitle":"REVOKE Statement","url":"en/sql-reference/statements/revoke#partial-revokes-dscr","content":"You can revoke a part of a privilege. For example, if a user has the SELECT *.* privilege you can revoke from it a privilege to read data from some table or a database. "},{"title":"Examples​","type":1,"pageTitle":"REVOKE Statement","url":"en/sql-reference/statements/revoke#revoke-example","content":"Grant the john user account with a privilege to select from all the databases, excepting the accounts one: GRANT SELECT ON *.* TO john; REVOKE SELECT ON accounts.* FROM john;  Grant the mira user account with a privilege to select from all the columns of the accounts.staff table, excepting the wage one. GRANT SELECT ON accounts.staff TO mira; REVOKE SELECT(wage) ON accounts.staff FROM mira;  Original article "},{"title":"FORMAT Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/format","content":"","keywords":""},{"title":"Default Format​","type":1,"pageTitle":"FORMAT Clause","url":"en/sql-reference/statements/select/format#default-format","content":"If the FORMAT clause is omitted, the default format is used, which depends on both the settings and the interface used for accessing the ClickHouse server. For the HTTP interface and the command-line client in batch mode, the default format is TabSeparated. For the command-line client in interactive mode, the default format is PrettyCompact (it produces compact human-readable tables). "},{"title":"Implementation Details​","type":1,"pageTitle":"FORMAT Clause","url":"en/sql-reference/statements/select/format#implementation-details","content":"When using the command-line client, data is always passed over the network in an internal efficient format (Native). The client independently interprets the FORMAT clause of the query and formats the data itself (thus relieving the network and the server from the extra load). "},{"title":"FROM Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/from","content":"","keywords":""},{"title":"FINAL Modifier​","type":1,"pageTitle":"FROM Clause","url":"en/sql-reference/statements/select/from#select-from-final","content":"When FINAL is specified, ClickHouse fully merges the data before returning the result and thus performs all data transformations that happen during merges for the given table engine. It is applicable when selecting data from tables that use the MergeTree-engine family. Also supported for: Replicated versions of MergeTree engines.View, Buffer, Distributed, and MaterializedView engines that operate over other engines, provided they were created over MergeTree-engine tables. Now SELECT queries with FINAL are executed in parallel and slightly faster. But there are drawbacks (see below). The max_final_threads setting limits the number of threads used. "},{"title":"Drawbacks​","type":1,"pageTitle":"FROM Clause","url":"en/sql-reference/statements/select/from#drawbacks","content":"Queries that use FINAL are executed slightly slower than similar queries that do not, because: Data is merged during query execution.Queries with FINAL read primary key columns in addition to the columns specified in the query. In most cases, avoid using FINAL. The common approach is to use different queries that assume the background processes of the MergeTree engine have’t happened yet and deal with it by applying aggregation (for example, to discard duplicates). "},{"title":"Implementation Details​","type":1,"pageTitle":"FROM Clause","url":"en/sql-reference/statements/select/from#implementation-details","content":"If the FROM clause is omitted, data will be read from the system.one table. The system.one table contains exactly one row (this table fulfills the same purpose as the DUAL table found in other DBMSs). To execute a query, all the columns listed in the query are extracted from the appropriate table. Any columns not needed for the external query are thrown out of the subqueries. If a query does not list any columns (for example, SELECT count() FROM t), some column is extracted from the table anyway (the smallest one is preferred), in order to calculate the number of rows. "},{"title":"DISTINCT Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/distinct","content":"","keywords":""},{"title":"DISTINCT and ORDER BY​","type":1,"pageTitle":"DISTINCT Clause","url":"en/sql-reference/statements/select/distinct#distinct-orderby","content":"ClickHouse supports using the DISTINCT and ORDER BY clauses for different columns in one query. The DISTINCT clause is executed before the ORDER BY clause. Consider the table: ┌─a─┬─b─┐ │ 2 │ 1 │ │ 1 │ 2 │ │ 3 │ 3 │ │ 2 │ 4 │ └───┴───┘  Selecting data: SELECT DISTINCT a FROM t1 ORDER BY b ASC;  ┌─a─┐ │ 2 │ │ 1 │ │ 3 │ └───┘  Selecting data with the different sorting direction: SELECT DISTINCT a FROM t1 ORDER BY b DESC;  ┌─a─┐ │ 3 │ │ 1 │ │ 2 │ └───┘  Row 2, 4 was cut before sorting. Take this implementation specificity into account when programming queries. "},{"title":"Null Processing​","type":1,"pageTitle":"DISTINCT Clause","url":"en/sql-reference/statements/select/distinct#null-processing","content":"DISTINCT works with NULL as if NULL were a specific value, and NULL==NULL. In other words, in the DISTINCT results, different combinations with NULL occur only once. It differs from NULL processing in most other contexts. "},{"title":"Alternatives​","type":1,"pageTitle":"DISTINCT Clause","url":"en/sql-reference/statements/select/distinct#alternatives","content":"It is possible to obtain the same result by applying GROUP BY across the same set of values as specified as SELECT clause, without using any aggregate functions. But there are few differences from GROUP BY approach: DISTINCT can be applied together with GROUP BY.When ORDER BY is omitted and LIMIT is defined, the query stops running immediately after the required number of different rows has been read.Data blocks are output as they are processed, without waiting for the entire query to finish running. "},{"title":"INSERT INTO Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/insert-into","content":"","keywords":""},{"title":"Constraints​","type":1,"pageTitle":"INSERT INTO Statement","url":"en/sql-reference/statements/insert-into#constraints","content":"If table has constraints, their expressions will be checked for each row of inserted data. If any of those constraints is not satisfied — server will raise an exception containing constraint name and expression, the query will be stopped. "},{"title":"Inserting the Results of SELECT​","type":1,"pageTitle":"INSERT INTO Statement","url":"en/sql-reference/statements/insert-into#insert_query_insert-select","content":"Syntax INSERT INTO [db.]table [(c1, c2, c3)] SELECT ...  Columns are mapped according to their position in the SELECT clause. However, their names in the SELECT expression and the table for INSERT may differ. If necessary, type casting is performed. None of the data formats except Values allow setting values to expressions such as now(), 1 + 2, and so on. The Values format allows limited use of expressions, but this is not recommended, because in this case inefficient code is used for their execution. Other queries for modifying data parts are not supported: UPDATE, DELETE, REPLACE, MERGE, UPSERT, INSERT UPDATE. However, you can delete old data using ALTER TABLE ... DROP PARTITION. FORMAT clause must be specified in the end of query if SELECT clause contains table function input(). To insert a default value instead of NULL into a column with not nullable data type, enable insert_null_as_default setting. "},{"title":"Inserting Data from a File​","type":1,"pageTitle":"INSERT INTO Statement","url":"en/sql-reference/statements/insert-into#inserting-data-from-a-file","content":"Syntax INSERT INTO [db.]table [(c1, c2, c3)] FROM INFILE file_name [COMPRESSION type] FORMAT format_name  Use the syntax above to insert data from a file stored on a client side. file_name and type are string literals. Input file format must be set in the FORMAT clause. Compressed files are supported. Compression type is detected by the extension of the file name. Or it can be explicitly specified in a COMPRESSION clause. Supported types are: 'none', 'gzip', 'deflate', 'br', 'xz', 'zstd', 'lz4', 'bz2'. This functionality is available in the command-line client and clickhouse-local. Example Execute the following queries using command-line client: echo 1,A &gt; input.csv ; echo 2,B &gt;&gt; input.csv clickhouse-client --query=&quot;CREATE TABLE table_from_file (id UInt32, text String) ENGINE=MergeTree() ORDER BY id;&quot; clickhouse-client --query=&quot;INSERT INTO table_from_file FROM INFILE 'input.csv' FORMAT CSV;&quot; clickhouse-client --query=&quot;SELECT * FROM table_from_file FORMAT PrettyCompact;&quot;  Result: ┌─id─┬─text─┐ │ 1 │ A │ │ 2 │ B │ └────┴──────┘  "},{"title":"Inserting into Table Function​","type":1,"pageTitle":"INSERT INTO Statement","url":"en/sql-reference/statements/insert-into#inserting-into-table-function","content":"Data can be inserted into tables referenced by table functions. Syntax INSERT INTO [TABLE] FUNCTION table_func ...  Example remote table function is used in the following queries: CREATE TABLE simple_table (id UInt32, text String) ENGINE=MergeTree() ORDER BY id; INSERT INTO TABLE FUNCTION remote('localhost', default.simple_table) VALUES (100, 'inserted via remote()'); SELECT * FROM simple_table;  Result: ┌──id─┬─text──────────────────┐ │ 100 │ inserted via remote() │ └─────┴───────────────────────┘  "},{"title":"Performance Considerations​","type":1,"pageTitle":"INSERT INTO Statement","url":"en/sql-reference/statements/insert-into#performance-considerations","content":"INSERT sorts the input data by primary key and splits them into partitions by a partition key. If you insert data into several partitions at once, it can significantly reduce the performance of the INSERT query. To avoid this: Add data in fairly large batches, such as 100,000 rows at a time.Group data by a partition key before uploading it to ClickHouse. Performance will not decrease if: Data is added in real time.You upload data that is usually sorted by time. It's also possible to asynchronously insert data in small but frequent inserts. The data from such insertions is combined into batches and then safely inserted into a table. To enable the asynchronous mode, switch on the async_insert setting. Note that asynchronous insertions are supported only over HTTP protocol, and deduplication is not supported for them. See Also async_insertasync_insert_threadswait_for_async_insertwait_for_async_insert_timeoutasync_insert_max_data_sizeasync_insert_busy_timeout_msasync_insert_stale_timeout_ms "},{"title":"EXCEPT Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/except","content":"EXCEPT Clause The EXCEPT clause returns only those rows that result from the first query without the second. The queries must match the number of columns, order, and type. The result of EXCEPT can contain duplicate rows. Multiple EXCEPT statements are executed left to right if parenthesis are not specified. The EXCEPT operator has the same priority as the UNION clause and lower priority than the INTERSECT clause. SELECT column1 [, column2 ] FROM table1 [WHERE condition] EXCEPT SELECT column1 [, column2 ] FROM table2 [WHERE condition] The condition could be any expression based on your requirements. Examples Query: SELECT number FROM numbers(1,10) EXCEPT SELECT number FROM numbers(3,6); Result: ┌─number─┐ │ 1 │ │ 2 │ │ 9 │ │ 10 │ └────────┘ Query: CREATE TABLE t1(one String, two String, three String) ENGINE=Memory(); CREATE TABLE t2(four String, five String, six String) ENGINE=Memory(); INSERT INTO t1 VALUES ('q', 'm', 'b'), ('s', 'd', 'f'), ('l', 'p', 'o'), ('s', 'd', 'f'), ('s', 'd', 'f'), ('k', 't', 'd'), ('l', 'p', 'o'); INSERT INTO t2 VALUES ('q', 'm', 'b'), ('b', 'd', 'k'), ('s', 'y', 't'), ('s', 'd', 'f'), ('m', 'f', 'o'), ('k', 'k', 'd'); SELECT * FROM t1 EXCEPT SELECT * FROM t2; Result: ┌─one─┬─two─┬─three─┐ │ l │ p │ o │ │ k │ t │ d │ │ l │ p │ o │ └─────┴─────┴───────┘ See Also UNIONINTERSECT","keywords":""},{"title":"HAVING Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/having","content":"","keywords":""},{"title":"Limitations​","type":1,"pageTitle":"HAVING Clause","url":"en/sql-reference/statements/select/having#limitations","content":"HAVING can’t be used if aggregation is not performed. Use WHERE instead. "},{"title":"INTO OUTFILE Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/into-outfile","content":"","keywords":""},{"title":"Implementation Details​","type":1,"pageTitle":"INTO OUTFILE Clause","url":"en/sql-reference/statements/select/into-outfile#implementation-details","content":"This functionality is available in the command-line client and clickhouse-local. Thus a query sent via HTTP interface will fail.The query will fail if a file with the same file name already exists.The default output format is TabSeparated (like in the command-line client batch mode). Use FORMAT clause to change it. Example Execute the following query using command-line client: clickhouse-client --query=&quot;SELECT 1,'ABC' INTO OUTFILE 'select.gz' FORMAT CSV;&quot; zcat select.gz  Result: 1,&quot;ABC&quot;  "},{"title":"CREATE TABLE","type":0,"sectionRef":"#","url":"en/sql-reference/statements/create/table","content":"","keywords":""},{"title":"Syntax Forms​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#syntax-forms","content":""},{"title":"With Explicit Schema​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#with-explicit-schema","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [NULL|NOT NULL] [DEFAULT|MATERIALIZED|EPHEMERAL|ALIAS expr1] [compression_codec] [TTL expr1], name2 [type2] [NULL|NOT NULL] [DEFAULT|MATERIALIZED|EPHEMERAL|ALIAS expr2] [compression_codec] [TTL expr2], ... ) ENGINE = engine  Creates a table named table_name in the db database or the current database if db is not set, with the structure specified in brackets and the engine engine. The structure of the table is a list of column descriptions, secondary indexes and constraints . If primary key is supported by the engine, it will be indicated as parameter for the table engine. A column description is name type in the simplest case. Example: RegionID UInt32. Expressions can also be defined for default values (see below). If necessary, primary key can be specified, with one or more key expressions. "},{"title":"With a Schema Similar to Other Table​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#with-a-schema-similar-to-other-table","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name AS [db2.]name2 [ENGINE = engine]  Creates a table with the same structure as another table. You can specify a different engine for the table. If the engine is not specified, the same engine will be used as for the db2.name2 table. "},{"title":"From a Table Function​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#from-a-table-function","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name AS table_function()  Creates a table with the same result as that of the table function specified. The created table will also work in the same way as the corresponding table function that was specified. "},{"title":"From SELECT query​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#from-select-query","content":"CREATE TABLE [IF NOT EXISTS] [db.]table_name[(name1 [type1], name2 [type2], ...)] ENGINE = engine AS SELECT ...  Creates a table with a structure like the result of the SELECT query, with the engine engine, and fills it with data from SELECT. Also you can explicitly specify columns description. If the table already exists and IF NOT EXISTS is specified, the query won’t do anything. There can be other clauses after the ENGINE clause in the query. See detailed documentation on how to create tables in the descriptions of table engines. Example Query: CREATE TABLE t1 (x String) ENGINE = Memory AS SELECT 1; SELECT x, toTypeName(x) FROM t1;  Result: ┌─x─┬─toTypeName(x)─┐ │ 1 │ String │ └───┴───────────────┘  "},{"title":"NULL Or NOT NULL Modifiers​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#null-modifiers","content":"NULL and NOT NULL modifiers after data type in column definition allow or do not allow it to be Nullable. If the type is not Nullable and if NULL is specified, it will be treated as Nullable; if NOT NULL is specified, then no. For example, INT NULL is the same as Nullable(INT). If the type is Nullable and NULL or NOT NULL modifiers are specified, the exception will be thrown. See also data_type_default_nullable setting. "},{"title":"Default Values​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#create-default-values","content":"The column description can specify an expression for a default value, in one of the following ways: DEFAULT expr, MATERIALIZED expr, ALIAS expr. Example: URLDomain String DEFAULT domain(URL). If an expression for the default value is not defined, the default values will be set to zeros for numbers, empty strings for strings, empty arrays for arrays, and 1970-01-01 for dates or zero unix timestamp for DateTime, NULL for Nullable. If the default expression is defined, the column type is optional. If there isn’t an explicitly defined type, the default expression type is used. Example: EventDate DEFAULT toDate(EventTime) – the ‘Date’ type will be used for the ‘EventDate’ column. If the data type and default expression are defined explicitly, this expression will be cast to the specified type using type casting functions. Example: Hits UInt32 DEFAULT 0 means the same thing as Hits UInt32 DEFAULT toUInt32(0). Default expressions may be defined as an arbitrary expression from table constants and columns. When creating and changing the table structure, it checks that expressions do not contain loops. For INSERT, it checks that expressions are resolvable – that all columns they can be calculated from have been passed. "},{"title":"DEFAULT​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#default","content":"DEFAULT expr Normal default value. If the INSERT query does not specify the corresponding column, it will be filled in by computing the corresponding expression. "},{"title":"MATERIALIZED​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#materialized","content":"MATERIALIZED expr Materialized expression. Such a column can’t be specified for INSERT, because it is always calculated. For an INSERT without a list of columns, these columns are not considered. In addition, this column is not substituted when using an asterisk in a SELECT query. This is to preserve the invariant that the dump obtained using SELECT * can be inserted back into the table using INSERT without specifying the list of columns. "},{"title":"EPHEMERAL​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#ephemeral","content":"EPHEMERAL [expr] Ephemeral column. Such a column isn't stored in the table and cannot be SELECTed, but can be referenced in the defaults of CREATE statement. If expr is omitted type for column is required. INSERT without list of columns will skip such column, so SELECT/INSERT invariant is preserved - the dump obtained using SELECT * can be inserted back into the table using INSERT without specifying the list of columns. "},{"title":"ALIAS​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#alias","content":"ALIAS expr Synonym. Such a column isn’t stored in the table at all. Its values can’t be inserted in a table, and it is not substituted when using an asterisk in a SELECT query. It can be used in SELECTs if the alias is expanded during query parsing. When using the ALTER query to add new columns, old data for these columns is not written. Instead, when reading old data that does not have values for the new columns, expressions are computed on the fly by default. However, if running the expressions requires different columns that are not indicated in the query, these columns will additionally be read, but only for the blocks of data that need it. If you add a new column to a table but later change its default expression, the values used for old data will change (for data where values were not stored on the disk). Note that when running background merges, data for columns that are missing in one of the merging parts is written to the merged part. It is not possible to set default values for elements in nested data structures. "},{"title":"Primary Key​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#primary-key","content":"You can define a primary key when creating a table. Primary key can be specified in two ways: Inside the column list CREATE TABLE db.table_name ( name1 type1, name2 type2, ..., PRIMARY KEY(expr1[, expr2,...])] ) ENGINE = engine;  Outside the column list CREATE TABLE db.table_name ( name1 type1, name2 type2, ... ) ENGINE = engine PRIMARY KEY(expr1[, expr2,...]);  warning You can't combine both ways in one query. "},{"title":"Constraints​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#constraints","content":"Along with columns descriptions constraints could be defined: CREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster] ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1] [compression_codec] [TTL expr1], ... CONSTRAINT constraint_name_1 CHECK boolean_expr_1, ... ) ENGINE = engine  boolean_expr_1 could by any boolean expression. If constraints are defined for the table, each of them will be checked for every row in INSERT query. If any constraint is not satisfied — server will raise an exception with constraint name and checking expression. Adding large amount of constraints can negatively affect performance of big INSERT queries. "},{"title":"TTL Expression​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#ttl-expression","content":"Defines storage time for values. Can be specified only for MergeTree-family tables. For the detailed description, see TTL for columns and tables. "},{"title":"Column Compression Codecs​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#codecs","content":"By default, ClickHouse applies the lz4 compression method. For MergeTree-engine family you can change the default compression method in the compression section of a server configuration. You can also define the compression method for each individual column in the CREATE TABLE query. CREATE TABLE codec_example ( dt Date CODEC(ZSTD), ts DateTime CODEC(LZ4HC), float_value Float32 CODEC(NONE), double_value Float64 CODEC(LZ4HC(9)), value Float32 CODEC(Delta, ZSTD) ) ENGINE = &lt;Engine&gt; ...  The Default codec can be specified to reference default compression which may depend on different settings (and properties of data) in runtime. Example: value UInt64 CODEC(Default) — the same as lack of codec specification. Also you can remove current CODEC from the column and use default compression from config.xml: ALTER TABLE codec_example MODIFY COLUMN float_value CODEC(Default);  Codecs can be combined in a pipeline, for example, CODEC(Delta, Default). warning You can’t decompress ClickHouse database files with external utilities like lz4. Instead, use the special clickhouse-compressor utility. Compression is supported for the following table engines: MergeTree family. Supports column compression codecs and selecting the default compression method by compression settings.Log family. Uses the lz4 compression method by default and supports column compression codecs.Set. Only supported the default compression.Join. Only supported the default compression. ClickHouse supports general purpose codecs and specialized codecs. "},{"title":"General Purpose Codecs​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#create-query-general-purpose-codecs","content":"Codecs: NONE — No compression.LZ4 — Lossless data compression algorithm used by default. Applies LZ4 fast compression.LZ4HC[(level)] — LZ4 HC (high compression) algorithm with configurable level. Default level: 9. Setting level &lt;= 0 applies the default level. Possible levels: [1, 12]. Recommended level range: [4, 9].ZSTD[(level)] — ZSTD compression algorithm with configurable level. Possible levels: [1, 22]. Default value: 1. High compression levels are useful for asymmetric scenarios, like compress once, decompress repeatedly. Higher levels mean better compression and higher CPU usage. "},{"title":"Specialized Codecs​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#specialized-codecs","content":"These codecs are designed to make compression more effective by using specific features of data. Some of these codecs do not compress data themself. Instead, they prepare the data for a common purpose codec, which compresses it better than without this preparation. Specialized codecs: Delta(delta_bytes) — Compression approach in which raw values are replaced by the difference of two neighboring values, except for the first value that stays unchanged. Up to delta_bytes are used for storing delta values, so delta_bytes is the maximum size of raw values. Possible delta_bytes values: 1, 2, 4, 8. The default value for delta_bytes is sizeof(type) if equal to 1, 2, 4, or 8. In all other cases, it’s 1.DoubleDelta — Calculates delta of deltas and writes it in compact binary form. Optimal compression rates are achieved for monotonic sequences with a constant stride, such as time series data. Can be used with any fixed-width type. Implements the algorithm used in Gorilla TSDB, extending it to support 64-bit types. Uses 1 extra bit for 32-byte deltas: 5-bit prefixes instead of 4-bit prefixes. For additional information, see Compressing Time Stamps in Gorilla: A Fast, Scalable, In-Memory Time Series Database.Gorilla — Calculates XOR between current and previous value and writes it in compact binary form. Efficient when storing a series of floating point values that change slowly, because the best compression rate is achieved when neighboring values are binary equal. Implements the algorithm used in Gorilla TSDB, extending it to support 64-bit types. For additional information, see Compressing Values in Gorilla: A Fast, Scalable, In-Memory Time Series Database.T64 — Compression approach that crops unused high bits of values in integer data types (including Enum, Date and DateTime). At each step of its algorithm, codec takes a block of 64 values, puts them into 64x64 bit matrix, transposes it, crops the unused bits of values and returns the rest as a sequence. Unused bits are the bits, that do not differ between maximum and minimum values in the whole data part for which the compression is used. DoubleDelta and Gorilla codecs are used in Gorilla TSDB as the components of its compressing algorithm. Gorilla approach is effective in scenarios when there is a sequence of slowly changing values with their timestamps. Timestamps are effectively compressed by the DoubleDelta codec, and values are effectively compressed by the Gorilla codec. For example, to get an effectively stored table, you can create it in the following configuration: CREATE TABLE codec_example ( timestamp DateTime CODEC(DoubleDelta), slow_values Float32 CODEC(Gorilla) ) ENGINE = MergeTree()  "},{"title":"Encryption Codecs​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#create-query-encryption-codecs","content":"These codecs don't actually compress data, but instead encrypt data on disk. These are only available when an encryption key is specified by encryption settings. Note that encryption only makes sense at the end of codec pipelines, because encrypted data usually can't be compressed in any meaningful way. Encryption codecs: CODEC('AES-128-GCM-SIV') — Encrypts data with AES-128 in RFC 8452 GCM-SIV mode. CODEC('AES-256-GCM-SIV') — Encrypts data with AES-256 in GCM-SIV mode. These codecs use a fixed nonce and encryption is therefore deterministic. This makes it compatible with deduplicating engines such as ReplicatedMergeTree but has a weakness: when the same data block is encrypted twice, the resulting ciphertext will be exactly the same so an adversary who can read the disk can see this equivalence (although only the equivalence, without getting its content). warning Most engines including the &quot;*MergeTree&quot; family create index files on disk without applying codecs. This means plaintext will appear on disk if an encrypted column is indexed. warning If you perform a SELECT query mentioning a specific value in an encrypted column (such as in its WHERE clause), the value may appear in system.query_log. You may want to disable the logging. Example CREATE TABLE mytable ( x String Codec(AES_128_GCM_SIV) ) ENGINE = MergeTree ORDER BY x;  note If compression needs to be applied, it must be explicitly specified. Otherwise, only encryption will be applied to data. Example CREATE TABLE mytable ( x String Codec(Delta, LZ4, AES_128_GCM_SIV) ) ENGINE = MergeTree ORDER BY x;  "},{"title":"Temporary Tables​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#temporary-tables","content":"ClickHouse supports temporary tables which have the following characteristics: Temporary tables disappear when the session ends, including if the connection is lost.A temporary table uses the Memory engine only.The DB can’t be specified for a temporary table. It is created outside of databases.Impossible to create a temporary table with distributed DDL query on all cluster servers (by using ON CLUSTER): this table exists only in the current session.If a temporary table has the same name as another one and a query specifies the table name without specifying the DB, the temporary table will be used.For distributed query processing, temporary tables used in a query are passed to remote servers. To create a temporary table, use the following syntax: CREATE TEMPORARY TABLE [IF NOT EXISTS] table_name ( name1 [type1] [DEFAULT|MATERIALIZED|ALIAS expr1], name2 [type2] [DEFAULT|MATERIALIZED|ALIAS expr2], ... )  In most cases, temporary tables are not created manually, but when using external data for a query, or for distributed (GLOBAL) IN. For more information, see the appropriate sections It’s possible to use tables with ENGINE = Memory instead of temporary tables. "},{"title":"REPLACE TABLE​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#replace-table-query","content":"'REPLACE' query allows you to update the table atomically. note This query is supported only for Atomic database engine. If you need to delete some data from a table, you can create a new table and fill it with a SELECT statement that does not retrieve unwanted data, then drop the old table and rename the new one: CREATE TABLE myNewTable AS myOldTable; INSERT INTO myNewTable SELECT * FROM myOldTable WHERE CounterID &lt;12345; DROP TABLE myOldTable; RENAME TABLE myNewTable TO myOldTable;  Instead of above, you can use the following: REPLACE TABLE myOldTable SELECT * FROM myOldTable WHERE CounterID &lt;12345;  "},{"title":"Syntax​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#syntax","content":"{CREATE [OR REPLACE] | REPLACE} TABLE [db.]table_name  All syntax forms for CREATE query also work for this query. REPLACE for a non-existent table will cause an error. "},{"title":"Examples:​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#examples","content":"Consider the table: CREATE DATABASE base ENGINE = Atomic; CREATE OR REPLACE TABLE base.t1 (n UInt64, s String) ENGINE = MergeTree ORDER BY n; INSERT INTO base.t1 VALUES (1, 'test'); SELECT * FROM base.t1;  ┌─n─┬─s────┐ │ 1 │ test │ └───┴──────┘  Using REPLACE query to clear all data: CREATE OR REPLACE TABLE base.t1 (n UInt64, s Nullable(String)) ENGINE = MergeTree ORDER BY n; INSERT INTO base.t1 VALUES (2, null); SELECT * FROM base.t1;  ┌─n─┬─s──┐ │ 2 │ \\N │ └───┴────┘  Using REPLACE query to change table structure: REPLACE TABLE base.t1 (n UInt64) ENGINE = MergeTree ORDER BY n; INSERT INTO base.t1 VALUES (3); SELECT * FROM base.t1;  ┌─n─┐ │ 3 │ └───┘  "},{"title":"COMMENT Clause​","type":1,"pageTitle":"CREATE TABLE","url":"en/sql-reference/statements/create/table#comment-table","content":"You can add a comment to the table when you creating it. note The comment is supported for all table engines except Kafka, RabbitMQ and EmbeddedRocksDB. Syntax CREATE TABLE db.table_name ( name1 type1, name2 type2, ... ) ENGINE = engine COMMENT 'Comment'  Example Query: CREATE TABLE t1 (x String) ENGINE = Memory COMMENT 'The temporary table'; SELECT name, comment FROM system.tables WHERE name = 't1';  Result: ┌─name─┬─comment─────────────┐ │ t1 │ The temporary table │ └──────┴─────────────────────┘  "},{"title":"CREATE VIEW","type":0,"sectionRef":"#","url":"en/sql-reference/statements/create/view","content":"","keywords":""},{"title":"Normal View​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#normal","content":"Syntax: CREATE [OR REPLACE] VIEW [IF NOT EXISTS] [db.]table_name [ON CLUSTER] AS SELECT ...  Normal views do not store any data. They just perform a read from another table on each access. In other words, a normal view is nothing more than a saved query. When reading from a view, this saved query is used as a subquery in the FROM clause. As an example, assume you’ve created a view: CREATE VIEW view AS SELECT ...  and written a query: SELECT a, b, c FROM view  This query is fully equivalent to using the subquery: SELECT a, b, c FROM (SELECT ...)  "},{"title":"Materialized View​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#materialized","content":"CREATE MATERIALIZED VIEW [IF NOT EXISTS] [db.]table_name [ON CLUSTER] [TO[db.]name] [ENGINE = engine] [POPULATE] AS SELECT ...  Materialized views store data transformed by the corresponding SELECT query. When creating a materialized view without TO [db].[table], you must specify ENGINE – the table engine for storing data. When creating a materialized view with TO [db].[table], you must not use POPULATE. A materialized view is implemented as follows: when inserting data to the table specified in SELECT, part of the inserted data is converted by this SELECT query, and the result is inserted in the view. note Materialized views in ClickHouse use column names instead of column order during insertion into destination table. If some column names are not present in the SELECT query result, ClickHouse uses a default value, even if the column is not Nullable. A safe practice would be to add aliases for every column when using Materialized views. Materialized views in ClickHouse are implemented more like insert triggers. If there’s some aggregation in the view query, it’s applied only to the batch of freshly inserted data. Any changes to existing data of source table (like update, delete, drop partition, etc.) does not change the materialized view. If you specify POPULATE, the existing table data is inserted into the view when creating it, as if making a CREATE TABLE ... AS SELECT ... . Otherwise, the query contains only the data inserted in the table after creating the view. We do not recommend using POPULATE, since data inserted in the table during the view creation will not be inserted in it. A SELECT query can contain DISTINCT, GROUP BY, ORDER BY, LIMIT. Note that the corresponding conversions are performed independently on each block of inserted data. For example, if GROUP BY is set, data is aggregated during insertion, but only within a single packet of inserted data. The data won’t be further aggregated. The exception is when using an ENGINE that independently performs data aggregation, such as SummingMergeTree. The execution of ALTER queries on materialized views has limitations, so they might be inconvenient. If the materialized view uses the construction TO [db.]name, you can DETACH the view, run ALTER for the target table, and then ATTACH the previously detached (DETACH) view. Note that materialized view is influenced by optimize_on_insert setting. The data is merged before the insertion into a view. Views look the same as normal tables. For example, they are listed in the result of the SHOW TABLES query. To delete a view, use DROP VIEW. Although DROP TABLE works for VIEWs as well. "},{"title":"Live View [Experimental]​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#live-view","content":"note This is an experimental feature that may change in backwards-incompatible ways in the future releases. Enable usage of live views and WATCH query using allow_experimental_live_view setting. Input the command set allow_experimental_live_view = 1. CREATE LIVE VIEW [IF NOT EXISTS] [db.]table_name [WITH [TIMEOUT [value_in_sec] [AND]] [REFRESH [value_in_sec]]] AS SELECT ...  Live views store result of the corresponding SELECT query and are updated any time the result of the query changes. Query result as well as partial result needed to combine with new data are stored in memory providing increased performance for repeated queries. Live views can provide push notifications when query result changes using the WATCH query. Live views are triggered by insert into the innermost table specified in the query. Live views work similarly to how a query in a distributed table works. But instead of combining partial results from different servers they combine partial result from current data with partial result from the new data. When a live view query includes a subquery then the cached partial result is only stored for the innermost subquery. info Table function is not supported as the innermost table.Tables that do not have inserts such as a dictionary, system table, a normal view, or a materialized view will not trigger a live view.Only queries where one can combine partial result from the old data plus partial result from the new data will work. Live view will not work for queries that require the complete data set to compute the final result or aggregations where the state of the aggregation must be preserved.Does not work with replicated or distributed tables where inserts are performed on different nodes.Can't be triggered by multiple tables. See WITH REFRESH to force periodic updates of a live view that in some cases can be used as a workaround. "},{"title":"Monitoring Live View Changes​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#live-view-monitoring","content":"You can monitor changes in the LIVE VIEW query result using WATCH query. WATCH [db.]live_view  Example: CREATE TABLE mt (x Int8) Engine = MergeTree ORDER BY x; CREATE LIVE VIEW lv AS SELECT sum(x) FROM mt;  Watch a live view while doing a parallel insert into the source table. WATCH lv;  ┌─sum(x)─┬─_version─┐ │ 1 │ 1 │ └────────┴──────────┘ ┌─sum(x)─┬─_version─┐ │ 3 │ 2 │ └────────┴──────────┘ ┌─sum(x)─┬─_version─┐ │ 6 │ 3 │ └────────┴──────────┘  INSERT INTO mt VALUES (1); INSERT INTO mt VALUES (2); INSERT INTO mt VALUES (3);  Or add EVENTS clause to just get change events. WATCH [db.]live_view EVENTS;  Example: WATCH lv EVENTS;  ┌─version─┐ │ 1 │ └─────────┘ ┌─version─┐ │ 2 │ └─────────┘ ┌─version─┐ │ 3 │ └─────────┘  You can execute SELECT query on a live view in the same way as for any regular view or a table. If the query result is cached it will return the result immediately without running the stored query on the underlying tables. SELECT * FROM [db.]live_view WHERE ...  "},{"title":"Force Live View Refresh​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#live-view-alter-refresh","content":"You can force live view refresh using the ALTER LIVE VIEW [db.]table_name REFRESH statement. "},{"title":"WITH TIMEOUT Clause​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#live-view-with-timeout","content":"When a live view is created with a WITH TIMEOUT clause then the live view will be dropped automatically after the specified number of seconds elapse since the end of the last WATCH query that was watching the live view. CREATE LIVE VIEW [db.]table_name WITH TIMEOUT [value_in_sec] AS SELECT ...  If the timeout value is not specified then the value specified by the temporary_live_view_timeout setting is used. Example: CREATE TABLE mt (x Int8) Engine = MergeTree ORDER BY x; CREATE LIVE VIEW lv WITH TIMEOUT 15 AS SELECT sum(x) FROM mt;  "},{"title":"WITH REFRESH Clause​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#live-view-with-refresh","content":"When a live view is created with a WITH REFRESH clause then it will be automatically refreshed after the specified number of seconds elapse since the last refresh or trigger. CREATE LIVE VIEW [db.]table_name WITH REFRESH [value_in_sec] AS SELECT ...  If the refresh value is not specified then the value specified by the periodic_live_view_refresh setting is used. Example: CREATE LIVE VIEW lv WITH REFRESH 5 AS SELECT now(); WATCH lv  ┌───────────────now()─┬─_version─┐ │ 2021-02-21 08:47:05 │ 1 │ └─────────────────────┴──────────┘ ┌───────────────now()─┬─_version─┐ │ 2021-02-21 08:47:10 │ 2 │ └─────────────────────┴──────────┘ ┌───────────────now()─┬─_version─┐ │ 2021-02-21 08:47:15 │ 3 │ └─────────────────────┴──────────┘  You can combine WITH TIMEOUT and WITH REFRESH clauses using an AND clause. CREATE LIVE VIEW [db.]table_name WITH TIMEOUT [value_in_sec] AND REFRESH [value_in_sec] AS SELECT ...  Example: CREATE LIVE VIEW lv WITH TIMEOUT 15 AND REFRESH 5 AS SELECT now();  After 15 sec the live view will be automatically dropped if there are no active WATCH queries. WATCH lv  Code: 60. DB::Exception: Received from localhost:9000. DB::Exception: Table default.lv does not exist..  "},{"title":"Live View Usage​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#live-view-usage","content":"Most common uses of live view tables include: Providing push notifications for query result changes to avoid polling.Caching results of most frequent queries to provide immediate query results.Watching for table changes and triggering a follow-up select queries.Watching metrics from system tables using periodic refresh. See Also ALTER LIVE VIEW "},{"title":"Window View [Experimental]​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#window-view","content":"info This is an experimental feature that may change in backwards-incompatible ways in the future releases. Enable usage of window views and WATCH query using allow_experimental_window_view setting. Input the command set allow_experimental_window_view = 1. CREATE WINDOW VIEW [IF NOT EXISTS] [db.]table_name [TO [db.]table_name] [ENGINE = engine] [WATERMARK = strategy] [ALLOWED_LATENESS = interval_function] AS SELECT ... GROUP BY time_window_function  Window view can aggregate data by time window and output the results when the window is ready to fire. It stores the partial aggregation results in an inner(or specified) table to reduce latency and can push the processing result to a specified table or push notifications using the WATCH query. Creating a window view is similar to creating MATERIALIZED VIEW. Window view needs an inner storage engine to store intermediate data. The inner storage will use AggregatingMergeTree as the default engine. "},{"title":"Time Window Functions​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#window-view-timewindowfunctions","content":"Time window functions are used to get the lower and upper window bound of records. The window view needs to be used with a time window function. "},{"title":"TIME ATTRIBUTES​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#window-view-timeattributes","content":"Window view supports processing time and event time process. Processing time allows window view to produce results based on the local machine's time and is used by default. It is the most straightforward notion of time but does not provide determinism. The processing time attribute can be defined by setting the time_attr of the time window function to a table column or using the function now(). The following query creates a window view with processing time. CREATE WINDOW VIEW wv AS SELECT count(number), tumbleStart(w_id) as w_start from date GROUP BY tumble(now(), INTERVAL '5' SECOND) as w_id  Event time is the time that each individual event occurred on its producing device. This time is typically embedded within the records when it is generated. Event time processing allows for consistent results even in case of out-of-order events or late events. Window view supports event time processing by using WATERMARK syntax. Window view provides three watermark strategies: STRICTLY_ASCENDING: Emits a watermark of the maximum observed timestamp so far. Rows that have a timestamp smaller to the max timestamp are not late.ASCENDING: Emits a watermark of the maximum observed timestamp so far minus 1. Rows that have a timestamp equal and smaller to the max timestamp are not late.BOUNDED: WATERMARK=INTERVAL. Emits watermarks, which are the maximum observed timestamp minus the specified delay. The following queries are examples of creating a window view with WATERMARK: CREATE WINDOW VIEW wv WATERMARK=STRICTLY_ASCENDING AS SELECT count(number) FROM date GROUP BY tumble(timestamp, INTERVAL '5' SECOND); CREATE WINDOW VIEW wv WATERMARK=ASCENDING AS SELECT count(number) FROM date GROUP BY tumble(timestamp, INTERVAL '5' SECOND); CREATE WINDOW VIEW wv WATERMARK=INTERVAL '3' SECOND AS SELECT count(number) FROM date GROUP BY tumble(timestamp, INTERVAL '5' SECOND);  By default, the window will be fired when the watermark comes, and elements that arrived behind the watermark will be dropped. Window view supports late event processing by setting ALLOWED_LATENESS=INTERVAL. An example of lateness handling is: CREATE WINDOW VIEW test.wv TO test.dst WATERMARK=ASCENDING ALLOWED_LATENESS=INTERVAL '2' SECOND AS SELECT count(a) AS count, tumbleEnd(wid) AS w_end FROM test.mt GROUP BY tumble(timestamp, INTERVAL '5' SECOND) AS wid;  Note that elements emitted by a late firing should be treated as updated results of a previous computation. Instead of firing at the end of windows, the window view will fire immediately when the late event arrives. Thus, it will result in multiple outputs for the same window. Users need to take these duplicated results into account or deduplicate them. "},{"title":"Monitoring New Windows​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#window-view-monitoring","content":"Window view supports the WATCH query to monitoring changes, or use TO syntax to output the results to a table. WATCH [db.]window_view [EVENTS] [LIMIT n] [FORMAT format]  WATCH query acts similar as in LIVE VIEW. A LIMIT can be specified to set the number of updates to receive before terminating the query. The EVENTS clause can be used to obtain a short form of the WATCH query where instead of the query result you will just get the latest query watermark. "},{"title":"Settings​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#window-view-settings","content":"window_view_clean_interval: The clean interval of window view in seconds to free outdated data. The system will retain the windows that have not been fully triggered according to the system time or WATERMARK configuration, and the other data will be deleted.window_view_heartbeat_interval: The heartbeat interval in seconds to indicate the watch query is alive. "},{"title":"Example​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#window-view-example","content":"Suppose we need to count the number of click logs per 10 seconds in a log table called data, and its table structure is: CREATE TABLE data ( `id` UInt64, `timestamp` DateTime) ENGINE = Memory;  First, we create a window view with tumble window of 10 seconds interval: CREATE WINDOW VIEW wv as select count(id), tumbleStart(w_id) as window_start from data group by tumble(timestamp, INTERVAL '10' SECOND) as w_id  Then, we use the WATCH query to get the results. WATCH wv  When logs are inserted into table data, INSERT INTO data VALUES(1,now())  The WATCH query should print the results as follows: ┌─count(id)─┬────────window_start─┐ │ 1 │ 2020-01-14 16:56:40 │ └───────────┴─────────────────────┘  Alternatively, we can attach the output to another table using TO syntax. CREATE WINDOW VIEW wv TO dst AS SELECT count(id), tumbleStart(w_id) as window_start FROM data GROUP BY tumble(timestamp, INTERVAL '10' SECOND) as w_id  Additional examples can be found among stateful tests of ClickHouse (they are named *window_view* there). "},{"title":"Window View Usage​","type":1,"pageTitle":"CREATE VIEW","url":"en/sql-reference/statements/create/view#window-view-usage","content":"The window view is useful in the following scenarios: Monitoring: Aggregate and calculate the metrics logs by time, and output the results to a target table. The dashboard can use the target table as a source table.Analyzing: Automatically aggregate and preprocess data in the time window. This can be useful when analyzing a large number of logs. The preprocessing eliminates repeated calculations in multiple queries and reduces query latency. "},{"title":"ARRAY JOIN Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/array-join","content":"","keywords":""},{"title":"Basic ARRAY JOIN Examples​","type":1,"pageTitle":"ARRAY JOIN Clause","url":"en/sql-reference/statements/select/array-join#basic-array-join-examples","content":"The examples below demonstrate the usage of the ARRAY JOIN and LEFT ARRAY JOIN clauses. Let’s create a table with an Array type column and insert values into it: CREATE TABLE arrays_test ( s String, arr Array(UInt8) ) ENGINE = Memory; INSERT INTO arrays_test VALUES ('Hello', [1,2]), ('World', [3,4,5]), ('Goodbye', []);  ┌─s───────────┬─arr─────┐ │ Hello │ [1,2] │ │ World │ [3,4,5] │ │ Goodbye │ [] │ └─────────────┴─────────┘  The example below uses the ARRAY JOIN clause: SELECT s, arr FROM arrays_test ARRAY JOIN arr;  ┌─s─────┬─arr─┐ │ Hello │ 1 │ │ Hello │ 2 │ │ World │ 3 │ │ World │ 4 │ │ World │ 5 │ └───────┴─────┘  The next example uses the LEFT ARRAY JOIN clause: SELECT s, arr FROM arrays_test LEFT ARRAY JOIN arr;  ┌─s───────────┬─arr─┐ │ Hello │ 1 │ │ Hello │ 2 │ │ World │ 3 │ │ World │ 4 │ │ World │ 5 │ │ Goodbye │ 0 │ └─────────────┴─────┘  "},{"title":"Using Aliases​","type":1,"pageTitle":"ARRAY JOIN Clause","url":"en/sql-reference/statements/select/array-join#using-aliases","content":"An alias can be specified for an array in the ARRAY JOIN clause. In this case, an array item can be accessed by this alias, but the array itself is accessed by the original name. Example: SELECT s, arr, a FROM arrays_test ARRAY JOIN arr AS a;  ┌─s─────┬─arr─────┬─a─┐ │ Hello │ [1,2] │ 1 │ │ Hello │ [1,2] │ 2 │ │ World │ [3,4,5] │ 3 │ │ World │ [3,4,5] │ 4 │ │ World │ [3,4,5] │ 5 │ └───────┴─────────┴───┘  Using aliases, you can perform ARRAY JOIN with an external array. For example: SELECT s, arr_external FROM arrays_test ARRAY JOIN [1, 2, 3] AS arr_external;  ┌─s───────────┬─arr_external─┐ │ Hello │ 1 │ │ Hello │ 2 │ │ Hello │ 3 │ │ World │ 1 │ │ World │ 2 │ │ World │ 3 │ │ Goodbye │ 1 │ │ Goodbye │ 2 │ │ Goodbye │ 3 │ └─────────────┴──────────────┘  Multiple arrays can be comma-separated in the ARRAY JOIN clause. In this case, JOIN is performed with them simultaneously (the direct sum, not the cartesian product). Note that all the arrays must have the same size by default. Example: SELECT s, arr, a, num, mapped FROM arrays_test ARRAY JOIN arr AS a, arrayEnumerate(arr) AS num, arrayMap(x -&gt; x + 1, arr) AS mapped;  ┌─s─────┬─arr─────┬─a─┬─num─┬─mapped─┐ │ Hello │ [1,2] │ 1 │ 1 │ 2 │ │ Hello │ [1,2] │ 2 │ 2 │ 3 │ │ World │ [3,4,5] │ 3 │ 1 │ 4 │ │ World │ [3,4,5] │ 4 │ 2 │ 5 │ │ World │ [3,4,5] │ 5 │ 3 │ 6 │ └───────┴─────────┴───┴─────┴────────┘  The example below uses the arrayEnumerate function: SELECT s, arr, a, num, arrayEnumerate(arr) FROM arrays_test ARRAY JOIN arr AS a, arrayEnumerate(arr) AS num;  ┌─s─────┬─arr─────┬─a─┬─num─┬─arrayEnumerate(arr)─┐ │ Hello │ [1,2] │ 1 │ 1 │ [1,2] │ │ Hello │ [1,2] │ 2 │ 2 │ [1,2] │ │ World │ [3,4,5] │ 3 │ 1 │ [1,2,3] │ │ World │ [3,4,5] │ 4 │ 2 │ [1,2,3] │ │ World │ [3,4,5] │ 5 │ 3 │ [1,2,3] │ └───────┴─────────┴───┴─────┴─────────────────────┘  Multiple arrays with different sizes can be joined by using: SETTINGS enable_unaligned_array_join = 1. Example: SELECT s, arr, a, b FROM arrays_test ARRAY JOIN arr as a, [['a','b'],['c']] as b SETTINGS enable_unaligned_array_join = 1;  ┌─s───────┬─arr─────┬─a─┬─b─────────┐ │ Hello │ [1,2] │ 1 │ ['a','b'] │ │ Hello │ [1,2] │ 2 │ ['c'] │ │ World │ [3,4,5] │ 3 │ ['a','b'] │ │ World │ [3,4,5] │ 4 │ ['c'] │ │ World │ [3,4,5] │ 5 │ [] │ │ Goodbye │ [] │ 0 │ ['a','b'] │ │ Goodbye │ [] │ 0 │ ['c'] │ └─────────┴─────────┴───┴───────────┘  "},{"title":"ARRAY JOIN with Nested Data Structure​","type":1,"pageTitle":"ARRAY JOIN Clause","url":"en/sql-reference/statements/select/array-join#array-join-with-nested-data-structure","content":"ARRAY JOIN also works with nested data structures: CREATE TABLE nested_test ( s String, nest Nested( x UInt8, y UInt32) ) ENGINE = Memory; INSERT INTO nested_test VALUES ('Hello', [1,2], [10,20]), ('World', [3,4,5], [30,40,50]), ('Goodbye', [], []);  ┌─s───────┬─nest.x──┬─nest.y─────┐ │ Hello │ [1,2] │ [10,20] │ │ World │ [3,4,5] │ [30,40,50] │ │ Goodbye │ [] │ [] │ └─────────┴─────────┴────────────┘  SELECT s, `nest.x`, `nest.y` FROM nested_test ARRAY JOIN nest;  ┌─s─────┬─nest.x─┬─nest.y─┐ │ Hello │ 1 │ 10 │ │ Hello │ 2 │ 20 │ │ World │ 3 │ 30 │ │ World │ 4 │ 40 │ │ World │ 5 │ 50 │ └───────┴────────┴────────┘  When specifying names of nested data structures in ARRAY JOIN, the meaning is the same as ARRAY JOIN with all the array elements that it consists of. Examples are listed below: SELECT s, `nest.x`, `nest.y` FROM nested_test ARRAY JOIN `nest.x`, `nest.y`;  ┌─s─────┬─nest.x─┬─nest.y─┐ │ Hello │ 1 │ 10 │ │ Hello │ 2 │ 20 │ │ World │ 3 │ 30 │ │ World │ 4 │ 40 │ │ World │ 5 │ 50 │ └───────┴────────┴────────┘  This variation also makes sense: SELECT s, `nest.x`, `nest.y` FROM nested_test ARRAY JOIN `nest.x`;  ┌─s─────┬─nest.x─┬─nest.y─────┐ │ Hello │ 1 │ [10,20] │ │ Hello │ 2 │ [10,20] │ │ World │ 3 │ [30,40,50] │ │ World │ 4 │ [30,40,50] │ │ World │ 5 │ [30,40,50] │ └───────┴────────┴────────────┘  An alias may be used for a nested data structure, in order to select either the JOIN result or the source array. Example: SELECT s, `n.x`, `n.y`, `nest.x`, `nest.y` FROM nested_test ARRAY JOIN nest AS n;  ┌─s─────┬─n.x─┬─n.y─┬─nest.x──┬─nest.y─────┐ │ Hello │ 1 │ 10 │ [1,2] │ [10,20] │ │ Hello │ 2 │ 20 │ [1,2] │ [10,20] │ │ World │ 3 │ 30 │ [3,4,5] │ [30,40,50] │ │ World │ 4 │ 40 │ [3,4,5] │ [30,40,50] │ │ World │ 5 │ 50 │ [3,4,5] │ [30,40,50] │ └───────┴─────┴─────┴─────────┴────────────┘  Example of using the arrayEnumerate function: SELECT s, `n.x`, `n.y`, `nest.x`, `nest.y`, num FROM nested_test ARRAY JOIN nest AS n, arrayEnumerate(`nest.x`) AS num;  ┌─s─────┬─n.x─┬─n.y─┬─nest.x──┬─nest.y─────┬─num─┐ │ Hello │ 1 │ 10 │ [1,2] │ [10,20] │ 1 │ │ Hello │ 2 │ 20 │ [1,2] │ [10,20] │ 2 │ │ World │ 3 │ 30 │ [3,4,5] │ [30,40,50] │ 1 │ │ World │ 4 │ 40 │ [3,4,5] │ [30,40,50] │ 2 │ │ World │ 5 │ 50 │ [3,4,5] │ [30,40,50] │ 3 │ └───────┴─────┴─────┴─────────┴────────────┴─────┘  "},{"title":"Implementation Details​","type":1,"pageTitle":"ARRAY JOIN Clause","url":"en/sql-reference/statements/select/array-join#implementation-details","content":"The query execution order is optimized when running ARRAY JOIN. Although ARRAY JOIN must always be specified before the WHERE/PREWHERE clause in a query, technically they can be performed in any order, unless result of ARRAY JOIN is used for filtering. The processing order is controlled by the query optimizer. "},{"title":"SELECT Query","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/","content":"","keywords":""},{"title":"Syntax​","type":1,"pageTitle":"SELECT Query","url":"en/sql-reference/statements/select/#syntax","content":"[WITH expr_list|(subquery)] SELECT [DISTINCT [ON (column1, column2, ...)]] expr_list [FROM [db.]table | (subquery) | table_function] [FINAL] [SAMPLE sample_coeff] [ARRAY JOIN ...] [GLOBAL] [ANY|ALL|ASOF] [INNER|LEFT|RIGHT|FULL|CROSS] [OUTER|SEMI|ANTI] JOIN (subquery)|table (ON &lt;expr_list&gt;)|(USING &lt;column_list&gt;) [PREWHERE expr] [WHERE expr] [GROUP BY expr_list] [WITH ROLLUP|WITH CUBE] [WITH TOTALS] [HAVING expr] [ORDER BY expr_list] [WITH FILL] [FROM expr] [TO expr] [STEP expr] [INTERPOLATE [(expr_list)]] [LIMIT [offset_value, ]n BY columns] [LIMIT [n, ]m] [WITH TIES] [SETTINGS ...] [UNION ...] [INTO OUTFILE filename [COMPRESSION type] ] [FORMAT format]  All clauses are optional, except for the required list of expressions immediately after SELECT which is covered in more detail below. Specifics of each optional clause are covered in separate sections, which are listed in the same order as they are executed: WITH clauseSELECT clauseDISTINCT clauseFROM clauseSAMPLE clauseJOIN clausePREWHERE clauseWHERE clauseGROUP BY clauseLIMIT BY clauseHAVING clauseLIMIT clauseOFFSET clauseUNION clauseINTERSECT clauseEXCEPT clauseINTO OUTFILE clauseFORMAT clause "},{"title":"SELECT Clause​","type":1,"pageTitle":"SELECT Query","url":"en/sql-reference/statements/select/#select-clause","content":"Expressions specified in the SELECT clause are calculated after all the operations in the clauses described above are finished. These expressions work as if they apply to separate rows in the result. If expressions in the SELECT clause contain aggregate functions, then ClickHouse processes aggregate functions and expressions used as their arguments during the GROUP BY aggregation. If you want to include all columns in the result, use the asterisk (*) symbol. For example, SELECT * FROM .... "},{"title":"COLUMNS expression​","type":1,"pageTitle":"SELECT Query","url":"en/sql-reference/statements/select/#columns-expression","content":"To match some columns in the result with a re2 regular expression, you can use the COLUMNS expression. COLUMNS('regexp')  For example, consider the table: CREATE TABLE default.col_names (aa Int8, ab Int8, bc Int8) ENGINE = TinyLog  The following query selects data from all the columns containing the a symbol in their name. SELECT COLUMNS('a') FROM col_names  ┌─aa─┬─ab─┐ │ 1 │ 1 │ └────┴────┘  The selected columns are returned not in the alphabetical order. You can use multiple COLUMNS expressions in a query and apply functions to them. For example: SELECT COLUMNS('a'), COLUMNS('c'), toTypeName(COLUMNS('c')) FROM col_names  ┌─aa─┬─ab─┬─bc─┬─toTypeName(bc)─┐ │ 1 │ 1 │ 1 │ Int8 │ └────┴────┴────┴────────────────┘  Each column returned by the COLUMNS expression is passed to the function as a separate argument. Also you can pass other arguments to the function if it supports them. Be careful when using functions. If a function does not support the number of arguments you have passed to it, ClickHouse throws an exception. For example: SELECT COLUMNS('a') + COLUMNS('c') FROM col_names  Received exception from server (version 19.14.1): Code: 42. DB::Exception: Received from localhost:9000. DB::Exception: Number of arguments for function plus does not match: passed 3, should be 2.  In this example, COLUMNS('a') returns two columns: aa and ab. COLUMNS('c') returns the bc column. The + operator can’t apply to 3 arguments, so ClickHouse throws an exception with the relevant message. Columns that matched the COLUMNS expression can have different data types. If COLUMNS does not match any columns and is the only expression in SELECT, ClickHouse throws an exception. "},{"title":"Asterisk​","type":1,"pageTitle":"SELECT Query","url":"en/sql-reference/statements/select/#asterisk","content":"You can put an asterisk in any part of a query instead of an expression. When the query is analyzed, the asterisk is expanded to a list of all table columns (excluding the MATERIALIZED and ALIAS columns). There are only a few cases when using an asterisk is justified: When creating a table dump.For tables containing just a few columns, such as system tables.For getting information about what columns are in a table. In this case, set LIMIT 1. But it is better to use the DESC TABLE query.When there is strong filtration on a small number of columns using PREWHERE.In subqueries (since columns that aren’t needed for the external query are excluded from subqueries). In all other cases, we do not recommend using the asterisk, since it only gives you the drawbacks of a columnar DBMS instead of the advantages. In other words using the asterisk is not recommended. "},{"title":"Extreme Values​","type":1,"pageTitle":"SELECT Query","url":"en/sql-reference/statements/select/#extreme-values","content":"In addition to results, you can also get minimum and maximum values for the results columns. To do this, set the extremes setting to 1. Minimums and maximums are calculated for numeric types, dates, and dates with times. For other columns, the default values are output. An extra two rows are calculated – the minimums and maximums, respectively. These extra two rows are output in JSON*, TabSeparated*, and Pretty* formats, separate from the other rows. They are not output for other formats. In JSON* formats, the extreme values are output in a separate ‘extremes’ field. In TabSeparated* formats, the row comes after the main result, and after ‘totals’ if present. It is preceded by an empty row (after the other data). In Pretty* formats, the row is output as a separate table after the main result, and after totals if present. Extreme values are calculated for rows before LIMIT, but after LIMIT BY. However, when using LIMIT offset, size, the rows before offset are included in extremes. In stream requests, the result may also include a small number of rows that passed through LIMIT. "},{"title":"Notes​","type":1,"pageTitle":"SELECT Query","url":"en/sql-reference/statements/select/#notes","content":"You can use synonyms (AS aliases) in any part of a query. The GROUP BY, ORDER BY, and LIMIT BY clauses can support positional arguments. To enable this, switch on the enable_positional_arguments setting. Then, for example, ORDER BY 1,2 will be sorting rows in the table on the first and then the second column. "},{"title":"Implementation Details​","type":1,"pageTitle":"SELECT Query","url":"en/sql-reference/statements/select/#implementation-details","content":"If the query omits the DISTINCT, GROUP BY and ORDER BY clauses and the IN and JOIN subqueries, the query will be completely stream processed, using O(1) amount of RAM. Otherwise, the query might consume a lot of RAM if the appropriate restrictions are not specified: max_memory_usagemax_rows_to_group_bymax_rows_to_sortmax_rows_in_distinctmax_bytes_in_distinctmax_rows_in_setmax_bytes_in_setmax_rows_in_joinmax_bytes_in_joinmax_bytes_before_external_sortmax_bytes_before_external_group_by For more information, see the section “Settings”. It is possible to use external sorting (saving temporary tables to a disk) and external aggregation. "},{"title":"SELECT modifiers​","type":1,"pageTitle":"SELECT Query","url":"en/sql-reference/statements/select/#select-modifiers","content":"You can use the following modifiers in SELECT queries. "},{"title":"APPLY​","type":1,"pageTitle":"SELECT Query","url":"en/sql-reference/statements/select/#apply-modifier","content":"Allows you to invoke some function for each row returned by an outer table expression of a query. Syntax: SELECT &lt;expr&gt; APPLY( &lt;func&gt; ) FROM [db.]table_name  Example: CREATE TABLE columns_transformers (i Int64, j Int16, k Int64) ENGINE = MergeTree ORDER by (i); INSERT INTO columns_transformers VALUES (100, 10, 324), (120, 8, 23); SELECT * APPLY(sum) FROM columns_transformers;  ┌─sum(i)─┬─sum(j)─┬─sum(k)─┐ │ 220 │ 18 │ 347 │ └────────┴────────┴────────┘  "},{"title":"EXCEPT​","type":1,"pageTitle":"SELECT Query","url":"en/sql-reference/statements/select/#except-modifier","content":"Specifies the names of one or more columns to exclude from the result. All matching column names are omitted from the output. Syntax: SELECT &lt;expr&gt; EXCEPT ( col_name1 [, col_name2, col_name3, ...] ) FROM [db.]table_name  Example: SELECT * EXCEPT (i) from columns_transformers;  ┌──j─┬───k─┐ │ 10 │ 324 │ │ 8 │ 23 │ └────┴─────┘  "},{"title":"REPLACE​","type":1,"pageTitle":"SELECT Query","url":"en/sql-reference/statements/select/#replace-modifier","content":"Specifies one or more expression aliases. Each alias must match a column name from the SELECT * statement. In the output column list, the column that matches the alias is replaced by the expression in that REPLACE. This modifier does not change the names or order of columns. However, it can change the value and the value type. Syntax: SELECT &lt;expr&gt; REPLACE( &lt;expr&gt; AS col_name) from [db.]table_name  Example: SELECT * REPLACE(i + 1 AS i) from columns_transformers;  ┌───i─┬──j─┬───k─┐ │ 101 │ 10 │ 324 │ │ 121 │ 8 │ 23 │ └─────┴────┴─────┘  "},{"title":"Modifier Combinations​","type":1,"pageTitle":"SELECT Query","url":"en/sql-reference/statements/select/#modifier-combinations","content":"You can use each modifier separately or combine them. Examples: Using the same modifier multiple times. SELECT COLUMNS('[jk]') APPLY(toString) APPLY(length) APPLY(max) from columns_transformers;  ┌─max(length(toString(j)))─┬─max(length(toString(k)))─┐ │ 2 │ 3 │ └──────────────────────────┴──────────────────────────┘  Using multiple modifiers in a single query. SELECT * REPLACE(i + 1 AS i) EXCEPT (j) APPLY(sum) from columns_transformers;  ┌─sum(plus(i, 1))─┬─sum(k)─┐ │ 222 │ 347 │ └─────────────────┴────────┘  "},{"title":"SETTINGS in SELECT Query​","type":1,"pageTitle":"SELECT Query","url":"en/sql-reference/statements/select/#settings-in-select","content":"You can specify the necessary settings right in the SELECT query. The setting value is applied only to this query and is reset to default or previous value after the query is executed. Other ways to make settings see here. Example SELECT * FROM some_table SETTINGS optimize_read_in_order=1, cast_keep_nullable=1;  Original article "},{"title":"LIMIT Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/limit","content":"","keywords":""},{"title":"LIMIT … WITH TIES Modifier​","type":1,"pageTitle":"LIMIT Clause","url":"en/sql-reference/statements/select/limit#limit-with-ties","content":"When you set WITH TIES modifier for LIMIT n[,m] and specify ORDER BY expr_list, you will get in result first n or n,m rows and all rows with same ORDER BY fields values equal to row at position n for LIMIT n and m for LIMIT n,m. This modifier also can be combined with ORDER BY … WITH FILL modifier. For example, the following query SELECT * FROM ( SELECT number%50 AS n FROM numbers(100) ) ORDER BY n LIMIT 0,5  returns ┌─n─┐ │ 0 │ │ 0 │ │ 1 │ │ 1 │ │ 2 │ └───┘  but after apply WITH TIES modifier SELECT * FROM ( SELECT number%50 AS n FROM numbers(100) ) ORDER BY n LIMIT 0,5 WITH TIES  it returns another rows set ┌─n─┐ │ 0 │ │ 0 │ │ 1 │ │ 1 │ │ 2 │ │ 2 │ └───┘  cause row number 6 have same value “2” for field n as row number 5 "},{"title":"INTERSECT Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/intersect","content":"INTERSECT Clause The INTERSECT clause returns only those rows that result from both the first and the second queries. The queries must match the number of columns, order, and type. The result of INTERSECT can contain duplicate rows. Multiple INTERSECT statements are executes left to right if parenthesis are not specified. The INTERSECT operator has a higher priority than the UNION and EXCEPT clause. SELECT column1 [, column2 ] FROM table1 [WHERE condition] INTERSECT SELECT column1 [, column2 ] FROM table2 [WHERE condition] The condition could be any expression based on your requirements. Examples Query: SELECT number FROM numbers(1,10) INTERSECT SELECT number FROM numbers(3,6); Result: ┌─number─┐ │ 3 │ │ 4 │ │ 5 │ │ 6 │ │ 7 │ │ 8 │ └────────┘ Query: CREATE TABLE t1(one String, two String, three String) ENGINE=Memory(); CREATE TABLE t2(four String, five String, six String) ENGINE=Memory(); INSERT INTO t1 VALUES ('q', 'm', 'b'), ('s', 'd', 'f'), ('l', 'p', 'o'), ('s', 'd', 'f'), ('s', 'd', 'f'), ('k', 't', 'd'), ('l', 'p', 'o'); INSERT INTO t2 VALUES ('q', 'm', 'b'), ('b', 'd', 'k'), ('s', 'y', 't'), ('s', 'd', 'f'), ('m', 'f', 'o'), ('k', 'k', 'd'); SELECT * FROM t1 INTERSECT SELECT * FROM t2; Result: ┌─one─┬─two─┬─three─┐ │ q │ m │ b │ │ s │ d │ f │ │ s │ d │ f │ │ s │ d │ f │ └─────┴─────┴───────┘ See Also UNIONEXCEPT","keywords":""},{"title":"LIMIT BY Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/limit-by","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"LIMIT BY Clause","url":"en/sql-reference/statements/select/limit-by#examples","content":"Sample table: CREATE TABLE limit_by(id Int, val Int) ENGINE = Memory; INSERT INTO limit_by VALUES (1, 10), (1, 11), (1, 12), (2, 20), (2, 21);  Queries: SELECT * FROM limit_by ORDER BY id, val LIMIT 2 BY id  ┌─id─┬─val─┐ │ 1 │ 10 │ │ 1 │ 11 │ │ 2 │ 20 │ │ 2 │ 21 │ └────┴─────┘  SELECT * FROM limit_by ORDER BY id, val LIMIT 1, 2 BY id  ┌─id─┬─val─┐ │ 1 │ 11 │ │ 1 │ 12 │ │ 2 │ 21 │ └────┴─────┘  The SELECT * FROM limit_by ORDER BY id, val LIMIT 2 OFFSET 1 BY id query returns the same result. The following query returns the top 5 referrers for each domain, device_type pair with a maximum of 100 rows in total (LIMIT n BY + LIMIT). SELECT domainWithoutWWW(URL) AS domain, domainWithoutWWW(REFERRER_URL) AS referrer, device_type, count() cnt FROM hits GROUP BY domain, referrer, device_type ORDER BY cnt DESC LIMIT 5 BY domain, device_type LIMIT 100  "},{"title":"EXPLAIN Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/explain","content":"","keywords":""},{"title":"EXPLAIN Types​","type":1,"pageTitle":"EXPLAIN Statement","url":"en/sql-reference/statements/explain#explain-types","content":"AST — Abstract syntax tree.SYNTAX — Query text after AST-level optimizations.PLAN — Query execution plan.PIPELINE — Query execution pipeline. "},{"title":"EXPLAIN AST​","type":1,"pageTitle":"EXPLAIN Statement","url":"en/sql-reference/statements/explain#explain-ast","content":"Dump query AST. Supports all types of queries, not only SELECT. Examples: EXPLAIN AST SELECT 1;  SelectWithUnionQuery (children 1) ExpressionList (children 1) SelectQuery (children 1) ExpressionList (children 1) Literal UInt64_1  EXPLAIN AST ALTER TABLE t1 DELETE WHERE date = today();   explain AlterQuery t1 (children 1) ExpressionList (children 1) AlterCommand 27 (children 1) Function equals (children 1) ExpressionList (children 2) Identifier date Function today (children 1) ExpressionList  "},{"title":"EXPLAIN SYNTAX​","type":1,"pageTitle":"EXPLAIN Statement","url":"en/sql-reference/statements/explain#explain-syntax","content":"Returns query after syntax optimizations. Example: EXPLAIN SYNTAX SELECT * FROM system.numbers AS a, system.numbers AS b, system.numbers AS c;  SELECT `--a.number` AS `a.number`, `--b.number` AS `b.number`, number AS `c.number` FROM ( SELECT number AS `--a.number`, b.number AS `--b.number` FROM system.numbers AS a CROSS JOIN system.numbers AS b ) AS `--.s` CROSS JOIN system.numbers AS c  "},{"title":"EXPLAIN PLAN​","type":1,"pageTitle":"EXPLAIN Statement","url":"en/sql-reference/statements/explain#explain-plan","content":"Dump query plan steps. Settings: header — Prints output header for step. Default: 0.description — Prints step description. Default: 1.indexes — Shows used indexes, the number of filtered parts and the number of filtered granules for every index applied. Default: 0. Supported for MergeTree tables.actions — Prints detailed information about step actions. Default: 0.json — Prints query plan steps as a row in JSON format. Default: 0. It is recommended to use TSVRaw format to avoid unnecessary escaping. Example: EXPLAIN SELECT sum(number) FROM numbers(10) GROUP BY number % 4;  Union Expression (Projection) Expression (Before ORDER BY and SELECT) Aggregating Expression (Before GROUP BY) SettingQuotaAndLimits (Set limits and quota after reading from storage) ReadFromStorage (SystemNumbers)  note Step and query cost estimation is not supported. When json = 1, the query plan is represented in JSON format. Every node is a dictionary that always has the keys Node Type and Plans. Node Type is a string with a step name. Plans is an array with child step descriptions. Other optional keys may be added depending on node type and settings. Example: EXPLAIN json = 1, description = 0 SELECT 1 UNION ALL SELECT 2 FORMAT TSVRaw;  [ { &quot;Plan&quot;: { &quot;Node Type&quot;: &quot;Union&quot;, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;Expression&quot;, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;SettingQuotaAndLimits&quot;, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;ReadFromStorage&quot; } ] } ] }, { &quot;Node Type&quot;: &quot;Expression&quot;, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;SettingQuotaAndLimits&quot;, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;ReadFromStorage&quot; } ] } ] } ] } } ]  With description = 1, the Description key is added to the step: { &quot;Node Type&quot;: &quot;ReadFromStorage&quot;, &quot;Description&quot;: &quot;SystemOne&quot; }  With header = 1, the Header key is added to the step as an array of columns. Example: EXPLAIN json = 1, description = 0, header = 1 SELECT 1, 2 + dummy;  [ { &quot;Plan&quot;: { &quot;Node Type&quot;: &quot;Expression&quot;, &quot;Header&quot;: [ { &quot;Name&quot;: &quot;1&quot;, &quot;Type&quot;: &quot;UInt8&quot; }, { &quot;Name&quot;: &quot;plus(2, dummy)&quot;, &quot;Type&quot;: &quot;UInt16&quot; } ], &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;SettingQuotaAndLimits&quot;, &quot;Header&quot;: [ { &quot;Name&quot;: &quot;dummy&quot;, &quot;Type&quot;: &quot;UInt8&quot; } ], &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;ReadFromStorage&quot;, &quot;Header&quot;: [ { &quot;Name&quot;: &quot;dummy&quot;, &quot;Type&quot;: &quot;UInt8&quot; } ] } ] } ] } } ]  With indexes = 1, the Indexes key is added. It contains an array of used indexes. Each index is described as JSON with Type key (a string MinMax, Partition, PrimaryKey or Skip) and optional keys: Name — An index name (for now, is used only for Skip index).Keys — An array of columns used by the index.Condition — A string with condition used.Description — An index (for now, is used only for Skip index).Initial Parts — A number of parts before the index is applied.Selected Parts — A number of parts after the index is applied.Initial Granules — A number of granules before the index is applied.Selected Granulesis — A number of granules after the index is applied. Example: &quot;Node Type&quot;: &quot;ReadFromMergeTree&quot;, &quot;Indexes&quot;: [ { &quot;Type&quot;: &quot;MinMax&quot;, &quot;Keys&quot;: [&quot;y&quot;], &quot;Condition&quot;: &quot;(y in [1, +inf))&quot;, &quot;Initial Parts&quot;: 5, &quot;Selected Parts&quot;: 4, &quot;Initial Granules&quot;: 12, &quot;Selected Granules&quot;: 11 }, { &quot;Type&quot;: &quot;Partition&quot;, &quot;Keys&quot;: [&quot;y&quot;, &quot;bitAnd(z, 3)&quot;], &quot;Condition&quot;: &quot;and((bitAnd(z, 3) not in [1, 1]), and((y in [1, +inf)), (bitAnd(z, 3) not in [1, 1])))&quot;, &quot;Initial Parts&quot;: 4, &quot;Selected Parts&quot;: 3, &quot;Initial Granules&quot;: 11, &quot;Selected Granules&quot;: 10 }, { &quot;Type&quot;: &quot;PrimaryKey&quot;, &quot;Keys&quot;: [&quot;x&quot;, &quot;y&quot;], &quot;Condition&quot;: &quot;and((x in [11, +inf)), (y in [1, +inf)))&quot;, &quot;Initial Parts&quot;: 3, &quot;Selected Parts&quot;: 2, &quot;Initial Granules&quot;: 10, &quot;Selected Granules&quot;: 6 }, { &quot;Type&quot;: &quot;Skip&quot;, &quot;Name&quot;: &quot;t_minmax&quot;, &quot;Description&quot;: &quot;minmax GRANULARITY 2&quot;, &quot;Initial Parts&quot;: 2, &quot;Selected Parts&quot;: 1, &quot;Initial Granules&quot;: 6, &quot;Selected Granules&quot;: 2 }, { &quot;Type&quot;: &quot;Skip&quot;, &quot;Name&quot;: &quot;t_set&quot;, &quot;Description&quot;: &quot;set GRANULARITY 2&quot;, &quot;Initial Parts&quot;: 1, &quot;Selected Parts&quot;: 1, &quot;Initial Granules&quot;: 2, &quot;Selected Granules&quot;: 1 } ]  With actions = 1, added keys depend on step type. Example: EXPLAIN json = 1, actions = 1, description = 0 SELECT 1 FORMAT TSVRaw;  [ { &quot;Plan&quot;: { &quot;Node Type&quot;: &quot;Expression&quot;, &quot;Expression&quot;: { &quot;Inputs&quot;: [], &quot;Actions&quot;: [ { &quot;Node Type&quot;: &quot;Column&quot;, &quot;Result Type&quot;: &quot;UInt8&quot;, &quot;Result Type&quot;: &quot;Column&quot;, &quot;Column&quot;: &quot;Const(UInt8)&quot;, &quot;Arguments&quot;: [], &quot;Removed Arguments&quot;: [], &quot;Result&quot;: 0 } ], &quot;Outputs&quot;: [ { &quot;Name&quot;: &quot;1&quot;, &quot;Type&quot;: &quot;UInt8&quot; } ], &quot;Positions&quot;: [0], &quot;Project Input&quot;: true }, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;SettingQuotaAndLimits&quot;, &quot;Plans&quot;: [ { &quot;Node Type&quot;: &quot;ReadFromStorage&quot; } ] } ] } } ]  "},{"title":"EXPLAIN PIPELINE​","type":1,"pageTitle":"EXPLAIN Statement","url":"en/sql-reference/statements/explain#explain-pipeline","content":"Settings: header — Prints header for each output port. Default: 0.graph — Prints a graph described in the DOT graph description language. Default: 0.compact — Prints graph in compact mode if graph setting is enabled. Default: 1. Example: EXPLAIN PIPELINE SELECT sum(number) FROM numbers_mt(100000) GROUP BY number % 4;  (Union) (Expression) ExpressionTransform (Expression) ExpressionTransform (Aggregating) Resize 2 → 1 AggregatingTransform × 2 (Expression) ExpressionTransform × 2 (SettingQuotaAndLimits) (ReadFromStorage) NumbersMt × 2 0 → 1  "},{"title":"EXPLAIN ESTIMATE​","type":1,"pageTitle":"EXPLAIN Statement","url":"en/sql-reference/statements/explain#explain-estimate","content":"Shows the estimated number of rows, marks and parts to be read from the tables while processing the query. Works with tables in the MergeTree family. Example Creating a table: CREATE TABLE ttt (i Int64) ENGINE = MergeTree() ORDER BY i SETTINGS index_granularity = 16, write_final_mark = 0; INSERT INTO ttt SELECT number FROM numbers(128); OPTIMIZE TABLE ttt;  Query: EXPLAIN ESTIMATE SELECT * FROM ttt;  Result: ┌─database─┬─table─┬─parts─┬─rows─┬─marks─┐ │ default │ ttt │ 1 │ 128 │ 8 │ └──────────┴───────┴───────┴──────┴───────┘  "},{"title":"EXPLAIN TABLE OVERRIDE​","type":1,"pageTitle":"EXPLAIN Statement","url":"en/sql-reference/statements/explain#explain-table-override","content":"Shows the result of a table override on a table schema accessed through a table function. Also does some validation, throwing an exception if the override would have caused some kind of failure. Example Assume you have a remote MySQL table like this: CREATE TABLE db.tbl ( id INT PRIMARY KEY, created DATETIME DEFAULT now() )  EXPLAIN TABLE OVERRIDE mysql('127.0.0.1:3306', 'db', 'tbl', 'root', 'clickhouse') PARTITION BY toYYYYMM(assumeNotNull(created))  Result: ┌─explain─────────────────────────────────────────────────┐ │ PARTITION BY uses columns: `created` Nullable(DateTime) │ └─────────────────────────────────────────────────────────┘  note The validation is not complete, so a successfull query does not guarantee that the override would not cause issues. Оriginal article "},{"title":"GRANT Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/grant","content":"","keywords":""},{"title":"Granting Privilege Syntax​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-privigele-syntax","content":"GRANT [ON CLUSTER cluster_name] privilege[(column_name [,...])] [,...] ON {db.table|db.*|*.*|table|*} TO {user | role | CURRENT_USER} [,...] [WITH GRANT OPTION] [WITH REPLACE OPTION]  privilege — Type of privilege.role — ClickHouse user role.user — ClickHouse user account. The WITH GRANT OPTION clause grants user or role with permission to execute the GRANT query. Users can grant privileges of the same scope they have and less. The WITH REPLACE OPTION clause replace old privileges by new privileges for the user or role, if is not specified it appends privileges. "},{"title":"Assigning Role Syntax​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#assign-role-syntax","content":"GRANT [ON CLUSTER cluster_name] role [,...] TO {user | another_role | CURRENT_USER} [,...] [WITH ADMIN OPTION] [WITH REPLACE OPTION]  role — ClickHouse user role.user — ClickHouse user account. The WITH ADMIN OPTION clause grants ADMIN OPTION privilege to user or role. The WITH REPLACE OPTION clause replace old roles by new role for the user or role, if is not specified it appends roles. "},{"title":"Usage​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-usage","content":"To use GRANT, your account must have the GRANT OPTION privilege. You can grant privileges only inside the scope of your account privileges. For example, administrator has granted privileges to the john account by the query: GRANT SELECT(x,y) ON db.table TO john WITH GRANT OPTION  It means that john has the permission to execute: SELECT x,y FROM db.table.SELECT x FROM db.table.SELECT y FROM db.table. john can’t execute SELECT z FROM db.table. The SELECT * FROM db.table also is not available. Processing this query, ClickHouse does not return any data, even x and y. The only exception is if a table contains only x and y columns. In this case ClickHouse returns all the data. Also john has the GRANT OPTION privilege, so it can grant other users with privileges of the same or smaller scope. Specifying privileges you can use asterisk (*) instead of a table or a database name. For example, the GRANT SELECT ON db.* TO john query allows john to execute the SELECT query over all the tables in db database. Also, you can omit database name. In this case privileges are granted for current database. For example, GRANT SELECT ON * TO john grants the privilege on all the tables in the current database, GRANT SELECT ON mytable TO john grants the privilege on the mytable table in the current database. Access to the system database is always allowed (since this database is used for processing queries). You can grant multiple privileges to multiple accounts in one query. The query GRANT SELECT, INSERT ON *.* TO john, robin allows accounts john and robin to execute the INSERT and SELECT queries over all the tables in all the databases on the server. "},{"title":"Privileges​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-privileges","content":"Privilege is a permission to execute specific kind of queries. Privileges have a hierarchical structure. A set of permitted queries depends on the privilege scope. Hierarchy of privileges: SELECTINSERTALTER ALTER TABLE ALTER UPDATEALTER DELETEALTER COLUMN ALTER ADD COLUMNALTER DROP COLUMNALTER MODIFY COLUMNALTER COMMENT COLUMNALTER CLEAR COLUMNALTER RENAME COLUMN ALTER INDEX ALTER ORDER BYALTER SAMPLE BYALTER ADD INDEXALTER DROP INDEXALTER MATERIALIZE INDEXALTER CLEAR INDEX ALTER CONSTRAINT ALTER ADD CONSTRAINTALTER DROP CONSTRAINT ALTER TTL ALTER MATERIALIZE TTL ALTER SETTINGSALTER MOVE PARTITIONALTER FETCH PARTITIONALTER FREEZE PARTITION ALTER VIEW ALTER VIEW REFRESHALTER VIEW MODIFY QUERY CREATE CREATE DATABASECREATE TABLE CREATE TEMPORARY TABLE CREATE VIEWCREATE DICTIONARYCREATE FUNCTION DROP DROP DATABASEDROP TABLEDROP VIEWDROP DICTIONARYDROP FUNCTION TRUNCATEOPTIMIZESHOW SHOW DATABASESSHOW TABLESSHOW COLUMNSSHOW DICTIONARIES KILL QUERYACCESS MANAGEMENT CREATE USERALTER USERDROP USERCREATE ROLEALTER ROLEDROP ROLECREATE ROW POLICYALTER ROW POLICYDROP ROW POLICYCREATE QUOTAALTER QUOTADROP QUOTACREATE SETTINGS PROFILEALTER SETTINGS PROFILEDROP SETTINGS PROFILESHOW ACCESS SHOW_USERSSHOW_ROLESSHOW_ROW_POLICIESSHOW_QUOTASSHOW_SETTINGS_PROFILES ROLE ADMIN SYSTEM SYSTEM SHUTDOWNSYSTEM DROP CACHE SYSTEM DROP DNS CACHESYSTEM DROP MARK CACHESYSTEM DROP UNCOMPRESSED CACHE SYSTEM RELOAD SYSTEM RELOAD CONFIGSYSTEM RELOAD DICTIONARY SYSTEM RELOAD EMBEDDED DICTIONARIES SYSTEM RELOAD FUNCTIONSYSTEM RELOAD FUNCTIONS SYSTEM MERGESSYSTEM TTL MERGESSYSTEM FETCHESSYSTEM MOVESSYSTEM SENDS SYSTEM DISTRIBUTED SENDSSYSTEM REPLICATED SENDS SYSTEM REPLICATION QUEUESSYSTEM SYNC REPLICASYSTEM RESTART REPLICASYSTEM FLUSH SYSTEM FLUSH DISTRIBUTEDSYSTEM FLUSH LOGS INTROSPECTION addressToLineaddressToLineWithInlinesaddressToSymboldemangle SOURCES FILEURLREMOTEYSQLODBCJDBCHDFSS3 dictGet Examples of how this hierarchy is treated: The ALTER privilege includes all other ALTER* privileges.ALTER CONSTRAINT includes ALTER ADD CONSTRAINT and ALTER DROP CONSTRAINT privileges. Privileges are applied at different levels. Knowing of a level suggests syntax available for privilege. Levels (from lower to higher): COLUMN — Privilege can be granted for column, table, database, or globally.TABLE — Privilege can be granted for table, database, or globally.VIEW — Privilege can be granted for view, database, or globally.DICTIONARY — Privilege can be granted for dictionary, database, or globally.DATABASE — Privilege can be granted for database or globally.GLOBAL — Privilege can be granted only globally.GROUP — Groups privileges of different levels. When GROUP-level privilege is granted, only that privileges from the group are granted which correspond to the used syntax. Examples of allowed syntax: GRANT SELECT(x) ON db.table TO userGRANT SELECT ON db.* TO user Examples of disallowed syntax: GRANT CREATE USER(x) ON db.table TO userGRANT CREATE USER ON db.* TO user The special privilege ALL grants all the privileges to a user account or a role. By default, a user account or a role has no privileges. If a user or a role has no privileges, it is displayed as NONE privilege. Some queries by their implementation require a set of privileges. For example, to execute the RENAME query you need the following privileges: SELECT, CREATE TABLE, INSERT and DROP TABLE. "},{"title":"SELECT​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-select","content":"Allows executing SELECT queries. Privilege level: COLUMN. Description User granted with this privilege can execute SELECT queries over a specified list of columns in the specified table and database. If user includes other columns then specified a query returns no data. Consider the following privilege: GRANT SELECT(x,y) ON db.table TO john  This privilege allows john to execute any SELECT query that involves data from the x and/or y columns in db.table, for example, SELECT x FROM db.table. john can’t execute SELECT z FROM db.table. The SELECT * FROM db.table also is not available. Processing this query, ClickHouse does not return any data, even x and y. The only exception is if a table contains only x and y columns, in this case ClickHouse returns all the data. "},{"title":"INSERT​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-insert","content":"Allows executing INSERT queries. Privilege level: COLUMN. Description User granted with this privilege can execute INSERT queries over a specified list of columns in the specified table and database. If user includes other columns then specified a query does not insert any data. Example GRANT INSERT(x,y) ON db.table TO john  The granted privilege allows john to insert data to the x and/or y columns in db.table. "},{"title":"ALTER​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-alter","content":"Allows executing ALTER queries according to the following hierarchy of privileges: ALTER. Level: COLUMN. ALTER TABLE. Level: GROUP ALTER UPDATE. Level: COLUMN. Aliases: UPDATEALTER DELETE. Level: COLUMN. Aliases: DELETEALTER COLUMN. Level: GROUP ALTER ADD COLUMN. Level: COLUMN. Aliases: ADD COLUMNALTER DROP COLUMN. Level: COLUMN. Aliases: DROP COLUMNALTER MODIFY COLUMN. Level: COLUMN. Aliases: MODIFY COLUMNALTER COMMENT COLUMN. Level: COLUMN. Aliases: COMMENT COLUMNALTER CLEAR COLUMN. Level: COLUMN. Aliases: CLEAR COLUMNALTER RENAME COLUMN. Level: COLUMN. Aliases: RENAME COLUMN ALTER INDEX. Level: GROUP. Aliases: INDEX ALTER ORDER BY. Level: TABLE. Aliases: ALTER MODIFY ORDER BY, MODIFY ORDER BYALTER SAMPLE BY. Level: TABLE. Aliases: ALTER MODIFY SAMPLE BY, MODIFY SAMPLE BYALTER ADD INDEX. Level: TABLE. Aliases: ADD INDEXALTER DROP INDEX. Level: TABLE. Aliases: DROP INDEXALTER MATERIALIZE INDEX. Level: TABLE. Aliases: MATERIALIZE INDEXALTER CLEAR INDEX. Level: TABLE. Aliases: CLEAR INDEX ALTER CONSTRAINT. Level: GROUP. Aliases: CONSTRAINT ALTER ADD CONSTRAINT. Level: TABLE. Aliases: ADD CONSTRAINTALTER DROP CONSTRAINT. Level: TABLE. Aliases: DROP CONSTRAINT ALTER TTL. Level: TABLE. Aliases: ALTER MODIFY TTL, MODIFY TTL ALTER MATERIALIZE TTL. Level: TABLE. Aliases: MATERIALIZE TTL ALTER SETTINGS. Level: TABLE. Aliases: ALTER SETTING, ALTER MODIFY SETTING, MODIFY SETTINGALTER MOVE PARTITION. Level: TABLE. Aliases: ALTER MOVE PART, MOVE PARTITION, MOVE PARTALTER FETCH PARTITION. Level: TABLE. Aliases: ALTER FETCH PART, FETCH PARTITION, FETCH PARTALTER FREEZE PARTITION. Level: TABLE. Aliases: FREEZE PARTITION ALTER VIEW Level: GROUP ALTER VIEW REFRESH. Level: VIEW. Aliases: ALTER LIVE VIEW REFRESH, REFRESH VIEWALTER VIEW MODIFY QUERY. Level: VIEW. Aliases: ALTER TABLE MODIFY QUERY Examples of how this hierarchy is treated: The ALTER privilege includes all other ALTER* privileges.ALTER CONSTRAINT includes ALTER ADD CONSTRAINT and ALTER DROP CONSTRAINT privileges. Notes The MODIFY SETTING privilege allows modifying table engine settings. It does not affect settings or server configuration parameters.The ATTACH operation needs the CREATE privilege.The DETACH operation needs the DROP privilege.To stop mutation by the KILL MUTATION query, you need to have a privilege to start this mutation. For example, if you want to stop the ALTER UPDATE query, you need the ALTER UPDATE, ALTER TABLE, or ALTER privilege. "},{"title":"CREATE​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-create","content":"Allows executing CREATE and ATTACH DDL-queries according to the following hierarchy of privileges: CREATE. Level: GROUP CREATE DATABASE. Level: DATABASECREATE TABLE. Level: TABLE CREATE TEMPORARY TABLE. Level: GLOBAL CREATE VIEW. Level: VIEWCREATE DICTIONARY. Level: DICTIONARY Notes To delete the created table, a user needs DROP. "},{"title":"DROP​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-drop","content":"Allows executing DROP and DETACH queries according to the following hierarchy of privileges: DROP. Level: GROUP DROP DATABASE. Level: DATABASEDROP TABLE. Level: TABLEDROP VIEW. Level: VIEWDROP DICTIONARY. Level: DICTIONARY "},{"title":"TRUNCATE​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-truncate","content":"Allows executing TRUNCATE queries. Privilege level: TABLE. "},{"title":"OPTIMIZE​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-optimize","content":"Allows executing OPTIMIZE TABLE queries. Privilege level: TABLE. "},{"title":"SHOW​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-show","content":"Allows executing SHOW, DESCRIBE, USE, and EXISTS queries according to the following hierarchy of privileges: SHOW. Level: GROUP SHOW DATABASES. Level: DATABASE. Allows to execute SHOW DATABASES, SHOW CREATE DATABASE, USE &lt;database&gt; queries.SHOW TABLES. Level: TABLE. Allows to execute SHOW TABLES, EXISTS &lt;table&gt;, CHECK &lt;table&gt; queries.SHOW COLUMNS. Level: COLUMN. Allows to execute SHOW CREATE TABLE, DESCRIBE queries.SHOW DICTIONARIES. Level: DICTIONARY. Allows to execute SHOW DICTIONARIES, SHOW CREATE DICTIONARY, EXISTS &lt;dictionary&gt; queries. Notes A user has the SHOW privilege if it has any other privilege concerning the specified table, dictionary or database. "},{"title":"KILL QUERY​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-kill-query","content":"Allows executing KILL queries according to the following hierarchy of privileges: Privilege level: GLOBAL. Notes KILL QUERY privilege allows one user to kill queries of other users. "},{"title":"ACCESS MANAGEMENT​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-access-management","content":"Allows a user to execute queries that manage users, roles and row policies. ACCESS MANAGEMENT. Level: GROUP CREATE USER. Level: GLOBALALTER USER. Level: GLOBALDROP USER. Level: GLOBALCREATE ROLE. Level: GLOBALALTER ROLE. Level: GLOBALDROP ROLE. Level: GLOBALROLE ADMIN. Level: GLOBALCREATE ROW POLICY. Level: GLOBAL. Aliases: CREATE POLICYALTER ROW POLICY. Level: GLOBAL. Aliases: ALTER POLICYDROP ROW POLICY. Level: GLOBAL. Aliases: DROP POLICYCREATE QUOTA. Level: GLOBALALTER QUOTA. Level: GLOBALDROP QUOTA. Level: GLOBALCREATE SETTINGS PROFILE. Level: GLOBAL. Aliases: CREATE PROFILEALTER SETTINGS PROFILE. Level: GLOBAL. Aliases: ALTER PROFILEDROP SETTINGS PROFILE. Level: GLOBAL. Aliases: DROP PROFILESHOW ACCESS. Level: GROUP SHOW_USERS. Level: GLOBAL. Aliases: SHOW CREATE USERSHOW_ROLES. Level: GLOBAL. Aliases: SHOW CREATE ROLESHOW_ROW_POLICIES. Level: GLOBAL. Aliases: SHOW POLICIES, SHOW CREATE ROW POLICY, SHOW CREATE POLICYSHOW_QUOTAS. Level: GLOBAL. Aliases: SHOW CREATE QUOTASHOW_SETTINGS_PROFILES. Level: GLOBAL. Aliases: SHOW PROFILES, SHOW CREATE SETTINGS PROFILE, SHOW CREATE PROFILE The ROLE ADMIN privilege allows a user to assign and revoke any roles including those which are not assigned to the user with the admin option. "},{"title":"SYSTEM​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-system","content":"Allows a user to execute SYSTEM queries according to the following hierarchy of privileges. SYSTEM. Level: GROUP SYSTEM SHUTDOWN. Level: GLOBAL. Aliases: SYSTEM KILL, SHUTDOWNSYSTEM DROP CACHE. Aliases: DROP CACHE SYSTEM DROP DNS CACHE. Level: GLOBAL. Aliases: SYSTEM DROP DNS, DROP DNS CACHE, DROP DNSSYSTEM DROP MARK CACHE. Level: GLOBAL. Aliases: SYSTEM DROP MARK, DROP MARK CACHE, DROP MARKSSYSTEM DROP UNCOMPRESSED CACHE. Level: GLOBAL. Aliases: SYSTEM DROP UNCOMPRESSED, DROP UNCOMPRESSED CACHE, DROP UNCOMPRESSED SYSTEM RELOAD. Level: GROUP SYSTEM RELOAD CONFIG. Level: GLOBAL. Aliases: RELOAD CONFIGSYSTEM RELOAD DICTIONARY. Level: GLOBAL. Aliases: SYSTEM RELOAD DICTIONARIES, RELOAD DICTIONARY, RELOAD DICTIONARIES SYSTEM RELOAD EMBEDDED DICTIONARIES. Level: GLOBAL. Aliases: RELOAD EMBEDDED DICTIONARIES SYSTEM MERGES. Level: TABLE. Aliases: SYSTEM STOP MERGES, SYSTEM START MERGES, STOP MERGES, START MERGESSYSTEM TTL MERGES. Level: TABLE. Aliases: SYSTEM STOP TTL MERGES, SYSTEM START TTL MERGES, STOP TTL MERGES, START TTL MERGESSYSTEM FETCHES. Level: TABLE. Aliases: SYSTEM STOP FETCHES, SYSTEM START FETCHES, STOP FETCHES, START FETCHESSYSTEM MOVES. Level: TABLE. Aliases: SYSTEM STOP MOVES, SYSTEM START MOVES, STOP MOVES, START MOVESSYSTEM SENDS. Level: GROUP. Aliases: SYSTEM STOP SENDS, SYSTEM START SENDS, STOP SENDS, START SENDS SYSTEM DISTRIBUTED SENDS. Level: TABLE. Aliases: SYSTEM STOP DISTRIBUTED SENDS, SYSTEM START DISTRIBUTED SENDS, STOP DISTRIBUTED SENDS, START DISTRIBUTED SENDSSYSTEM REPLICATED SENDS. Level: TABLE. Aliases: SYSTEM STOP REPLICATED SENDS, SYSTEM START REPLICATED SENDS, STOP REPLICATED SENDS, START REPLICATED SENDS SYSTEM REPLICATION QUEUES. Level: TABLE. Aliases: SYSTEM STOP REPLICATION QUEUES, SYSTEM START REPLICATION QUEUES, STOP REPLICATION QUEUES, START REPLICATION QUEUESSYSTEM SYNC REPLICA. Level: TABLE. Aliases: SYNC REPLICASYSTEM RESTART REPLICA. Level: TABLE. Aliases: RESTART REPLICASYSTEM FLUSH. Level: GROUP SYSTEM FLUSH DISTRIBUTED. Level: TABLE. Aliases: FLUSH DISTRIBUTEDSYSTEM FLUSH LOGS. Level: GLOBAL. Aliases: FLUSH LOGS The SYSTEM RELOAD EMBEDDED DICTIONARIES privilege implicitly granted by the SYSTEM RELOAD DICTIONARY ON *.* privilege. "},{"title":"INTROSPECTION​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-introspection","content":"Allows using introspection functions. INTROSPECTION. Level: GROUP. Aliases: INTROSPECTION FUNCTIONS addressToLine. Level: GLOBALaddressToLineWithInlines. Level: GLOBALaddressToSymbol. Level: GLOBALdemangle. Level: GLOBAL "},{"title":"SOURCES​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-sources","content":"Allows using external data sources. Applies to table engines and table functions. SOURCES. Level: GROUP FILE. Level: GLOBALURL. Level: GLOBALREMOTE. Level: GLOBALYSQL. Level: GLOBALODBC. Level: GLOBALJDBC. Level: GLOBALHDFS. Level: GLOBALS3. Level: GLOBAL The SOURCES privilege enables use of all the sources. Also you can grant a privilege for each source individually. To use sources, you need additional privileges. Examples: To create a table with the MySQL table engine, you need CREATE TABLE (ON db.table_name) and MYSQL privileges.To use the mysql table function, you need CREATE TEMPORARY TABLE and MYSQL privileges. "},{"title":"dictGet​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-dictget","content":"dictGet. Aliases: dictHas, dictGetHierarchy, dictIsIn Allows a user to execute dictGet, dictHas, dictGetHierarchy, dictIsIn functions. Privilege level: DICTIONARY. Examples GRANT dictGet ON mydb.mydictionary TO johnGRANT dictGet ON mydictionary TO john "},{"title":"ALL​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-all","content":"Grants all the privileges on regulated entity to a user account or a role. "},{"title":"NONE​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#grant-none","content":"Doesn’t grant any privileges. "},{"title":"ADMIN OPTION​","type":1,"pageTitle":"GRANT Statement","url":"en/sql-reference/statements/grant#admin-option-privilege","content":"The ADMIN OPTION privilege allows a user to grant their role to another user. "},{"title":"OFFSET FETCH Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/offset","content":"","keywords":""},{"title":"Examples​","type":1,"pageTitle":"OFFSET FETCH Clause","url":"en/sql-reference/statements/select/offset#examples","content":"Input table: ┌─a─┬─b─┐ │ 1 │ 1 │ │ 2 │ 1 │ │ 3 │ 4 │ │ 1 │ 3 │ │ 5 │ 4 │ │ 0 │ 6 │ │ 5 │ 7 │ └───┴───┘  Usage of the ONLY option: SELECT * FROM test_fetch ORDER BY a OFFSET 3 ROW FETCH FIRST 3 ROWS ONLY;  Result: ┌─a─┬─b─┐ │ 2 │ 1 │ │ 3 │ 4 │ │ 5 │ 4 │ └───┴───┘  Usage of the WITH TIES option: SELECT * FROM test_fetch ORDER BY a OFFSET 3 ROW FETCH FIRST 3 ROWS WITH TIES;  Result: ┌─a─┬─b─┐ │ 2 │ 1 │ │ 3 │ 4 │ │ 5 │ 4 │ │ 5 │ 7 │ └───┴───┘  "},{"title":"GROUP BY Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/group-by","content":"","keywords":""},{"title":"NULL Processing​","type":1,"pageTitle":"GROUP BY Clause","url":"en/sql-reference/statements/select/group-by#null-processing","content":"For grouping, ClickHouse interprets NULL as a value, and NULL==NULL. It differs from NULL processing in most other contexts. Here’s an example to show what this means. Assume you have this table: ┌─x─┬────y─┐ │ 1 │ 2 │ │ 2 │ ᴺᵁᴸᴸ │ │ 3 │ 2 │ │ 3 │ 3 │ │ 3 │ ᴺᵁᴸᴸ │ └───┴──────┘  The query SELECT sum(x), y FROM t_null_big GROUP BY y results in: ┌─sum(x)─┬────y─┐ │ 4 │ 2 │ │ 3 │ 3 │ │ 5 │ ᴺᵁᴸᴸ │ └────────┴──────┘  You can see that GROUP BY for y = NULL summed up x, as if NULL is this value. If you pass several keys to GROUP BY, the result will give you all the combinations of the selection, as if NULL were a specific value. "},{"title":"WITH ROLLUP Modifier​","type":1,"pageTitle":"GROUP BY Clause","url":"en/sql-reference/statements/select/group-by#with-rollup-modifier","content":"WITH ROLLUP modifier is used to calculate subtotals for the key expressions, based on their order in the GROUP BY list. The subtotals rows are added after the result table. The subtotals are calculated in the reverse order: at first subtotals are calculated for the last key expression in the list, then for the previous one, and so on up to the first key expression. In the subtotals rows the values of already &quot;grouped&quot; key expressions are set to 0 or empty line. note Mind that HAVING clause can affect the subtotals results. Example Consider the table t: ┌─year─┬─month─┬─day─┐ │ 2019 │ 1 │ 5 │ │ 2019 │ 1 │ 15 │ │ 2020 │ 1 │ 5 │ │ 2020 │ 1 │ 15 │ │ 2020 │ 10 │ 5 │ │ 2020 │ 10 │ 15 │ └──────┴───────┴─────┘  Query: SELECT year, month, day, count(*) FROM t GROUP BY year, month, day WITH ROLLUP;  As GROUP BY section has three key expressions, the result contains four tables with subtotals &quot;rolled up&quot; from right to left: GROUP BY year, month, day;GROUP BY year, month (and day column is filled with zeros);GROUP BY year (now month, day columns are both filled with zeros);and totals (and all three key expression columns are zeros). ┌─year─┬─month─┬─day─┬─count()─┐ │ 2020 │ 10 │ 15 │ 1 │ │ 2020 │ 1 │ 5 │ 1 │ │ 2019 │ 1 │ 5 │ 1 │ │ 2020 │ 1 │ 15 │ 1 │ │ 2019 │ 1 │ 15 │ 1 │ │ 2020 │ 10 │ 5 │ 1 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 2019 │ 1 │ 0 │ 2 │ │ 2020 │ 1 │ 0 │ 2 │ │ 2020 │ 10 │ 0 │ 2 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 2019 │ 0 │ 0 │ 2 │ │ 2020 │ 0 │ 0 │ 4 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 0 │ 0 │ 0 │ 6 │ └──────┴───────┴─────┴─────────┘  "},{"title":"WITH CUBE Modifier​","type":1,"pageTitle":"GROUP BY Clause","url":"en/sql-reference/statements/select/group-by#with-cube-modifier","content":"WITH CUBE modifier is used to calculate subtotals for every combination of the key expressions in the GROUP BY list. The subtotals rows are added after the result table. In the subtotals rows the values of all &quot;grouped&quot; key expressions are set to 0 or empty line. note Mind that HAVING clause can affect the subtotals results. Example Consider the table t: ┌─year─┬─month─┬─day─┐ │ 2019 │ 1 │ 5 │ │ 2019 │ 1 │ 15 │ │ 2020 │ 1 │ 5 │ │ 2020 │ 1 │ 15 │ │ 2020 │ 10 │ 5 │ │ 2020 │ 10 │ 15 │ └──────┴───────┴─────┘  Query: SELECT year, month, day, count(*) FROM t GROUP BY year, month, day WITH CUBE;  As GROUP BY section has three key expressions, the result contains eight tables with subtotals for all key expression combinations: GROUP BY year, month, dayGROUP BY year, monthGROUP BY year, dayGROUP BY yearGROUP BY month, dayGROUP BY monthGROUP BY dayand totals. Columns, excluded from GROUP BY, are filled with zeros. ┌─year─┬─month─┬─day─┬─count()─┐ │ 2020 │ 10 │ 15 │ 1 │ │ 2020 │ 1 │ 5 │ 1 │ │ 2019 │ 1 │ 5 │ 1 │ │ 2020 │ 1 │ 15 │ 1 │ │ 2019 │ 1 │ 15 │ 1 │ │ 2020 │ 10 │ 5 │ 1 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 2019 │ 1 │ 0 │ 2 │ │ 2020 │ 1 │ 0 │ 2 │ │ 2020 │ 10 │ 0 │ 2 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 2020 │ 0 │ 5 │ 2 │ │ 2019 │ 0 │ 5 │ 1 │ │ 2020 │ 0 │ 15 │ 2 │ │ 2019 │ 0 │ 15 │ 1 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 2019 │ 0 │ 0 │ 2 │ │ 2020 │ 0 │ 0 │ 4 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 0 │ 1 │ 5 │ 2 │ │ 0 │ 10 │ 15 │ 1 │ │ 0 │ 10 │ 5 │ 1 │ │ 0 │ 1 │ 15 │ 2 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 0 │ 1 │ 0 │ 4 │ │ 0 │ 10 │ 0 │ 2 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 0 │ 0 │ 5 │ 3 │ │ 0 │ 0 │ 15 │ 3 │ └──────┴───────┴─────┴─────────┘ ┌─year─┬─month─┬─day─┬─count()─┐ │ 0 │ 0 │ 0 │ 6 │ └──────┴───────┴─────┴─────────┘  "},{"title":"WITH TOTALS Modifier​","type":1,"pageTitle":"GROUP BY Clause","url":"en/sql-reference/statements/select/group-by#with-totals-modifier","content":"If the WITH TOTALS modifier is specified, another row will be calculated. This row will have key columns containing default values (zeros or empty lines), and columns of aggregate functions with the values calculated across all the rows (the “total” values). This extra row is only produced in JSON*, TabSeparated*, and Pretty* formats, separately from the other rows: In JSON* formats, this row is output as a separate ‘totals’ field.In TabSeparated* formats, the row comes after the main result, preceded by an empty row (after the other data).In Pretty* formats, the row is output as a separate table after the main result.In the other formats it is not available. note totals is output in the results of SELECT queries, and is not output in INSERT INTO ... SELECT. WITH TOTALS can be run in different ways when HAVING is present. The behavior depends on the totals_mode setting. "},{"title":"Configuring Totals Processing​","type":1,"pageTitle":"GROUP BY Clause","url":"en/sql-reference/statements/select/group-by#configuring-totals-processing","content":"By default, totals_mode = 'before_having'. In this case, ‘totals’ is calculated across all rows, including the ones that do not pass through HAVING and max_rows_to_group_by. The other alternatives include only the rows that pass through HAVING in ‘totals’, and behave differently with the setting max_rows_to_group_by and group_by_overflow_mode = 'any'. after_having_exclusive – Don’t include rows that didn’t pass through max_rows_to_group_by. In other words, ‘totals’ will have less than or the same number of rows as it would if max_rows_to_group_by were omitted. after_having_inclusive – Include all the rows that didn’t pass through ‘max_rows_to_group_by’ in ‘totals’. In other words, ‘totals’ will have more than or the same number of rows as it would if max_rows_to_group_by were omitted. after_having_auto – Count the number of rows that passed through HAVING. If it is more than a certain amount (by default, 50%), include all the rows that didn’t pass through ‘max_rows_to_group_by’ in ‘totals’. Otherwise, do not include them. totals_auto_threshold – By default, 0.5. The coefficient for after_having_auto. If max_rows_to_group_by and group_by_overflow_mode = 'any' are not used, all variations of after_having are the same, and you can use any of them (for example, after_having_auto). You can use WITH TOTALS in subqueries, including subqueries in the JOIN clause (in this case, the respective total values are combined). "},{"title":"Examples​","type":1,"pageTitle":"GROUP BY Clause","url":"en/sql-reference/statements/select/group-by#examples","content":"Example: SELECT count(), median(FetchTiming &gt; 60 ? 60 : FetchTiming), count() - sum(Refresh) FROM hits  As opposed to MySQL (and conforming to standard SQL), you can’t get some value of some column that is not in a key or aggregate function (except constant expressions). To work around this, you can use the ‘any’ aggregate function (get the first encountered value) or ‘min/max’. Example: SELECT domainWithoutWWW(URL) AS domain, count(), any(Title) AS title -- getting the first occurred page header for each domain. FROM hits GROUP BY domain  For every different key value encountered, GROUP BY calculates a set of aggregate function values. "},{"title":"Implementation Details​","type":1,"pageTitle":"GROUP BY Clause","url":"en/sql-reference/statements/select/group-by#implementation-details","content":"Aggregation is one of the most important features of a column-oriented DBMS, and thus it’s implementation is one of the most heavily optimized parts of ClickHouse. By default, aggregation is done in memory using a hash-table. It has 40+ specializations that are chosen automatically depending on “grouping key” data types. "},{"title":"GROUP BY Optimization Depending on Table Sorting Key​","type":1,"pageTitle":"GROUP BY Clause","url":"en/sql-reference/statements/select/group-by#aggregation-in-order","content":"The aggregation can be performed more effectively, if a table is sorted by some key, and GROUP BY expression contains at least prefix of sorting key or injective functions. In this case when a new key is read from table, the in-between result of aggregation can be finalized and sent to client. This behaviour is switched on by the optimize_aggregation_in_order setting. Such optimization reduces memory usage during aggregation, but in some cases may slow down the query execution. "},{"title":"GROUP BY in External Memory​","type":1,"pageTitle":"GROUP BY Clause","url":"en/sql-reference/statements/select/group-by#select-group-by-in-external-memory","content":"You can enable dumping temporary data to the disk to restrict memory usage during GROUP BY. The max_bytes_before_external_group_by setting determines the threshold RAM consumption for dumping GROUP BY temporary data to the file system. If set to 0 (the default), it is disabled. When using max_bytes_before_external_group_by, we recommend that you set max_memory_usage about twice as high. This is necessary because there are two stages to aggregation: reading the data and forming intermediate data (1) and merging the intermediate data (2). Dumping data to the file system can only occur during stage 1. If the temporary data wasn’t dumped, then stage 2 might require up to the same amount of memory as in stage 1. For example, if max_memory_usage was set to 10000000000 and you want to use external aggregation, it makes sense to set max_bytes_before_external_group_by to 10000000000, and max_memory_usage to 20000000000. When external aggregation is triggered (if there was at least one dump of temporary data), maximum consumption of RAM is only slightly more than max_bytes_before_external_group_by. With distributed query processing, external aggregation is performed on remote servers. In order for the requester server to use only a small amount of RAM, set distributed_aggregation_memory_efficient to 1. When merging data flushed to the disk, as well as when merging results from remote servers when the distributed_aggregation_memory_efficient setting is enabled, consumes up to 1/256 * the_number_of_threads from the total amount of RAM. When external aggregation is enabled, if there was less than max_bytes_before_external_group_by of data (i.e. data was not flushed), the query runs just as fast as without external aggregation. If any temporary data was flushed, the run time will be several times longer (approximately three times). If you have an ORDER BY with a LIMIT after GROUP BY, then the amount of used RAM depends on the amount of data in LIMIT, not in the whole table. But if the ORDER BY does not have LIMIT, do not forget to enable external sorting (max_bytes_before_external_sort). "},{"title":"ORDER BY Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/order-by","content":"","keywords":""},{"title":"Sorting of Special Values​","type":1,"pageTitle":"ORDER BY Clause","url":"en/sql-reference/statements/select/order-by#sorting-of-special-values","content":"There are two approaches to NaN and NULL sorting order: By default or with the NULLS LAST modifier: first the values, then NaN, then NULL.With the NULLS FIRST modifier: first NULL, then NaN, then other values. "},{"title":"Example​","type":1,"pageTitle":"ORDER BY Clause","url":"en/sql-reference/statements/select/order-by#example","content":"For the table ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ │ 2 │ 2 │ │ 1 │ nan │ │ 2 │ 2 │ │ 3 │ 4 │ │ 5 │ 6 │ │ 6 │ nan │ │ 7 │ ᴺᵁᴸᴸ │ │ 6 │ 7 │ │ 8 │ 9 │ └───┴──────┘  Run the query SELECT * FROM t_null_nan ORDER BY y NULLS FIRST to get: ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ │ 7 │ ᴺᵁᴸᴸ │ │ 1 │ nan │ │ 6 │ nan │ │ 2 │ 2 │ │ 2 │ 2 │ │ 3 │ 4 │ │ 5 │ 6 │ │ 6 │ 7 │ │ 8 │ 9 │ └───┴──────┘  When floating point numbers are sorted, NaNs are separate from the other values. Regardless of the sorting order, NaNs come at the end. In other words, for ascending sorting they are placed as if they are larger than all the other numbers, while for descending sorting they are placed as if they are smaller than the rest. "},{"title":"Collation Support​","type":1,"pageTitle":"ORDER BY Clause","url":"en/sql-reference/statements/select/order-by#collation-support","content":"For sorting by String values, you can specify collation (comparison). Example: ORDER BY SearchPhrase COLLATE 'tr' - for sorting by keyword in ascending order, using the Turkish alphabet, case insensitive, assuming that strings are UTF-8 encoded. COLLATE can be specified or not for each expression in ORDER BY independently. If ASC or DESC is specified, COLLATE is specified after it. When using COLLATE, sorting is always case-insensitive. Collate is supported in LowCardinality, Nullable, Array and Tuple. We only recommend using COLLATE for final sorting of a small number of rows, since sorting with COLLATE is less efficient than normal sorting by bytes. "},{"title":"Collation Examples​","type":1,"pageTitle":"ORDER BY Clause","url":"en/sql-reference/statements/select/order-by#collation-examples","content":"Example only with String values: Input table: ┌─x─┬─s────┐ │ 1 │ bca │ │ 2 │ ABC │ │ 3 │ 123a │ │ 4 │ abc │ │ 5 │ BCA │ └───┴──────┘  Query: SELECT * FROM collate_test ORDER BY s ASC COLLATE 'en';  Result: ┌─x─┬─s────┐ │ 3 │ 123a │ │ 4 │ abc │ │ 2 │ ABC │ │ 1 │ bca │ │ 5 │ BCA │ └───┴──────┘  Example with Nullable: Input table: ┌─x─┬─s────┐ │ 1 │ bca │ │ 2 │ ᴺᵁᴸᴸ │ │ 3 │ ABC │ │ 4 │ 123a │ │ 5 │ abc │ │ 6 │ ᴺᵁᴸᴸ │ │ 7 │ BCA │ └───┴──────┘  Query: SELECT * FROM collate_test ORDER BY s ASC COLLATE 'en';  Result: ┌─x─┬─s────┐ │ 4 │ 123a │ │ 5 │ abc │ │ 3 │ ABC │ │ 1 │ bca │ │ 7 │ BCA │ │ 6 │ ᴺᵁᴸᴸ │ │ 2 │ ᴺᵁᴸᴸ │ └───┴──────┘  Example with Array: Input table: ┌─x─┬─s─────────────┐ │ 1 │ ['Z'] │ │ 2 │ ['z'] │ │ 3 │ ['a'] │ │ 4 │ ['A'] │ │ 5 │ ['z','a'] │ │ 6 │ ['z','a','a'] │ │ 7 │ [''] │ └───┴───────────────┘  Query: SELECT * FROM collate_test ORDER BY s ASC COLLATE 'en';  Result: ┌─x─┬─s─────────────┐ │ 7 │ [''] │ │ 3 │ ['a'] │ │ 4 │ ['A'] │ │ 2 │ ['z'] │ │ 5 │ ['z','a'] │ │ 6 │ ['z','a','a'] │ │ 1 │ ['Z'] │ └───┴───────────────┘  Example with LowCardinality string: Input table: ┌─x─┬─s───┐ │ 1 │ Z │ │ 2 │ z │ │ 3 │ a │ │ 4 │ A │ │ 5 │ za │ │ 6 │ zaa │ │ 7 │ │ └───┴─────┘  Query: SELECT * FROM collate_test ORDER BY s ASC COLLATE 'en';  Result: ┌─x─┬─s───┐ │ 7 │ │ │ 3 │ a │ │ 4 │ A │ │ 2 │ z │ │ 1 │ Z │ │ 5 │ za │ │ 6 │ zaa │ └───┴─────┘  Example with Tuple: ┌─x─┬─s───────┐ │ 1 │ (1,'Z') │ │ 2 │ (1,'z') │ │ 3 │ (1,'a') │ │ 4 │ (2,'z') │ │ 5 │ (1,'A') │ │ 6 │ (2,'Z') │ │ 7 │ (2,'A') │ └───┴─────────┘  Query: SELECT * FROM collate_test ORDER BY s ASC COLLATE 'en';  Result: ┌─x─┬─s───────┐ │ 3 │ (1,'a') │ │ 5 │ (1,'A') │ │ 2 │ (1,'z') │ │ 1 │ (1,'Z') │ │ 7 │ (2,'A') │ │ 4 │ (2,'z') │ │ 6 │ (2,'Z') │ └───┴─────────┘  "},{"title":"Implementation Details​","type":1,"pageTitle":"ORDER BY Clause","url":"en/sql-reference/statements/select/order-by#implementation-details","content":"Less RAM is used if a small enough LIMIT is specified in addition to ORDER BY. Otherwise, the amount of memory spent is proportional to the volume of data for sorting. For distributed query processing, if GROUP BY is omitted, sorting is partially done on remote servers, and the results are merged on the requestor server. This means that for distributed sorting, the volume of data to sort can be greater than the amount of memory on a single server. If there is not enough RAM, it is possible to perform sorting in external memory (creating temporary files on a disk). Use the setting max_bytes_before_external_sort for this purpose. If it is set to 0 (the default), external sorting is disabled. If it is enabled, when the volume of data to sort reaches the specified number of bytes, the collected data is sorted and dumped into a temporary file. After all data is read, all the sorted files are merged and the results are output. Files are written to the /var/lib/clickhouse/tmp/ directory in the config (by default, but you can use the tmp_path parameter to change this setting). Running a query may use more memory than max_bytes_before_external_sort. For this reason, this setting must have a value significantly smaller than max_memory_usage. As an example, if your server has 128 GB of RAM and you need to run a single query, set max_memory_usage to 100 GB, and max_bytes_before_external_sort to 80 GB. External sorting works much less effectively than sorting in RAM. "},{"title":"Optimization of Data Reading​","type":1,"pageTitle":"ORDER BY Clause","url":"en/sql-reference/statements/select/order-by#optimize_read_in_order","content":"If ORDER BY expression has a prefix that coincides with the table sorting key, you can optimize the query by using the optimize_read_in_order setting. When the optimize_read_in_order setting is enabled, the ClickHouse server uses the table index and reads the data in order of the ORDER BY key. This allows to avoid reading all data in case of specified LIMIT. So queries on big data with small limit are processed faster. Optimization works with both ASC and DESC and does not work together with GROUP BY clause and FINAL modifier. When the optimize_read_in_order setting is disabled, the ClickHouse server does not use the table index while processing SELECT queries. Consider disabling optimize_read_in_order manually, when running queries that have ORDER BY clause, large LIMIT and WHERE condition that requires to read huge amount of records before queried data is found. Optimization is supported in the following table engines: MergeTreeMerge, Buffer, and MaterializedView table engines over MergeTree-engine tables In MaterializedView-engine tables the optimization works with views like SELECT ... FROM merge_tree_table ORDER BY pk. But it is not supported in the queries like SELECT ... FROM view ORDER BY pk if the view query does not have the ORDER BY clause. "},{"title":"ORDER BY Expr WITH FILL Modifier​","type":1,"pageTitle":"ORDER BY Clause","url":"en/sql-reference/statements/select/order-by#orderby-with-fill","content":"This modifier also can be combined with LIMIT … WITH TIES modifier. WITH FILL modifier can be set after ORDER BY expr with optional FROM expr, TO expr and STEP expr parameters. All missed values of expr column will be filled sequentially and other columns will be filled as defaults. To fill multiple columns, add WITH FILL modifier with optional parameters after each field name in ORDER BY section. ORDER BY expr [WITH FILL] [FROM const_expr] [TO const_expr] [STEP const_numeric_expr], ... exprN [WITH FILL] [FROM expr] [TO expr] [STEP numeric_expr] [INTERPOLATE [(col [AS expr], ... colN [AS exprN])]]  WITH FILL can be applied for fields with Numeric (all kinds of float, decimal, int) or Date/DateTime types. When applied for String fields, missed values are filled with empty strings. When FROM const_expr not defined sequence of filling use minimal expr field value from ORDER BY. When TO const_expr not defined sequence of filling use maximum expr field value from ORDER BY. When STEP const_numeric_expr defined then const_numeric_expr interprets as is for numeric types, as days for Date type, as seconds for DateTime type. It also supports INTERVAL data type representing time and date intervals. When STEP const_numeric_expr omitted then sequence of filling use 1.0 for numeric type, 1 day for Date type and 1 second for DateTime type.INTERPOLATE can be applied to columns not participating in ORDER BY WITH FILL. Such columns are filled based on previous fields values by applying expr. If expr is not present will repeate previous value. Omitted list will result in including all allowed columns. Example of a query without WITH FILL: SELECT n, source FROM ( SELECT toFloat32(number % 10) AS n, 'original' AS source FROM numbers(10) WHERE number % 3 = 1 ) ORDER BY n;  Result: ┌─n─┬─source───┐ │ 1 │ original │ │ 4 │ original │ │ 7 │ original │ └───┴──────────┘  Same query after applying WITH FILL modifier: SELECT n, source FROM ( SELECT toFloat32(number % 10) AS n, 'original' AS source FROM numbers(10) WHERE number % 3 = 1 ) ORDER BY n WITH FILL FROM 0 TO 5.51 STEP 0.5;  Result: ┌───n─┬─source───┐ │ 0 │ │ │ 0.5 │ │ │ 1 │ original │ │ 1.5 │ │ │ 2 │ │ │ 2.5 │ │ │ 3 │ │ │ 3.5 │ │ │ 4 │ original │ │ 4.5 │ │ │ 5 │ │ │ 5.5 │ │ │ 7 │ original │ └─────┴──────────┘  For the case with multiple fields ORDER BY field2 WITH FILL, field1 WITH FILL order of filling will follow the order of fields in the ORDER BY clause. Example: SELECT toDate((number * 10) * 86400) AS d1, toDate(number * 86400) AS d2, 'original' AS source FROM numbers(10) WHERE (number % 3) = 1 ORDER BY d2 WITH FILL, d1 WITH FILL STEP 5;  Result: ┌───d1───────┬───d2───────┬─source───┐ │ 1970-01-11 │ 1970-01-02 │ original │ │ 1970-01-01 │ 1970-01-03 │ │ │ 1970-01-01 │ 1970-01-04 │ │ │ 1970-02-10 │ 1970-01-05 │ original │ │ 1970-01-01 │ 1970-01-06 │ │ │ 1970-01-01 │ 1970-01-07 │ │ │ 1970-03-12 │ 1970-01-08 │ original │ └────────────┴────────────┴──────────┘  Field d1 does not fill in and use the default value cause we do not have repeated values for d2 value, and the sequence for d1 can’t be properly calculated. The following query with the changed field in ORDER BY: SELECT toDate((number * 10) * 86400) AS d1, toDate(number * 86400) AS d2, 'original' AS source FROM numbers(10) WHERE (number % 3) = 1 ORDER BY d1 WITH FILL STEP 5, d2 WITH FILL;  Result: ┌───d1───────┬───d2───────┬─source───┐ │ 1970-01-11 │ 1970-01-02 │ original │ │ 1970-01-16 │ 1970-01-01 │ │ │ 1970-01-21 │ 1970-01-01 │ │ │ 1970-01-26 │ 1970-01-01 │ │ │ 1970-01-31 │ 1970-01-01 │ │ │ 1970-02-05 │ 1970-01-01 │ │ │ 1970-02-10 │ 1970-01-05 │ original │ │ 1970-02-15 │ 1970-01-01 │ │ │ 1970-02-20 │ 1970-01-01 │ │ │ 1970-02-25 │ 1970-01-01 │ │ │ 1970-03-02 │ 1970-01-01 │ │ │ 1970-03-07 │ 1970-01-01 │ │ │ 1970-03-12 │ 1970-01-08 │ original │ └────────────┴────────────┴──────────┘  The following query uses the INTERVAL data type of 1 day for each data filled on column d1: SELECT toDate((number * 10) * 86400) AS d1, toDate(number * 86400) AS d2, 'original' AS source FROM numbers(10) WHERE (number % 3) = 1 ORDER BY d1 WITH FILL STEP INTERVAL 1 DAY, d2 WITH FILL;  Result: ┌─────────d1─┬─────────d2─┬─source───┐ │ 1970-01-11 │ 1970-01-02 │ original │ │ 1970-01-12 │ 1970-01-01 │ │ │ 1970-01-13 │ 1970-01-01 │ │ │ 1970-01-14 │ 1970-01-01 │ │ │ 1970-01-15 │ 1970-01-01 │ │ │ 1970-01-16 │ 1970-01-01 │ │ │ 1970-01-17 │ 1970-01-01 │ │ │ 1970-01-18 │ 1970-01-01 │ │ │ 1970-01-19 │ 1970-01-01 │ │ │ 1970-01-20 │ 1970-01-01 │ │ │ 1970-01-21 │ 1970-01-01 │ │ │ 1970-01-22 │ 1970-01-01 │ │ │ 1970-01-23 │ 1970-01-01 │ │ │ 1970-01-24 │ 1970-01-01 │ │ │ 1970-01-25 │ 1970-01-01 │ │ │ 1970-01-26 │ 1970-01-01 │ │ │ 1970-01-27 │ 1970-01-01 │ │ │ 1970-01-28 │ 1970-01-01 │ │ │ 1970-01-29 │ 1970-01-01 │ │ │ 1970-01-30 │ 1970-01-01 │ │ │ 1970-01-31 │ 1970-01-01 │ │ │ 1970-02-01 │ 1970-01-01 │ │ │ 1970-02-02 │ 1970-01-01 │ │ │ 1970-02-03 │ 1970-01-01 │ │ │ 1970-02-04 │ 1970-01-01 │ │ │ 1970-02-05 │ 1970-01-01 │ │ │ 1970-02-06 │ 1970-01-01 │ │ │ 1970-02-07 │ 1970-01-01 │ │ │ 1970-02-08 │ 1970-01-01 │ │ │ 1970-02-09 │ 1970-01-01 │ │ │ 1970-02-10 │ 1970-01-05 │ original │ │ 1970-02-11 │ 1970-01-01 │ │ │ 1970-02-12 │ 1970-01-01 │ │ │ 1970-02-13 │ 1970-01-01 │ │ │ 1970-02-14 │ 1970-01-01 │ │ │ 1970-02-15 │ 1970-01-01 │ │ │ 1970-02-16 │ 1970-01-01 │ │ │ 1970-02-17 │ 1970-01-01 │ │ │ 1970-02-18 │ 1970-01-01 │ │ │ 1970-02-19 │ 1970-01-01 │ │ │ 1970-02-20 │ 1970-01-01 │ │ │ 1970-02-21 │ 1970-01-01 │ │ │ 1970-02-22 │ 1970-01-01 │ │ │ 1970-02-23 │ 1970-01-01 │ │ │ 1970-02-24 │ 1970-01-01 │ │ │ 1970-02-25 │ 1970-01-01 │ │ │ 1970-02-26 │ 1970-01-01 │ │ │ 1970-02-27 │ 1970-01-01 │ │ │ 1970-02-28 │ 1970-01-01 │ │ │ 1970-03-01 │ 1970-01-01 │ │ │ 1970-03-02 │ 1970-01-01 │ │ │ 1970-03-03 │ 1970-01-01 │ │ │ 1970-03-04 │ 1970-01-01 │ │ │ 1970-03-05 │ 1970-01-01 │ │ │ 1970-03-06 │ 1970-01-01 │ │ │ 1970-03-07 │ 1970-01-01 │ │ │ 1970-03-08 │ 1970-01-01 │ │ │ 1970-03-09 │ 1970-01-01 │ │ │ 1970-03-10 │ 1970-01-01 │ │ │ 1970-03-11 │ 1970-01-01 │ │ │ 1970-03-12 │ 1970-01-08 │ original │ └────────────┴────────────┴──────────┘  Example of a query without INTERPOLATE: SELECT n, source, inter FROM ( SELECT toFloat32(number % 10) AS n, 'original' AS source, number as inter FROM numbers(10) WHERE number % 3 = 1 ) ORDER BY n WITH FILL FROM 0 TO 5.51 STEP 0.5;  Result: ┌───n─┬─source───┬─inter─┐ │ 0 │ │ 0 │ │ 0.5 │ │ 0 │ │ 1 │ original │ 1 │ │ 1.5 │ │ 0 │ │ 2 │ │ 0 │ │ 2.5 │ │ 0 │ │ 3 │ │ 0 │ │ 3.5 │ │ 0 │ │ 4 │ original │ 4 │ │ 4.5 │ │ 0 │ │ 5 │ │ 0 │ │ 5.5 │ │ 0 │ │ 7 │ original │ 7 │ └─────┴──────────┴───────┘  Same query after applying INTERPOLATE: SELECT n, source, inter FROM ( SELECT toFloat32(number % 10) AS n, 'original' AS source, number as inter FROM numbers(10) WHERE number % 3 = 1 ) ORDER BY n WITH FILL FROM 0 TO 5.51 STEP 0.5 INTERPOLATE (inter AS inter + 1);  Result: ┌───n─┬─source───┬─inter─┐ │ 0 │ │ 0 │ │ 0.5 │ │ 0 │ │ 1 │ original │ 1 │ │ 1.5 │ │ 2 │ │ 2 │ │ 3 │ │ 2.5 │ │ 4 │ │ 3 │ │ 5 │ │ 3.5 │ │ 6 │ │ 4 │ original │ 4 │ │ 4.5 │ │ 5 │ │ 5 │ │ 6 │ │ 5.5 │ │ 7 │ │ 7 │ original │ 7 │ └─────┴──────────┴───────┘  Original article "},{"title":"OPTIMIZE Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/optimize","content":"","keywords":""},{"title":"BY expression​","type":1,"pageTitle":"OPTIMIZE Statement","url":"en/sql-reference/statements/optimize#by-expression","content":"If you want to perform deduplication on custom set of columns rather than on all, you can specify list of columns explicitly or use any combination of *, COLUMNS or EXCEPT expressions. The explictly written or implicitly expanded list of columns must include all columns specified in row ordering expression (both primary and sorting keys) and partitioning expression (partitioning key). note Notice that * behaves just like in SELECT: MATERIALIZED and ALIAS columns are not used for expansion. Also, it is an error to specify empty list of columns, or write an expression that results in an empty list of columns, or deduplicate by an ALIAS column. Syntax OPTIMIZE TABLE table DEDUPLICATE; -- all columns OPTIMIZE TABLE table DEDUPLICATE BY *; -- excludes MATERIALIZED and ALIAS columns OPTIMIZE TABLE table DEDUPLICATE BY colX,colY,colZ; OPTIMIZE TABLE table DEDUPLICATE BY * EXCEPT colX; OPTIMIZE TABLE table DEDUPLICATE BY * EXCEPT (colX, colY); OPTIMIZE TABLE table DEDUPLICATE BY COLUMNS('column-matched-by-regex'); OPTIMIZE TABLE table DEDUPLICATE BY COLUMNS('column-matched-by-regex') EXCEPT colX; OPTIMIZE TABLE table DEDUPLICATE BY COLUMNS('column-matched-by-regex') EXCEPT (colX, colY);  Examples Consider the table: CREATE TABLE example ( primary_key Int32, secondary_key Int32, value UInt32, partition_key UInt32, materialized_value UInt32 MATERIALIZED 12345, aliased_value UInt32 ALIAS 2, PRIMARY KEY primary_key ) ENGINE=MergeTree PARTITION BY partition_key ORDER BY (primary_key, secondary_key);  INSERT INTO example (primary_key, secondary_key, value, partition_key) VALUES (0, 0, 0, 0), (0, 0, 0, 0), (1, 1, 2, 2), (1, 1, 2, 3), (1, 1, 3, 3);  SELECT * FROM example;  Result:  ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 0 │ 0 │ 0 │ 0 │ │ 0 │ 0 │ 0 │ 0 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 2 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 3 │ │ 1 │ 1 │ 3 │ 3 │ └─────────────┴───────────────┴───────┴───────────────┘  When columns for deduplication are not specified, all of them are taken into account. Row is removed only if all values in all columns are equal to corresponding values in previous row: OPTIMIZE TABLE example FINAL DEDUPLICATE;  SELECT * FROM example;  Result: ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 2 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 0 │ 0 │ 0 │ 0 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 3 │ │ 1 │ 1 │ 3 │ 3 │ └─────────────┴───────────────┴───────┴───────────────┘  When columns are specified implicitly, the table is deduplicated by all columns that are not ALIAS or MATERIALIZED. Considering the table above, these are primary_key, secondary_key, value, and partition_key columns: OPTIMIZE TABLE example FINAL DEDUPLICATE BY *;  SELECT * FROM example;  Result: ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 2 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 0 │ 0 │ 0 │ 0 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 3 │ │ 1 │ 1 │ 3 │ 3 │ └─────────────┴───────────────┴───────┴───────────────┘  Deduplicate by all columns that are not ALIAS or MATERIALIZED and explicitly not value: primary_key, secondary_key, and partition_key columns. OPTIMIZE TABLE example FINAL DEDUPLICATE BY * EXCEPT value;  SELECT * FROM example;  Result: ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 2 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 0 │ 0 │ 0 │ 0 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 3 │ └─────────────┴───────────────┴───────┴───────────────┘  Deduplicate explicitly by primary_key, secondary_key, and partition_key columns: OPTIMIZE TABLE example FINAL DEDUPLICATE BY primary_key, secondary_key, partition_key;  SELECT * FROM example;  Result: ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 2 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 0 │ 0 │ 0 │ 0 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 3 │ └─────────────┴───────────────┴───────┴───────────────┘  Deduplicate by any column matching a regex: primary_key, secondary_key, and partition_key columns: OPTIMIZE TABLE example FINAL DEDUPLICATE BY COLUMNS('.*_key');  SELECT * FROM example;  Result: ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 0 │ 0 │ 0 │ 0 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 2 │ └─────────────┴───────────────┴───────┴───────────────┘ ┌─primary_key─┬─secondary_key─┬─value─┬─partition_key─┐ │ 1 │ 1 │ 2 │ 3 │ └─────────────┴───────────────┴───────┴───────────────┘  "},{"title":"PREWHERE Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/prewhere","content":"","keywords":""},{"title":"Controlling Prewhere Manually​","type":1,"pageTitle":"PREWHERE Clause","url":"en/sql-reference/statements/select/prewhere#controlling-prewhere-manually","content":"The clause has the same meaning as the WHERE clause. The difference is in which data is read from the table. When manually controlling PREWHERE for filtration conditions that are used by a minority of the columns in the query, but that provide strong data filtration. This reduces the volume of data to read. A query may simultaneously specify PREWHERE and WHERE. In this case, PREWHERE precedes WHERE. If the optimize_move_to_prewhere setting is set to 0, heuristics to automatically move parts of expressions from WHERE to PREWHERE are disabled. If query has FINAL modifier, the PREWHERE optimization is not always correct. It is enabled only if both settings optimize_move_to_prewhere and optimize_move_to_prewhere_if_final are turned on. note The PREWHERE section is executed before FINAL, so the results of FROM ... FINAL queries may be skewed when using PREWHERE with fields not in the ORDER BY section of a table. "},{"title":"Limitations​","type":1,"pageTitle":"PREWHERE Clause","url":"en/sql-reference/statements/select/prewhere#limitations","content":"PREWHERE is only supported by tables from the *MergeTree family. "},{"title":"SAMPLE Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/sample","content":"","keywords":""},{"title":"SAMPLE K​","type":1,"pageTitle":"SAMPLE Clause","url":"en/sql-reference/statements/select/sample#select-sample-k","content":"Here k is the number from 0 to 1 (both fractional and decimal notations are supported). For example, SAMPLE 1/2 or SAMPLE 0.5. In a SAMPLE k clause, the sample is taken from the k fraction of data. The example is shown below: SELECT Title, count() * 10 AS PageViews FROM hits_distributed SAMPLE 0.1 WHERE CounterID = 34 GROUP BY Title ORDER BY PageViews DESC LIMIT 1000  In this example, the query is executed on a sample from 0.1 (10%) of data. Values of aggregate functions are not corrected automatically, so to get an approximate result, the value count() is manually multiplied by 10. "},{"title":"SAMPLE N​","type":1,"pageTitle":"SAMPLE Clause","url":"en/sql-reference/statements/select/sample#select-sample-n","content":"Here n is a sufficiently large integer. For example, SAMPLE 10000000. In this case, the query is executed on a sample of at least n rows (but not significantly more than this). For example, SAMPLE 10000000 runs the query on a minimum of 10,000,000 rows. Since the minimum unit for data reading is one granule (its size is set by the index_granularity setting), it makes sense to set a sample that is much larger than the size of the granule. When using the SAMPLE n clause, you do not know which relative percent of data was processed. So you do not know the coefficient the aggregate functions should be multiplied by. Use the _sample_factor virtual column to get the approximate result. The _sample_factor column contains relative coefficients that are calculated dynamically. This column is created automatically when you create a table with the specified sampling key. The usage examples of the _sample_factor column are shown below. Let’s consider the table visits, which contains the statistics about site visits. The first example shows how to calculate the number of page views: SELECT sum(PageViews * _sample_factor) FROM visits SAMPLE 10000000  The next example shows how to calculate the total number of visits: SELECT sum(_sample_factor) FROM visits SAMPLE 10000000  The example below shows how to calculate the average session duration. Note that you do not need to use the relative coefficient to calculate the average values. SELECT avg(Duration) FROM visits SAMPLE 10000000  "},{"title":"SAMPLE K OFFSET M​","type":1,"pageTitle":"SAMPLE Clause","url":"en/sql-reference/statements/select/sample#select-sample-offset","content":"Here k and m are numbers from 0 to 1. Examples are shown below. Example 1 SAMPLE 1/10  In this example, the sample is 1/10th of all data: [++------------] Example 2 SAMPLE 1/10 OFFSET 1/2  Here, a sample of 10% is taken from the second half of the data. [------++------] "},{"title":"UNION Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/union","content":"UNION Clause You can use UNION with explicitly specifying UNION ALL or UNION DISTINCT. If you don't specify ALL or DISTINCT, it will depend on the union_default_mode setting. The difference between UNION ALL and UNION DISTINCT is that UNION DISTINCT will do a distinct transform for union result, it is equivalent to SELECT DISTINCT from a subquery containing UNION ALL. You can use UNION to combine any number of SELECT queries by extending their results. Example: SELECT CounterID, 1 AS table, toInt64(count()) AS c FROM test.hits GROUP BY CounterID UNION ALL SELECT CounterID, 2 AS table, sum(Sign) AS c FROM test.visits GROUP BY CounterID HAVING c &gt; 0 Result columns are matched by their index (order inside SELECT). If column names do not match, names for the final result are taken from the first query. Type casting is performed for unions. For example, if two queries being combined have the same field with non-Nullable and Nullable types from a compatible type, the resulting UNION has a Nullable type field. Queries that are parts of UNION can be enclosed in round brackets. ORDER BY and LIMIT are applied to separate queries, not to the final result. If you need to apply a conversion to the final result, you can put all the queries with UNION in a subquery in the FROM clause. If you use UNION without explicitly specifying UNION ALL or UNION DISTINCT, you can specify the union mode using the union_default_mode setting. The setting values can be ALL, DISTINCT or an empty string. However, if you use UNION with union_default_mode setting to empty string, it will throw an exception. The following examples demonstrate the results of queries with different values setting. Query: SET union_default_mode = 'DISTINCT'; SELECT 1 UNION SELECT 2 UNION SELECT 3 UNION SELECT 2; Result: ┌─1─┐ │ 1 │ └───┘ ┌─1─┐ │ 2 │ └───┘ ┌─1─┐ │ 3 │ └───┘ Query: SET union_default_mode = 'ALL'; SELECT 1 UNION SELECT 2 UNION SELECT 3 UNION SELECT 2; Result: ┌─1─┐ │ 1 │ └───┘ ┌─1─┐ │ 2 │ └───┘ ┌─1─┐ │ 2 │ └───┘ ┌─1─┐ │ 3 │ └───┘ Queries that are parts of UNION/UNION ALL/UNION DISTINCT can be run simultaneously, and their results can be mixed together. See Also insert_null_as_default setting.union_default_mode setting. Original article","keywords":""},{"title":"JOIN Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/join","content":"","keywords":""},{"title":"Supported Types of JOIN​","type":1,"pageTitle":"JOIN Clause","url":"en/sql-reference/statements/select/join#select-join-types","content":"All standard SQL JOIN types are supported: INNER JOIN, only matching rows are returned.LEFT OUTER JOIN, non-matching rows from left table are returned in addition to matching rows.RIGHT OUTER JOIN, non-matching rows from right table are returned in addition to matching rows.FULL OUTER JOIN, non-matching rows from both tables are returned in addition to matching rows.CROSS JOIN, produces cartesian product of whole tables, “join keys” are not specified. JOIN without specified type implies INNER. Keyword OUTER can be safely omitted. Alternative syntax for CROSS JOIN is specifying multiple tables in FROM clause separated by commas. Additional join types available in ClickHouse: LEFT SEMI JOIN and RIGHT SEMI JOIN, a whitelist on “join keys”, without producing a cartesian product.LEFT ANTI JOIN and RIGHT ANTI JOIN, a blacklist on “join keys”, without producing a cartesian product.LEFT ANY JOIN, RIGHT ANY JOIN and INNER ANY JOIN, partially (for opposite side of LEFT and RIGHT) or completely (for INNER and FULL) disables the cartesian product for standard JOIN types.ASOF JOIN and LEFT ASOF JOIN, joining sequences with a non-exact match. ASOF JOIN usage is described below. note When join_algorithm is set to partial_merge, RIGHT JOIN and FULL JOIN are supported only with ALL strictness (SEMI, ANTI, ANY, and ASOF are not supported). "},{"title":"Settings​","type":1,"pageTitle":"JOIN Clause","url":"en/sql-reference/statements/select/join#join-settings","content":"The default join type can be overridden using join_default_strictness setting. The behavior of ClickHouse server for ANY JOIN operations depends on the any_join_distinct_right_table_keys setting. See also join_algorithmjoin_any_take_last_rowjoin_use_nullspartial_merge_join_optimizationspartial_merge_join_rows_in_right_blocksjoin_on_disk_max_files_to_mergeany_join_distinct_right_table_keys "},{"title":"ON Section Conditions​","type":1,"pageTitle":"JOIN Clause","url":"en/sql-reference/statements/select/join#on-section-conditions","content":"An ON section can contain several conditions combined using the AND and OR operators. Conditions specifying join keys must refer both left and right tables and must use the equality operator. Other conditions may use other logical operators but they must refer either the left or the right table of a query. Rows are joined if the whole complex condition is met. If the conditions are not met, still rows may be included in the result depending on the JOIN type. Note that if the same conditions are placed in a WHERE section and they are not met, then rows are always filtered out from the result. The OR operator inside the ON clause works using the hash join algorithm — for each OR argument with join keys for JOIN, a separate hash table is created, so memory consumption and query execution time grow linearly with an increase in the number of expressions OR of the ON clause. note If a condition refers columns from different tables, then only the equality operator (=) is supported so far. Example Consider table_1 and table_2: ┌─Id─┬─name─┐ ┌─Id─┬─text───────────┬─scores─┐ │ 1 │ A │ │ 1 │ Text A │ 10 │ │ 2 │ B │ │ 1 │ Another text A │ 12 │ │ 3 │ C │ │ 2 │ Text B │ 15 │ └────┴──────┘ └────┴────────────────┴────────┘  Query with one join key condition and an additional condition for table_2: SELECT name, text FROM table_1 LEFT OUTER JOIN table_2 ON table_1.Id = table_2.Id AND startsWith(table_2.text, 'Text');  Note that the result contains the row with the name C and the empty text column. It is included into the result because an OUTER type of a join is used. ┌─name─┬─text───┐ │ A │ Text A │ │ B │ Text B │ │ C │ │ └──────┴────────┘  Query with INNER type of a join and multiple conditions: SELECT name, text, scores FROM table_1 INNER JOIN table_2 ON table_1.Id = table_2.Id AND table_2.scores &gt; 10 AND startsWith(table_2.text, 'Text');  Result: ┌─name─┬─text───┬─scores─┐ │ B │ Text B │ 15 │ └──────┴────────┴────────┘  Query with INNER type of a join and condition with OR: CREATE TABLE t1 (`a` Int64, `b` Int64) ENGINE = MergeTree() ORDER BY a; CREATE TABLE t2 (`key` Int32, `val` Int64) ENGINE = MergeTree() ORDER BY key; INSERT INTO t1 SELECT number as a, -a as b from numbers(5); INSERT INTO t2 SELECT if(number % 2 == 0, toInt64(number), -number) as key, number as val from numbers(5); SELECT a, b, val FROM t1 INNER JOIN t2 ON t1.a = t2.key OR t1.b = t2.key;  Result: ┌─a─┬──b─┬─val─┐ │ 0 │ 0 │ 0 │ │ 1 │ -1 │ 1 │ │ 2 │ -2 │ 2 │ │ 3 │ -3 │ 3 │ │ 4 │ -4 │ 4 │ └───┴────┴─────┘  Query with INNER type of a join and conditions with OR and AND: SELECT a, b, val FROM t1 INNER JOIN t2 ON t1.a = t2.key OR t1.b = t2.key AND t2.val &gt; 3;  Result: ┌─a─┬──b─┬─val─┐ │ 0 │ 0 │ 0 │ │ 2 │ -2 │ 2 │ │ 4 │ -4 │ 4 │ └───┴────┴─────┘  "},{"title":"ASOF JOIN Usage​","type":1,"pageTitle":"JOIN Clause","url":"en/sql-reference/statements/select/join#asof-join-usage","content":"ASOF JOIN is useful when you need to join records that have no exact match. Algorithm requires the special column in tables. This column: Must contain an ordered sequence.Can be one of the following types: Int, UInt, Float, Date, DateTime, Decimal.Can’t be the only column in the JOIN clause. Syntax ASOF JOIN ... ON: SELECT expressions_list FROM table_1 ASOF LEFT JOIN table_2 ON equi_cond AND closest_match_cond  You can use any number of equality conditions and exactly one closest match condition. For example, SELECT count() FROM table_1 ASOF LEFT JOIN table_2 ON table_1.a == table_2.b AND table_2.t &lt;= table_1.t. Conditions supported for the closest match: &gt;, &gt;=, &lt;, &lt;=. Syntax ASOF JOIN ... USING: SELECT expressions_list FROM table_1 ASOF JOIN table_2 USING (equi_column1, ... equi_columnN, asof_column)  ASOF JOIN uses equi_columnX for joining on equality and asof_column for joining on the closest match with the table_1.asof_column &gt;= table_2.asof_column condition. The asof_column column is always the last one in the USING clause. For example, consider the following tables:  table_1 table_2 event | ev_time | user_id event | ev_time | user_id ----------|---------|---------- ----------|---------|---------- ... ... event_1_1 | 12:00 | 42 event_2_1 | 11:59 | 42 ... event_2_2 | 12:30 | 42 event_1_2 | 13:00 | 42 event_2_3 | 13:00 | 42 ... ...  ASOF JOIN can take the timestamp of a user event from table_1 and find an event in table_2 where the timestamp is closest to the timestamp of the event from table_1 corresponding to the closest match condition. Equal timestamp values are the closest if available. Here, the user_id column can be used for joining on equality and the ev_time column can be used for joining on the closest match. In our example, event_1_1 can be joined with event_2_1 and event_1_2 can be joined with event_2_3, but event_2_2 can’t be joined. note ASOF join is not supported in the Join table engine. "},{"title":"Distributed JOIN​","type":1,"pageTitle":"JOIN Clause","url":"en/sql-reference/statements/select/join#global-join","content":"There are two ways to execute join involving distributed tables: When using a normal JOIN, the query is sent to remote servers. Subqueries are run on each of them in order to make the right table, and the join is performed with this table. In other words, the right table is formed on each server separately.When using GLOBAL ... JOIN, first the requestor server runs a subquery to calculate the right table. This temporary table is passed to each remote server, and queries are run on them using the temporary data that was transmitted. Be careful when using GLOBAL. For more information, see the Distributed subqueries section. "},{"title":"Implicit Type Conversion​","type":1,"pageTitle":"JOIN Clause","url":"en/sql-reference/statements/select/join#implicit-type-conversion","content":"INNER JOIN, LEFT JOIN, RIGHT JOIN, and FULL JOIN queries support the implicit type conversion for &quot;join keys&quot;. However the query can not be executed, if join keys from the left and the right tables cannot be converted to a single type (for example, there is no data type that can hold all values from both UInt64 and Int64, or String and Int32). Example Consider the table t_1: ┌─a─┬─b─┬─toTypeName(a)─┬─toTypeName(b)─┐ │ 1 │ 1 │ UInt16 │ UInt8 │ │ 2 │ 2 │ UInt16 │ UInt8 │ └───┴───┴───────────────┴───────────────┘  and the table t_2: ┌──a─┬────b─┬─toTypeName(a)─┬─toTypeName(b)───┐ │ -1 │ 1 │ Int16 │ Nullable(Int64) │ │ 1 │ -1 │ Int16 │ Nullable(Int64) │ │ 1 │ 1 │ Int16 │ Nullable(Int64) │ └────┴──────┴───────────────┴─────────────────┘  The query SELECT a, b, toTypeName(a), toTypeName(b) FROM t_1 FULL JOIN t_2 USING (a, b);  returns the set: ┌──a─┬────b─┬─toTypeName(a)─┬─toTypeName(b)───┐ │ 1 │ 1 │ Int32 │ Nullable(Int64) │ │ 2 │ 2 │ Int32 │ Nullable(Int64) │ │ -1 │ 1 │ Int32 │ Nullable(Int64) │ │ 1 │ -1 │ Int32 │ Nullable(Int64) │ └────┴──────┴───────────────┴─────────────────┘  "},{"title":"Usage Recommendations​","type":1,"pageTitle":"JOIN Clause","url":"en/sql-reference/statements/select/join#usage-recommendations","content":""},{"title":"Processing of Empty or NULL Cells​","type":1,"pageTitle":"JOIN Clause","url":"en/sql-reference/statements/select/join#processing-of-empty-or-null-cells","content":"While joining tables, the empty cells may appear. The setting join_use_nulls define how ClickHouse fills these cells. If the JOIN keys are Nullable fields, the rows where at least one of the keys has the value NULL are not joined. "},{"title":"Syntax​","type":1,"pageTitle":"JOIN Clause","url":"en/sql-reference/statements/select/join#syntax","content":"The columns specified in USING must have the same names in both subqueries, and the other columns must be named differently. You can use aliases to change the names of columns in subqueries. The USING clause specifies one or more columns to join, which establishes the equality of these columns. The list of columns is set without brackets. More complex join conditions are not supported. "},{"title":"Syntax Limitations​","type":1,"pageTitle":"JOIN Clause","url":"en/sql-reference/statements/select/join#syntax-limitations","content":"For multiple JOIN clauses in a single SELECT query: Taking all the columns via * is available only if tables are joined, not subqueries.The PREWHERE clause is not available. For ON, WHERE, and GROUP BY clauses: Arbitrary expressions cannot be used in ON, WHERE, and GROUP BY clauses, but you can define an expression in a SELECT clause and then use it in these clauses via an alias. "},{"title":"Performance​","type":1,"pageTitle":"JOIN Clause","url":"en/sql-reference/statements/select/join#performance","content":"When running a JOIN, there is no optimization of the order of execution in relation to other stages of the query. The join (a search in the right table) is run before filtering in WHERE and before aggregation. Each time a query is run with the same JOIN, the subquery is run again because the result is not cached. To avoid this, use the special Join table engine, which is a prepared array for joining that is always in RAM. In some cases, it is more efficient to use IN instead of JOIN. If you need a JOIN for joining with dimension tables (these are relatively small tables that contain dimension properties, such as names for advertising campaigns), a JOIN might not be very convenient due to the fact that the right table is re-accessed for every query. For such cases, there is an “external dictionaries” feature that you should use instead of JOIN. For more information, see the External dictionaries section. "},{"title":"Memory Limitations​","type":1,"pageTitle":"JOIN Clause","url":"en/sql-reference/statements/select/join#memory-limitations","content":"By default, ClickHouse uses the hash join algorithm. ClickHouse takes the right_table and creates a hash table for it in RAM. If join_algorithm = 'auto' is enabled, then after some threshold of memory consumption, ClickHouse falls back to merge join algorithm. For JOIN algorithms description see the join_algorithm setting. If you need to restrict JOIN operation memory consumption use the following settings: max_rows_in_join — Limits number of rows in the hash table.max_bytes_in_join — Limits size of the hash table. When any of these limits is reached, ClickHouse acts as the join_overflow_mode setting instructs. "},{"title":"Examples​","type":1,"pageTitle":"JOIN Clause","url":"en/sql-reference/statements/select/join#examples","content":"Example: SELECT CounterID, hits, visits FROM ( SELECT CounterID, count() AS hits FROM test.hits GROUP BY CounterID ) ANY LEFT JOIN ( SELECT CounterID, sum(Sign) AS visits FROM test.visits GROUP BY CounterID ) USING CounterID ORDER BY hits DESC LIMIT 10  ┌─CounterID─┬───hits─┬─visits─┐ │ 1143050 │ 523264 │ 13665 │ │ 731962 │ 475698 │ 102716 │ │ 722545 │ 337212 │ 108187 │ │ 722889 │ 252197 │ 10547 │ │ 2237260 │ 196036 │ 9522 │ │ 23057320 │ 147211 │ 7689 │ │ 722818 │ 90109 │ 17847 │ │ 48221 │ 85379 │ 4652 │ │ 19762435 │ 77807 │ 7026 │ │ 722884 │ 77492 │ 11056 │ └───────────┴────────┴────────┘  "},{"title":"WHERE Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/where","content":"WHERE Clause WHERE clause allows to filter the data that is coming from FROM clause of SELECT. If there is a WHERE clause, it must contain an expression with the UInt8 type. This is usually an expression with comparison and logical operators. Rows where this expression evaluates to 0 are excluded from further transformations or result. WHERE expression is evaluated on the ability to use indexes and partition pruning, if the underlying table engine supports that. note There is a filtering optimization called PREWHERE. If you need to test a value for NULL, use IS NULL and IS NOT NULL operators or isNull and isNotNull functions. Otherwise an expression with NULL never passes. Example To find numbers that are multiples of 3 and are greater than 10 execute the following query on the numbers table: SELECT number FROM numbers(20) WHERE (number &gt; 10) AND (number % 3 == 0); Result: ┌─number─┐ │ 12 │ │ 15 │ │ 18 │ └────────┘ Queries with NULL values: CREATE TABLE t_null(x Int8, y Nullable(Int8)) ENGINE=MergeTree() ORDER BY x; INSERT INTO t_null VALUES (1, NULL), (2, 3); SELECT * FROM t_null WHERE y IS NULL; SELECT * FROM t_null WHERE y != 0; Result: ┌─x─┬────y─┐ │ 1 │ ᴺᵁᴸᴸ │ └───┴──────┘ ┌─x─┬─y─┐ │ 2 │ 3 │ └───┴───┘ ","keywords":""},{"title":"WITH Clause","type":0,"sectionRef":"#","url":"en/sql-reference/statements/select/with","content":"","keywords":""},{"title":"Syntax​","type":1,"pageTitle":"WITH Clause","url":"en/sql-reference/statements/select/with#syntax","content":"WITH &lt;expression&gt; AS &lt;identifier&gt;  or WITH &lt;identifier&gt; AS &lt;subquery expression&gt;  "},{"title":"Examples​","type":1,"pageTitle":"WITH Clause","url":"en/sql-reference/statements/select/with#examples","content":"Example 1: Using constant expression as “variable” WITH '2019-08-01 15:23:00' as ts_upper_bound SELECT * FROM hits WHERE EventDate = toDate(ts_upper_bound) AND EventTime &lt;= ts_upper_bound;  Example 2: Evicting a sum(bytes) expression result from the SELECT clause column list WITH sum(bytes) as s SELECT formatReadableSize(s), table FROM system.parts GROUP BY table ORDER BY s;  Example 3: Using results of a scalar subquery /* this example would return TOP 10 of most huge tables */ WITH ( SELECT sum(bytes) FROM system.parts WHERE active ) AS total_disk_usage SELECT (sum(bytes) / total_disk_usage) * 100 AS table_disk_usage, table FROM system.parts GROUP BY table ORDER BY table_disk_usage DESC LIMIT 10;  Example 4: Reusing expression in a subquery WITH test1 AS (SELECT i + 1, j + 1 FROM test1) SELECT * FROM test1;  Original article "},{"title":"SET Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/set","content":"SET Statement SET param = value Assigns value to the param setting for the current session. You cannot change server settings this way. You can also set all the values from the specified settings profile in a single query. SET profile = 'profile-name-from-the-settings-file' For more information, see Settings.","keywords":""},{"title":"SET ROLE Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/set-role","content":"","keywords":""},{"title":"SET DEFAULT ROLE​","type":1,"pageTitle":"SET ROLE Statement","url":"en/sql-reference/statements/set-role#set-default-role-statement","content":"Sets default roles to a user. Default roles are automatically activated at user login. You can set as default only the previously granted roles. If the role isn’t granted to a user, ClickHouse throws an exception. SET DEFAULT ROLE {NONE | role [,...] | ALL | ALL EXCEPT role [,...]} TO {user|CURRENT_USER} [,...]  "},{"title":"Examples​","type":1,"pageTitle":"SET ROLE Statement","url":"en/sql-reference/statements/set-role#set-default-role-examples","content":"Set multiple default roles to a user: SET DEFAULT ROLE role1, role2, ... TO user  Set all the granted roles as default to a user: SET DEFAULT ROLE ALL TO user  Purge default roles from a user: SET DEFAULT ROLE NONE TO user  Set all the granted roles as default excepting some of them: SET DEFAULT ROLE ALL EXCEPT role1, role2 TO user  "},{"title":"SHOW Statements","type":0,"sectionRef":"#","url":"en/sql-reference/statements/show","content":"","keywords":""},{"title":"SHOW CREATE TABLE​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-create-table","content":"SHOW CREATE [TEMPORARY] [TABLE|DICTIONARY|VIEW] [db.]table|view [INTO OUTFILE filename] [FORMAT format]  Returns a single String-type ‘statement’ column, which contains a single value – the CREATE query used for creating the specified object. Note that if you use this statement to get CREATE query of system tables, you will get a fake query, which only declares table structure, but cannot be used to create table. "},{"title":"SHOW DATABASES​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-databases","content":"Prints a list of all databases. SHOW DATABASES [LIKE | ILIKE | NOT LIKE '&lt;pattern&gt;'] [LIMIT &lt;N&gt;] [INTO OUTFILE filename] [FORMAT format]  This statement is identical to the query: SELECT name FROM system.databases [WHERE name LIKE | ILIKE | NOT LIKE '&lt;pattern&gt;'] [LIMIT &lt;N&gt;] [INTO OUTFILE filename] [FORMAT format]  "},{"title":"Examples​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#examples","content":"Getting database names, containing the symbols sequence 'de' in their names: SHOW DATABASES LIKE '%de%'  Result: ┌─name────┐ │ default │ └─────────┘  Getting database names, containing symbols sequence 'de' in their names, in the case insensitive manner: SHOW DATABASES ILIKE '%DE%'  Result: ┌─name────┐ │ default │ └─────────┘  Getting database names, not containing the symbols sequence 'de' in their names: SHOW DATABASES NOT LIKE '%de%'  Result: ┌─name───────────────────────────┐ │ _temporary_and_external_tables │ │ system │ │ test │ │ tutorial │ └────────────────────────────────┘  Getting the first two rows from database names: SHOW DATABASES LIMIT 2  Result: ┌─name───────────────────────────┐ │ _temporary_and_external_tables │ │ default │ └────────────────────────────────┘  "},{"title":"See Also​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#see-also","content":"CREATE DATABASE "},{"title":"SHOW PROCESSLIST​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-processlist","content":"SHOW PROCESSLIST [INTO OUTFILE filename] [FORMAT format]  Outputs the content of the system.processes table, that contains a list of queries that is being processed at the moment, excepting SHOW PROCESSLIST queries. The SELECT * FROM system.processes query returns data about all the current queries. Tip (execute in the console): $ watch -n1 &quot;clickhouse-client --query='SHOW PROCESSLIST'&quot;  "},{"title":"SHOW TABLES​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-tables","content":"Displays a list of tables. SHOW [TEMPORARY] TABLES [{FROM | IN} &lt;db&gt;] [LIKE | ILIKE | NOT LIKE '&lt;pattern&gt;'] [LIMIT &lt;N&gt;] [INTO OUTFILE &lt;filename&gt;] [FORMAT &lt;format&gt;]  If the FROM clause is not specified, the query returns the list of tables from the current database. This statement is identical to the query: SELECT name FROM system.tables [WHERE name LIKE | ILIKE | NOT LIKE '&lt;pattern&gt;'] [LIMIT &lt;N&gt;] [INTO OUTFILE &lt;filename&gt;] [FORMAT &lt;format&gt;]  "},{"title":"Examples​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#examples","content":"Getting table names, containing the symbols sequence 'user' in their names: SHOW TABLES FROM system LIKE '%user%'  Result: ┌─name─────────────┐ │ user_directories │ │ users │ └──────────────────┘  Getting table names, containing sequence 'user' in their names, in the case insensitive manner: SHOW TABLES FROM system ILIKE '%USER%'  Result: ┌─name─────────────┐ │ user_directories │ │ users │ └──────────────────┘  Getting table names, not containing the symbol sequence 's' in their names: SHOW TABLES FROM system NOT LIKE '%s%'  Result: ┌─name─────────┐ │ metric_log │ │ metric_log_0 │ │ metric_log_1 │ └──────────────┘  Getting the first two rows from table names: SHOW TABLES FROM system LIMIT 2  Result: ┌─name───────────────────────────┐ │ aggregate_function_combinators │ │ asynchronous_metric_log │ └────────────────────────────────┘  "},{"title":"See Also​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#see-also","content":"Create TablesSHOW CREATE TABLE "},{"title":"SHOW DICTIONARIES​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-dictionaries","content":"Displays a list of external dictionaries. SHOW DICTIONARIES [FROM &lt;db&gt;] [LIKE '&lt;pattern&gt;'] [LIMIT &lt;N&gt;] [INTO OUTFILE &lt;filename&gt;] [FORMAT &lt;format&gt;]  If the FROM clause is not specified, the query returns the list of dictionaries from the current database. You can get the same results as the SHOW DICTIONARIES query in the following way: SELECT name FROM system.dictionaries WHERE database = &lt;db&gt; [AND name LIKE &lt;pattern&gt;] [LIMIT &lt;N&gt;] [INTO OUTFILE &lt;filename&gt;] [FORMAT &lt;format&gt;]  Example The following query selects the first two rows from the list of tables in the system database, whose names contain reg. SHOW DICTIONARIES FROM db LIKE '%reg%' LIMIT 2  ┌─name─────────┐ │ regions │ │ region_names │ └──────────────┘  "},{"title":"SHOW GRANTS​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-grants-statement","content":"Shows privileges for a user. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-grants-syntax","content":"SHOW GRANTS [FOR user1 [, user2 ...]]  If user is not specified, the query returns privileges for the current user. "},{"title":"SHOW CREATE USER​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-create-user-statement","content":"Shows parameters that were used at a user creation. SHOW CREATE USER does not output user passwords. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-create-user-syntax","content":"SHOW CREATE USER [name1 [, name2 ...] | CURRENT_USER]  "},{"title":"SHOW CREATE ROLE​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-create-role-statement","content":"Shows parameters that were used at a role creation. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-create-role-syntax","content":"SHOW CREATE ROLE name1 [, name2 ...]  "},{"title":"SHOW CREATE ROW POLICY​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-create-row-policy-statement","content":"Shows parameters that were used at a row policy creation. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-create-row-policy-syntax","content":"SHOW CREATE [ROW] POLICY name ON [database1.]table1 [, [database2.]table2 ...]  "},{"title":"SHOW CREATE QUOTA​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-create-quota-statement","content":"Shows parameters that were used at a quota creation. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-create-quota-syntax","content":"SHOW CREATE QUOTA [name1 [, name2 ...] | CURRENT]  "},{"title":"SHOW CREATE SETTINGS PROFILE​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-create-settings-profile-statement","content":"Shows parameters that were used at a settings profile creation. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-create-settings-profile-syntax","content":"SHOW CREATE [SETTINGS] PROFILE name1 [, name2 ...]  "},{"title":"SHOW USERS​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-users-statement","content":"Returns a list of user account names. To view user accounts parameters, see the system table system.users. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-users-syntax","content":"SHOW USERS  "},{"title":"SHOW ROLES​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-roles-statement","content":"Returns a list of roles. To view another parameters, see system tables system.roles and system.role-grants. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-roles-syntax","content":"SHOW [CURRENT|ENABLED] ROLES  "},{"title":"SHOW PROFILES​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-profiles-statement","content":"Returns a list of setting profiles. To view user accounts parameters, see the system table settings_profiles. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-profiles-syntax","content":"SHOW [SETTINGS] PROFILES  "},{"title":"SHOW POLICIES​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-policies-statement","content":"Returns a list of row policies for the specified table. To view user accounts parameters, see the system table system.row_policies. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-policies-syntax","content":"SHOW [ROW] POLICIES [ON [db.]table]  "},{"title":"SHOW QUOTAS​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-quotas-statement","content":"Returns a list of quotas. To view quotas parameters, see the system table system.quotas. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-quotas-syntax","content":"SHOW QUOTAS  "},{"title":"SHOW QUOTA​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-quota-statement","content":"Returns a quota consumption for all users or for current user. To view another parameters, see system tables system.quotas_usage and system.quota_usage. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-quota-syntax","content":"SHOW [CURRENT] QUOTA  "},{"title":"SHOW ACCESS​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-access-statement","content":"Shows all users, roles, profiles, etc. and all their grants. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-access-syntax","content":"SHOW ACCESS  "},{"title":"SHOW CLUSTER(s)​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-cluster-statement","content":"Returns a list of clusters. All available clusters are listed in the system.clusters table. note SHOW CLUSTER name query displays the contents of system.clusters table for this cluster. "},{"title":"Syntax​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-cluster-syntax","content":"SHOW CLUSTER '&lt;name&gt;' SHOW CLUSTERS [LIKE|NOT LIKE '&lt;pattern&gt;'] [LIMIT &lt;N&gt;]  "},{"title":"Examples​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-cluster-examples","content":"Query: SHOW CLUSTERS;  Result: ┌─cluster──────────────────────────────────────┐ │ test_cluster_two_shards │ │ test_cluster_two_shards_internal_replication │ │ test_cluster_two_shards_localhost │ │ test_shard_localhost │ │ test_shard_localhost_secure │ │ test_unavailable_shard │ └──────────────────────────────────────────────┘  Query: SHOW CLUSTERS LIKE 'test%' LIMIT 1;  Result: ┌─cluster─────────────────┐ │ test_cluster_two_shards │ └─────────────────────────┘  Query: SHOW CLUSTER 'test_shard_localhost' FORMAT Vertical;  Result: Row 1: ────── cluster: test_shard_localhost shard_num: 1 shard_weight: 1 replica_num: 1 host_name: localhost host_address: 127.0.0.1 port: 9000 is_local: 1 user: default default_database: errors_count: 0 estimated_recovery_time: 0  "},{"title":"SHOW SETTINGS​","type":1,"pageTitle":"SHOW Statements","url":"en/sql-reference/statements/show#show-settings","content":"Returns a list of system settings and their values. Selects data from the system.settings table. Syntax SHOW [CHANGED] SETTINGS LIKE|ILIKE &lt;name&gt;  Clauses LIKE|ILIKE allow to specify a matching pattern for the setting name. It can contain globs such as % or _. LIKE clause is case-sensitive, ILIKE — case insensitive. When the CHANGED clause is used, the query returns only settings changed from their default values. Examples Query with the LIKE clause: SHOW SETTINGS LIKE 'send_timeout';  Result: ┌─name─────────┬─type────┬─value─┐ │ send_timeout │ Seconds │ 300 │ └──────────────┴─────────┴───────┘  Query with the ILIKE clause: SHOW SETTINGS ILIKE '%CONNECT_timeout%'  Result: ┌─name────────────────────────────────────┬─type─────────┬─value─┐ │ connect_timeout │ Seconds │ 10 │ │ connect_timeout_with_failover_ms │ Milliseconds │ 50 │ │ connect_timeout_with_failover_secure_ms │ Milliseconds │ 100 │ └─────────────────────────────────────────┴──────────────┴───────┘  Query with the CHANGED clause: SHOW CHANGED SETTINGS ILIKE '%MEMORY%'  Result: ┌─name─────────────┬─type───┬─value───────┐ │ max_memory_usage │ UInt64 │ 10000000000 │ └──────────────────┴────────┴─────────────┘  See Also system.settings table Original article "},{"title":"SYSTEM Statements","type":0,"sectionRef":"#","url":"en/sql-reference/statements/system","content":"","keywords":""},{"title":"RELOAD EMBEDDED DICTIONARIES​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-reload-emdedded-dictionaries","content":"Reload all Internal dictionaries. By default, internal dictionaries are disabled. Always returns Ok. regardless of the result of the internal dictionary update. "},{"title":"RELOAD DICTIONARIES​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-reload-dictionaries","content":"Reloads all dictionaries that have been successfully loaded before. By default, dictionaries are loaded lazily (see dictionaries_lazy_load), so instead of being loaded automatically at startup, they are initialized on first access through dictGet function or SELECT from tables with ENGINE = Dictionary. The SYSTEM RELOAD DICTIONARIES query reloads such dictionaries (LOADED). Always returns Ok. regardless of the result of the dictionary update. "},{"title":"RELOAD DICTIONARY​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-reload-dictionary","content":"Completely reloads a dictionary dictionary_name, regardless of the state of the dictionary (LOADED / NOT_LOADED / FAILED). Always returns Ok. regardless of the result of updating the dictionary. The status of the dictionary can be checked by querying the system.dictionaries table. SELECT name, status FROM system.dictionaries;  "},{"title":"RELOAD MODELS​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-reload-models","content":"Reloads all CatBoost models if the configuration was updated without restarting the server. Syntax SYSTEM RELOAD MODELS [ON CLUSTER cluster_name]  "},{"title":"RELOAD MODEL​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-reload-model","content":"Completely reloads a CatBoost model model_name if the configuration was updated without restarting the server. Syntax SYSTEM RELOAD MODEL [ON CLUSTER cluster_name] &lt;model_name&gt;  "},{"title":"RELOAD FUNCTIONS​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-reload-functions","content":"Reloads all registered executable user defined functions or one of them from a configuration file. Syntax RELOAD FUNCTIONS [ON CLUSTER cluster_name] RELOAD FUNCTION [ON CLUSTER cluster_name] function_name  "},{"title":"DROP DNS CACHE​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-drop-dns-cache","content":"Resets ClickHouse’s internal DNS cache. Sometimes (for old ClickHouse versions) it is necessary to use this command when changing the infrastructure (changing the IP address of another ClickHouse server or the server used by dictionaries). For more convenient (automatic) cache management, see disable_internal_dns_cache, dns_cache_update_period parameters. "},{"title":"DROP MARK CACHE​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-drop-mark-cache","content":"Resets the mark cache. Used in development of ClickHouse and performance tests. "},{"title":"DROP REPLICA​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-drop-replica","content":"Dead replicas can be dropped using following syntax: SYSTEM DROP REPLICA 'replica_name' FROM TABLE database.table; SYSTEM DROP REPLICA 'replica_name' FROM DATABASE database; SYSTEM DROP REPLICA 'replica_name'; SYSTEM DROP REPLICA 'replica_name' FROM ZKPATH '/path/to/table/in/zk';  Queries will remove the replica path in ZooKeeper. It is useful when the replica is dead and its metadata cannot be removed from ZooKeeper by DROP TABLE because there is no such table anymore. It will only drop the inactive/stale replica, and it cannot drop local replica, please use DROP TABLE for that. DROP REPLICA does not drop any tables and does not remove any data or metadata from disk. The first one removes metadata of 'replica_name' replica of database.table table. The second one does the same for all replicated tables in the database. The third one does the same for all replicated tables on the local server. The fourth one is useful to remove metadata of dead replica when all other replicas of a table were dropped. It requires the table path to be specified explicitly. It must be the same path as was passed to the first argument of ReplicatedMergeTree engine on table creation. "},{"title":"DROP UNCOMPRESSED CACHE​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-drop-uncompressed-cache","content":"Reset the uncompressed data cache. Used in development of ClickHouse and performance tests. For manage uncompressed data cache parameters use following server level settings uncompressed_cache_size and query/user/profile level settings use_uncompressed_cache "},{"title":"DROP COMPILED EXPRESSION CACHE​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-drop-compiled-expression-cache","content":"Reset the compiled expression cache. Used in development of ClickHouse and performance tests. Compiled expression cache used when query/user/profile enable option compile-expressions "},{"title":"FLUSH LOGS​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-flush_logs","content":"Flushes buffers of log messages to system tables (e.g. system.query_log). Allows you to not wait 7.5 seconds when debugging. This will also create system tables even if message queue is empty. "},{"title":"RELOAD CONFIG​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-reload-config","content":"Reloads ClickHouse configuration. Used when configuration is stored in ZooKeeper. "},{"title":"SHUTDOWN​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-shutdown","content":"Normally shuts down ClickHouse (like service clickhouse-server stop / kill {$pid_clickhouse-server}) "},{"title":"KILL​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-kill","content":"Aborts ClickHouse process (like kill -9 {$ pid_clickhouse-server}) "},{"title":"Managing Distributed Tables​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query-language-system-distributed","content":"ClickHouse can manage distributed tables. When a user inserts data into these tables, ClickHouse first creates a queue of the data that should be sent to cluster nodes, then asynchronously sends it. You can manage queue processing with the STOP DISTRIBUTED SENDS, FLUSH DISTRIBUTED, and START DISTRIBUTED SENDS queries. You can also synchronously insert distributed data with the insert_distributed_sync setting. "},{"title":"STOP DISTRIBUTED SENDS​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-stop-distributed-sends","content":"Disables background data distribution when inserting data into distributed tables. SYSTEM STOP DISTRIBUTED SENDS [db.]&lt;distributed_table_name&gt;  "},{"title":"FLUSH DISTRIBUTED​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-flush-distributed","content":"Forces ClickHouse to send data to cluster nodes synchronously. If any nodes are unavailable, ClickHouse throws an exception and stops query execution. You can retry the query until it succeeds, which will happen when all nodes are back online. SYSTEM FLUSH DISTRIBUTED [db.]&lt;distributed_table_name&gt;  "},{"title":"START DISTRIBUTED SENDS​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-start-distributed-sends","content":"Enables background data distribution when inserting data into distributed tables. SYSTEM START DISTRIBUTED SENDS [db.]&lt;distributed_table_name&gt;  "},{"title":"Managing MergeTree Tables​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query-language-system-mergetree","content":"ClickHouse can manage background processes in MergeTree tables. "},{"title":"STOP MERGES​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-stop-merges","content":"Provides possibility to stop background merges for tables in the MergeTree family: SYSTEM STOP MERGES [ON VOLUME &lt;volume_name&gt; | [db.]merge_tree_family_table_name]  note DETACH / ATTACH table will start background merges for the table even in case when merges have been stopped for all MergeTree tables before. "},{"title":"START MERGES​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-start-merges","content":"Provides possibility to start background merges for tables in the MergeTree family: SYSTEM START MERGES [ON VOLUME &lt;volume_name&gt; | [db.]merge_tree_family_table_name]  "},{"title":"STOP TTL MERGES​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-stop-ttl-merges","content":"Provides possibility to stop background delete old data according to TTL expression for tables in the MergeTree family: Returns Ok. even if table does not exist or table has not MergeTree engine. Returns error when database does not exist: SYSTEM STOP TTL MERGES [[db.]merge_tree_family_table_name]  "},{"title":"START TTL MERGES​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-start-ttl-merges","content":"Provides possibility to start background delete old data according to TTL expression for tables in the MergeTree family: Returns Ok. even if table does not exist. Returns error when database does not exist: SYSTEM START TTL MERGES [[db.]merge_tree_family_table_name]  "},{"title":"STOP MOVES​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-stop-moves","content":"Provides possibility to stop background move data according to TTL table expression with TO VOLUME or TO DISK clause for tables in the MergeTree family: Returns Ok. even if table does not exist. Returns error when database does not exist: SYSTEM STOP MOVES [[db.]merge_tree_family_table_name]  "},{"title":"START MOVES​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-start-moves","content":"Provides possibility to start background move data according to TTL table expression with TO VOLUME and TO DISK clause for tables in the MergeTree family: Returns Ok. even if table does not exist. Returns error when database does not exist: SYSTEM START MOVES [[db.]merge_tree_family_table_name]  "},{"title":"Managing ReplicatedMergeTree Tables​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query-language-system-replicated","content":"ClickHouse can manage background replication related processes in ReplicatedMergeTree tables. "},{"title":"STOP FETCHES​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-stop-fetches","content":"Provides possibility to stop background fetches for inserted parts for tables in the ReplicatedMergeTree family: Always returns Ok. regardless of the table engine and even if table or database does not exist. SYSTEM STOP FETCHES [[db.]replicated_merge_tree_family_table_name]  "},{"title":"START FETCHES​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-start-fetches","content":"Provides possibility to start background fetches for inserted parts for tables in the ReplicatedMergeTree family: Always returns Ok. regardless of the table engine and even if table or database does not exist. SYSTEM START FETCHES [[db.]replicated_merge_tree_family_table_name]  "},{"title":"STOP REPLICATED SENDS​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-start-replicated-sends","content":"Provides possibility to stop background sends to other replicas in cluster for new inserted parts for tables in the ReplicatedMergeTree family: SYSTEM STOP REPLICATED SENDS [[db.]replicated_merge_tree_family_table_name]  "},{"title":"START REPLICATED SENDS​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-start-replicated-sends","content":"Provides possibility to start background sends to other replicas in cluster for new inserted parts for tables in the ReplicatedMergeTree family: SYSTEM START REPLICATED SENDS [[db.]replicated_merge_tree_family_table_name]  "},{"title":"STOP REPLICATION QUEUES​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-stop-replication-queues","content":"Provides possibility to stop background fetch tasks from replication queues which stored in Zookeeper for tables in the ReplicatedMergeTree family. Possible background tasks types - merges, fetches, mutation, DDL statements with ON CLUSTER clause: SYSTEM STOP REPLICATION QUEUES [[db.]replicated_merge_tree_family_table_name]  "},{"title":"START REPLICATION QUEUES​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-start-replication-queues","content":"Provides possibility to start background fetch tasks from replication queues which stored in Zookeeper for tables in the ReplicatedMergeTree family. Possible background tasks types - merges, fetches, mutation, DDL statements with ON CLUSTER clause: SYSTEM START REPLICATION QUEUES [[db.]replicated_merge_tree_family_table_name]  "},{"title":"SYNC REPLICA​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-sync-replica","content":"Wait until a ReplicatedMergeTree table will be synced with other replicas in a cluster. Will run until receive_timeout if fetches currently disabled for the table. SYSTEM SYNC REPLICA [db.]replicated_merge_tree_family_table_name  After running this statement the [db.]replicated_merge_tree_family_table_name fetches commands from the common replicated log into its own replication queue, and then the query waits till the replica processes all of the fetched commands. "},{"title":"RESTART REPLICA​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-restart-replica","content":"Provides possibility to reinitialize Zookeeper sessions state for ReplicatedMergeTree table, will compare current state with Zookeeper as source of true and add tasks to Zookeeper queue if needed. Initialization replication queue based on ZooKeeper date happens in the same way as ATTACH TABLE statement. For a short time the table will be unavailable for any operations. SYSTEM RESTART REPLICA [db.]replicated_merge_tree_family_table_name  "},{"title":"RESTORE REPLICA​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-restore-replica","content":"Restores a replica if data is [possibly] present but Zookeeper metadata is lost. Works only on readonly ReplicatedMergeTree tables. One may execute query after: ZooKeeper root / loss.Replicas path /replicas loss.Individual replica path /replicas/replica_name/ loss. Replica attaches locally found parts and sends info about them to Zookeeper. Parts present on a replica before metadata loss are not re-fetched from other ones if not being outdated (so replica restoration does not mean re-downloading all data over the network). warning Parts in all states are moved to detached/ folder. Parts active before data loss (committed) are attached. Syntax SYSTEM RESTORE REPLICA [db.]replicated_merge_tree_family_table_name [ON CLUSTER cluster_name]  Alternative syntax: SYSTEM RESTORE REPLICA [ON CLUSTER cluster_name] [db.]replicated_merge_tree_family_table_name  Example Creating a table on multiple servers. After the replica's metadata in ZooKeeper is lost, the table will attach as read-only as metadata is missing. The last query needs to execute on every replica. CREATE TABLE test(n UInt32) ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/', '{replica}') ORDER BY n PARTITION BY n % 10; INSERT INTO test SELECT * FROM numbers(1000); -- zookeeper_delete_path(&quot;/clickhouse/tables/test&quot;, recursive=True) &lt;- root loss. SYSTEM RESTART REPLICA test; SYSTEM RESTORE REPLICA test;  Another way: SYSTEM RESTORE REPLICA test ON CLUSTER cluster;  "},{"title":"RESTART REPLICAS​","type":1,"pageTitle":"SYSTEM Statements","url":"en/sql-reference/statements/system#query_language-system-restart-replicas","content":"Provides possibility to reinitialize Zookeeper sessions state for all ReplicatedMergeTree tables, will compare current state with Zookeeper as source of true and add tasks to Zookeeper queue if needed "},{"title":"TRUNCATE Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/truncate","content":"TRUNCATE Statement TRUNCATE TABLE [IF EXISTS] [db.]name [ON CLUSTER cluster] Removes all data from a table. When the clause IF EXISTS is omitted, the query returns an error if the table does not exist. The TRUNCATE query is not supported for View, File, URL, Buffer and Null table engines. You can use the replication_alter_partitions_sync setting to set up waiting for actions to be executed on replicas. You can specify how long (in seconds) to wait for inactive replicas to execute TRUNCATE queries with the replication_wait_for_inactive_replica_timeout setting. note If the replication_alter_partitions_sync is set to 2 and some replicas are not active for more than the time, specified by the replication_wait_for_inactive_replica_timeout setting, then an exception UNFINISHED is thrown.","keywords":""},{"title":"USE Statement","type":0,"sectionRef":"#","url":"en/sql-reference/statements/use","content":"USE Statement USE db Lets you set the current database for the session. The current database is used for searching for tables if the database is not explicitly defined in the query with a dot before the table name. This query can’t be made when using the HTTP protocol, since there is no concept of a session.","keywords":""},{"title":"WATCH Statement (Experimental)","type":0,"sectionRef":"#","url":"en/sql-reference/statements/watch","content":"","keywords":""},{"title":"Virtual columns​","type":1,"pageTitle":"WATCH Statement (Experimental)","url":"en/sql-reference/statements/watch#watch-virtual-columns","content":"The virtual _version column in the query result indicates the current result version. Example: CREATE LIVE VIEW lv WITH REFRESH 5 AS SELECT now(); WATCH lv;  ┌───────────────now()─┬─_version─┐ │ 2021-02-21 09:17:21 │ 1 │ └─────────────────────┴──────────┘ ┌───────────────now()─┬─_version─┐ │ 2021-02-21 09:17:26 │ 2 │ └─────────────────────┴──────────┘ ┌───────────────now()─┬─_version─┐ │ 2021-02-21 09:17:31 │ 3 │ └─────────────────────┴──────────┘ ...  By default, the requested data is returned to the client, while in conjunction with INSERT INTO it can be forwarded to a different table. Example: INSERT INTO [db.]table WATCH [db.]live_view ...  "},{"title":"EVENTS Clause​","type":1,"pageTitle":"WATCH Statement (Experimental)","url":"en/sql-reference/statements/watch#events-clause","content":"The EVENTS clause can be used to obtain a short form of the WATCH query where instead of the query result you will just get the latest query result version. WATCH [db.]live_view EVENTS;  Example: CREATE LIVE VIEW lv WITH REFRESH 5 AS SELECT now(); WATCH lv EVENTS;  ┌─version─┐ │ 1 │ └─────────┘ ┌─version─┐ │ 2 │ └─────────┘ ...  "},{"title":"LIMIT Clause​","type":1,"pageTitle":"WATCH Statement (Experimental)","url":"en/sql-reference/statements/watch#limit-clause","content":"The LIMIT n clause specifies the number of updates the WATCH query should wait for before terminating. By default there is no limit on the number of updates and therefore the query will not terminate. The value of 0 indicates that the WATCH query should not wait for any new query results and therefore will return immediately once query result is evaluated. WATCH [db.]live_view LIMIT 1;  Example: CREATE LIVE VIEW lv WITH REFRESH 5 AS SELECT now(); WATCH lv EVENTS LIMIT 1;  ┌─version─┐ │ 1 │ └─────────┘  "},{"title":"FORMAT Clause​","type":1,"pageTitle":"WATCH Statement (Experimental)","url":"en/sql-reference/statements/watch#format-clause","content":"The FORMAT clause works the same way as for the SELECT. note The JSONEachRowWithProgress format should be used when watching LIVE VIEW tables over the HTTP interface. The progress messages will be added to the output to keep the long-lived HTTP connection alive until the query result changes. The interval between progress messages is controlled using the live_view_heartbeat_interval setting. "},{"title":"Syntax","type":0,"sectionRef":"#","url":"en/sql-reference/syntax","content":"","keywords":""},{"title":"Spaces​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#spaces","content":"There may be any number of space symbols between syntactical constructions (including the beginning and end of a query). Space symbols include the space, tab, line feed, CR, and form feed. "},{"title":"Comments​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#comments","content":"ClickHouse supports either SQL-style and C-style comments: SQL-style comments start with --, #! or # and continue to the end of the line, a space after -- and #! can be omitted.C-style are from /* to */and can be multiline, spaces are not required either. "},{"title":"Keywords​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#syntax-keywords","content":"Keywords are case-insensitive when they correspond to: SQL standard. For example, SELECT, select and SeLeCt are all valid.Implementation in some popular DBMS (MySQL or Postgres). For example, DateTime is the same as datetime. You can check whether a data type name is case-sensitive in the system.data_type_families table. In contrast to standard SQL, all other keywords (including functions names) are case-sensitive. Keywords are not reserved; they are treated as such only in the corresponding context. If you use identifiers with the same name as the keywords, enclose them into double-quotes or backticks. For example, the query SELECT &quot;FROM&quot; FROM table_name is valid if the table table_name has column with the name &quot;FROM&quot;. "},{"title":"Identifiers​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#syntax-identifiers","content":"Identifiers are: Cluster, database, table, partition, and column names.Functions.Data types.Expression aliases. Identifiers can be quoted or non-quoted. The latter is preferred. Non-quoted identifiers must match the regex ^[a-zA-Z_][0-9a-zA-Z_]*$ and can not be equal to keywords. Examples: x, _1, X_y__Z123_. If you want to use identifiers the same as keywords or you want to use other symbols in identifiers, quote it using double quotes or backticks, for example, &quot;id&quot;, `id`. "},{"title":"Literals​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#literals","content":"There are numeric, string, compound, and NULL literals. "},{"title":"Numeric​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#numeric","content":"Numeric literal tries to be parsed: First, as a 64-bit signed number, using the strtoull function.If unsuccessful, as a 64-bit unsigned number, using the strtoll function.If unsuccessful, as a floating-point number using the strtod function.Otherwise, it returns an error. Literal value has the smallest type that the value fits in. For example, 1 is parsed as UInt8, but 256 is parsed as UInt16. For more information, see Data types. Examples: 1, 18446744073709551615, 0xDEADBEEF, 01, 0.1, 1e100, -1e-100, inf, nan. "},{"title":"String​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#syntax-string-literal","content":"Only string literals in single quotes are supported. The enclosed characters can be backslash-escaped. The following escape sequences have a corresponding special value: \\b, \\f, \\r, \\n, \\t, \\0, \\a, \\v, \\xHH. In all other cases, escape sequences in the format \\c, where c is any character, are converted to c. It means that you can use the sequences \\'and\\\\. The value will have the String type. In string literals, you need to escape at least ' and \\. Single quotes can be escaped with the single quote, literals 'It\\'s' and 'It''s' are equal. "},{"title":"Compound​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#compound","content":"Arrays are constructed with square brackets [1, 2, 3]. Tuples are constructed with round brackets (1, 'Hello, world!', 2). Technically these are not literals, but expressions with the array creation operator and the tuple creation operator, respectively. An array must consist of at least one item, and a tuple must have at least two items. There’s a separate case when tuples appear in the IN clause of a SELECT query. Query results can include tuples, but tuples can’t be saved to a database (except of tables with Memory engine). "},{"title":"NULL​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#null-literal","content":"Indicates that the value is missing. In order to store NULL in a table field, it must be of the Nullable type. Depending on the data format (input or output), NULL may have a different representation. For more information, see the documentation for data formats. There are many nuances to processing NULL. For example, if at least one of the arguments of a comparison operation is NULL, the result of this operation is also NULL. The same is true for multiplication, addition, and other operations. For more information, read the documentation for each operation. In queries, you can check NULL using the IS NULL and IS NOT NULL operators and the related functions isNull and isNotNull. "},{"title":"Heredoc​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#heredeoc","content":"A heredoc is a way to define a string (often multiline), while maintaining the original formatting. A heredoc is defined as a custom string literal, placed between two $ symbols, for example $heredoc$. A value between two heredocs is processed &quot;as-is&quot;. You can use a heredoc to embed snippets of SQL, HTML, or XML code, etc. Example Query: SELECT $smth$SHOW CREATE VIEW my_view$smth$;  Result: ┌─'SHOW CREATE VIEW my_view'─┐ │ SHOW CREATE VIEW my_view │ └────────────────────────────┘  "},{"title":"Functions​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#functions","content":"Function calls are written like an identifier with a list of arguments (possibly empty) in round brackets. In contrast to standard SQL, the brackets are required, even for an empty argument list. Example: now(). There are regular and aggregate functions (see the section “Aggregate functions”). Some aggregate functions can contain two lists of arguments in brackets. Example: quantile (0.9) (x). These aggregate functions are called “parametric” functions, and the arguments in the first list are called “parameters”. The syntax of aggregate functions without parameters is the same as for regular functions. "},{"title":"Operators​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#operators","content":"Operators are converted to their corresponding functions during query parsing, taking their priority and associativity into account. For example, the expression 1 + 2 * 3 + 4 is transformed to plus(plus(1, multiply(2, 3)), 4). "},{"title":"Data Types and Database Table Engines​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#data_types-and-database-table-engines","content":"Data types and table engines in the CREATE query are written the same way as identifiers or functions. In other words, they may or may not contain an argument list in brackets. For more information, see the sections “Data types,” “Table engines,” and “CREATE”. "},{"title":"Expression Aliases​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#syntax-expression_aliases","content":"An alias is a user-defined name for expression in a query. expr AS alias  AS — The keyword for defining aliases. You can define the alias for a table name or a column name in a SELECT clause without using the AS keyword. For example, `SELECT table_name_alias.column_name FROM table_name table_name_alias`. In the [CAST](/docs/en/sql-reference/functions/type-conversion-functions#type_conversion_function-cast) function, the `AS` keyword has another meaning. See the description of the function. expr — Any expression supported by ClickHouse. For example, `SELECT column_name * 2 AS double FROM some_table`. alias — Name for expr. Aliases should comply with the identifiers syntax. For example, `SELECT &quot;table t&quot;.column_name FROM table_name AS &quot;table t&quot;`.  "},{"title":"Notes on Usage​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#notes-on-usage","content":"Aliases are global for a query or subquery, and you can define an alias in any part of a query for any expression. For example, SELECT (1 AS n) + 2, n. Aliases are not visible in subqueries and between subqueries. For example, while executing the query SELECT (SELECT sum(b.a) + num FROM b) - a.a AS num FROM a ClickHouse generates the exception Unknown identifier: num. If an alias is defined for the result columns in the SELECT clause of a subquery, these columns are visible in the outer query. For example, SELECT n + m FROM (SELECT 1 AS n, 2 AS m). Be careful with aliases that are the same as column or table names. Let’s consider the following example: CREATE TABLE t ( a Int, b Int ) ENGINE = TinyLog()  SELECT argMax(a, b), sum(b) AS b FROM t  Received exception from server (version 18.14.17): Code: 184. DB::Exception: Received from localhost:9000, 127.0.0.1. DB::Exception: Aggregate function sum(b) is found inside another aggregate function in query.  In this example, we declared table t with column b. Then, when selecting data, we defined the sum(b) AS b alias. As aliases are global, ClickHouse substituted the literal b in the expression argMax(a, b) with the expression sum(b). This substitution caused the exception. You can change this default behavior by setting prefer_column_name_to_alias to 1. "},{"title":"Asterisk​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#asterisk","content":"In a SELECT query, an asterisk can replace the expression. For more information, see the section “SELECT”. "},{"title":"Expressions​","type":1,"pageTitle":"Syntax","url":"en/sql-reference/syntax#syntax-expressions","content":"An expression is a function, identifier, literal, application of an operator, expression in brackets, subquery, or asterisk. It can also contain an alias. A list of expressions is one or more expressions separated by commas. Functions and operators, in turn, can have expressions as arguments. Original article "},{"title":"Table Functions","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/","content":"Table Functions Table functions are methods for constructing tables. You can use table functions in: FROM clause of the SELECT query. The method for creating a temporary table that is available only in the current query. The table is deleted when the query finishes. CREATE TABLE AS table_function() query. It's one of the methods of creating a table. INSERT INTO TABLE FUNCTION query. warning You can’t use table functions if the allow_ddl setting is disabled. Function\tDescriptionfile\tCreates a File-engine table. merge\tCreates a Merge-engine table. numbers\tCreates a table with a single column filled with integer numbers. remote\tAllows you to access remote servers without creating a Distributed-engine table. url\tCreates a Url-engine table. mysql\tCreates a MySQL-engine table. postgresql\tCreates a PostgreSQL-engine table. jdbc\tCreates a JDBC-engine table. odbc\tCreates a ODBC-engine table. hdfs\tCreates a HDFS-engine table. s3\tCreates a S3-engine table. sqlite\tCreates a sqlite-engine table. Original article","keywords":""},{"title":"cluster, clusterAllReplicas","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/cluster","content":"cluster, clusterAllReplicas Allows to access all shards in an existing cluster which configured in remote_servers section without creating a Distributed table. One replica of each shard is queried. clusterAllReplicas function — same as cluster, but all replicas are queried. Each replica in a cluster is used as a separate shard/connection. note All available clusters are listed in the system.clusters table. Syntax cluster('cluster_name', db.table[, sharding_key]) cluster('cluster_name', db, table[, sharding_key]) clusterAllReplicas('cluster_name', db.table[, sharding_key]) clusterAllReplicas('cluster_name', db, table[, sharding_key]) Arguments cluster_name – Name of a cluster that is used to build a set of addresses and connection parameters to remote and local servers. db.table or db, table - Name of a database and a table. sharding_key - A sharding key. Optional. Needs to be specified if the cluster has more than one shard. Returned value The dataset from clusters. Using Macros cluster_name can contain macros — substitution in curly brackets. The substituted value is taken from the macros section of the server configuration file. Example: SELECT * FROM cluster('{cluster}', default.example_table); Usage and Recommendations Using the cluster and clusterAllReplicas table functions are less efficient than creating a Distributed table because in this case, the server connection is re-established for every request. When processing a large number of queries, please always create the Distributed table ahead of time, and do not use the cluster and clusterAllReplicas table functions. The cluster and clusterAllReplicas table functions can be useful in the following cases: Accessing a specific cluster for data comparison, debugging, and testing.Queries to various ClickHouse clusters and replicas for research purposes.Infrequent distributed requests that are made manually. Connection settings like host, port, user, password, compression, secure are taken from &lt;remote_servers&gt; config section. See details in Distributed engine. See Also skip_unavailable_shardsload_balancing","keywords":""},{"title":"dictionary","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/dictionary","content":"dictionary Displays the dictionary data as a ClickHouse table. Works the same way as Dictionary engine. Syntax dictionary('dict') Arguments dict — A dictionary name. String. Returned value A ClickHouse table. Example Input table dictionary_source_table: ┌─id─┬─value─┐ │ 0 │ 0 │ │ 1 │ 1 │ └────┴───────┘ Create a dictionary: CREATE DICTIONARY new_dictionary(id UInt64, value UInt64 DEFAULT 0) PRIMARY KEY id SOURCE(CLICKHOUSE(HOST 'localhost' PORT tcpPort() USER 'default' TABLE 'dictionary_source_table')) LAYOUT(DIRECT()); Query: SELECT * FROM dictionary('new_dictionary'); Result: ┌─id─┬─value─┐ │ 0 │ 0 │ │ 1 │ 1 │ └────┴───────┘ See Also Dictionary engine","keywords":""},{"title":"file","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/file","content":"","keywords":""},{"title":"Globs in Path​","type":1,"pageTitle":"file","url":"en/sql-reference/table-functions/file#globs-in-path","content":"Multiple path components can have globs. For being processed file must exist and match to the whole path pattern (not only suffix or prefix). * — Substitutes any number of any characters except / including empty string.? — Substitutes any single character.{some_string,another_string,yet_another_one} — Substitutes any of strings 'some_string', 'another_string', 'yet_another_one'.{N..M} — Substitutes any number in range from N to M including both borders. Constructions with {} are similar to the remote table function. Example Suppose we have several files with the following relative paths: 'some_dir/some_file_1''some_dir/some_file_2''some_dir/some_file_3''another_dir/some_file_1''another_dir/some_file_2''another_dir/some_file_3' Query the number of rows in these files: SELECT count(*) FROM file('{some,another}_dir/some_file_{1..3}', 'TSV', 'name String, value UInt32');  Query the number of rows in all files of these two directories: SELECT count(*) FROM file('{some,another}_dir/*', 'TSV', 'name String, value UInt32');  warning If your listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?. Example Query the data from files named file000, file001, … , file999: SELECT count(*) FROM file('big_dir/file{0..9}{0..9}{0..9}', 'CSV', 'name String, value UInt32');  "},{"title":"Virtual Columns​","type":1,"pageTitle":"file","url":"en/sql-reference/table-functions/file#virtual-columns","content":"_path — Path to the file._file — Name of the file. See Also Virtual columns Original article "},{"title":"generateRandom","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/generate","content":"","keywords":""},{"title":"Usage Example​","type":1,"pageTitle":"generateRandom","url":"en/sql-reference/table-functions/generate#usage-example","content":"SELECT * FROM generateRandom('a Array(Int8), d Decimal32(4), c Tuple(DateTime64(3), UUID)', 1, 10, 2) LIMIT 3;  ┌─a────────┬────────────d─┬─c──────────────────────────────────────────────────────────────────┐ │ [77] │ -124167.6723 │ ('2061-04-17 21:59:44.573','3f72f405-ec3e-13c8-44ca-66ef335f7835') │ │ [32,110] │ -141397.7312 │ ('1979-02-09 03:43:48.526','982486d1-5a5d-a308-e525-7bd8b80ffa73') │ │ [68] │ -67417.0770 │ ('2080-03-12 14:17:31.269','110425e5-413f-10a6-05ba-fa6b3e929f15') │ └──────────┴──────────────┴────────────────────────────────────────────────────────────────────┘  "},{"title":"hdfs","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/hdfs","content":"","keywords":""},{"title":"Virtual Columns​","type":1,"pageTitle":"hdfs","url":"en/sql-reference/table-functions/hdfs#virtual-columns","content":"_path — Path to the file._file — Name of the file. See Also Virtual columns "},{"title":"hdfsCluster Table Function","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/hdfsCluster","content":"hdfsCluster Table Function Allows processing files from HDFS in parallel from many nodes in a specified cluster. On initiator it creates a connection to all nodes in the cluster, discloses asterics in HDFS file path, and dispatches each file dynamically. On the worker node it asks the initiator about the next task to process and processes it. This is repeated until all tasks are finished. Syntax hdfsCluster(cluster_name, URI, format, structure) Arguments cluster_name — Name of a cluster that is used to build a set of addresses and connection parameters to remote and local servers.URI — URI to a file or a bunch of files. Supports following wildcards in readonly mode: *, ?, {'abc','def'} and {N..M} where N, M — numbers, abc, def — strings. For more information see Wildcards In Path.format — The format of the file.structure — Structure of the table. Format 'column1_name column1_type, column2_name column2_type, ...'. Returned value A table with the specified structure for reading data in the specified file. Examples Suppose that we have a ClickHouse cluster named cluster_simple, and several files with following URIs on HDFS: ‘hdfs://hdfs1:9000/some_dir/some_file_1’‘hdfs://hdfs1:9000/some_dir/some_file_2’‘hdfs://hdfs1:9000/some_dir/some_file_3’‘hdfs://hdfs1:9000/another_dir/some_file_1’‘hdfs://hdfs1:9000/another_dir/some_file_2’‘hdfs://hdfs1:9000/another_dir/some_file_3’ Query the amount of rows in these files: SELECT count(*) FROM hdfsCluster('cluster_simple', 'hdfs://hdfs1:9000/{some,another}_dir/some_file_{1..3}', 'TSV', 'name String, value UInt32') Query the amount of rows in all files of these two directories: SELECT count(*) FROM hdfsCluster('cluster_simple', 'hdfs://hdfs1:9000/{some,another}_dir/*', 'TSV', 'name String, value UInt32') warning If your listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?. See Also HDFS engineHDFS table function","keywords":""},{"title":"input","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/input","content":"input input(structure) - table function that allows effectively convert and insert data sent to the server with given structure to the table with another structure. structure - structure of data sent to the server in following format 'column1_name column1_type, column2_name column2_type, ...'. For example, 'id UInt32, name String'. This function can be used only in INSERT SELECT query and only once but otherwise behaves like ordinary table function (for example, it can be used in subquery, etc.). Data can be sent in any way like for ordinary INSERT query and passed in any available formatthat must be specified in the end of query (unlike ordinary INSERT SELECT). The main feature of this function is that when server receives data from client it simultaneously converts it according to the list of expressions in the SELECT clause and inserts into the target table. Temporary table with all transferred data is not created. Examples Let the test table has the following structure (a String, b String)and data in data.csv has a different structure (col1 String, col2 Date, col3 Int32). Query for insert data from the data.csv into the test table with simultaneous conversion looks like this: $ cat data.csv | clickhouse-client --query=&quot;INSERT INTO test SELECT lower(col1), col3 * col3 FROM input('col1 String, col2 Date, col3 Int32') FORMAT CSV&quot;; If data.csv contains data of the same structure test_structure as the table test then these two queries are equal: $ cat data.csv | clickhouse-client --query=&quot;INSERT INTO test FORMAT CSV&quot; $ cat data.csv | clickhouse-client --query=&quot;INSERT INTO test SELECT * FROM input('test_structure') FORMAT CSV&quot; ","keywords":""},{"title":"jdbc","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/jdbc","content":"jdbc jdbc(datasource, schema, table) - returns table that is connected via JDBC driver. This table function requires separate clickhouse-jdbc-bridge program to be running. It supports Nullable types (based on DDL of remote table that is queried). Examples SELECT * FROM jdbc('jdbc:mysql://localhost:3306/?user=root&amp;password=root', 'schema', 'table') SELECT * FROM jdbc('mysql://localhost:3306/?user=root&amp;password=root', 'select * from schema.table') SELECT * FROM jdbc('mysql-dev?p1=233', 'num Int32', 'select toInt32OrZero(''{{p1}}'') as num') SELECT * FROM jdbc('mysql-dev?p1=233', 'num Int32', 'select toInt32OrZero(''{{p1}}'') as num') SELECT a.datasource AS server1, b.datasource AS server2, b.name AS db FROM jdbc('mysql-dev?datasource_column', 'show databases') a INNER JOIN jdbc('self?datasource_column', 'show databases') b ON a.Database = b.name Original article","keywords":""},{"title":"merge","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/merge","content":"merge Creates a temporary Merge table. The table structure is taken from the first table encountered that matches the regular expression. Syntax merge('db_name', 'tables_regexp') Arguments db_name — Possible values: database name, constant expression that returns a string with a database name, for example, currentDatabase(),REGEXP(expression), where expression is a regular expression to match the DB names. tables_regexp — A regular expression to match the table names in the specified DB or DBs. See Also Merge table engine","keywords":""},{"title":"mysql","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/mysql","content":"mysql Allows SELECT and INSERT queries to be performed on data that is stored on a remote MySQL server. Syntax mysql('host:port', 'database', 'table', 'user', 'password'[, replace_query, 'on_duplicate_clause']) Arguments host:port — MySQL server address. database — Remote database name. table — Remote table name. user — MySQL user. password — User password. replace_query — Flag that converts INSERT INTO queries to REPLACE INTO. Possible values: 0 - The query is executed as INSERT INTO.1 - The query is executed as REPLACE INTO. on_duplicate_clause — The ON DUPLICATE KEY on_duplicate_clause expression that is added to the INSERT query. Can be specified only with replace_query = 0 (if you simultaneously pass replace_query = 1 and on_duplicate_clause, ClickHouse generates an exception). Example: INSERT INTO t (c1,c2) VALUES ('a', 2) ON DUPLICATE KEY UPDATE c2 = c2 + 1; on_duplicate_clause here is UPDATE c2 = c2 + 1. See the MySQL documentation to find which on_duplicate_clause you can use with the ON DUPLICATE KEY clause. Simple WHERE clauses such as =, !=, &gt;, &gt;=, &lt;, &lt;= are currently executed on the MySQL server. The rest of the conditions and the LIMIT sampling constraint are executed in ClickHouse only after the query to MySQL finishes. Supports multiple replicas that must be listed by |. For example: SELECT name FROM mysql(`mysql{1|2|3}:3306`, 'mysql_database', 'mysql_table', 'user', 'password'); or SELECT name FROM mysql(`mysql1:3306|mysql2:3306|mysql3:3306`, 'mysql_database', 'mysql_table', 'user', 'password'); Returned Value A table object with the same columns as the original MySQL table. note In the INSERT query to distinguish table function mysql(...) from table name with column names list, you must use keywords FUNCTION or TABLE FUNCTION. See examples below. Examples Table in MySQL: mysql&gt; CREATE TABLE `test`.`test` ( -&gt; `int_id` INT NOT NULL AUTO_INCREMENT, -&gt; `float` FLOAT NOT NULL, -&gt; PRIMARY KEY (`int_id`)); mysql&gt; INSERT INTO test (`int_id`, `float`) VALUES (1,2); mysql&gt; SELECT * FROM test; +--------+-------+ | int_id | float | +--------+-------+ | 1 | 2 | +--------+-------+ Selecting data from ClickHouse: SELECT * FROM mysql('localhost:3306', 'test', 'test', 'bayonet', '123'); ┌─int_id─┬─float─┐ │ 1 │ 2 │ └────────┴───────┘ Replacing and inserting: INSERT INTO FUNCTION mysql('localhost:3306', 'test', 'test', 'bayonet', '123', 1) (int_id, float) VALUES (1, 3); INSERT INTO TABLE FUNCTION mysql('localhost:3306', 'test', 'test', 'bayonet', '123', 0, 'UPDATE int_id = int_id + 1') (int_id, float) VALUES (1, 4); SELECT * FROM mysql('localhost:3306', 'test', 'test', 'bayonet', '123'); ┌─int_id─┬─float─┐ │ 1 │ 3 │ │ 2 │ 4 │ └────────┴───────┘ See Also The ‘MySQL’ table engineUsing MySQL as a source of external dictionary Original article","keywords":""},{"title":"null","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/null","content":"null Creates a temporary table of the specified structure with the Null table engine. According to the Null-engine properties, the table data is ignored and the table itself is immediately droped right after the query execution. The function is used for the convenience of test writing and demonstrations. Syntax null('structure') Parameter structure — A list of columns and column types. String. Returned value A temporary Null-engine table with the specified structure. Example Query with the null function: INSERT INTO function null('x UInt64') SELECT * FROM numbers_mt(1000000000); can replace three queries: CREATE TABLE t (x UInt64) ENGINE = Null; INSERT INTO t SELECT * FROM numbers_mt(1000000000); DROP TABLE IF EXISTS t; See also: Null table engine Original article","keywords":""},{"title":"numbers","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/numbers","content":"numbers numbers(N) – Returns a table with the single ‘number’ column (UInt64) that contains integers from 0 to N-1.numbers(N, M) - Returns a table with the single ‘number’ column (UInt64) that contains integers from N to (N + M - 1). Similar to the system.numbers table, it can be used for testing and generating successive values, numbers(N, M) more efficient than system.numbers. The following queries are equivalent: SELECT * FROM numbers(10); SELECT * FROM numbers(0, 10); SELECT * FROM system.numbers LIMIT 10; Examples: -- Generate a sequence of dates from 2010-01-01 to 2010-12-31 select toDate('2010-01-01') + number as d FROM numbers(365); ","keywords":""},{"title":"odbc","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/odbc","content":"","keywords":""},{"title":"Usage Example​","type":1,"pageTitle":"odbc","url":"en/sql-reference/table-functions/odbc#usage-example","content":"Getting data from the local MySQL installation via ODBC This example is checked for Ubuntu Linux 18.04 and MySQL server 5.7. Ensure that unixODBC and MySQL Connector are installed. By default (if installed from packages), ClickHouse starts as user clickhouse. Thus you need to create and configure this user in the MySQL server. $ sudo mysql  mysql&gt; CREATE USER 'clickhouse'@'localhost' IDENTIFIED BY 'clickhouse'; mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'clickhouse'@'clickhouse' WITH GRANT OPTION;  Then configure the connection in /etc/odbc.ini. $ cat /etc/odbc.ini [mysqlconn] DRIVER = /usr/local/lib/libmyodbc5w.so SERVER = 127.0.0.1 PORT = 3306 DATABASE = test USERNAME = clickhouse PASSWORD = clickhouse  You can check the connection using the isql utility from the unixODBC installation. $ isql -v mysqlconn +-------------------------+ | Connected! | | | ...  Table in MySQL: mysql&gt; CREATE TABLE `test`.`test` ( -&gt; `int_id` INT NOT NULL AUTO_INCREMENT, -&gt; `int_nullable` INT NULL DEFAULT NULL, -&gt; `float` FLOAT NOT NULL, -&gt; `float_nullable` FLOAT NULL DEFAULT NULL, -&gt; PRIMARY KEY (`int_id`)); Query OK, 0 rows affected (0,09 sec) mysql&gt; insert into test (`int_id`, `float`) VALUES (1,2); Query OK, 1 row affected (0,00 sec) mysql&gt; select * from test; +------+----------+-----+----------+ | int_id | int_nullable | float | float_nullable | +------+----------+-----+----------+ | 1 | NULL | 2 | NULL | +------+----------+-----+----------+ 1 row in set (0,00 sec)  Retrieving data from the MySQL table in ClickHouse: SELECT * FROM odbc('DSN=mysqlconn', 'test', 'test')  ┌─int_id─┬─int_nullable─┬─float─┬─float_nullable─┐ │ 1 │ 0 │ 2 │ 0 │ └────────┴──────────────┴───────┴────────────────┘  "},{"title":"See Also​","type":1,"pageTitle":"odbc","url":"en/sql-reference/table-functions/odbc#see-also","content":"ODBC external dictionariesODBC table engine. "},{"title":"postgresql","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/postgresql","content":"","keywords":""},{"title":"Implementation Details​","type":1,"pageTitle":"postgresql","url":"en/sql-reference/table-functions/postgresql#implementation-details","content":"SELECT queries on PostgreSQL side run as COPY (SELECT ...) TO STDOUT inside read-only PostgreSQL transaction with commit after each SELECT query. Simple WHERE clauses such as =, !=, &gt;, &gt;=, &lt;, &lt;=, and IN are executed on the PostgreSQL server. All joins, aggregations, sorting, IN [ array ] conditions and the LIMIT sampling constraint are executed in ClickHouse only after the query to PostgreSQL finishes. INSERT queries on PostgreSQL side run as COPY &quot;table_name&quot; (field1, field2, ... fieldN) FROM STDIN inside PostgreSQL transaction with auto-commit after each INSERT statement. PostgreSQL Array types converts into ClickHouse arrays. note Be careful, in PostgreSQL an array data type column like Integer[] may contain arrays of different dimensions in different rows, but in ClickHouse it is only allowed to have multidimensional arrays of the same dimension in all rows. Supports multiple replicas that must be listed by |. For example: SELECT name FROM postgresql(`postgres{1|2|3}:5432`, 'postgres_database', 'postgres_table', 'user', 'password');  or SELECT name FROM postgresql(`postgres1:5431|postgres2:5432`, 'postgres_database', 'postgres_table', 'user', 'password');  Supports replicas priority for PostgreSQL dictionary source. The bigger the number in map, the less the priority. The highest priority is 0. Examples Table in PostgreSQL: postgres=# CREATE TABLE &quot;public&quot;.&quot;test&quot; ( &quot;int_id&quot; SERIAL, &quot;int_nullable&quot; INT NULL DEFAULT NULL, &quot;float&quot; FLOAT NOT NULL, &quot;str&quot; VARCHAR(100) NOT NULL DEFAULT '', &quot;float_nullable&quot; FLOAT NULL DEFAULT NULL, PRIMARY KEY (int_id)); CREATE TABLE postgres=# INSERT INTO test (int_id, str, &quot;float&quot;) VALUES (1,'test',2); INSERT 0 1 postgresql&gt; SELECT * FROM test; int_id | int_nullable | float | str | float_nullable --------+--------------+-------+------+---------------- 1 | | 2 | test | (1 row)  Selecting data from ClickHouse: SELECT * FROM postgresql('localhost:5432', 'test', 'test', 'postgresql_user', 'password') WHERE str IN ('test');  ┌─int_id─┬─int_nullable─┬─float─┬─str──┬─float_nullable─┐ │ 1 │ ᴺᵁᴸᴸ │ 2 │ test │ ᴺᵁᴸᴸ │ └────────┴──────────────┴───────┴──────┴────────────────┘  Inserting: INSERT INTO TABLE FUNCTION postgresql('localhost:5432', 'test', 'test', 'postgrsql_user', 'password') (int_id, float) VALUES (2, 3); SELECT * FROM postgresql('localhost:5432', 'test', 'test', 'postgresql_user', 'password');  ┌─int_id─┬─int_nullable─┬─float─┬─str──┬─float_nullable─┐ │ 1 │ ᴺᵁᴸᴸ │ 2 │ test │ ᴺᵁᴸᴸ │ │ 2 │ ᴺᵁᴸᴸ │ 3 │ │ ᴺᵁᴸᴸ │ └────────┴──────────────┴───────┴──────┴────────────────┘  Using Non-default Schema: postgres=# CREATE SCHEMA &quot;nice.schema&quot;; postgres=# CREATE TABLE &quot;nice.schema&quot;.&quot;nice.table&quot; (a integer); postgres=# INSERT INTO &quot;nice.schema&quot;.&quot;nice.table&quot; SELECT i FROM generate_series(0, 99) as t(i)  CREATE TABLE pg_table_schema_with_dots (a UInt32) ENGINE PostgreSQL('localhost:5432', 'clickhouse', 'nice.table', 'postgrsql_user', 'password', 'nice.schema');  See Also The PostgreSQL table engineUsing PostgreSQL as a source of external dictionary Original article "},{"title":"remote, remoteSecure","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/remote","content":"","keywords":""},{"title":"Globs in Addresses {globs-in-addresses}​","type":1,"pageTitle":"remote, remoteSecure","url":"en/sql-reference/table-functions/remote#globs-in-addresses-globs-in-addresses","content":"Patterns in curly brackets { } are used to generate a set of shards and to specify replicas. If there are multiple pairs of curly brackets, then the direct product of the corresponding sets is generated. The following pattern types are supported. {a,b} - Any number of variants separated by a comma. The pattern is replaced with a in the first shard address and it is replaced with b in the second shard address and so on. For instance, example0{1,2}-1 generates addresses example01-1 and example02-1.{n..m} - A range of numbers. This pattern generates shard addresses with incrementing indices from n to m. example0{1..2}-1 generates example01-1 and example02-1.{0n..0m} - A range of numbers with leading zeroes. This modification preserves leading zeroes in indices. The pattern example{01..03}-1 generates example01-1, example02-1 and example03-1.{a|b} - Any number of variants separated by a |. The pattern specifies replicas. For instance, example01-{1|2} generates replicas example01-1 and example01-2. The query will be sent to the first healthy replica. However, for remote the replicas are iterated in the order currently set in the load_balancing setting. The number of generated addresses is limited by table_function_remote_max_addresses setting. "},{"title":"s3 Table Function","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/s3","content":"","keywords":""},{"title":"Usage​","type":1,"pageTitle":"s3 Table Function","url":"en/sql-reference/table-functions/s3#usage-examples","content":"Suppose that we have several files with following URIs on S3: 'https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_1.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_2.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_3.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/some_prefix/some_file_4.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_1.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_2.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_3.csv''https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/another_prefix/some_file_4.csv' Count the amount of rows in files ending with numbers from 1 to 3: SELECT count(*) FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/{some,another}_prefix/some_file_{1..3}.csv', 'CSV', 'name String, value UInt32')  ┌─count()─┐ │ 18 │ └─────────┘  Count the total amount of rows in all files in these two directories: SELECT count(*) FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/{some,another}_prefix/*', 'CSV', 'name String, value UInt32')  ┌─count()─┐ │ 24 │ └─────────┘  warning If your listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?. Count the total amount of rows in files named file-000.csv, file-001.csv, … , file-999.csv: SELECT count(*) FROM s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/big_prefix/file-{000..999}.csv', 'CSV', 'name String, value UInt32');  ┌─count()─┐ │ 12 │ └─────────┘  Insert data into file test-data.csv.gz: INSERT INTO FUNCTION s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip') VALUES ('test-data', 1), ('test-data-2', 2);  Insert data into file test-data.csv.gz from existing table: INSERT INTO FUNCTION s3('https://clickhouse-public-datasets.s3.amazonaws.com/my-test-bucket-768/test-data.csv.gz', 'CSV', 'name String, value UInt32', 'gzip') SELECT name, value FROM existing_table;  "},{"title":"Partitioned Write​","type":1,"pageTitle":"s3 Table Function","url":"en/sql-reference/table-functions/s3#partitioned-write","content":"If you specify PARTITION BY expression when inserting data into S3 table, a separate file is created for each partition value. Splitting the data into separate files helps to improve reading operations efficiency. Examples Using partition ID in a key creates separate files: INSERT INTO TABLE FUNCTION s3('http://bucket.amazonaws.com/my_bucket/file_{_partition_id}.csv', 'CSV', 'a String, b UInt32, c UInt32') PARTITION BY a VALUES ('x', 2, 3), ('x', 4, 5), ('y', 11, 12), ('y', 13, 14), ('z', 21, 22), ('z', 23, 24);  As a result, the data is written into three files: file_x.csv, file_y.csv, and file_z.csv. Using partition ID in a bucket name creates files in different buckets: INSERT INTO TABLE FUNCTION s3('http://bucket.amazonaws.com/my_bucket_{_partition_id}/file.csv', 'CSV', 'a UInt32, b UInt32, c UInt32') PARTITION BY a VALUES (1, 2, 3), (1, 4, 5), (10, 11, 12), (10, 13, 14), (20, 21, 22), (20, 23, 24);  As a result, the data is written into three files in different buckets: my_bucket_1/file.csv, my_bucket_10/file.csv, and my_bucket_20/file.csv. See Also S3 engine Original article "},{"title":"s3Cluster Table Function","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/s3Cluster","content":"s3Cluster Table Function Allows processing files from Amazon S3 in parallel from many nodes in a specified cluster. On initiator it creates a connection to all nodes in the cluster, discloses asterics in S3 file path, and dispatches each file dynamically. On the worker node it asks the initiator about the next task to process and processes it. This is repeated until all tasks are finished. Syntax s3Cluster(cluster_name, source, [access_key_id, secret_access_key,] format, structure) Arguments cluster_name — Name of a cluster that is used to build a set of addresses and connection parameters to remote and local servers.source — URL to a file or a bunch of files. Supports following wildcards in readonly mode: *, ?, {'abc','def'} and {N..M} where N, M — numbers, abc, def — strings. For more information see Wildcards In Path.access_key_id and secret_access_key — Keys that specify credentials to use with given endpoint. Optional.format — The format of the file.structure — Structure of the table. Format 'column1_name column1_type, column2_name column2_type, ...'. Returned value A table with the specified structure for reading or writing data in the specified file. Examples Select the data from all files in the cluster cluster_simple: SELECT * FROM s3Cluster('cluster_simple', 'http://minio1:9001/root/data/{clickhouse,database}/*', 'minio', 'minio123', 'CSV', 'name String, value UInt32, polygon Array(Array(Tuple(Float64, Float64)))') ORDER BY (name, value, polygon); Count the total amount of rows in all files in the cluster cluster_simple: SELECT count(*) FROM s3Cluster('cluster_simple', 'http://minio1:9001/root/data/{clickhouse,database}/*', 'minio', 'minio123', 'CSV', 'name String, value UInt32, polygon Array(Array(Tuple(Float64, Float64)))'); warning If your listing of files contains number ranges with leading zeros, use the construction with braces for each digit separately or use ?. See Also S3 engines3 table function","keywords":""},{"title":"sqlite","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/sqlite","content":"","keywords":""},{"title":"sqlite​","type":1,"pageTitle":"sqlite","url":"en/sql-reference/table-functions/sqlite#sqlite","content":"Allows to perform queries on a data stored in an SQLite database. Syntax  sqlite('db_path', 'table_name')  Arguments db_path — Path to a file with an SQLite database. String.table_name — Name of a table in the SQLite database. String. Returned value A table object with the same columns as in the original SQLite table. Example Query: SELECT * FROM sqlite('sqlite.db', 'table1') ORDER BY col2;  Result: ┌─col1──┬─col2─┐ │ line1 │ 1 │ │ line2 │ 2 │ │ line3 │ 3 │ └───────┴──────┘  See Also SQLite table engine "},{"title":"url","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/url","content":"","keywords":""},{"title":"Globs in URL {globs-in-url}​","type":1,"pageTitle":"url","url":"en/sql-reference/table-functions/url#globs-in-url-globs-in-url","content":"Patterns in curly brackets { } are used to generate a set of shards or to specify failover addresses. Supported pattern types and examples see in the description of the remote function. Character | inside patterns is used to specify failover addresses. They are iterated in the same order as listed in the pattern. The number of generated addresses is limited by glob_expansion_max_elements setting. "},{"title":"view","type":0,"sectionRef":"#","url":"en/sql-reference/table-functions/view","content":"","keywords":""},{"title":"view​","type":1,"pageTitle":"view","url":"en/sql-reference/table-functions/view#view","content":"Turns a subquery into a table. The function implements views (see CREATE VIEW). The resulting table does not store data, but only stores the specified SELECT query. When reading from the table, ClickHouse executes the query and deletes all unnecessary columns from the result. Syntax view(subquery)  Arguments subquery — SELECT query. Returned value A table. Example Input table: ┌─id─┬─name─────┬─days─┐ │ 1 │ January │ 31 │ │ 2 │ February │ 29 │ │ 3 │ March │ 31 │ │ 4 │ April │ 30 │ └────┴──────────┴──────┘  Query: SELECT * FROM view(SELECT name FROM months);  Result: ┌─name─────┐ │ January │ │ February │ │ March │ │ April │ └──────────┘  You can use the view function as a parameter of the remote and cluster table functions: SELECT * FROM remote(`127.0.0.1`, view(SELECT a, b, c FROM table_name));  SELECT * FROM cluster(`cluster_name`, view(SELECT a, b, c FROM table_name));  See Also View Table Engine Original article "},{"title":"Window Functions","type":0,"sectionRef":"#","url":"en/sql-reference/window-functions/","content":"","keywords":""},{"title":"References​","type":1,"pageTitle":"Window Functions","url":"en/sql-reference/window-functions/#references","content":""},{"title":"GitHub Issues​","type":1,"pageTitle":"Window Functions","url":"en/sql-reference/window-functions/#github-issues","content":"The roadmap for the initial support of window functions is in this issue. All GitHub issues related to window funtions have the comp-window-functions tag. "},{"title":"Tests​","type":1,"pageTitle":"Window Functions","url":"en/sql-reference/window-functions/#tests","content":"These tests contain the examples of the currently supported grammar: https://github.com/ClickHouse/ClickHouse/blob/master/tests/performance/window_functions.xml https://github.com/ClickHouse/ClickHouse/blob/master/tests/queries/0_stateless/01591_window_functions.sql "},{"title":"Postgres Docs​","type":1,"pageTitle":"Window Functions","url":"en/sql-reference/window-functions/#postgres-docs","content":"https://www.postgresql.org/docs/current/sql-select.html#SQL-WINDOW https://www.postgresql.org/docs/devel/sql-expressions.html#SYNTAX-WINDOW-FUNCTIONS https://www.postgresql.org/docs/devel/functions-window.html https://www.postgresql.org/docs/devel/tutorial-window.html "},{"title":"MySQL Docs​","type":1,"pageTitle":"Window Functions","url":"en/sql-reference/window-functions/#mysql-docs","content":"https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html https://dev.mysql.com/doc/refman/8.0/en/window-functions-usage.html https://dev.mysql.com/doc/refman/8.0/en/window-functions-frames.html "},{"title":"Roadmap","type":0,"sectionRef":"#","url":"en/whats-new/roadmap","content":"Roadmap The roadmap for the year 2022 is published for open discussion here.","keywords":""},{"title":"Security Changelog","type":0,"sectionRef":"#","url":"en/whats-new/security-changelog","content":"","keywords":""},{"title":"Fixed in ClickHouse 21.10.2.15, 2021-10-18​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#fixed-in-clickhouse-release-21-10-2-215-2021-10-18","content":""},{"title":"CVE-2021-43304​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2021-43304","content":"Heap buffer overflow in Clickhouse's LZ4 compression codec when parsing a malicious query. There is no verification that the copy operations in the LZ4::decompressImpl loop and especially the arbitrary copy operation wildCopy&lt;copy_amount&gt;(op, ip, copy_end), don’t exceed the destination buffer’s limits. Credits: JFrog Security Research Team "},{"title":"CVE-2021-43305​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2021-43305","content":"Heap buffer overflow in Clickhouse's LZ4 compression codec when parsing a malicious query. There is no verification that the copy operations in the LZ4::decompressImpl loop and especially the arbitrary copy operation wildCopy&lt;copy_amount&gt;(op, ip, copy_end), don’t exceed the destination buffer’s limits. This issue is very similar to CVE-2021-43304, but the vulnerable copy operation is in a different wildCopy call. Credits: JFrog Security Research Team "},{"title":"CVE-2021-42387​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2021-42387","content":"Heap out-of-bounds read in Clickhouse's LZ4 compression codec when parsing a malicious query. As part of the LZ4::decompressImpl() loop, a 16-bit unsigned user-supplied value ('offset') is read from the compressed data. The offset is later used in the length of a copy operation, without checking the upper bounds of the source of the copy operation. Credits: JFrog Security Research Team "},{"title":"CVE-2021-42388​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2021-42388","content":"Heap out-of-bounds read in Clickhouse's LZ4 compression codec when parsing a malicious query. As part of the LZ4::decompressImpl() loop, a 16-bit unsigned user-supplied value ('offset') is read from the compressed data. The offset is later used in the length of a copy operation, without checking the lower bounds of the source of the copy operation. Credits: JFrog Security Research Team "},{"title":"CVE-2021-42389​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2021-42389","content":"Divide-by-zero in Clickhouse's Delta compression codec when parsing a malicious query. The first byte of the compressed buffer is used in a modulo operation without being checked for 0. Credits: JFrog Security Research Team "},{"title":"CVE-2021-42390​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2021-42390","content":"Divide-by-zero in Clickhouse's DeltaDouble compression codec when parsing a malicious query. The first byte of the compressed buffer is used in a modulo operation without being checked for 0. Credits: JFrog Security Research Team "},{"title":"CVE-2021-42391​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2021-42391","content":"Divide-by-zero in Clickhouse's Gorilla compression codec when parsing a malicious query. The first byte of the compressed buffer is used in a modulo operation without being checked for 0. Credits: JFrog Security Research Team "},{"title":"Fixed in ClickHouse 21.4.3.21, 2021-04-12​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#fixed-in-clickhouse-release-21-4-3-21-2021-04-12","content":""},{"title":"CVE-2021-25263​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2021-25263","content":"An attacker that has CREATE DICTIONARY privilege, can read arbitary file outside permitted directory. Fix has been pushed to versions 20.8.18.32-lts, 21.1.9.41-stable, 21.2.9.41-stable, 21.3.6.55-lts, 21.4.3.21-stable and later. Credits: Vyacheslav Egoshin "},{"title":"Fixed in ClickHouse Release 19.14.3.3, 2019-09-10​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#fixed-in-clickhouse-release-19-14-3-3-2019-09-10","content":""},{"title":"CVE-2019-15024​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2019-15024","content":"Аn attacker that has write access to ZooKeeper and who can run a custom server available from the network where ClickHouse runs, can create a custom-built malicious server that will act as a ClickHouse replica and register it in ZooKeeper. When another replica will fetch data part from the malicious replica, it can force clickhouse-server to write to arbitrary path on filesystem. Credits: Eldar Zaitov of Yandex Information Security Team "},{"title":"CVE-2019-16535​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2019-16535","content":"Аn OOB read, OOB write and integer underflow in decompression algorithms can be used to achieve RCE or DoS via native protocol. Credits: Eldar Zaitov of Yandex Information Security Team "},{"title":"CVE-2019-16536​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2019-16536","content":"Stack overflow leading to DoS can be triggered by a malicious authenticated client. Credits: Eldar Zaitov of Yandex Information Security Team "},{"title":"Fixed in ClickHouse Release 19.13.6.1, 2019-09-20​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#fixed-in-clickhouse-release-19-13-6-1-2019-09-20","content":""},{"title":"CVE-2019-18657​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2019-18657","content":"Table function url had the vulnerability allowed the attacker to inject arbitrary HTTP headers in the request. Credits: Nikita Tikhomirov "},{"title":"Fixed in ClickHouse Release 18.12.13, 2018-09-10​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#fixed-in-clickhouse-release-18-12-13-2018-09-10","content":""},{"title":"CVE-2018-14672​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2018-14672","content":"Functions for loading CatBoost models allowed path traversal and reading arbitrary files through error messages. Credits: Andrey Krasichkov of Yandex Information Security Team "},{"title":"Fixed in ClickHouse Release 18.10.3, 2018-08-13​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#fixed-in-clickhouse-release-18-10-3-2018-08-13","content":""},{"title":"CVE-2018-14671​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2018-14671","content":"unixODBC allowed loading arbitrary shared objects from the file system which led to a Remote Code Execution vulnerability. Credits: Andrey Krasichkov and Evgeny Sidorov of Yandex Information Security Team "},{"title":"Fixed in ClickHouse Release 1.1.54388, 2018-06-28​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#fixed-in-clickhouse-release-1-1-54388-2018-06-28","content":""},{"title":"CVE-2018-14668​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2018-14668","content":"“remote” table function allowed arbitrary symbols in “user”, “password” and “default_database” fields which led to Cross Protocol Request Forgery Attacks. Credits: Andrey Krasichkov of Yandex Information Security Team "},{"title":"Fixed in ClickHouse Release 1.1.54390, 2018-07-06​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#fixed-in-clickhouse-release-1-1-54390-2018-07-06","content":""},{"title":"CVE-2018-14669​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2018-14669","content":"ClickHouse MySQL client had “LOAD DATA LOCAL INFILE” functionality enabled that allowed a malicious MySQL database read arbitrary files from the connected ClickHouse server. Credits: Andrey Krasichkov and Evgeny Sidorov of Yandex Information Security Team "},{"title":"Fixed in ClickHouse Release 1.1.54131, 2017-01-10​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#fixed-in-clickhouse-release-1-1-54131-2017-01-10","content":""},{"title":"CVE-2018-14670​","type":1,"pageTitle":"Security Changelog","url":"en/whats-new/security-changelog#cve-2018-14670","content":"Incorrect configuration in deb package could lead to the unauthorized use of the database. Credits: the UK’s National Cyber Security Centre (NCSC) "},{"title":"Что такое ClickHouse","type":0,"sectionRef":"#","url":"ru/","content":"","keywords":""},{"title":"Ключевые особенности OLAP сценария работы​","type":1,"pageTitle":"Что такое ClickHouse","url":"ru/#kliuchevye-osobennosti-olap-stsenariia-raboty","content":"подавляющее большинство запросов - на чтение;данные обновляются достаточно большими пачками (&gt; 1000 строк), а не по одной строке, или не обновляются вообще;данные добавляются в БД, но не изменяются;при чтении, вынимается достаточно большое количество строк из БД, но только небольшое подмножество столбцов;таблицы являются «широкими», то есть, содержат большое количество столбцов;запросы идут сравнительно редко (обычно не более сотни в секунду на сервер);при выполнении простых запросов, допустимы задержки в районе 50 мс;значения в столбцах достаточно мелкие - числа и небольшие строки (пример - 60 байт на URL);требуется высокая пропускная способность при обработке одного запроса (до миллиардов строк в секунду на один сервер);транзакции отсутствуют;низкие требования к консистентности данных;в запросе одна большая таблица, все таблицы кроме одной маленькие;результат выполнения запроса существенно меньше исходных данных - то есть, данные фильтруются или агрегируются; результат выполнения помещается в оперативку на одном сервере. Легко видеть, что OLAP сценарий работы существенно отличается от других распространённых сценариев работы (например, OLTP или Key-Value сценариев работы). Таким образом, не имеет никакого смысла пытаться использовать OLTP или Key-Value БД для обработки аналитических запросов, если вы хотите получить приличную производительность («выше плинтуса»). Например, если вы попытаетесь использовать для аналитики MongoDB или Redis - вы получите анекдотически низкую производительность по сравнению с OLAP-СУБД. "},{"title":"Причины, по которым столбцовые СУБД лучше подходят для OLAP сценария​","type":1,"pageTitle":"Что такое ClickHouse","url":"ru/#prichiny-po-kotorym-stolbtsovye-subd-luchshe-podkhodiat-dlia-olap-stsenariia","content":"Столбцовые СУБД лучше (от 100 раз по скорости обработки большинства запросов) подходят для OLAP сценария работы. Причины в деталях будут разъяснены ниже, а сам факт проще продемонстрировать визуально: Строковые СУБД  Столбцовые СУБД  Видите разницу? "},{"title":"По вводу-выводу​","type":1,"pageTitle":"Что такое ClickHouse","url":"ru/#po-vvodu-vyvodu","content":"Для выполнения аналитического запроса, требуется прочитать небольшое количество столбцов таблицы. В столбцовой БД для этого можно читать только нужные данные. Например, если вам требуется только 5 столбцов из 100, то следует рассчитывать на 20-кратное уменьшение ввода-вывода.Так как данные читаются пачками, то их проще сжимать. Данные, лежащие по столбцам также лучше сжимаются. За счёт этого, дополнительно уменьшается объём ввода-вывода.За счёт уменьшения ввода-вывода, больше данных влезает в системный кэш. Например, для запроса «посчитать количество записей для каждой рекламной системы», требуется прочитать один столбец «идентификатор рекламной системы», который занимает 1 байт в несжатом виде. Если большинство переходов было не с рекламных систем, то можно рассчитывать хотя бы на десятикратное сжатие этого столбца. При использовании быстрого алгоритма сжатия, возможно разжатие данных со скоростью более нескольких гигабайт несжатых данных в секунду. То есть, такой запрос может выполняться со скоростью около нескольких миллиардов строк в секунду на одном сервере. На практике, такая скорость действительно достигается. "},{"title":"По вычислениям​","type":1,"pageTitle":"Что такое ClickHouse","url":"ru/#po-vychisleniiam","content":"Так как для выполнения запроса надо обработать достаточно большое количество строк, становится актуальным диспетчеризовывать все операции не для отдельных строк, а для целых векторов, или реализовать движок выполнения запроса так, чтобы издержки на диспетчеризацию были примерно нулевыми. Если этого не делать, то при любой не слишком плохой дисковой подсистеме, интерпретатор запроса неизбежно упрётся в CPU. Имеет смысл не только хранить данные по столбцам, но и обрабатывать их, по возможности, тоже по столбцам. Есть два способа это сделать: Векторный движок. Все операции пишутся не для отдельных значений, а для векторов. То есть, вызывать операции надо достаточно редко, и издержки на диспетчеризацию становятся пренебрежимо маленькими. Код операции содержит в себе хорошо оптимизированный внутренний цикл. Кодогенерация. Для запроса генерируется код, в котором подставлены все косвенные вызовы. В «обычных» БД этого не делается, так как не имеет смысла при выполнении простых запросов. Хотя есть исключения. Например, в MemSQL кодогенерация используется для уменьшения latency при выполнении SQL запросов. Для сравнения, в аналитических СУБД требуется оптимизация throughput, а не latency. Стоит заметить, что для эффективности по CPU требуется, чтобы язык запросов был декларативным (SQL, MDX) или хотя бы векторным (J, K). То есть, чтобы запрос содержал циклы только в неявном виде, открывая возможности для оптимизации. "},{"title":"什么是ClickHouse？","type":0,"sectionRef":"#","url":"zh/","content":"","keywords":""},{"title":"OLAP场景的关键特征​","type":1,"pageTitle":"什么是ClickHouse？","url":"zh/#olapchang-jing-de-guan-jian-te-zheng","content":"绝大多数是读请求数据以相当大的批次(&gt; 1000行)更新，而不是单行更新;或者根本没有更新。已添加到数据库的数据不能修改。对于读取，从数据库中提取相当多的行，但只提取列的一小部分。宽表，即每个表包含着大量的列查询相对较少(通常每台服务器每秒查询数百次或更少)对于简单查询，允许延迟大约50毫秒列中的数据相对较小：数字和短字符串(例如，每个URL 60个字节)处理单个查询时需要高吞吐量(每台服务器每秒可达数十亿行)事务不是必须的对数据一致性要求低每个查询有一个大表。除了他以外，其他的都很小。查询结果明显小于源数据。换句话说，数据经过过滤或聚合，因此结果适合于单个服务器的RAM中 很容易可以看出，OLAP场景与其他通常业务场景(例如,OLTP或K/V)有很大的不同， 因此想要使用OLTP或Key-Value数据库去高效的处理分析查询场景，并不是非常完美的适用方案。例如，使用OLAP数据库去处理分析请求通常要优于使用MongoDB或Redis去处理分析请求。 "},{"title":"列式数据库更适合OLAP场景的原因​","type":1,"pageTitle":"什么是ClickHouse？","url":"zh/#lie-shi-shu-ju-ku-geng-gua-he-olapchang-jing-de-yuan-yin","content":"列式数据库更适合于OLAP场景(对于大多数查询而言，处理速度至少提高了100倍)，下面详细解释了原因(通过图片更有利于直观理解)： 行式  列式  看到差别了么？下面将详细介绍为什么会发生这种情况。 "},{"title":"输入/输出​","type":1,"pageTitle":"什么是ClickHouse？","url":"zh/#inputoutput","content":"针对分析类查询，通常只需要读取表的一小部分列。在列式数据库中你可以只读取你需要的数据。例如，如果只需要读取100列中的5列，这将帮助你最少减少20倍的I/O消耗。由于数据总是打包成批量读取的，所以压缩是非常容易的。同时数据按列分别存储这也更容易压缩。这进一步降低了I/O的体积。由于I/O的降低，这将帮助更多的数据被系统缓存。 例如，查询«统计每个广告平台的记录数量»需要读取«广告平台ID»这一列，它在未压缩的情况下需要1个字节进行存储。如果大部分流量不是来自广告平台，那么这一列至少可以以十倍的压缩率被压缩。当采用快速压缩算法，它的解压速度最少在十亿字节(未压缩数据)每秒。换句话说，这个查询可以在单个服务器上以每秒大约几十亿行的速度进行处理。这实际上是当前实现的速度。 "},{"title":"CPU​","type":1,"pageTitle":"什么是ClickHouse？","url":"zh/#cpu","content":"由于执行一个查询需要处理大量的行，因此在整个向量上执行所有操作将比在每一行上执行所有操作更加高效。同时这将有助于实现一个几乎没有调用成本的查询引擎。如果你不这样做，使用任何一个机械硬盘，查询引擎都不可避免的停止CPU进行等待。所以，在数据按列存储并且按列执行是很有意义的。 有两种方法可以做到这一点： 向量引擎：所有的操作都是为向量而不是为单个值编写的。这意味着多个操作之间的不再需要频繁的调用，并且调用的成本基本可以忽略不计。操作代码包含一个优化的内部循环。 代码生成：生成一段代码，包含查询中的所有操作。 这是不应该在一个通用数据库中实现的，因为这在运行简单查询时是没有意义的。但是也有例外，例如，MemSQL使用代码生成来减少处理SQL查询的延迟(只是为了比较，分析型数据库通常需要优化的是吞吐而不是延迟)。 请注意，为了提高CPU效率，查询语言必须是声明型的(SQL或MDX)， 或者至少一个向量(J，K)。 查询应该只包含隐式循环，允许进行优化。 来源文章 "},{"title":"ClickHouse Tutorial","type":0,"sectionRef":"#","url":"en/tutorial","content":"","keywords":"clickhouse install tutorial"},{"title":"What to Expect from This Tutorial?​","type":1,"pageTitle":"ClickHouse Tutorial","url":"en/tutorial#what-to-expect-from-this-tutorial","content":"In this tutorial, you will create a table and insert a large dataset (two million rows of the New York taxi data). Then you will execute queries on the dataset, including an example of how to create a dictionary from an external data source and use it to perform a JOIN. note This tutorial assumes you have already the ClickHouse server up and running as described in the Quick Start. "},{"title":"1. Create a New Table​","type":1,"pageTitle":"ClickHouse Tutorial","url":"en/tutorial#1-create-a-new-table","content":"The New York City taxi data contains the details of millions of taxi rides, with columns like pickup and dropoff times and locations, cost, tip amount, tolls, payment type and so on. Let's create a table to store this data... Either open your Play UI at http://localhost:8123/play or startup the clickhouse-client: clickhouse-client Create the following trips table in the default database: CREATE TABLE trips ( `trip_id` UInt32, `vendor_id` Enum8('1' = 1, '2' = 2, '3' = 3, '4' = 4, 'CMT' = 5, 'VTS' = 6, 'DDS' = 7, 'B02512' = 10, 'B02598' = 11, 'B02617' = 12, 'B02682' = 13, 'B02764' = 14, '' = 15), `pickup_date` Date, `pickup_datetime` DateTime, `dropoff_date` Date, `dropoff_datetime` DateTime, `store_and_fwd_flag` UInt8, `rate_code_id` UInt8, `pickup_longitude` Float64, `pickup_latitude` Float64, `dropoff_longitude` Float64, `dropoff_latitude` Float64, `passenger_count` UInt8, `trip_distance` Float64, `fare_amount` Float32, `extra` Float32, `mta_tax` Float32, `tip_amount` Float32, `tolls_amount` Float32, `ehail_fee` Float32, `improvement_surcharge` Float32, `total_amount` Float32, `payment_type` Enum8('UNK' = 0, 'CSH' = 1, 'CRE' = 2, 'NOC' = 3, 'DIS' = 4), `trip_type` UInt8, `pickup` FixedString(25), `dropoff` FixedString(25), `cab_type` Enum8('yellow' = 1, 'green' = 2, 'uber' = 3), `pickup_nyct2010_gid` Int8, `pickup_ctlabel` Float32, `pickup_borocode` Int8, `pickup_ct2010` String, `pickup_boroct2010` String, `pickup_cdeligibil` String, `pickup_ntacode` FixedString(4), `pickup_ntaname` String, `pickup_puma` UInt16, `dropoff_nyct2010_gid` UInt8, `dropoff_ctlabel` Float32, `dropoff_borocode` UInt8, `dropoff_ct2010` String, `dropoff_boroct2010` String, `dropoff_cdeligibil` String, `dropoff_ntacode` FixedString(4), `dropoff_ntaname` String, `dropoff_puma` UInt16 ) ENGINE = MergeTree PARTITION BY toYYYYMM(pickup_date) ORDER BY pickup_datetime;  "},{"title":"2. Insert the Dataset​","type":1,"pageTitle":"ClickHouse Tutorial","url":"en/tutorial#2-insert-the-dataset","content":"Now that you have a table created, let's add the NYC taxi data. It is in CSV files in S3, and you can simply load the data from there. The following command inserts ~2,000,000 rows into your trips table from two different files in S3: trips_1.tsv.gz and trips_2.tsv.gz: INSERT INTO trips SELECT * FROM s3( 'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/trips_{1..2}.gz', 'TabSeparatedWithNames' ) Wait for the INSERT to execute - it might take a moment for the 150MB of data to be downloaded. note The s3 function cleverly knows how to decompress the data, and the TabSeparatedWithNames format tells ClickHouse that the data is tab-separated and also to skip the header row of each file. When the data is finished being inserted, verify it worked: SELECT count() FROM trips You should see about 2M rows (1,999,657 rows, to be precise). note Notice how quickly and how few rows ClickHouse had to process to determine the count. You can get back the count in 0.001 seconds with only 6 rows processed. (6 just happens to be the number of parts that the trips table currently has, and parts know how many rows they have.) If you run a query that needs to hit every row, you will notice considerably more rows need to be processed, but the execution time is still blazing fast: SELECT DISTINCT(pickup_ntaname) FROM trips This query has to process 2M rows and return 190 values, but notice it does this in about 0.05 seconds. The pickup_ntaname column represents the name of the neighborhood in New York City where the taxi ride originated. "},{"title":"3. Analyze the Data​","type":1,"pageTitle":"ClickHouse Tutorial","url":"en/tutorial#3-analyze-the-data","content":"Let's see how quickly ClickHouse can process 2M rows of data... We will start with some simple and fast calculations, like computing the average tip amount (which is right on $1) SELECT avg(tip_amount) FROM trips The response is almost immediate: ┌────avg(tip_amount)─┐ │ 1.6847585806972212 │ └────────────────────┘ 1 rows in set. Elapsed: 0.113 sec. Processed 2.00 million rows, 8.00 MB (17.67 million rows/s., 70.69 MB/s.) This query computes the average cost based on the number of passengers: SELECT passenger_count, ceil(avg(total_amount),2) AS average_total_amount FROM trips GROUP BY passenger_count The passenger_count ranges from 0 to 9: ┌─passenger_count─┬─average_total_amount─┐ │ 0 │ 22.69 │ │ 1 │ 15.97 │ │ 2 │ 17.15 │ │ 3 │ 16.76 │ │ 4 │ 17.33 │ │ 5 │ 16.35 │ │ 6 │ 16.04 │ │ 7 │ 59.8 │ │ 8 │ 36.41 │ │ 9 │ 9.81 │ └─────────────────┴──────────────────────┘ 10 rows in set. Elapsed: 0.015 sec. Processed 2.00 million rows, 10.00 MB (129.00 million rows/s., 645.01 MB/s.) Here is a query that calculates the daily number of pickups per neighborhood: SELECT pickup_date, pickup_ntaname, SUM(1) AS number_of_trips FROM trips GROUP BY pickup_date, pickup_ntaname ORDER BY pickup_date ASC The result looks like: ┌─pickup_date─┬─pickup_ntaname───────────────────────────────────────────┬─number_of_trips─┐ │ 2015-07-01 │ Brooklyn Heights-Cobble Hill │ 13 │ │ 2015-07-01 │ Old Astoria │ 5 │ │ 2015-07-01 │ Flushing │ 1 │ │ 2015-07-01 │ Yorkville │ 378 │ │ 2015-07-01 │ Gramercy │ 344 │ │ 2015-07-01 │ Fordham South │ 2 │ │ 2015-07-01 │ SoHo-TriBeCa-Civic Center-Little Italy │ 621 │ │ 2015-07-01 │ Park Slope-Gowanus │ 29 │ │ 2015-07-01 │ Bushwick South │ 5 │  This query computes the length of the trip and groups the results by that value: SELECT avg(tip_amount) AS avg_tip, avg(fare_amount) AS avg_fare, avg(passenger_count) AS avg_passenger, count() AS count, truncate(date_diff('second', pickup_datetime, dropoff_datetime)/3600) as trip_minutes FROM trips WHERE trip_minutes &gt; 0 GROUP BY trip_minutes ORDER BY trip_minutes DESC The result looks like: ┌────────────avg_tip─┬───────────avg_fare─┬──────avg_passenger─┬─count─┬─trip_minutes─┐ │ 0.9800000190734863 │ 10 │ 1.5 │ 2 │ 458 │ │ 1.18236789075801 │ 14.493377928590297 │ 2.060200668896321 │ 1495 │ 23 │ │ 2.1159574744549206 │ 23.22872340425532 │ 2.4680851063829787 │ 47 │ 22 │ │ 1.1218181631781838 │ 13.681818181818182 │ 1.9090909090909092 │ 11 │ 21 │ │ 0.3218181837688793 │ 18.045454545454547 │ 2.3636363636363638 │ 11 │ 20 │ │ 2.1490000009536745 │ 17.55 │ 1.5 │ 10 │ 19 │ │ 4.537058907396653 │ 37 │ 1.7647058823529411 │ 17 │ 18 │  This query shows the number of pickups in each neighborhood, broken down by hour of the day: SELECT pickup_ntaname, toHour(pickup_datetime) as pickup_hour, SUM(1) AS pickups FROM trips WHERE pickup_ntaname != '' GROUP BY pickup_ntaname, pickup_hour ORDER BY pickup_ntaname, pickup_hour The result looks like: ┌─pickup_ntaname───────────────────────────────────────────┬─pickup_hour─┬─pickups─┐ │ Airport │ 0 │ 3509 │ │ Airport │ 1 │ 1184 │ │ Airport │ 2 │ 401 │ │ Airport │ 3 │ 152 │ │ Airport │ 4 │ 213 │ │ Airport │ 5 │ 955 │ │ Airport │ 6 │ 2161 │ │ Airport │ 7 │ 3013 │ │ Airport │ 8 │ 3601 │ │ Airport │ 9 │ 3792 │ │ Airport │ 10 │ 4546 │ │ Airport │ 11 │ 4659 │ │ Airport │ 12 │ 4621 │ │ Airport │ 13 │ 5348 │ │ Airport │ 14 │ 5889 │ │ Airport │ 15 │ 6505 │ │ Airport │ 16 │ 6119 │ │ Airport │ 17 │ 6341 │ │ Airport │ 18 │ 6173 │ │ Airport │ 19 │ 6329 │ │ Airport │ 20 │ 6271 │ │ Airport │ 21 │ 6649 │ │ Airport │ 22 │ 6356 │ │ Airport │ 23 │ 6016 │ │ Allerton-Pelham Gardens │ 4 │ 1 │ │ Allerton-Pelham Gardens │ 6 │ 1 │ │ Allerton-Pelham Gardens │ 7 │ 1 │ │ Allerton-Pelham Gardens │ 9 │ 5 │ │ Allerton-Pelham Gardens │ 10 │ 3 │ │ Allerton-Pelham Gardens │ 15 │ 1 │ │ Allerton-Pelham Gardens │ 20 │ 2 │ │ Allerton-Pelham Gardens │ 23 │ 1 │ │ Annadale-Huguenot-Prince's Bay-Eltingville │ 23 │ 1 │ │ Arden Heights │ 11 │ 1 │ Let's look at rides to LaGuardia or JFK airports, which requires all 2M rows to be processed and returns in less than 0.04 seconds: SELECT pickup_datetime, dropoff_datetime, total_amount, pickup_nyct2010_gid, dropoff_nyct2010_gid, CASE WHEN dropoff_nyct2010_gid = 138 THEN 'LGA' WHEN dropoff_nyct2010_gid = 132 THEN 'JFK' END AS airport_code, EXTRACT(YEAR FROM pickup_datetime) AS year, EXTRACT(DAY FROM pickup_datetime) AS day, EXTRACT(HOUR FROM pickup_datetime) AS hour FROM trips WHERE dropoff_nyct2010_gid IN (132, 138) ORDER BY pickup_datetime The response is: ┌─────pickup_datetime─┬────dropoff_datetime─┬─total_amount─┬─pickup_nyct2010_gid─┬─dropoff_nyct2010_gid─┬─airport_code─┬─year─┬─day─┬─hour─┐ │ 2015-07-01 00:04:14 │ 2015-07-01 00:15:29 │ 13.3 │ -34 │ 132 │ JFK │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:09:42 │ 2015-07-01 00:12:55 │ 6.8 │ 50 │ 138 │ LGA │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:23:04 │ 2015-07-01 00:24:39 │ 4.8 │ -125 │ 132 │ JFK │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:27:51 │ 2015-07-01 00:39:02 │ 14.72 │ -101 │ 138 │ LGA │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:32:03 │ 2015-07-01 00:55:39 │ 39.34 │ 48 │ 138 │ LGA │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:34:12 │ 2015-07-01 00:40:48 │ 9.95 │ -93 │ 132 │ JFK │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:38:26 │ 2015-07-01 00:49:00 │ 13.3 │ -11 │ 138 │ LGA │ 2015 │ 1 │ 0 │ │ 2015-07-01 00:41:48 │ 2015-07-01 00:44:45 │ 6.3 │ -94 │ 132 │ JFK │ 2015 │ 1 │ 0 │ │ 2015-07-01 01:06:18 │ 2015-07-01 01:14:43 │ 11.76 │ 37 │ 132 │ JFK │ 2015 │ 1 │ 1 │ note As you can see, it doesn't seem to matter what type of grouping or calculation that is being performed, ClickHouse retrieves the results almost immediately! "},{"title":"4. Create a Dictionary​","type":1,"pageTitle":"ClickHouse Tutorial","url":"en/tutorial#4-create-a-dictionary","content":"If you are new to ClickHouse, it is important to understand how dictionaries work. A dictionary is a mapping of key-&gt;value pairs that is stored in memory. They often are associated with data in a file or external database (and they can periodically update with their external data source). Let's see how to create a dictionary associated with a file in S3. The file contains 265 rows, one row for each neighborhood in NYC. The neighborhoods are mapped to the names of the NYC boroughs (NYC has 5 boroughs: the Bronx, Booklyn, Manhattan, Queens and Staten Island), and this file counts Newark Airport (EWR) as a borough as well. The LocationID column in the our file maps to the pickup_nyct2010_gid and dropoff_nyct2010_gid columns in your trips table. Here are a few rows from the CSV file: LocationID\tBorough\tZone\tservice_zone1\tEWR\tNewark Airport\tEWR 2\tQueens\tJamaica Bay\tBoro Zone 3\tBronx\tAllerton/Pelham Gardens\tBoro Zone 4\tManhattan\tAlphabet City\tYellow Zone 5\tStaten Island\tArden Heights\tBoro Zone The URL for the file is https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/taxi_zone_lookup.csv. Run the following SQL, which creates a new dictionary named taxi_zone_dictionary that is based on this file in S3: CREATE DICTIONARY taxi_zone_dictionary ( LocationID UInt16 DEFAULT 0, Borough String, Zone String, service_zone String ) PRIMARY KEY LocationID SOURCE(HTTP( url 'https://datasets-documentation.s3.eu-west-3.amazonaws.com/nyc-taxi/taxi_zone_lookup.csv' format 'CSVWithNames' )) LIFETIME(0) LAYOUT(HASHED()) note Setting LIFETIME to 0 means this dictionary will never update with its source. It is used here to not send unnecessary traffic to our S3 bucket, but in general you could specify any lifetime values you prefer. For example: LIFETIME(MIN 1 MAX 10) specifies the dictionary to update after some random time between 1 and 10 seconds. (The random time is necessary in order to distribute the load on the dictionary source when updating on a large number of servers.) Verify it worked - you should get 265 rows (one row for each neighborhood): SELECT * FROM taxi_zone_dictionary Use the dictGet function (or its variations) to retrieve a value from a dictionary. You pass in the name of the dictionary, the value you want, and the key (which in our example is the LocationID column of taxi_zone_dictionary). For example, the following query returns the Borough whose LocationID is 132 (which as we saw above is JFK airport): SELECT dictGet('taxi_zone_dictionary', 'Borough', 132) JFK is in Queens, and notice the time to retrieve the value is essentially 0: ┌─dictGet('taxi_zone_dictionary', 'Borough', 132)─┐ │ Queens │ └─────────────────────────────────────────────────┘ 1 rows in set. Elapsed: 0.004 sec. Use the dictHas function to see if a key is present in the dictionary. For example, the following query returns 1 (which is &quot;true&quot; in ClickHouse): SELECT dictHas('taxi_zone_dictionary', 132) The following query returns 0 because 4567 is not a value of LocationID in the dictionary: SELECT dictHas('taxi_zone_dictionary', 4567) Use the dictGet function to retrieve a borough's name in a query. For example: SELECT count(1) AS total, dictGetOrDefault('taxi_zone_dictionary','Borough', toUInt64(pickup_nyct2010_gid), 'Unknown') AS borough_name FROM trips WHERE dropoff_nyct2010_gid = 132 OR dropoff_nyct2010_gid = 138 GROUP BY borough_name ORDER BY total DESC This query sums up the number of taxi rides per borough that end at either the LaGuardia or JFK airport. The result looks like the following, and notice there are quite a few trips where the dropoff neighborhood is unknown: ┌─total─┬─borough_name──┐ │ 23683 │ Unknown │ │ 7053 │ Manhattan │ │ 6828 │ Brooklyn │ │ 4458 │ Queens │ │ 2670 │ Bronx │ │ 554 │ Staten Island │ │ 53 │ EWR │ └───────┴───────────────┘ 7 rows in set. Elapsed: 0.019 sec. Processed 2.00 million rows, 4.00 MB (105.70 million rows/s., 211.40 MB/s.)  "},{"title":"5. Perform a Join​","type":1,"pageTitle":"ClickHouse Tutorial","url":"en/tutorial#5-perform-a-join","content":"Let's write some queries that join the taxi_zone_dictionary with your trips table. We can start with a simple JOIN that acts similarly to the previous airport query above: SELECT count(1) AS total, Borough FROM trips JOIN taxi_zone_dictionary ON toUInt64(trips.pickup_nyct2010_gid) = taxi_zone_dictionary.LocationID WHERE dropoff_nyct2010_gid = 132 OR dropoff_nyct2010_gid = 138 GROUP BY Borough ORDER BY total DESC The response looks familiar: ┌─total─┬─Borough───────┐ │ 7053 │ Manhattan │ │ 6828 │ Brooklyn │ │ 4458 │ Queens │ │ 2670 │ Bronx │ │ 554 │ Staten Island │ │ 53 │ EWR │ └───────┴───────────────┘ 6 rows in set. Elapsed: 0.034 sec. Processed 2.00 million rows, 4.00 MB (59.14 million rows/s., 118.29 MB/s.) note Notice the output of the above JOIN query is the same as the query before it that used dictGetOrDefault (except that the Unknown values are not included). Behind the scenes, ClickHouse is actually calling the dictGet function for the taxi_zone_dictionary dictionary, but the JOIN syntax is more familiar for SQL developers. We do not use SELECT * often in ClickHouse - you should only retrieve the columns you actually need! But it is difficult to find a query that takes a long time, so this query purposely selects every column and returns every row (except there is a built-in 10,000 row maximum in the response by default), and also does a right join of every row with the dictionary: SELECT * FROM trips JOIN taxi_zone_dictionary ON trips.dropoff_nyct2010_gid = taxi_zone_dictionary.LocationID WHERE tip_amount &gt; 0 ORDER BY tip_amount DESC It is the slowest query in this tutorial, yet it only takes about 0.8 seconds to process all 2M rows. Nice! Congrats!​ Well done, you made it through the tutorial, and hopefully you have a better understanding of how to use ClickHouse. Here are some options for what to do next: View the Getting Started with ClickHouse video, an excellent introduction to ClickHouseRead how primary keys work in ClickHouse - this knowledge will move you a long ways forward along your journey to becoming a ClickHouse expertIntegrate an external data source like files, Kafka, PostgreSQL, data pipelines, or lots of other data sourcesConnect your favorite UI/BI tool to ClickHouseCheck out the SQL Reference and browse through the various functions. ClickHouse has an amazing collection of functions for transforming, processing and analyzing data "},{"title":"2022 Changelog","type":0,"sectionRef":"#","url":"en/whats-new/changelog/","content":"","keywords":""},{"title":"ClickHouse release v22.4, 2022-040-20​","type":1,"pageTitle":"2022 Changelog","url":"en/whats-new/changelog/#clickhouse-release-v224-2022-040-20","content":"Backward Incompatible Change​ Do not allow SETTINGS after FORMAT for INSERT queries (there is compatibility setting parser_settings_after_format_compact to accept such queries, but it is turned OFF by default). #35883 (Azat Khuzhin).Function yandexConsistentHash (consistent hashing algorithm by Konstantin &quot;kostik&quot; Oblakov) is renamed to kostikConsistentHash. The old name is left as an alias for compatibility. Although this change is backward compatible, we may remove the alias in subsequent releases, that's why it's recommended to update the usages of this function in your apps. #35553 (Alexey Milovidov). New Feature​ Added INTERPOLATE extension to the ORDER BY ... WITH FILL. Closes #34903. #35349 (Yakov Olkhovskiy).Profiling on Processors level (under log_processors_profiles setting, ClickHouse will write time that processor spent during execution/waiting for data to system.processors_profile_log table). #34355 (Azat Khuzhin).Added functions makeDate(year, month, day), makeDate32(year, month, day). #35628 (Alexander Gololobov). Implementation of makeDateTime() and makeDateTIme64(). #35934 (Alexander Gololobov).Support new type of quota WRITTEN BYTES to limit amount of written bytes during insert queries. #35736 (Anton Popov).Added function flattenTuple. It receives nested named Tuple as an argument and returns a flatten Tuple which elements are the paths from the original Tuple. E.g.: Tuple(a Int, Tuple(b Int, c Int)) -&gt; Tuple(a Int, b Int, c Int). flattenTuple can be used to select all paths from type Object as separate columns. #35690 (Anton Popov).Added functions arrayFirstOrNull, arrayLastOrNull. Closes #35238. #35414 (Maksim Kita).Added functions minSampleSizeContinous and minSampleSizeConversion. Author achimbab. #35360 (Maksim Kita).New functions minSampleSizeContinous and minSampleSizeConversion. #34354 (achimbab).Introduce format ProtobufList (all records as repeated messages in out Protobuf). Closes #16436. #35152 (Nikolai Kochetov).Add h3PointDistM, h3PointDistKm, h3PointDistRads, h3GetRes0Indexes, h3GetPentagonIndexes functions. #34568 (Bharat Nallan).Add toLastDayOfMonth function which rounds up a date or date with time to the last day of the month. #33501. #34394 (Habibullah Oladepo).New aggregation function groupSortedArray to obtain an array of first N values. #34055 (palegre-tiny).Added load balancing setting for [Zoo]Keeper client. Closes #29617. #30325 (小路).Add a new kind of row policies named simple. Before this PR we had two kinds or row policies: permissive and restrictive. A simple row policy adds a new filter on a table without any side-effects like it was for permissive and restrictive policies. #35345 (Vitaly Baranov).Added an ability to specify cluster secret in replicated database. #35333 (Nikita Mikhaylov).Added sanity checks on server startup (available memory and disk space, max thread count, etc). #34566 (Sergei Trifonov).INTERVAL improvement - can be used with [MILLI|MICRO|NANO]SECOND. Added toStartOf[Milli|Micro|Nano]second() functions. Added [add|subtract][Milli|Micro|Nano]seconds(). #34353 (Andrey Zvonov). Experimental Feature​ Added support for transactions for simple MergeTree tables. This feature is highly experimental and not recommended for production. Part of #22086. #24258 (tavplubix).Support schema inference for type Object in format JSONEachRow. Allow to convert columns of type Map to columns of type Object. #35629 (Anton Popov).Allow to write remote FS cache on all write operations. Add system.remote_filesystem_cache table. Add drop remote filesystem cache query. Add introspection for s3 metadata with system.remote_data_paths table. Closes #34021. Add cache option for merges by adding mode read_from_filesystem_cache_if_exists_otherwise_bypass_cache (turned on by default for merges and can also be turned on by query setting with the same name). Rename cache related settings (remote_fs_enable_cache -&gt; enable_filesystem_cache, etc). #35475 (Kseniia Sumarokova).An option to store parts metadata in RocksDB. Speed up parts loading process of MergeTree to accelerate starting up of clickhouse-server. With this improvement, clickhouse-server was able to decrease starting up time from 75 minutes to 20 seconds, with 700k mergetree parts. #32928 (李扬). Performance Improvement​ A new query plan optimization. Evaluate functions after ORDER BY when possible. As an example, for a query SELECT sipHash64(number) FROM numbers(1e8) ORDER BY number LIMIT 5, function sipHash64 would be evaluated after ORDER BY and LIMIT, which gives ~20x speed up. #35623 (Nikita Taranov).Sizes of hash tables used during aggregation now collected and used in later queries to avoid hash tables resizes. #33439 (Nikita Taranov).Improvement for hasAll function using SIMD instructions (SSE and AVX2). #27653 (youennL-cs). #35723 (Maksim Kita).Multiple changes to improve ASOF JOIN performance (1.2 - 1.6x as fast). It also adds support to use big integers. #34733 (Raúl Marín).Improve performance of ASOF JOIN if key is native integer. #35525 (Maksim Kita).Parallelization of multipart upload into S3 storage. #35343 (Sergei Trifonov).URL storage engine now downloads multiple chunks in parallel if the endpoint supports HTTP Range. Two additional settings were added, max_download_threads and max_download_buffer_size, which control maximum number of threads a single query can use to download the file and the maximum number of bytes each thread can process. #35150 (Antonio Andelic).Use multiple threads to download objects from S3. Downloading is controllable using max_download_threads and max_download_buffer_size settings. #35571 (Antonio Andelic).Narrow mutex scope when interacting with HDFS. Related to #35292. #35646 (shuchaome).Require mutations for per-table TTL only when it had been changed. #35953 (Azat Khuzhin). Improvement​ Multiple improvements for schema inference. Use some tweaks and heuristics to determine numbers, strings, arrays, tuples and maps in CSV, TSV and TSVRaw data formats. Add setting input_format_csv_use_best_effort_in_schema_inference for CSV format that enables/disables using these heuristics, if it's disabled, we treat everything as string. Add similar setting input_format_tsv_use_best_effort_in_schema_inference for TSV/TSVRaw format. These settings are enabled by default. - Add Maps support for schema inference in Values format. - Fix possible segfault in schema inference in Values format. - Allow to skip columns with unsupported types in Arrow/ORC/Parquet formats. Add corresponding settings for it: input_format_{parquet|orc|arrow}_skip_columns_with_unsupported_types_in_schema_inference. These settings are disabled by default. - Allow to convert a column with type Null to a Nullable column with all NULL values in Arrow/Parquet formats. - Allow to specify column names in schema inference via setting column_names_for_schema_inference for formats that don't contain column names (like CSV, TSV, JSONCompactEachRow, etc) - Fix schema inference in ORC/Arrow/Parquet formats in terms of working with Nullable columns. Previously all inferred types were not Nullable and it blocked reading Nullable columns from data, now it's fixed and all inferred types are always Nullable (because we cannot understand that column is Nullable or not by reading the schema). - Fix schema inference in Template format with CSV escaping rules. #35582 (Kruglov Pavel).Add parallel parsing and schema inference for format JSONAsObject. #35592 (Anton Popov).Added a support for automatic schema inference to s3Cluster table function. Synced the signatures of s3 and s3Cluster. #35544 (Nikita Mikhaylov).Added support for schema inference for hdfsCluster. #35602 (Nikita Mikhaylov).Add new setting input_format_json_read_bools_as_numbers that allows to infer and parse bools as numbers in JSON input formats. It's enabled by default. Suggested by @alexey-milovidov. #35735 (Kruglov Pavel).Improve columns ordering in schema inference for formats TSKV and JSONEachRow, closes #35640. Don't stop schema inference when reading empty row in schema inference for formats TSKV and JSONEachRow. #35724 (Kruglov Pavel).Add settings input_format_orc_case_insensitive_column_matching, input_format_arrow_case_insensitive_column_matching, and input_format_parquet_case_insensitive_column_matching which allows ClickHouse to use case insensitive matching of columns while reading data from ORC, Arrow or Parquet files. #35459 (Antonio Andelic).Added is_secure column to system.query_log which denotes if the client is using a secure connection over TCP or HTTP. #35705 (Antonio Andelic).Now kafka_num_consumers can be bigger than amount of physical cores in case of low resource machine (less than 16 cores). #35926 (alesapin).Add some basic metrics to monitor engine=Kafka tables. #35916 (filimonov).Now it's not allowed to ALTER TABLE ... RESET SETTING for non-existing settings for MergeTree engines family. Fixes #35816. #35884 (alesapin).Now some ALTER MODIFY COLUMN queries for Arrays and Nullable types can be done at metadata level without mutations. For example, alter from Array(Enum8('Option1'=1)) to Array(Enum8('Option1'=1, 'Option2'=2)). #35882 (alesapin).Added an animation to the hourglass icon to indicate to the user that a query is running. #35860 (peledni).support ALTER TABLE t DETACH PARTITION (ALL). #35794 (awakeljw).Improve projection analysis to optimize trivial queries such as count(). #35788 (Amos Bird).Support schema inference for insert select with using input table function. Get schema from insertion table instead of inferring it from the data in case of insert select from table functions that support schema inference. Closes #35639. #35760 (Kruglov Pavel).Respect remote_url_allow_hosts for Hive tables. #35743 (李扬).Implement send_logs_level for clickhouse-local. Closes #35653. #35716 (Kseniia Sumarokova).Closes #35641 Allow EPHEMERAL columns without explicit default expression. #35706 (Yakov Olkhovskiy).Add profile event counter AsyncInsertBytes about size of async INSERTs. #35644 (Alexey Milovidov).Improve the pipeline description for JOIN. #35612 (何李夫).Deduce absolute hdfs config path. #35572 (李扬).Improve pasting performance and compatibility of clickhouse-client. This helps #35501. #35541 (Amos Bird).It was possible to get stack overflow in distributed queries if one of the settings async_socket_for_remote and use_hedged_requests is enabled while parsing very deeply nested data type (at least in debug build). Closes #35509. #35524 (Kruglov Pavel).Add sizes of subcolumns to system.parts_columns table. #35488 (Anton Popov).Add explicit table info to the scan node of query plan and pipeline. #35460 (何李夫).Allow server to bind to low-numbered ports (e.g. 443). ClickHouse installation script will set cap_net_bind_service to the binary file. #35451 (Alexey Milovidov).Fix INSERT INTO table FROM INFILE: it did not display the progress bar. #35429 (xiedeyantu).Add arguments --user, --password, --host, --port for clickhouse-diagnostics tool. #35422 (李扬).Support uuid for Postgres engines. Closes #35384. #35403 (Kseniia Sumarokova).For table function s3cluster or HDFSCluster or hive, we can't get right AccessType by StorageFactory::instance().getSourceAccessType(getStorageTypeName()). This pr fix it. #35365 (李扬).Remove --testmode option for clickhouse-client, enable it unconditionally. #35354 (Kseniia Sumarokova).Don't allow wchc operation (four letter command) for clickhouse-keeper. #35320 (zhangyuli1).Add function getTypeSerializationStreams. For a specified type (which is detected from column), it returns an array with all the serialization substream paths. This function is useful mainly for developers. #35290 (李扬).If port is not specified in cluster configuration, default server port will be used. This closes #34769. #34772 (Alexey Milovidov).Use minmax index for orc/parquet file in Hive Engine. Related PR: https://github.com/ClickHouse/arrow/pull/10. #34631 (李扬).System log tables now allow to specify COMMENT in ENGINE declaration. Closes #33768. #34536 (Maksim Kita).Proper support of setting max_rows_to_read in case of reading in order of sorting key and specified limit. Previously the exception Limit for rows or bytes to read exceeded could be thrown even if query actually requires to read less amount of rows. #33230 (Anton Popov).Respect only quota &amp; period from cgroups, ignore shares (which are not really limit the number of the cores which can be used). #35815 (filimonov). Build/Testing/Packaging Improvement​ Add next batch of randomization settings in functional tests. #35047 (Kruglov Pavel).Add backward compatibility check in stress test. Closes #25088. #27928 (Kruglov Pavel).Migrate package building to nfpm - Deprecate release script in favor of packages/build - Build everything in clickhouse/binary-builder image (cleanup: clickhouse/deb-builder) - Add symbol stripping to cmake (todo: use $prefix/lib/$bin_dir/clickhouse/$binary.debug) - Fix issue with DWARF symbols - Add Alpine APK packages - Rename alien to additional_pkgs. #33664 (Mikhail f. Shiryaev).Add a night scan and upload for Coverity. #34895 (Boris Kuschel).A dedicated small package for clickhouse-keeper. #35308 (Mikhail f. Shiryaev).Running with podman was failing: it complains about specifying the same volume twice. #35978 (Roman Nikonov).Minor improvement in contrib/krb5 build configuration. #35832 (Anton Kozlov).Add a label to recognize a building task for every image. #35583 (Mikhail f. Shiryaev).Apply black formatter to python code and add a per-commit check. #35466 (Mikhail f. Shiryaev).Redo alpine image to use clean Dockerfile. Create a script in tests/ci to build both ubuntu and alpine images. Add clickhouse-keeper image (cc @nikitamikhaylov). Add build check to PullRequestCI. Add a job to a ReleaseCI. Add a job to MasterCI to build and push clickhouse/clickhouse-server:head and clickhouse/clickhouse-keeper:head images for each merged PR. #35211 (Mikhail f. Shiryaev).Fix stress-test report in CI, now we upload the runlog with information about started stress tests only once. #35093 (Mikhail f. Shiryaev).Switch to libcxx / libcxxabi from LLVM 14. #34906 (Raúl Marín).Update unixodbc to mitigate CVE-2018-7485. Note: this CVE is not relevant for ClickHouse as it implements its own isolation layer for ODBC. #35943 (Mikhail f. Shiryaev). Bug Fix​ Added settings input_format_ipv4_default_on_conversion_error, input_format_ipv6_default_on_conversion_error to allow insert of invalid ip address values as default into tables. Closes #35726. #35733 (Maksim Kita).Avoid erasing columns from a block if it doesn't exist while reading data from Hive. #35393 (lgbo).Add type checking when creating materialized view. Close: #23684. #24896 (hexiaoting).Fix formatting of INSERT INFILE queries (missing quotes). #35886 (Azat Khuzhin).Disable session_log because memory safety issue has been found by fuzzing. See #35714. #35873 (Alexey Milovidov).Avoid processing per-column TTL multiple times. #35820 (Azat Khuzhin).Fix inserts to columns of type Object in case when there is data related to several partitions in insert query. #35806 (Anton Popov).Fix bug in indexes of not presented columns in -WithNames formats that led to error INCORRECT_NUMBER_OF_COLUMNS when the number of columns is more than 256. Closes #35793. #35803 (Kruglov Pavel).Fixes #35751. #35799 (Nikolay Degterinsky).Fix for reading from HDFS in Snappy format. #35771 (shuchaome).Fix bug in conversion from custom types to string that could lead to segfault or unexpected error messages. Closes #35752. #35755 (Kruglov Pavel).Fix any/all (subquery) implementation. Closes #35489. #35727 (Kseniia Sumarokova).Fix dropping non-empty database in clickhouse-local. Closes #35692. #35711 (Kseniia Sumarokova).Fix bug in creating materialized view with subquery after server restart. Materialized view was not getting updated after inserts into underlying table after server restart. Closes #35511. #35691 (Kruglov Pavel).Fix possible Can't adjust last granule exception while reading subcolumns of experimental type Object. #35687 (Anton Popov).Enable build with JIT compilation by default. #35683 (Maksim Kita).Fix possible loss of subcolumns in experimental type Object. #35682 (Anton Popov).Fix check ASOF JOIN key nullability, close #35565. #35674 (Vladimir C).Fix part checking logic for parts with projections. Error happened when projection and main part had different types. This is similar to https://github.com/ClickHouse/ClickHouse/pull/33774 . The bug is addressed by @caoyang10. #35667 (Amos Bird).Fix server crash when large number of arguments are passed into format function. Please refer to the test file and see how to reproduce the crash. #35651 (Amos Bird).Fix usage of quotas with asynchronous inserts. #35645 (Anton Popov).Fix positional arguments with aliases. Closes #35600. #35620 (Kseniia Sumarokova).Check remote_url_allow_hosts before schema inference in URL engine Closes #35064. #35619 (Kruglov Pavel).Fix HashJoin when columns with LowCardinality type are used. This closes #35548. #35616 (Antonio Andelic).Fix possible segfault in MaterializedPostgreSQL which happened if exception occurred when data, collected in memory, was synced into underlying tables. Closes #35611. #35614 (Kseniia Sumarokova).Setting database_atomic_wait_for_drop_and_detach_synchronously worked incorrectly for ATTACH TABLE query when previously detached table is still in use, It's fixed. #35594 (tavplubix).Fix HTTP headers with named collections, add compression_method. Closes #35273. Closes #35269. #35593 (Kseniia Sumarokova).Fix s3 engine getting virtual columns. Closes #35411. #35586 (Kseniia Sumarokova).Fixed return type deduction for caseWithExpression. The type of the ELSE branch is now correctly taken into account. #35576 (Antonio Andelic).Fix parsing of IPv6 addresses longer than 39 characters. Closes #34022. #35539 (Maksim Kita).Fix cast into IPv4, IPv6 address in IN section. Fixes #35528. #35534 (Maksim Kita).Fix crash during short circuit function evaluation when one of arguments is nullable constant. Closes #35497. Closes #35496. #35502 (Maksim Kita).Fix crash for function throwIf with constant arguments. #35500 (Maksim Kita).Fix bug in Keeper which can lead to unstable client connections. Introduced in #35031. #35498 (alesapin).Fix bug in function if when resulting column type differs with resulting data type that led to logical errors like Logical error: 'Bad cast from type DB::ColumnVector&lt;int&gt; to DB::ColumnVector&lt;long&gt;'.. Closes #35367. #35476 (Kruglov Pavel).Fix excessive logging when using S3 as backend for MergeTree or as separate table engine/function. Fixes #30559. #35434 (alesapin).Now merges executed with zero copy replication (experimental) will not spam logs with message Found parts with the same min block and with the same max block as the missing part _ on replica _. Hoping that it will eventually appear as a result of a merge.. #35430 (alesapin).Skip possible exception if empty chunks appear in GroupingAggregatedTransform. #35417 (Nikita Taranov).Fix working with columns that are not needed in query in Arrow/Parquet/ORC formats, it prevents possible errors like Unsupported &lt;format&gt; type &lt;type&gt; of an input column &lt;column_name&gt; when file contains column with unsupported type and we don't use it in query. #35406 (Kruglov Pavel).Fix for local cache for remote filesystem (experimental feature) for high concurrency on corner cases. #35381 (Kseniia Sumarokova). Fix possible deadlock in cache. #35378 (Kseniia Sumarokova).Fix partition pruning in case of comparison with constant in WHERE. If column and constant had different types, overflow was possible. Query could return an incorrect empty result. This fixes #35304. #35334 (Amos Bird).Fix schema inference for TSKV format while using small max_read_buffer_size. #35332 (Kruglov Pavel).Fix mutations in tables with enabled sparse columns. #35284 (Anton Popov).Do not delay final part writing by default (fixes possible Memory limit exceeded during INSERT by adding max_insert_delayed_streams_for_parallel_write with default to 1000 for writes to s3 and disabled as before otherwise). #34780 (Azat Khuzhin). "},{"title":"ClickHouse release v22.3-lts, 2022-03-17​","type":1,"pageTitle":"2022 Changelog","url":"en/whats-new/changelog/#clickhouse-release-v223-lts-2022-03-17","content":"Backward Incompatible Change​ Make arrayCompact function behave as other higher-order functions: perform compaction not of lambda function results but on the original array. If you're using nontrivial lambda functions in arrayCompact you may restore old behaviour by wrapping arrayCompact arguments into arrayMap. Closes #34010 #18535 #14778. #34795 (Alexandre Snarskii).Change implementation specific behavior on overflow of function toDatetime. It will be saturated to the nearest min/max supported instant of datetime instead of wraparound. This change is highlighted as &quot;backward incompatible&quot; because someone may unintentionally rely on the old behavior. #32898 (HaiBo Li).Make function cast(value, 'IPv4'), cast(value, 'IPv6') behave same as toIPv4, toIPv6 functions. Changed behavior of incorrect IP address passed into functions toIPv4, toIPv6, now if invalid IP address passes into this functions exception will be raised, before this function return default value. Added functions IPv4StringToNumOrDefault, IPv4StringToNumOrNull, IPv6StringToNumOrDefault, IPv6StringOrNull toIPv4OrDefault, toIPv4OrNull, toIPv6OrDefault, toIPv6OrNull. Functions IPv4StringToNumOrDefault , toIPv4OrDefault , toIPv6OrDefault should be used if previous logic relied on IPv4StringToNum, toIPv4, toIPv6 returning default value for invalid address. Added setting cast_ipv4_ipv6_default_on_conversion_error, if this setting enabled, then IP address conversion functions will behave as before. Closes #22825. Closes #5799. Closes #35156. #35240 (Maksim Kita). New Feature​ Support for caching data locally for remote filesystems. It can be enabled for s3 disks. Closes #28961. #33717 (Kseniia Sumarokova). In the meantime, we enabled the test suite on s3 filesystem and no more known issues exist, so it is started to be production ready.Add new table function hive. It can be used as follows hive('&lt;hive metastore url&gt;', '&lt;hive database&gt;', '&lt;hive table name&gt;', '&lt;columns definition&gt;', '&lt;partition columns&gt;') for example SELECT * FROM hive('thrift://hivetest:9083', 'test', 'demo', 'id Nullable(String), score Nullable(Int32), day Nullable(String)', 'day'). #34946 (lgbo).Support authentication of users connected via SSL by their X.509 certificate. #31484 (eungenue).Support schema inference for inserting into table functions file/hdfs/s3/url. #34732 (Kruglov Pavel).Now you can read system.zookeeper table without restrictions on path or using like expression. This reads can generate quite heavy load for zookeeper so to enable this ability you have to enable setting allow_unrestricted_reads_from_keeper. #34609 (Sergei Trifonov).Display CPU and memory metrics in clickhouse-local. Close #34545. #34605 (李扬).Implement startsWith and endsWith function for arrays, closes #33982. #34368 (usurai).Add three functions for Map data type: 1. mapReplace(map1, map2) - replaces values for keys in map1 with the values of the corresponding keys in map2; adds keys from map2 that don't exist in map1. 2. mapFilter 3. mapMap. mapFilter and mapMap are higher order functions, accepting two arguments, the first argument is a lambda function with k, v pair as arguments, the second argument is a column of type Map. #33698 (hexiaoting).Allow getting default user and password for clickhouse-client from the CLICKHOUSE_USER and CLICKHOUSE_PASSWORD environment variables. Close #34538. #34947 (DR). Experimental Feature​ New data type Object(&lt;schema_format&gt;), which supports storing of semi-structured data (for now JSON only). Data is written to such types as string. Then all paths are extracted according to format of semi-structured data and written as separate columns in most optimal types, that can store all their values. Those columns can be queried by names that match paths in source data. E.g data.key1.key2 or with cast operator data.key1.key2::Int64.Add database_replicated_allow_only_replicated_engine setting. When enabled, it only allowed to only create Replicated tables or tables with stateless engines in Replicated databases. #35214 (Nikolai Kochetov). Note that Replicated database is still an experimental feature. Performance Improvement​ Improve performance of insertion into MergeTree tables by optimizing sorting. Up to 2x improvement is observed on realistic benchmarks. #34750 (Maksim Kita).Columns pruning when reading Parquet, ORC and Arrow files from URL and S3. Closes #34163. #34849 (Kseniia Sumarokova).Columns pruning when reading Parquet, ORC and Arrow files from Hive. #34954 (lgbo).A bunch of performance optimizations from a performance superhero. Improve performance of processing queries with large IN section. Improve performance of direct dictionary if its source is ClickHouse. Improve performance of detectCharset , detectLanguageUnknown functions. #34888 (Maksim Kita).Improve performance of any aggregate function by using more batching. #34760 (Raúl Marín).Multiple improvements for performance of clickhouse-keeper: less locking #35010 (zhanglistar), lower memory usage by streaming reading and writing of snapshot instead of full copy. #34584 (zhanglistar), optimizing compaction of log store in the RAFT implementation. #34534 (zhanglistar), versioning of the internal data structure #34486 (zhanglistar). Improvement​ Allow asynchronous inserts to table functions. Fixes #34864. #34866 (Anton Popov).Implicit type casting of the key argument for functions dictGetHierarchy, dictIsIn, dictGetChildren, dictGetDescendants. Closes #34970. #35027 (Maksim Kita).EXPLAIN AST query can output AST in form of a graph in Graphviz format: EXPLAIN AST graph = 1 SELECT * FROM system.parts. #35173 (李扬).When large files were written with s3 table function or table engine, the content type on the files was mistakenly set to application/xml due to a bug in the AWS SDK. This closes #33964. #34433 (Alexey Milovidov).Change restrictive row policies a bit to make them an easier alternative to permissive policies in easy cases. If for a particular table only restrictive policies exist (without permissive policies) users will be able to see some rows. Also SHOW CREATE ROW POLICY will always show AS permissive or AS restrictive in row policy's definition. #34596 (Vitaly Baranov).Improve schema inference with globs in File/S3/HDFS/URL engines. Try to use the next path for schema inference in case of error. #34465 (Kruglov Pavel).Play UI now correctly detects the preferred light/dark theme from the OS. #35068 (peledni).Added date_time_input_format = 'best_effort_us'. Closes #34799. #34982 (WenYao).A new settings called allow_plaintext_password and allow_no_password are added in server configuration which turn on/off authentication types that can be potentially insecure in some environments. They are allowed by default. #34738 (Heena Bansal).Support for DateTime64 data type in Arrow format, closes #8280 and closes #28574. #34561 (李扬).Reload remote_url_allow_hosts (filtering of outgoing connections) on config update. #35294 (Nikolai Kochetov).Support --testmode parameter for clickhouse-local. This parameter enables interpretation of test hints that we use in functional tests. #35264 (Kseniia Sumarokova).Add distributed_depth to query log. It is like a more detailed variant of is_initial_query #35207 (李扬).Respect remote_url_allow_hosts for MySQL and PostgreSQL table functions. #35191 (Heena Bansal).Added disk_name field to system.part_log. #35178 (Artyom Yurkov).Do not retry non-rertiable errors when querying remote URLs. Closes #35161. #35172 (Kseniia Sumarokova).Support distributed INSERT SELECT queries (the setting parallel_distributed_insert_select) table function view(). #35132 (Azat Khuzhin).More precise memory tracking during INSERT into Buffer with AggregateFunction. #35072 (Azat Khuzhin).Avoid division by zero in Query Profiler if Linux kernel has a bug. Closes #34787. #35032 (Alexey Milovidov).Add more sanity checks for keeper configuration: now mixing of localhost and non-local servers is not allowed, also add checks for same value of internal raft port and keeper client port. #35004 (alesapin).Currently, if the user changes the settings of the system tables there will be tons of logs and ClickHouse will rename the tables every minute. This fixes #34929. #34949 (Nikita Mikhaylov).Use connection pool for Hive metastore client. #34940 (lgbo).Ignore per-column TTL in CREATE TABLE AS if new table engine does not support it (i.e. if the engine is not of MergeTree family). #34938 (Azat Khuzhin).Allow LowCardinality strings for ngrambf_v1/tokenbf_v1 indexes. Closes #21865. #34911 (Lars Hiller Eidnes).Allow opening empty sqlite db if the file doesn't exist. Closes #33367. #34907 (Kseniia Sumarokova).Implement memory statistics for FreeBSD - this is required for max_server_memory_usage to work correctly. #34902 (Alexandre Snarskii).In previous versions the progress bar in clickhouse-client can jump forward near 50% for no reason. This closes #34324. #34801 (Alexey Milovidov).Now ALTER TABLE DROP COLUMN columnX queries for MergeTree table engines will work instantly when columnX is an ALIAS column. Fixes #34660. #34786 (alesapin).Show hints when user mistyped the name of a data skipping index. Closes #29698. #34764 (flynn).Support remote()/cluster() table functions for parallel_distributed_insert_select. #34728 (Azat Khuzhin).Do not reset logging that configured via --log-file/--errorlog-file command line options in case of empty configuration in the config file. #34718 (Amos Bird).Extract schema only once on table creation and prevent reading from local files/external sources to extract schema on each server startup. #34684 (Kruglov Pavel).Allow specifying argument names for executable UDFs. This is necessary for formats where argument name is part of serialization, like Native, JSONEachRow. Closes #34604. #34653 (Maksim Kita).MaterializedMySQL (experimental feature) now supports materialized_mysql_tables_list (a comma-separated list of MySQL database tables, which will be replicated by the MaterializedMySQL database engine. Default value: empty list — means all the tables will be replicated), mentioned at #32977. #34487 (zzsmdfj).Improve OpenTelemetry span logs for INSERT operation on distributed table. #34480 (Frank Chen).Make the znode ctime and mtime consistent between servers in ClickHouse Keeper. #33441 (小路). Build/Testing/Packaging Improvement​ Package repository is migrated to JFrog Artifactory (Mikhail f. Shiryaev).Randomize some settings in functional tests, so more possible combinations of settings will be tested. This is yet another fuzzing method to ensure better test coverage. This closes #32268. #34092 (Kruglov Pavel).Drop PVS-Studio from our CI. #34680 (Mikhail f. Shiryaev).Add an ability to build stripped binaries with CMake. In previous versions it was performed by dh-tools. #35196 (alesapin).Smaller &quot;fat-free&quot; clickhouse-keeper build. #35031 (alesapin).Use @robot-clickhouse as an author and committer for PRs like https://github.com/ClickHouse/ClickHouse/pull/34685. #34793 (Mikhail f. Shiryaev).Limit DWARF version for debug info by 4 max, because our internal stack symbolizer cannot parse DWARF version 5. This makes sense if you compile ClickHouse with clang-15. #34777 (Alexey Milovidov).Remove clickhouse-test debian package as unneeded complication. CI use tests from repository and standalone testing via deb package is no longer supported. #34606 (Ilya Yatsishin). Bug Fix (user-visible misbehaviour in official stable or prestable release)​ A fix for HDFS integration: When the inner buffer size is too small, NEED_MORE_INPUT in HadoopSnappyDecoder will run multi times (&gt;=3) for one compressed block. This makes the input data be copied into the wrong place in HadoopSnappyDecoder::buffer. #35116 (lgbo).Ignore obsolete grants in ATTACH GRANT statements. This PR fixes #34815. #34855 (Vitaly Baranov).Fix segfault in Postgres database when getting create table query if database was created using named collections. Closes #35312. #35313 (Kseniia Sumarokova).Fix partial merge join duplicate rows bug, close #31009. #35311 (Vladimir C).Fix possible Assertion 'position() != working_buffer.end()' failed while using bzip2 compression with small max_read_buffer_size setting value. The bug was found in https://github.com/ClickHouse/ClickHouse/pull/35047. #35300 (Kruglov Pavel). While using lz4 compression with a small max_read_buffer_size setting value. #35296 (Kruglov Pavel). While using lzma compression with small max_read_buffer_size setting value. #35295 (Kruglov Pavel). While using brotli compression with a small max_read_buffer_size setting value. The bug was found in https://github.com/ClickHouse/ClickHouse/pull/35047. #35281 (Kruglov Pavel).Fix possible segfault in JSONEachRow schema inference. #35291 (Kruglov Pavel).Fix CHECK TABLE query in case when sparse columns are enabled in table. #35274 (Anton Popov).Avoid std::terminate in case of exception in reading from remote VFS. #35257 (Azat Khuzhin).Fix reading port from config, close #34776. #35193 (Vladimir C).Fix error in query with WITH TOTALS in case if HAVING returned empty result. This fixes #33711. #35186 (Amos Bird).Fix a corner case of replaceRegexpAll, close #35117. #35182 (Vladimir C).Schema inference didn't work properly on case of INSERT INTO FUNCTION s3(...) FROM ..., it tried to read schema from s3 file instead of from select query. #35176 (Kruglov Pavel).Fix MaterializedPostgreSQL (experimental feature) table overrides for partition by, etc. Closes #35048. #35162 (Kseniia Sumarokova).Fix MaterializedPostgreSQL (experimental feature) adding new table to replication (ATTACH TABLE) after manually removing (DETACH TABLE). Closes #33800. Closes #34922. Closes #34315. #35158 (Kseniia Sumarokova).Fix partition pruning error when non-monotonic function is used with IN operator. This fixes #35136. #35146 (Amos Bird).Fixed slightly incorrect translation of YAML configs to XML. #35135 (Miel Donkers).Fix optimize_skip_unused_shards_rewrite_in for signed columns and negative values. #35134 (Azat Khuzhin).The update_lag external dictionary configuration option was unusable showing the error message Unexpected key `update_lag` in dictionary source configuration. #35089 (Jason Chu).Avoid possible deadlock on server shutdown. #35081 (Azat Khuzhin).Fix missing alias after function is optimized to a subcolumn when setting optimize_functions_to_subcolumns is enabled. Closes #33798. #35079 (qieqieplus).Fix reading from system.asynchronous_inserts table if there exists asynchronous insert into table function. #35050 (Anton Popov).Fix possible exception Reading for MergeTree family tables must be done with last position boundary (relevant to operation on remote VFS). Closes #34979. #35001 (Kseniia Sumarokova).Fix unexpected result when use -State type aggregate function in window frame. #34999 (metahys).Fix possible segfault in FileLog (experimental feature). Closes #30749. #34996 (Kseniia Sumarokova).Fix possible rare error Cannot push block to port which already has data. #34993 (Nikolai Kochetov).Fix wrong schema inference for unquoted dates in CSV. Closes #34768. #34961 (Kruglov Pavel).Integration with Hive: Fix unexpected result when use in in where in hive query. #34945 (lgbo).Avoid busy polling in ClickHouse Keeper while searching for changelog files to delete. #34931 (Azat Khuzhin).Fix DateTime64 conversion from PostgreSQL. Closes #33364. #34910 (Kseniia Sumarokova).Fix possible &quot;Part directory doesn't exist&quot; during INSERT into MergeTree table backed by VFS over s3. #34876 (Azat Khuzhin).Support DDLs like CREATE USER to be executed on cross replicated cluster. #34860 (Jianmei Zhang).Fix bugs for multiple columns group by in WindowView (experimental feature). #34859 (vxider).Fix possible failures in S2 functions when queries contain const columns. #34745 (Bharat Nallan).Fix bug for H3 funcs containing const columns which cause queries to fail. #34743 (Bharat Nallan).Fix No such file or directory with enabled fsync_part_directory and vertical merge. #34739 (Azat Khuzhin).Fix serialization/printing for system queries RELOAD MODEL, RELOAD FUNCTION, RESTART DISK when used ON CLUSTER. Closes #34514. #34696 (Maksim Kita).Fix allow_experimental_projection_optimization with enable_global_with_statement (before it may lead to Stack size too large error in case of multiple expressions in WITH clause, and also it executes scalar subqueries again and again, so not it will be more optimal). #34650 (Azat Khuzhin).Stop to select part for mutate when the other replica has already updated the transaction log for ReplatedMergeTree engine. #34633 (Jianmei Zhang).Fix incorrect result of trivial count query when part movement feature is used #34089. #34385 (nvartolomei).Fix inconsistency of max_query_size limitation in distributed subqueries. #34078 (Chao Ma). "},{"title":"ClickHouse release v22.2, 2022-02-17​","type":1,"pageTitle":"2022 Changelog","url":"en/whats-new/changelog/#clickhouse-release-v222-2022-02-17","content":"Upgrade Notes​ Applying data skipping indexes for queries with FINAL may produce incorrect result. In this release we disabled data skipping indexes by default for queries with FINAL (a new setting use_skip_indexes_if_final is introduced and disabled by default). #34243 (Azat Khuzhin). New Feature​ Projections are production ready. Set allow_experimental_projection_optimization by default and deprecate this setting. #34456 (Nikolai Kochetov).An option to create a new files on insert for File/S3/HDFS engines. Allow to overwrite a file in HDFS. Throw an exception in attempt to overwrite a file in S3 by default. Throw an exception in attempt to append data to file in formats that have a suffix (and thus don't support appends, like Parquet, ORC). Closes #31640 Closes #31622 Closes #23862 Closes #15022 Closes #16674. #33302 (Kruglov Pavel).Add a setting that allows a user to provide own deduplication semantic in MergeTree/ReplicatedMergeTree If provided, it's used instead of data digest to generate block ID. So, for example, by providing a unique value for the setting in each INSERT statement, the user can avoid the same inserted data being deduplicated. This closes: #7461. #32304 (Igor Nikonov).Add support of DEFAULT keyword for INSERT statements. Closes #6331. #33141 (Andrii Buriachevskyi).EPHEMERAL column specifier is added to CREATE TABLE query. Closes #9436. #34424 (yakov-olkhovskiy).Support IF EXISTS clause for TTL expr TO [DISK|VOLUME] [IF EXISTS] 'xxx' feature. Parts will be moved to disk or volume only if it exists on replica, so MOVE TTL rules will be able to behave differently on replicas according to the existing storage policies. Resolves #34455. #34504 (Anton Popov).Allow set default table engine and to create tables without specifying ENGINE. #34187 (Ilya Yatsishin).Add table function format(format_name, data). #34125 (Kruglov Pavel).Detect format in clickhouse-local by file name even in the case when it is passed to stdin. #33829 (Kruglov Pavel).Add schema inference for values table function. Closes #33811. #34017 (Kruglov Pavel).Dynamic reload of server TLS certificates on config reload. Closes #15764. #15765 (johnskopis). #31257 (Filatenkov Artur).Now ReplicatedMergeTree can recover data when some of its disks are broken. #13544 (Amos Bird).Fault-tolerant connections in clickhouse-client: clickhouse-client ... --host host1 --host host2 --port port2 --host host3 --port port --host host4. #34490 (Kruglov Pavel). #33824 (Filippov Denis).Add DEGREES and RADIANS functions for MySQL compatibility. #33769 (Bharat Nallan).Add h3ToCenterChild function. #33313 (Bharat Nallan). Add new h3 miscellaneous functions: edgeLengthKm,exactEdgeLengthKm,exactEdgeLengthM,exactEdgeLengthRads,numHexagons. #33621 (Bharat Nallan).Add function bitSlice to extract bit subsequences from String/FixedString. #33360 (RogerYK).Implemented meanZTest aggregate function. #33354 (achimbab).Add confidence intervals to T-tests aggregate functions. #33260 (achimbab).Add function addressToLineWithInlines. Close #26211. #33467 (SuperDJY).Added #! and # as a recognised start of a single line comment. Closes #34138. #34230 (Aaron Katz). Experimental Feature​ Functions for text classification: language and charset detection. See #23271. #33314 (Nikolay Degterinsky).Add memory overcommit to MemoryTracker. Added guaranteed settings for memory limits which represent soft memory limits. In case when hard memory limit is reached, MemoryTracker tries to cancel the most overcommited query. New setting memory_usage_overcommit_max_wait_microseconds specifies how long queries may wait another query to stop. Closes #28375. #31182 (Dmitry Novik).Enable stream to table join in WindowView. #33729 (vxider).Support SET, YEAR, TIME and GEOMETRY data types in MaterializedMySQL (experimental feature). Fixes #18091, #21536, #26361. #33429 (zzsmdfj).Fix various issues when projection is enabled by default. Each issue is described in separate commit. This is for #33678 . This fixes #34273. #34305 (Amos Bird). Performance Improvement​ Support optimize_read_in_order if prefix of sorting key is already sorted. E.g. if we have sorting key ORDER BY (a, b) in table and query with WHERE a = const ORDER BY b clauses, now it will be applied reading in order of sorting key instead of full sort. #32748 (Anton Popov).Improve performance of partitioned insert into table functions URL, S3, File, HDFS. Closes #34348. #34510 (Maksim Kita).Multiple performance improvements of clickhouse-keeper. #34484 #34587 (zhanglistar).FlatDictionary improve performance of dictionary data load. #33871 (Maksim Kita).Improve performance of mapPopulateSeries function. Closes #33944. #34318 (Maksim Kita)._file and _path virtual columns (in file-like table engines) are made LowCardinality - it will make queries for multiple files faster. Closes #34300. #34317 (flynn).Speed up loading of data parts. It was not parallelized before: the setting part_loading_threads did not have effect. See #4699. #34310 (alexey-milovidov).Improve performance of LineAsString format. This closes #34303. #34306 (alexey-milovidov).Optimize quantilesExact{Low,High} to use nth_element instead of sort. #34287 (Danila Kutenin).Slightly improve performance of Regexp format. #34202 (alexey-milovidov).Minor improvement for analysis of scalar subqueries. #34128 (Federico Rodriguez).Make ORDER BY tuple almost as fast as ORDER BY columns. We have special optimizations for multiple column ORDER BY: https://github.com/ClickHouse/ClickHouse/pull/10831 . It's beneficial to also apply to tuple columns. #34060 (Amos Bird).Rework and reintroduce the scalar subqueries cache to Materialized Views execution. #33958 (Raúl Marín).Slightly improve performance of ORDER BY by adding x86-64 AVX-512 support for memcmpSmall functions to accelerate memory comparison. It works only if you compile ClickHouse by yourself. #33706 (hanqf-git).Improve range_hashed dictionary performance if for key there are a lot of intervals. Fixes #23821. #33516 (Maksim Kita).For inserts and merges into S3, write files in parallel whenever possible (TODO: check if it's merged). #33291 (Nikolai Kochetov).Improve clickhouse-keeper performance and fix several memory leaks in NuRaft library. #33329 (alesapin). Improvement​ Support asynchronous inserts in clickhouse-client for queries with inlined data. #34267 (Anton Popov).Functions dictGet, dictHas implicitly cast key argument to dictionary key structure, if they are different. #33672 (Maksim Kita).Improvements for range_hashed dictionaries. Improve performance of load time if there are multiple attributes. Allow to create a dictionary without attributes. Added option to specify strategy when intervals start and end have Nullable type convert_null_range_bound_to_open by default is true. Closes #29791. Allow to specify Float, Decimal, DateTime64, Int128, Int256, UInt128, UInt256 as range types. RangeHashedDictionary added support for range values that extend Int64 type. Closes #28322. Added option range_lookup_strategy to specify range lookup type min, max by default is min . Closes #21647. Fixed allocated bytes calculations. Fixed type name in system.dictionaries in case of ComplexKeyHashedDictionary. #33927 (Maksim Kita).flat, hashed, hashed_array dictionaries now support creating with empty attributes, with support of reading the keys and using dictHas. Fixes #33820. #33918 (Maksim Kita).Added support for DateTime64 data type in dictionaries. #33914 (Maksim Kita).Allow to write s3(url, access_key_id, secret_access_key) (autodetect of data format and table structure, but with explicit credentials). #34503 (Kruglov Pavel).Added sending of the output format back to client like it's done in HTTP protocol as suggested in #34362. Closes #34362. #34499 (Vitaly Baranov).Send ProfileEvents statistics in case of INSERT SELECT query (to display query metrics in clickhouse-client for this type of queries). #34498 (Dmitry Novik).Recognize .jsonl extension for JSONEachRow format. #34496 (Kruglov Pavel).Improve schema inference in clickhouse-local. Allow to write just clickhouse-local -q &quot;select * from table&quot; &lt; data.format. #34495 (Kruglov Pavel).Privileges CREATE/ALTER/DROP ROW POLICY now can be granted on a table or on database.* as well as globally *.*. #34489 (Vitaly Baranov).Allow to export arbitrary large files to s3. Add two new settings: s3_upload_part_size_multiply_factor and s3_upload_part_size_multiply_parts_count_threshold. Now each time s3_upload_part_size_multiply_parts_count_threshold uploaded to S3 from a single query s3_min_upload_part_size multiplied by s3_upload_part_size_multiply_factor. Fixes #34244. #34422 (alesapin).Allow to skip not found (404) URLs for globs when using URL storage / table function. Also closes #34359. #34392 (Kseniia Sumarokova).Default input and output formats for clickhouse-local that can be overriden by --input-format and --output-format. Close #30631. #34352 (李扬).Add options for clickhouse-format. Which close #30528 - max_query_size - max_parser_depth. #34349 (李扬).Better handling of pre-inputs before client start. This is for #34308. #34336 (Amos Bird).REGEXP_MATCHES and REGEXP_REPLACE function aliases for compatibility with PostgreSQL. Close #30885. #34334 (李扬).Some servers expect a User-Agent header in their HTTP requests. A User-Agent header entry has been added to HTTP requests of the form: User-Agent: ClickHouse/VERSION_STRING. #34330 (Saad Ur Rahman).Cancel merges before acquiring table lock for TRUNCATE query to avoid DEADLOCK_AVOIDED error in some cases. Fixes #34302. #34304 (tavplubix).Change severity of the &quot;Cancelled merging parts&quot; message in logs, because it's not an error. This closes #34148. #34232 (alexey-milovidov).Add ability to compose PostgreSQL-style cast operator :: with expressions using [] and . operators (array and tuple indexing). #34229 (Nikolay Degterinsky).Recognize YYYYMMDD-hhmmss format in parseDateTimeBestEffort function. This closes #34206. #34208 (alexey-milovidov).Allow carriage return in the middle of the line while parsing by Regexp format. This closes #34200. #34205 (alexey-milovidov).Allow to parse dictionary's PRIMARY KEY as PRIMARY KEY (id, value); previously supported only PRIMARY KEY id, value. Closes #34135. #34141 (Maksim Kita).An optional argument for splitByChar to limit the number of resulting elements. close #34081. #34140 (李扬).Improving the experience of multiple line editing for clickhouse-client. This is a follow-up of #31123. #34114 (Amos Bird).Add UUID suport in MsgPack input/output format. #34065 (Kruglov Pavel).Tracing context (for OpenTelemetry) is now propagated from GRPC client metadata (this change is relevant for GRPC client-server protocol). #34064 (andremarianiello).Supports all types of SYSTEM queries with ON CLUSTER clause. #34005 (小路).Improve memory accounting for queries that are using less than max_untracker_memory. #34001 (Azat Khuzhin).Fixed UTF-8 string case-insensitive search when lowercase and uppercase characters are represented by different number of bytes. Example is ẞ and ß. This closes #7334. #33992 (Harry Lee).Detect format and schema from stdin in clickhouse-local. #33960 (Kruglov Pavel).Correctly handle the case of misconfiguration when multiple disks are using the same path on the filesystem. #29072. #33905 (zhongyuankai).Try every resolved IP address while getting S3 proxy. S3 proxies are rarely used, mostly in Yandex Cloud. #33862 (Nikolai Kochetov).Support EXPLAIN AST CREATE FUNCTION query EXPLAIN AST CREATE FUNCTION mycast AS (n) -&gt; cast(n as String) will return EXPLAIN AST CREATE FUNCTION mycast AS n -&gt; CAST(n, 'String'). #33819 (李扬).Added support for cast from Map(Key, Value) to Array(Tuple(Key, Value)). #33794 (Maksim Kita).Add some improvements and fixes for Bool data type. Fixes #33244. #33737 (Kruglov Pavel).Parse and store OpenTelemetry trace-id in big-endian order. #33723 (Frank Chen).Improvement for fromUnixTimestamp64 family functions.. They now accept any integer value that can be converted to Int64. This closes: #14648. #33505 (Andrey Zvonov).Reimplement _shard_num from constants (see #7624) with shardNum() function (seee #27020), to avoid possible issues (like those that had been found in #16947). #33392 (Azat Khuzhin).Enable binary arithmetic (plus, minus, multiply, division, least, greatest) between Decimal and Float. #33355 (flynn).Respect cgroups limits in max_threads autodetection. #33342 (JaySon).Add new clickhouse-keeper setting min_session_timeout_ms. Now clickhouse-keeper will determine client session timeout according to min_session_timeout_ms and session_timeout_ms settings. #33288 (JackyWoo).Added UUID data type support for functions hex and bin. #32170 (Frank Chen).Fix reading of subcolumns with dots in their names. In particular fixed reading of Nested columns, if their element names contain dots (e.g Nested(`keys.name` String, `keys.id` UInt64, values UInt64)). #34228 (Anton Popov).Fixes parallel_view_processing = 0 not working when inserting into a table using VALUES. - Fixes view_duration_ms in the query_views_log not being set correctly for materialized views. #34067 (Raúl Marín).Fix parsing tables structure from ZooKeeper: now metadata from ZooKeeper compared with local metadata in canonical form. It helps when canonical function names can change between ClickHouse versions. #33933 (sunny).Properly escape some characters for interaction with LDAP. #33401 (IlyaTsoi). Build/Testing/Packaging Improvement​ Remove unbundled build support. #33690 (Azat Khuzhin).Ensure that tests don't depend on the result of non-stable sorting of equal elements. Added equal items ranges randomization in debug after sort to prevent issues when we rely on equal items sort order. #34393 (Maksim Kita).Add verbosity to a style check. #34289 (Mikhail f. Shiryaev).Remove clickhouse-test debian package because it's obsolete. #33948 (Ilya Yatsishin).Multiple improvements for build system to remove the possibility of occasionally using packages from the OS and to enforce hermetic builds. #33695 (Amos Bird). Bug Fix (user-visible misbehaviour in official stable or prestable release)​ Fixed the assertion in case of using allow_experimental_parallel_reading_from_replicas with max_parallel_replicas equals to 1. This fixes #34525. #34613 (Nikita Mikhaylov).Fix rare bug while reading of empty arrays, which could lead to Data compressed with different methods error. It can reproduce if you have mostly empty arrays, but not always. And reading is performed in backward direction with ORDER BY ... DESC. This error is extremely unlikely to happen. #34327 (Anton Popov).Fix wrong result of round/roundBankers if integer values of small types are rounded. Closes #33267. #34562 (李扬).Sometimes query cancellation did not work immediately when we were reading multiple files from s3 or HDFS. Fixes #34301 Relates to #34397. #34539 (Dmitry Novik).Fix exception Chunk should have AggregatedChunkInfo in MergingAggregatedTransform (in case of optimize_aggregation_in_order = 1 and distributed_aggregation_memory_efficient = 0). Fixes #34526. #34532 (Anton Popov).Fix comparison between integers and floats in index analysis. Previously it could lead to skipping some granules for reading by mistake. Fixes #34493. #34528 (Anton Popov).Fix compression support in URL engine. #34524 (Frank Chen).Fix possible error 'file_size: Operation not supported' in files' schema autodetection. #34479 (Kruglov Pavel).Fixes possible race with table deletion. #34416 (Kseniia Sumarokova).Fix possible error Cannot convert column Function to mask in short circuit function evaluation. Closes #34171. #34415 (Kruglov Pavel).Fix potential crash when doing schema inference from url source. Closes #34147. #34405 (Kruglov Pavel).For UDFs access permissions were checked for database level instead of global level as it should be. Closes #34281. #34404 (Maksim Kita).Fix wrong engine syntax in result of SHOW CREATE DATABASE query for databases with engine Memory. This closes #34335. #34345 (alexey-milovidov).Fixed a couple of extremely rare race conditions that might lead to broken state of replication queue and &quot;intersecting parts&quot; error. #34297 (tavplubix).Fix progress bar width. It was incorrectly rounded to integer number of characters. #34275 (alexey-milovidov).Fix current_user/current_address client information fields for inter-server communication (before this patch current_user/current_address will be preserved from the previous query). #34263 (Azat Khuzhin).Fix memory leak in case of some Exception during query processing with optimize_aggregation_in_order=1. #34234 (Azat Khuzhin).Fix metric Query, which shows the number of executing queries. In last several releases it was always 0. #34224 (Anton Popov).Fix schema inference for table runction s3. #34186 (Kruglov Pavel).Fix rare and benign race condition in HDFS, S3 and URL storage engines which can lead to additional connections. #34172 (alesapin).Fix bug which can rarely lead to error &quot;Cannot read all data&quot; while reading LowCardinality columns of MergeTree table engines family which stores data on remote file system like S3 (virtual filesystem over s3 is an experimental feature that is not ready for production). #34139 (alesapin).Fix inserts to distributed tables in case of a change of native protocol. The last change was in the version 22.1, so there may be some failures of inserts to distributed tables after upgrade to that version. #34132 (Anton Popov).Fix possible data race in File table engine that was introduced in #33960. Closes #34111. #34113 (Kruglov Pavel).Fixed minor race condition that might cause &quot;intersecting parts&quot; error in extremely rare cases after ZooKeeper connection loss. #34096 (tavplubix).Fix asynchronous inserts with Native format. #34068 (Anton Popov).Fix bug which lead to inability for server to start when both replicated access storage and keeper (embedded in clickhouse-server) are used. Introduced two settings for keeper socket timeout instead of settings from default user: keeper_server.socket_receive_timeout_sec and keeper_server.socket_send_timeout_sec. Fixes #33973. #33988 (alesapin).Fix segfault while parsing ORC file with corrupted footer. Closes #33797. #33984 (Kruglov Pavel).Fix parsing IPv6 from query parameter (prepared statements) and fix IPv6 to string conversion. Closes #33928. #33971 (Kruglov Pavel).Fix crash while reading of nested tuples. Fixes #33838. #33956 (Anton Popov).Fix usage of functions array and tuple with literal arguments in distributed queries. Previously it could lead to Not found columns exception. #33938 (Anton Popov).Aggregate function combinator -If did not correctly process Nullable filter argument. This closes #27073. #33920 (alexey-milovidov).Fix potential race condition when doing remote disk read (virtual filesystem over s3 is an experimental feature that is not ready for production). #33912 (Amos Bird).Fix crash if SQL UDF is created with lambda with non identifier arguments. Closes #33866. #33868 (Maksim Kita).Fix usage of sparse columns (which can be enabled by experimental setting ratio_of_defaults_for_sparse_serialization). #33849 (Anton Popov).Fixed replica is not readonly logical error on SYSTEM RESTORE REPLICA query when replica is actually readonly. Fixes #33806. #33847 (tavplubix).Fix memory leak in clickhouse-keeper in case of compression is used (default). #33840 (Azat Khuzhin).Fix index analysis with no common types available. #33833 (Amos Bird).Fix schema inference for JSONEachRow and JSONCompactEachRow. #33830 (Kruglov Pavel).Fix usage of external dictionaries with redis source and large number of keys. #33804 (Anton Popov).Fix bug in client that led to 'Connection reset by peer' in server. Closes #33309. #33790 (Kruglov Pavel).Fix parsing query INSERT INTO ... VALUES SETTINGS ... (...), ... #33776 (Kruglov Pavel).Fix bug of check table when creating data part with wide format and projection. #33774 (李扬).Fix tiny race between count() and INSERT/merges/... in MergeTree (it is possible to return incorrect number of rows for SELECT with optimize_trivial_count_query). #33753 (Azat Khuzhin).Throw exception when directory listing request has failed in storage HDFS. #33724 (LiuNeng).Fix mutation when table contains projections. This fixes #33010. This fixes #33275. #33679 (Amos Bird).Correctly determine current database if CREATE TEMPORARY TABLE AS SELECT is queried inside a named HTTP session. This is a very rare use case. This closes #8340. #33676 (alexey-milovidov).Allow some queries with sorting, LIMIT BY, ARRAY JOIN and lambda functions. This closes #7462. #33675 (alexey-milovidov).Fix bug in &quot;zero copy replication&quot; (a feature that is under development and should not be used in production) which lead to data duplication in case of TTL move. Fixes #33643. #33642 (alesapin).Fix Chunk should have AggregatedChunkInfo in GroupingAggregatedTransform (in case of optimize_aggregation_in_order = 1). #33637 (Azat Khuzhin).Fix error Bad cast from type ... to DB::DataTypeArray which may happen when table has Nested column with dots in name, and default value is generated for it (e.g. during insert, when column is not listed). Continuation of #28762. #33588 (Alexey Pavlenko).Export into lz4 files has been fixed. Closes #31421. #31862 (Kruglov Pavel).Fix potential crash if group_by_overflow_mode was set to any (approximate GROUP BY) and aggregation was performed by single column of type LowCardinality. #34506 (DR).Fix inserting to temporary tables via gRPC client-server protocol. Fixes #34347, issue #2. #34364 (Vitaly Baranov).Fix issue #19429. #34225 (Vitaly Baranov).Fix issue #18206. #33977 (Vitaly Baranov).This PR allows using multiple LDAP storages in the same list of user directories. It worked earlier but was broken because LDAP tests are disabled (they are part of the testflows tests). #33574 (Vitaly Baranov). "},{"title":"ClickHouse release v22.1, 2022-01-18​","type":1,"pageTitle":"2022 Changelog","url":"en/whats-new/changelog/#clickhouse-release-v221-2022-01-18","content":"Upgrade Notes​ The functions left and right were previously implemented in parser and now full-featured. Distributed queries with left or right functions without aliases may throw exception if cluster contains different versions of clickhouse-server. If you are upgrading your cluster and encounter this error, you should finish upgrading your cluster to ensure all nodes have the same version. Also you can add aliases (AS something) to the columns in your queries to avoid this issue. #33407 (alexey-milovidov).Resource usage by scalar subqueries is fully accounted since this version. With this change, rows read in scalar subqueries are now reported in the query_log. If the scalar subquery is cached (repeated or called for several rows) the rows read are only counted once. This change allows KILLing queries and reporting progress while they are executing scalar subqueries. #32271 (Raúl Marín). New Feature​ Implement data schema inference for input formats. Allow to skip structure (or write just auto) in table functions file, url, s3, hdfs and in parameters of clickhouse-local . Allow to skip structure in create query for table engines File, HDFS, S3, URL, Merge, Buffer, Distributed and ReplicatedMergeTree (if we add new replicas). #32455 (Kruglov Pavel).Detect format by file extension in file/hdfs/s3/url table functions and HDFS/S3/URL table engines and also for SELECT INTO OUTFILE and INSERT FROM INFILE #33565 (Kruglov Pavel). Close #30918. #33443 (OnePiece).A tool for collecting diagnostics data if you need support. #33175 (Alexander Burmak).Automatic cluster discovery via Zoo/Keeper. It allows to add replicas to the cluster without changing configuration on every server. #31442 (vdimir).Implement hive table engine to access apache hive from clickhouse. This implements: #29245. #31104 (taiyang-li).Add aggregate functions cramersV, cramersVBiasCorrected, theilsU and contingency. These functions calculate dependency (measure of association) between categorical values. All these functions are using cross-tab (histogram on pairs) for implementation. You can imagine it like a correlation coefficient but for any discrete values (not necessary numbers). #33366 (alexey-milovidov). Initial implementation by Vanyok-All-is-OK and antikvist.Added table function hdfsCluster which allows processing files from HDFS in parallel from many nodes in a specified cluster, similarly to s3Cluster. #32400 (Zhichang Yu).Adding support for disks backed by Azure Blob Storage, in a similar way it has been done for disks backed by AWS S3. #31505 (Jakub Kuklis).Allow COMMENT in CREATE VIEW (for all VIEW kinds). #31062 (Vasily Nemkov).Dynamically reinitialize listening ports and protocols when configuration changes. #30549 (Kevin Michel).Added left, right, leftUTF8, rightUTF8 functions. Fix error in implementation of substringUTF8 function with negative offset (offset from the end of string). #33407 (alexey-milovidov).Add new functions for H3 coordinate system: h3HexAreaKm2, h3CellAreaM2, h3CellAreaRads2. #33479 (Bharat Nallan).Add MONTHNAME function. #33436 (usurai).Added function arrayLast. Closes #33390. #33415 Added function arrayLastIndex. #33465 (Maksim Kita).Add function decodeURLFormComponent slightly different to decodeURLComponent. Close #10298. #33451 (SuperDJY).Allow to split GraphiteMergeTree rollup rules for plain/tagged metrics (optional rule_type field). #33494 (Michail Safronov). Performance Improvement​ Support moving conditions to PREWHERE (setting optimize_move_to_prewhere) for tables of Merge engine if its all underlying tables supports PREWHERE. #33300 (Anton Popov).More efficient handling of globs for URL storage. Now you can easily query million URLs in parallel with retries. Closes #32866. #32907 (Kseniia Sumarokova).Avoid exponential backtracking in parser. This closes #20158. #33481 (alexey-milovidov).Abuse of untuple function was leading to exponential complexity of query analysis (found by fuzzer). This closes #33297. #33445 (alexey-milovidov).Reduce allocated memory for dictionaries with string attributes. #33466 (Maksim Kita).Slight performance improvement of reinterpret function. #32587 (alexey-milovidov).Non significant change. In extremely rare cases when data part is lost on every replica, after merging of some data parts, the subsequent queries may skip less amount of partitions during partition pruning. This hardly affects anything. #32220 (Azat Khuzhin).Improve clickhouse-keeper writing performance by optimization the size calculation logic. #32366 (zhanglistar).Optimize single part projection materialization. This closes #31669. #31885 (Amos Bird).Improve query performance of system tables. #33312 (OnePiece).Optimize selecting of MergeTree parts that can be moved between volumes. #33225 (OnePiece).Fix sparse_hashed dict performance with sequential keys (wrong hash function). #32536 (Azat Khuzhin). Experimental Feature​ Parallel reading from multiple replicas within a shard during distributed query without using sample key. To enable this, set allow_experimental_parallel_reading_from_replicas = 1 and max_parallel_replicas to any number. This closes #26748. #29279 (Nikita Mikhaylov).Implemented sparse serialization. It can reduce usage of disk space and improve performance of some queries for columns, which contain a lot of default (zero) values. It can be enabled by setting ratio_for_sparse_serialization. Sparse serialization will be chosen dynamically for column, if it has ratio of number of default values to number of all values above that threshold. Serialization (default or sparse) will be fixed for every column in part, but may varies between parts. #22535 (Anton Popov).Add &quot;TABLE OVERRIDE&quot; feature for customizing MaterializedMySQL table schemas. #32325 (Stig Bakken).Add EXPLAIN TABLE OVERRIDE query. #32836 (Stig Bakken).Support TABLE OVERRIDE clause for MaterializedPostgreSQL. RFC: #31480. #32749 (Kseniia Sumarokova).Change ZooKeeper path for zero-copy marks for shared data. Note that &quot;zero-copy replication&quot; is non-production feature (in early stages of development) that you shouldn't use anyway. But in case if you have used it, let you keep in mind this change. #32061 (ianton-ru).Events clause support for WINDOW VIEW watch query. #32607 (vxider).Fix ACL with explicit digit hash in clickhouse-keeper: now the behavior consistent with ZooKeeper and generated digest is always accepted. #33249 (小路). #33246.Fix unexpected projection removal when detaching parts. #32067 (Amos Bird). Improvement​ Now date time conversion functions that generates time before 1970-01-01 00:00:00 will be saturated to zero instead of overflow. #29953 (Amos Bird). It also fixes a bug in index analysis if date truncation function would yield result before the Unix epoch.Always display resource usage (total CPU usage, total RAM usage and max RAM usage per host) in client. #33271 (alexey-milovidov).Improve Bool type serialization and deserialization, check the range of values. #32984 (Kruglov Pavel).If an invalid setting is defined using the SET query or using the query parameters in the HTTP request, error message will contain suggestions that are similar to the invalid setting string (if any exists). #32946 (Antonio Andelic).Support hints for mistyped setting names for clickhouse-client and clickhouse-local. Closes #32237. #32841 (凌涛).Allow to use virtual columns in Materialized Views. Close #11210. #33482 (OnePiece).Add config to disable IPv6 in clickhouse-keeper if needed. This close #33381. #33450 (Wu Xueyang).Add more info to system.build_options about current git revision. #33431 (taiyang-li).clickhouse-local: track memory under --max_memory_usage_in_client option. #33341 (Azat Khuzhin).Allow negative intervals in function intervalLengthSum. Their length will be added as well. This closes #33323. #33335 (alexey-milovidov).LineAsString can be used as output format. This closes #30919. #33331 (Sergei Trifonov).Support &lt;secure/&gt; in cluster configuration, as an alternative form of &lt;secure&gt;1&lt;/secure&gt;. Close #33270. #33330 (SuperDJY).Pressing Ctrl+C twice will terminate clickhouse-benchmark immediately without waiting for in-flight queries. This closes #32586. #33303 (alexey-milovidov).Support Unix timestamp with milliseconds in parseDateTimeBestEffort function. #33276 (Ben).Allow to cancel query while reading data from external table in the formats: Arrow / Parquet / ORC - it failed to be cancelled it case of big files and setting input_format_allow_seeks as false. Closes #29678. #33238 (Kseniia Sumarokova).If table engine supports SETTINGS clause, allow to pass the settings as key-value or via config. Add this support for MySQL. #33231 (Kseniia Sumarokova).Correctly prevent Nullable primary keys if necessary. This is for #32780. #33218 (Amos Bird).Add retry for PostgreSQL connections in case nothing has been fetched yet. Closes #33199. #33209 (Kseniia Sumarokova).Validate config keys for external dictionaries. #33095. #33130 (Kseniia Sumarokova).Send profile info inside clickhouse-local. Closes #33093. #33097 (Kseniia Sumarokova).Short circuit evaluation: support for function throwIf. Closes #32969. #32973 (Maksim Kita).(This only happens in unofficial builds). Fixed segfault when inserting data into compressed Decimal, String, FixedString and Array columns. This closes #32939. #32940 (N. Kolotov).Added support for specifying subquery as SQL user defined function. Example: CREATE FUNCTION test AS () -&gt; (SELECT 1). Closes #30755. #32758 (Maksim Kita).Improve gRPC compression support for #28671. #32747 (Vitaly Baranov).Flush all In-Memory data parts when WAL is not enabled while shutdown server or detaching table. #32742 (nauta).Allow to control connection timeouts for MySQL (previously was supported only for dictionary source). Closes #16669. Previously default connect_timeout was rather small, now it is configurable. #32734 (Kseniia Sumarokova).Support authSource option for storage MongoDB. Closes #32594. #32702 (Kseniia Sumarokova).Support Date32 type in genarateRandom table function. #32643 (nauta).Add settings max_concurrent_select_queries and max_concurrent_insert_queries for control concurrent queries by query kind. Close #3575. #32609 (SuperDJY).Improve handling nested structures with missing columns while reading data in Protobuf format. Follow-up to https://github.com/ClickHouse/ClickHouse/pull/31988. #32531 (Vitaly Baranov).Allow empty credentials for MongoDB engine. Closes #26267. #32460 (Kseniia Sumarokova).Disable some optimizations for window functions that may lead to exceptions. Closes #31535. Closes #31620. #32453 (Kseniia Sumarokova).Allows to connect to MongoDB 5.0. Closes #31483,. #32416 (Kseniia Sumarokova).Enable comparison between Decimal and Float. Closes #22626. #31966 (flynn).Added settings command_read_timeout, command_write_timeout for StorageExecutable, StorageExecutablePool, ExecutableDictionary, ExecutablePoolDictionary, ExecutableUserDefinedFunctions. Setting command_read_timeout controls timeout for reading data from command stdout in milliseconds. Setting command_write_timeout timeout for writing data to command stdin in milliseconds. Added settings command_termination_timeout for ExecutableUserDefinedFunction, ExecutableDictionary, StorageExecutable. Added setting execute_direct for ExecutableUserDefinedFunction, by default true. Added setting execute_direct for ExecutableDictionary, ExecutablePoolDictionary, by default false. #30957 (Maksim Kita).Bitmap aggregate functions will give correct result for out of range argument instead of wraparound. #33127 (DR).Fix parsing incorrect queries with FROM INFILE statement. #33521 (Kruglov Pavel).Don't allow to write into S3 if path contains globs. #33142 (Kruglov Pavel).--echo option was not used by clickhouse-client in batch mode with single query. #32843 (N. Kolotov).Use --database option for clickhouse-local. #32797 (Kseniia Sumarokova).Fix surprisingly bad code in SQL ordinary function file. Now it supports symlinks. #32640 (alexey-milovidov).Updating modification_time for data part in system.parts after part movement #32964. #32965 (save-my-heart).Potential issue, cannot be exploited: integer overflow may happen in array resize. #33024 (varadarajkumar). Build/Testing/Packaging Improvement​ Add packages, functional tests and Docker builds for AArch64 (ARM) version of ClickHouse. #32911 (Mikhail f. Shiryaev). #32415Prepare ClickHouse to be built with musl-libc. It is not enabled by default. #33134 (alexey-milovidov).Make installation script working on FreeBSD. This closes #33384. #33418 (alexey-milovidov).Add actionlint for GitHub Actions workflows and verify workflow files via act --list to check the correct workflow syntax. #33612 (Mikhail f. Shiryaev).Add more tests for the nullable primary key feature. Add more tests with different types and merge tree kinds, plus randomly generated data. #33228 (Amos Bird).Add a simple tool to visualize flaky tests in web browser. #33185 (alexey-milovidov).Enable hermetic build for shared builds. This is mainly for developers. #32968 (Amos Bird).Update libc++ and libc++abi to the latest. #32484 (Raúl Marín).Added integration test for external .NET client (ClickHouse.Client). #23230 (Oleg V. Kozlyuk).Inject git information into clickhouse binary file. So we can get source code revision easily from clickhouse binary file. #33124 (taiyang-li).Remove obsolete code from ConfigProcessor. Yandex specific code is not used anymore. The code contained one minor defect. This defect was reported by Mallik Hassan in #33032. This closes #33032. #33026 (alexey-milovidov). Bug Fix (user-visible misbehavior in official stable or prestable release)​ Several fixes for format parsing. This is relevant if clickhouse-server is open for write access to adversary. Specifically crafted input data for Native format may lead to reading uninitialized memory or crash. This is relevant if clickhouse-server is open for write access to adversary. #33050 (Heena Bansal). Fixed Apache Avro Union type index out of boundary issue in Apache Avro binary format. #33022 (Harry Lee). Fix null pointer dereference in LowCardinality data when deserializing LowCardinality data in the Native format. #33021 (Harry Lee).ClickHouse Keeper handler will correctly remove operation when response sent. #32988 (JackyWoo).Potential off-by-one miscalculation of quotas: quota limit was not reached, but the limit was exceeded. This fixes #31174. #31656 (sunny).Fixed CASTing from String to IPv4 or IPv6 and back. Fixed error message in case of failed conversion. #29224 (Dmitry Novik) #27914 (Vasily Nemkov).Fixed an exception like Unknown aggregate function nothing during an execution on a remote server. This fixes #16689. #26074 (hexiaoting).Fix wrong database for JOIN without explicit database in distributed queries (Fixes: #10471). #33611 (Azat Khuzhin).Fix segfault in Apache Avro format that appears after the second insert into file. #33566 (Kruglov Pavel).Fix segfault in Apache Arrow format if schema contains Dictionary type. Closes #33507. #33529 (Kruglov Pavel).Out of band offset and limit settings may be applied incorrectly for views. Close #33289 #33518 (hexiaoting).Fix an exception Block structure mismatch which may happen during insertion into table with default nested LowCardinality column. Fixes #33028. #33504 (Nikolai Kochetov).Fix dictionary expressions for range_hashed range min and range max attributes when created using DDL. Closes #30809. #33478 (Maksim Kita).Fix possible use-after-free for INSERT into Materialized View with concurrent DROP (Azat Khuzhin).Do not try to read pass EOF (to workaround for a bug in the Linux kernel), this bug can be reproduced on kernels (3.14..5.9), and requires index_granularity_bytes=0 (i.e. turn off adaptive index granularity). #33372 (Azat Khuzhin).The commands SYSTEM SUSPEND and SYSTEM ... THREAD FUZZER missed access control. It is fixed. Author: Kevin Michel. #33333 (alexey-milovidov).Fix when COMMENT for dictionaries does not appear in system.tables, system.dictionaries. Allow to modify the comment for Dictionary engine. Closes #33251. #33261 (Maksim Kita).Add asynchronous inserts (with enabled setting async_insert) to query log. Previously such queries didn't appear in the query log. #33239 (Anton Popov).Fix sending WHERE 1 = 0 expressions for external databases query. Closes #33152. #33214 (Kseniia Sumarokova).Fix DDL validation for MaterializedPostgreSQL. Fix setting materialized_postgresql_allow_automatic_update. Closes #29535. #33200 (Kseniia Sumarokova). Make sure unused replication slots are always removed. Found in #26952. #33187 (Kseniia Sumarokova). Fix MaterializedPostreSQL detach/attach (removing / adding to replication) tables with non-default schema. Found in #29535. #33179 (Kseniia Sumarokova). Fix DROP MaterializedPostgreSQL database. #33468 (Kseniia Sumarokova).The metric StorageBufferBytes sometimes was miscalculated. #33159 (xuyatian).Fix error Invalid version for SerializationLowCardinality key column in case of reading from LowCardinality column with local_filesystem_read_prefetch or remote_filesystem_read_prefetch enabled. #33046 (Nikolai Kochetov).Fix s3 table function reading empty file. Closes #33008. #33037 (Kseniia Sumarokova).Fix Context leak in case of cancel_http_readonly_queries_on_client_close (i.e. leaking of external tables that had been uploaded the the server and other resources). #32982 (Azat Khuzhin).Fix wrong tuple output in CSV format in case of custom csv delimiter. #32981 (Kruglov Pavel).Fix HDFS URL check that didn't allow using HA namenode address. Bug was introduced in https://github.com/ClickHouse/ClickHouse/pull/31042. #32976 (Kruglov Pavel).Fix throwing exception like positional argument out of bounds for non-positional arguments. Closes #31173#event-5789668239. #32961 (Kseniia Sumarokova).Fix UB in case of unexpected EOF during filling a set from HTTP query (i.e. if the client interrupted in the middle, i.e. timeout 0.15s curl -Ss -F 's=@t.csv;' 'http://127.0.0.1:8123/?s_structure=key+Int&amp;query=SELECT+dummy+IN+s' and with large enough t.csv). #32955 (Azat Khuzhin).Fix a regression in replaceRegexpAll function. The function worked incorrectly when matched substring was empty. This closes #32777. This closes #30245. #32945 (alexey-milovidov).Fix ORC format stripe reading. #32929 (kreuzerkrieg).topKWeightedState failed for some input types. #32487. #32914 (vdimir).Fix exception Single chunk is expected from view inner query (LOGICAL_ERROR) in materialized view. Fixes #31419. #32862 (Nikolai Kochetov).Fix optimization with lazy seek for async reads from remote filesystems. Closes #32803. #32835 (Kseniia Sumarokova).MergeTree table engine might silently skip some mutations if there are too many running mutations or in case of high memory consumption, it's fixed. Fixes #17882. #32814 (tavplubix).Avoid reusing the scalar subquery cache when processing MV blocks. This fixes a bug when the scalar query reference the source table but it means that all subscalar queries in the MV definition will be calculated for each block. #32811 (Raúl Marín).Server might fail to start if database with MySQL engine cannot connect to MySQL server, it's fixed. Fixes #14441. #32802 (tavplubix).Fix crash when used fuzzBits function, close #32737. #32755 (SuperDJY).Fix error Column is not under aggregate function in case of MV with GROUP BY (list of columns) (which is pared as GROUP BY tuple(...)) over Kafka/RabbitMQ. Fixes #32668 and #32744. #32751 (Nikolai Kochetov).Fix ALTER TABLE ... MATERIALIZE TTL query with TTL ... DELETE WHERE ... and TTL ... GROUP BY ... modes. #32695 (Anton Popov).Fix optimize_read_in_order optimization in case when table engine is Distributed or Merge and its underlying MergeTree tables have monotonous function in prefix of sorting key. #32670 (Anton Popov).Fix LOGICAL_ERROR exception when the target of a materialized view is a JOIN or a SET table. #32669 (Raúl Marín).Inserting into S3 with multipart upload to Google Cloud Storage may trigger abort. #32504. #32649 (vdimir).Fix possible exception at RabbitMQ storage startup by delaying channel creation. #32584 (Kseniia Sumarokova).Fix table lifetime (i.e. possible use-after-free) in case of parallel DROP TABLE and INSERT. #32572 (Azat Khuzhin).Fix async inserts with formats CustomSeparated, Template, Regexp, MsgPack and JSONAsString. Previousely the async inserts with these formats didn't read any data. #32530 (Kruglov Pavel).Fix groupBitmapAnd function on distributed table. #32529 (minhthucdao).Fix crash in JOIN found by fuzzer, close #32458. #32508 (vdimir).Proper handling of the case with Apache Arrow column duplication. #32507 (Dmitriy Mokhnatkin).Fix issue with ambiguous query formatting in distributed queries that led to errors when some table columns were named ALL or DISTINCT. This closes #32391. #32490 (alexey-milovidov).Fix failures in queries that are trying to use skipping indices, which are not materialized yet. Fixes #32292 and #30343. #32359 (Anton Popov).Fix broken select query when there are more than 2 row policies on same column, begin at second queries on the same session. #31606. #32291 (SuperDJY).Fix fractional unix timestamp conversion to DateTime64, fractional part was reversed for negative unix timestamps (before 1970-01-01). #32240 (Ben).Some entries of replication queue might hang for temporary_directories_lifetime (1 day by default) with Directory tmp_merge_&lt;part_name&gt; or Part ... (state Deleting) already exists, but it will be deleted soon or similar error. It's fixed. Fixes #29616. #32201 (tavplubix).Fix parsing of APPLY lambda column transformer which could lead to client/server crash. #32138 (Kruglov Pavel).Fix base64Encode adding trailing bytes on small strings. #31797 (Kevin Michel).Fix possible crash (or incorrect result) in case of LowCardinality arguments of window function. Fixes #31114. #31888 (Nikolai Kochetov).Fix hang up with command DROP TABLE system.query_log sync. #33293 (zhanghuajie). "},{"title":"Changelog for 2021​","type":1,"pageTitle":"2022 Changelog","url":"en/whats-new/changelog/#changelog-for-2021","content":""}]